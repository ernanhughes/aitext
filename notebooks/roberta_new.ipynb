{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 3000 test samples.\n",
      "ðŸ‘€ The first 5 samples in the test set:\n",
      "                                               title  label  \\\n",
      "0  Face Synthesis from Visual Attributes via Sket...      0   \n",
      "1  Face Synthesis from Visual Attributes via Sket...      1   \n",
      "2  Conformal symmetry breaking and degeneracy of ...      0   \n",
      "3  Conformal symmetry breaking and degeneracy of ...      1   \n",
      "4  Sensitivity integrals and related inequalities...      0   \n",
      "\n",
      "                                                text  word_count  \n",
      "0  Automatic synthesis of faces from visual attri...         174  \n",
      "1  This paper presents an innovative method for f...         146  \n",
      "2  We show that though conformal symmetry can be ...         341  \n",
      "3  This research investigates the phenomenon of c...         312  \n",
      "4  This paper exhibits the closed-loop design con...         164  \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"NicolaiSivesind/human-vs-machine\", 'research_abstracts_labeled')\n",
    "\n",
    "# Convert the test set to a Pandas DataFrame\n",
    "df_test = pd.DataFrame(dataset['test'])  # Use the 'test' split\n",
    "\n",
    "print(f\"âœ… Loaded {len(df_test)} test samples.\")\n",
    "print(f\"ðŸ‘€ The first 5 samples in the test set:\\n{df_test.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     85\u001b[39m initialize_database()\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Run classification on the dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[43mclassify_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mclassify_dataset\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03mLoads the NicolaiSivesind/human-vs-machine dataset, classifies the text samples, and stores the results.\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     51\u001b[39m dataset = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mNicolaiSivesind/human-vs-machine\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mresearch_abstracts_labeled\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m texts = \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     54\u001b[39m sources = dataset[\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# Source: AI or Human\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Classify text samples\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\aitext\\venv\\Lib\\site-packages\\datasets\\dataset_dict.py:82\u001b[39m, in \u001b[36mDatasetDict.__getitem__\u001b[39m\u001b[34m(self, k)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) -> Dataset:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m         available_suggested_splits = [\n\u001b[32m     85\u001b[39m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split.TRAIN, Split.TEST, Split.VALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m     86\u001b[39m         ]\n",
      "\u001b[31mKeyError\u001b[39m: 'text'"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from typing import List, Tuple\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "\n",
    "# Set device for model\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base-openai-detector\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base-openai-detector\")\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0 if DEVICE == \"cuda:0\" else -1)\n",
    "\n",
    "# Initialize SQLite database\n",
    "DB_NAME = \"classification_results.db\"\n",
    "\n",
    "def initialize_database():\n",
    "    \"\"\"\n",
    "    Initializes the SQLite database and creates the classifications table if it doesn't exist.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS classifications (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            source TEXT,\n",
    "            line_text TEXT,\n",
    "            label TEXT,\n",
    "            confidence REAL\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def classify_text(sentences: List[str]) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Classifies a list of sentences as 'Human' or 'AI' with confidence scores.\n",
    "\n",
    "    :param sentences: List of text inputs to classify.\n",
    "    :return: List of tuples containing the predicted label ('Human' or 'AI') and confidence score.\n",
    "    \"\"\"\n",
    "    results = pipe(sentences)\n",
    "    return [(\"Human\" if res[\"label\"] == \"Real\" else \"AI\", res[\"score\"]) for res in results]\n",
    "\n",
    "def classify_dataset():\n",
    "    \"\"\"\n",
    "    Loads the NicolaiSivesind/human-vs-machine dataset, classifies the text samples, and stores the results.\n",
    "    \"\"\"\n",
    "    texts = dataset[\"text\"]\n",
    "    sources = dataset[\"source\"]  # Source: AI or Human\n",
    "\n",
    "    # Classify text samples\n",
    "    classifications = classify_text(texts)\n",
    "\n",
    "    # Store results in SQLite\n",
    "    save_to_database(sources, texts, classifications)\n",
    "\n",
    "    # Print sample results\n",
    "    for source, text, classification in zip(sources, texts, classifications[:10]):  # Show first 10 results\n",
    "        print(f\"Expected: {source}, Predicted: {classification} : {text[:100]}...\")\n",
    "\n",
    "def save_to_database(sources: List[str], lines: List[str], classifications: List[Tuple[str, float]]):\n",
    "    \"\"\"\n",
    "    Saves classification results to an SQLite database.\n",
    "\n",
    "    :param sources: List of sources (human or AI).\n",
    "    :param lines: List of text samples.\n",
    "    :param classifications: List of classification results (label, confidence).\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    data = [(source, line, label, confidence) for source, line, (label, confidence) in zip(sources, lines, classifications)]\n",
    "    cursor.executemany(\"INSERT INTO classifications (source, line_text, label, confidence) VALUES (?, ?, ?, ?)\", data)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Dataset classification results saved to database.\")\n",
    "\n",
    "# Initialize database\n",
    "initialize_database()\n",
    "\n",
    "# Run classification on the dataset\n",
    "classify_dataset()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
