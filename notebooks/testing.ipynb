{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 3000 test samples.\n",
      "ðŸ‘€ The first 5 samples in the test set:\n",
      "                                               title  label  \\\n",
      "0  Face Synthesis from Visual Attributes via Sket...      0   \n",
      "1  Face Synthesis from Visual Attributes via Sket...      1   \n",
      "2  Conformal symmetry breaking and degeneracy of ...      0   \n",
      "3  Conformal symmetry breaking and degeneracy of ...      1   \n",
      "4  Sensitivity integrals and related inequalities...      0   \n",
      "\n",
      "                                                text  word_count  \n",
      "0  Automatic synthesis of faces from visual attri...         174  \n",
      "1  This paper presents an innovative method for f...         146  \n",
      "2  We show that though conformal symmetry can be ...         341  \n",
      "3  This research investigates the phenomenon of c...         312  \n",
      "4  This paper exhibits the closed-loop design con...         164  \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"NicolaiSivesind/human-vs-machine\", 'research_abstracts_labeled')\n",
    "\n",
    "# Convert the test set to a Pandas DataFrame\n",
    "df_test = pd.DataFrame(dataset['test'])  # Use the 'test' split\n",
    "\n",
    "print(f\"âœ… Loaded {len(df_test)} test samples.\")\n",
    "print(f\"ðŸ‘€ The first 5 samples in the test set:\\n{df_test.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table 'test_text_embeddings' created successfully.\n",
      "Inserting test sample 1  Automatic synthesis of faces from visual attributes is an important problem in computer vision and has wide applications in law enforcement and entertainment. With the advent of deep generative convolutional neural networks (CNNs), attempts have been made to synthesize face images from attributes and text descriptions. In this paper, we take a different approach, where we formulate the original problem as a stage-wise learning problem. We first synthesize the facial sketch corresponding to the visual attributes and then we reconstruct the face image based on the synthesized sketch. The proposed Attribute2Sketch2Face framework, which is based on a combination of deep Conditional Variational Autoencoder (CVAE) and Generative Adversarial Networks (GANs), consists of three stages: (1) Synthesis of facial sketch from attributes using a CVAE architecture, (2) Enhancement of coarse sketches to produce sharper sketches using a GAN-based framework, and (3) Synthesis of face from sketch using another GAN-based network. Extensive experiments and comparison with recent methods are performed to verify the effectiveness of the proposed attribute-based three stage face synthesis method. 0 into PostgreSQL...\n",
      "Inserting test sample 2  This paper presents an innovative method for face synthesis from visual attributes using conditional Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). The proposed system allows generating high-quality facial images by feeding the model with sketch-based visual attributes as input. Specifically, we first train a conditional VAE to learn the face representation using labeled training images. Then, we introduce a set of binary attribute vectors that specify the distinctive features and attributes of a particular face. Finally, we employ the trained generator of a GAN to synthesize new facial images, conditioned on the learned visual attributes. Our experiments demonstrate that the proposed methodology outperforms other existing methods in terms of facial attribute manipulation, with the added benefit of producing more diverse and realistic outputs. Our approach enables a wide range of applications, including but not limited to, face recognition, retrieval, and virtual reality systems. 1 into PostgreSQL...\n",
      "Inserting test sample 3  We show that though conformal symmetry can be broken by the dilaton, such can happen without breaking the conformal degeneracy patterns in the spectra. We departure from R^1XS^3 slicing of AdS_5 noticing that the inverse radius, R, of S^3 relates to the temperature of the deconfinement phase transition and has to satisfy, \\hbar c/R >> \\Lambda_{QCD}. We then focus on the eigenvalue problem of the S^3 conformal Laplacian, given by 1/R^2 (K^2+1), with K^2 standing for the Casimir invariant of the so(4) algebra. Such a spectrum is characterized by a (K+1)^2 fold degeneracy of its levels, with K\\in [0,\\infty). We then break the conformal S^3 metric as, d\\tilde{s}^2=e^{-b\\chi} ((1+b^2/4) d\\chi^2 +\\sin^2\\chi (d\\theta ^2 +\\sin^2\\theta d\\varphi ^2)), and attribute the symmetry breaking scale, b\\hbar^2c^2/R^2, to the dilaton. We show that such a metric deformation is equivalent to a breaking of the conformal curvature of S^3 by a term proportional to b\\cot \\chi, and that the perturbed conformal Laplacian is equivalent to (\\tilde{K}^2 +c_K), with c_K a representation constant, and \\tilde{K}^2 being again an so(4) Casimir invariant, but this time in a representation unitarily inequivalent to the 4D rotational. In effect, the spectra before and after the symmetry breaking are determined each by eigenvalues of a Casimir invariant of an so(4) algebra, a reason for which the degeneracies remain unaltered though the conformal group symmetry breaks at the level of the representation of its algebra. We fit the S^3 radius and the \\hbar^2c^2b/R^2 scale to the high-lying excitations in the spectra of the unflavored mesons, and observe the correct tendency of the \\hbar c /R=373 MeV value to notably exceed \\Lambda_{QCD}. The size of the symmetry breaking scale is calculated as \\hbar c \\sqrt{b}/R=673.7 MeV. 0 into PostgreSQL...\n",
      "Inserting test sample 4  This research investigates the phenomenon of conformal symmetry breaking and its correlation with the degeneracy of high-lying unflavored mesons. Conformal symmetry, the invariance of a theory under changes of scale, has long been considered a fundamental tool for understanding high-energy physics. However, in recent years, it has become increasingly apparent that this symmetry is often broken, even in theories where conventional wisdom suggests it should be exact. A prominent example is QCD, the theory of the strong interaction, whose conformal symmetry is well-known to be broken at low energies.\n",
      "\n",
      "We focus on the impact of conformal symmetry breaking on the mass spectrum of high-lying unflavored mesons, a subject that has received comparatively little attention in the literature. We show that the presence of broken conformal symmetry can lead to splitting of the meson masses, even in the absence of explicit chiral symmetry breaking. Moreover, we find that this splitting depends strongly on the angular momentum of the mesons, and we provide a quantitative analysis of this dependence.\n",
      "\n",
      "Our results have important implications for the interpretation of meson spectroscopy data, as well as for the construction of effective theories of QCD. For instance, we discuss how our findings may help to resolve long-standing puzzles in the observed spectrum of mesons, and we propose a framework for systematically incorporating conformal symmetry breaking into calculations of meson properties. Furthermore, we highlight the potential relevance of our work for broader questions in high-energy physics, such as the study of holographic dualities and the search for new phenomena beyond the Standard Model.\n",
      "\n",
      "To summarize, our research sheds light on the interplay between conformal symmetry breaking and meson degeneracy, revealing new features of high-energy physics that were previously overlooked. Our results provide a quantitative understanding of how broken conformal symmetry affects the meson spectrum, and suggest novel directions for future investigations. 1 into PostgreSQL...\n",
      "Inserting test sample 5  This paper exhibits the closed-loop design constraints using the non-analytic function theory. First, the paper generalizes the sensitivity integral for linear feedback systems with the non-analytic sensitivity function. Sensitivity inequalities are determined by the integral relationships based on the presence of non-minimum phase zeros and right half plane poles. These inequalities are rephrased in plant parameter context, which must be satisfied by the feedback design. That indicates the ability of controllers under the influence of input disturbances and plant parameter variations. The paper then extends the integral to the analytic sensitivity function of the augmented linear feedback systems. This is useful to augment the ability of a linear feedback system to handle input disturbances and plant uncertainties, via modified sensitivity function theory. Numerical simulations are carried out to perform sensitivity analysis on three chemical control systems. That describes the usefulness and demonstrates the applicability of the result of this paper to examine and augment the ability of linear feedback system. 0 into PostgreSQL...\n",
      "Inserting test sample 6  This paper presents sensitivity integrals and related inequalities to enhance the performance and stability of process control systems. Sensitivity integrals quantify the effects of disturbances on the output of control systems, providing important insights into the system's behavior. By using these integrals, we derive inequalities that impose bounds on the sensitivity integrals, resulting in improved control system robustness and better tracking of desired set-points. Furthermore, we show how these inequalities can be used to design control systems that achieve specific performance objectives, such as minimizing the effect of disturbances or maximizing disturbance rejection. The proposed methodology is illustrated through several examples, demonstrating its effectiveness in practical settings. The results suggest that sensitivity integrals and related inequalities are valuable tools for engineers and researchers working in the field of process control, offering a powerful approach to enhance the performance and robustness of control systems. 1 into PostgreSQL...\n",
      "Inserting test sample 7  Software engineering (SE) research should be relevant to industrial practice.\n",
      "\n",
      "There have been regular discussions in the SE community on this issue since the 1980's, led by pioneers such as Robert Glass. As we recently passed the milestone of \"50 years of software engineering\", some recent positive efforts have been made in this direction, e.g., establishing \"industrial\" tracks in several SE conferences. However, many researchers and practitioners believe that we, as a community, are still struggling with research relevance and utility. The goal of this paper is to synthesize the evidence and experience-based opinions shared on this topic so far in the SE community, and to encourage the community to further reflect and act on the research relevance. For this purpose, we have conducted a Multi-vocal Literature Review (MLR) of 54 systematically-selected sources (papers and non peer-reviewed articles). Instead of relying on and considering the individual opinions on research relevance, mentioned in each of the sources, the MLR aims to synthesize and provide the \"holistic\" view on the topic. The highlights of our MLR findings are as follows. The top three root causes of low relevance, discussed in the community, are: (1) Researchers having simplistic views (or wrong assumptions) about SE in practice; (2) Lack of connection with industry; and (3) Wrong identification of research problems. The top three suggestions for improving research relevance are: (1) Using appropriate research approaches such as action-research; (2) Choosing relevant research problems; and (3) Collaborating with industry. By synthesizing all the discussions on this important topic so far, this paper aims to encourage further discussions and actions in the community to increase our collective efforts to improve the research relevance. 0 into PostgreSQL...\n",
      "Inserting test sample 8  This paper explores the practical relevance of software engineering research through a synthesis of the community's voice. While academic discussions of software engineering theory and practice abound, sceptics have criticized software engineering research for its lack of applicability to real-world situations. Accordingly, in our research, we aimed to understand how the software engineering community perceives the practical value of research in the field. To do so, we conducted a comprehensive review of the empirical studies, examining specifically the software engineering research endeavor, and taking note of discussions of applicability within them.\n",
      "\n",
      "Through our analysis, we identified key themes in the community's discussions of the relevance of software engineering research. Our results indicate that although software engineering research is frequently scrutinized for lacking practical relevance, community members acknowledge its significant impact. Furthermore, community members emphasized the importance of experimental rigor, generalization, and replication in research for it to be practically relevant.\n",
      "\n",
      "Our research presents several contributions. First, it provides a comprehensive review of the current state of research in software engineering, shedding new light on the community's beliefs about the field's practical relevance. Second, it provides significant insights into the challenges of conducting practically relevant research in software engineering, such as the tension between generalization and specificity. Finally, our research shows the numerous opportunities for future research to better understand the relationship between software engineering research and practical applications.\n",
      "\n",
      "In conclusion, this paper presents significant insights into the practical relevance of software engineering research through a synthesis of the community's voice. The software engineering community acknowledges that research has tangible, practical value, but requires experimental rigor, generalization, and replication to be effective. 1 into PostgreSQL...\n",
      "Inserting test sample 9  We present high resolution large scale observations of the molecular and atomic gas in the Local Group Galaxy M33. The observations were carried out using the HERA at the 30m IRAM telescope in the CO(2-1) line achieving a resolution of 12\"x2.6 km/s, enabling individual GMCs to be resolved. The observed region mainly along the major axis out to a radius of 8.5 kpc, and covers the strip observed with HIFI/PACS Spectrometers as part of the HERM33ES Herschel key program. The achieved sensitivity in main beam temperature is 20-50 mK at 2.6 km/s velocity resolution. The CO(2-1) luminosity of the observed region is 1.7\\pm0.1x10^7 Kkm/s pc^2, corresponding to H2 masses of 1.9x10^8 Msun (including He), calculated with a NH2/ICO twice the Galactic value due to the half-solar metallicity of M33. HI 21 cm VLA archive observations were reduced and the mosaic was imaged and cleaned using the multi-scale task in CASA, yielding a series of datacubes with resolutions ranging from 5\" to 25\". The HI mass within a radius of 8.5 kpc is estimated to be 1.4x10^9 Msun. The azimuthally averaged CO surface brightness decreases exponentially with a scale length of 1.9\\pm0.1 kpc whereas the atomic gas surface density is constant at Sigma_HI=6\\pm2 Msun/pc^2 deprojected to face-on.\n",
      "\n",
      "The central kiloparsec H_2 surface density is Sigma_H2=8.5\\pm0.2 Msun/pc^2. The star formation rate per unit molecular gas (SF Efficiency, the rate of transformation of molecular gas into stars), as traced by the ratio of CO to Halpha and FIR brightness, is constant with radius. The SFE appears 2-4 times greater than of large spiral galaxies. A morphological comparison of molecular and atomic gas with tracers of star formation shows good agreement between these maps both in terms of peaks and holes. A few exceptions are noted.\n",
      "\n",
      "Several spectra, including those of a molecular cloud situated more than 8 kpc from the galaxy center, are presented. 0 into PostgreSQL...\n",
      "Inserting test sample 10  Molecular gas, consisting mainly of molecular hydrogen (H2), represents the raw material for star formation in galaxies. Its distribution and properties therefore play a key role in shaping the galactic environment. We present results from deep CO(1-0) observations of the Local Group galaxy M33 obtained with the IRAM 30-m telescope. A 3D spectroscopic analysis of the CO emission line was performed by applying the CLUMPFIND algorithm to the cube, in order to identify and characterize individual molecular clouds. The molecular gas is concentrated in a few giant molecular clouds (GMCs), with a total molecular gas mass of approximately 2.5 x 10^8 solar masses, and a corresponding mean H2 gas surface density of around 12 Msun/pc^2. We find that the GMCs have an approximately log-normal mass distribution and a typical mass of around 2 x 10^6 solar masses. Moreover, the molecular-to-atomic gas mass ratio for M33 is found to be ~0.4, which is lower than the typical value for other galaxy types, suggesting that the molecular gas in M33 is relatively less efficiently converted into stars. Finally, we compare our results with a sample of other Local Group galaxies and find that the global molecular gas properties of M33 are broadly consistent with those of other late-type spirals, although it has somewhat lower molecular gas content at a given HI mass. The unique combination of high sensitivity, spatial resolution, and velocity resolution offered by our observations make them a valuable resource for the study of the interstellar medium and star formation in M33, and demonstrate the exquisite capabilities of current and future millimeter-wave telescopes for the study of nearby galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 11  In this note we demonstrate that the polynomials introduced by Dubov, Eleonskii, and Kulagin in relation to nonharmonic oscillators with equidistant spectra are a discrete Darboux transformation of Hermite polynomials. In particular, we obtain a modification of the Christoffel formula since its classical form cannot be applied in this case. 0 into PostgreSQL...\n",
      "Inserting test sample 12  We present a modification to the classical Christoffel formula, which involves the Dubov-Eleonskii-Kulagin (DEK) polynomials. Our approach provides an alternative algorithm for computing polynomial approximations to arbitrary functions on the real line. Numerical experiments show improvements over known methods. 1 into PostgreSQL...\n",
      "Inserting test sample 13  Thermonuclear explosions may arise in binaries in which a CO white dwarf (WD) accretes He from a companion. If the accretion rate allows a sufficiently large mass of He to accumulate prior to ignition of nuclear burning, the He surface layer may detonate, giving rise to an astrophysical transient. Detonation of the He layer generates shock waves that propagate into the underlying CO WD.\n",
      "\n",
      "This might directly ignite a detonation at the edge of the CO WD or compress the core of the WD sufficiently to trigger a CO detonation near the centre. If either ignition mechanism works, the two detonations can release sufficient energy to completely unbind the WD. Here we extend our 2D studies of this double-detonation model to low-mass CO WDs. We investigate the feasibility of triggering a secondary core detonation by shock convergence in low-mass CO WDs and the observable consequences of such a detonation. Our results suggest that core detonation is probable, even for the lowest CO core masses realized in nature. We compute spectra and light curves for models in which either an edge-lit or compression-triggered CO detonation is assumed to occur and compare these to models in which no CO detonation was allowed to occur. If significant shock compression of the CO WD occurs prior to detonation, explosion of the CO WD can produce a sufficiently large mass of radioactive iron-group nuclei to affect the light curves. In particular, this can lead to relatively slow post-maximum decline. If the secondary detonation is edge-lit, however, the CO WD explosion primarily yields intermediate-mass elements that affect the observables more subtly. In this case, NIR observations and detailed spectroscopic analysis would be needed to determine whether core detonation occurred. We comment on the implications of our results for understanding peculiar astrophysical transients including SN 2002bj, SN 2010X and SN 2005E. 0 into PostgreSQL...\n",
      "Inserting test sample 14  Thermonuclear transients from low-mass carbon-oxygen (CO) white dwarfs have attracted significant attention in astrophysics due to their role as cosmological probes. The Double-Detonation (DD) model is a popular theory to explain these transients, in which a deflagration wave ignites in the outer envelope, followed by a detonation wave that triggers the CO coreâ€™s complete thermonuclear explosion. This paper presents two-dimensional simulations of the DD model carried out using the FLASH code, which is based on the Eulerian hydrodynamics method, on a computational domain of 1,024 x 512 x 512 zones.\n",
      "\n",
      "We find that the simulations reproduce the key features of DD, including thermonuclear expansion, carbon detonation, and merging deflagration fronts. Our results indicate that a detonation wave can be generated in CO white dwarfs with a mass of less than 0.6 solar masses, in agreement with previous one-dimensional calculations. We also study several simulation parameters and provide interpretations for the observed phenomena.\n",
      "\n",
      "For instance, we find that the ignition process is sensitive to the composition of the accreted material, and the nucleosynthesis processing leads to significant production of stable iron-group elements, silicon, and calcium group species. The simulations further indicate that the emission light curves in the B, V, R, and I bands for low-mass CO white dwarfs remain similar to those reported in previous studies.\n",
      "\n",
      "Overall, the simulations presented in this study provide a clear picture of the mechanisms and parameters responsible for the DD model. Our findings suggest that the DD model can significantly contribute to Type Ia supernovae. Future work could explore the accuracy of these simulations by comparing them with observations. Moreover, the 2D simulations lay the foundation for building three-dimensional models, representing the next step toward improving our understanding of these unique astrophysical phenomena. 1 into PostgreSQL...\n",
      "Inserting test sample 15  Let $L$ be a periodic self-adjoint linear elliptic operator in $\\R^n$ with coefficients periodic with respect to a lattice $\\G$, e.g. Schr\\\"{o}dinger operator $(i^{-1}\\partial/\\partial_x-A(x))^2+V(x)$ with periodic magnetic and electric potentials $A,V$, or a Maxwell operator $\\nabla\\times\\varepsilon (x)^{-1}\\nabla\\times$ in a periodic medium. Let also $S$ be a finite part of its spectrum separated by gaps from the rest of the spectrum. We address here the question of existence of a finite set of exponentially decaying Wannier functions $w_j(x)$ such that their $\\G$-shifts $w_{j,\\g}(x)=w_j(x-\\g)$ for $\\g\\in\\G$ span the whole spectral subspace corresponding to $S$. It was shown by D.~Thouless in 1984 that a topological obstruction sometimes exists to finding exponentially decaying $w_{j,\\g}$ that form an orthonormal (or any) basis of the spectral subspace. This obstruction has the form of non-triviality of certain finite dimensional (with the dimension equal to the number of spectral bands in $S$) analytic vector bundle (Bloch bundle). It was shown in 2009 by one of the authors that it is always possible to find a finite number $l$ of exponentially decaying Wannier functions $w_j$ such that their $\\G$-shifts form a tight (Parseval) frame in the spectral subspace. This appears to be the best one can do when the topological obstruction is present.\n",
      "\n",
      "Here we significantly improve the estimate on the number of extra Wannier functions needed, showing that in physical dimensions the number $l$ can be chosen equal to $m+1$, i.e. only one extra family of Wannier functions is required. This is the lowest number possible in the presence of the topological obstacle. The result for dimension four is also stated (without a proof), in which case $m+2$ functions are needed.\n",
      "\n",
      "The main result of the paper was announced without a proof in Bull. AMS, July 2016. 0 into PostgreSQL...\n",
      "Inserting test sample 16  This research paper is focused on understanding the properties and characteristics of composite Wannier functions that decay exponentially. Specifically, we investigate Parseval frames for this class of functions. Our analysis shows that for a composite Wannier function satisfying certain conditions, there exist Parseval frames consisting of exponentials with some additional properties. \n",
      "\n",
      "We begin by examining the definition and properties of composite Wannier functions with exponentially decaying tails. We establish the conditions under which such functions form a tight frame, and we explore the behavior of the frame constants under various assumptions. We then introduce the notion of a Parseval frame and explain its relevance in frame theory. The main result of our paper is a characterization of the Parseval frames for exponentially decaying composite Wannier functions. \n",
      "\n",
      "In order to prove our main result, we use tools from harmonic analysis, such as the Fourier transform and the Plancherel theorem. We also utilize a variety of techniques from different areas of mathematics, including complex analysis, functional analysis and operator theory. In particular, we employ the concept of the adjoint operator, and we prove that the adjoint of a certain operator associated with the composite Wannier function is a bounded operator. \n",
      "\n",
      "Our analysis reveals interesting properties of Parseval frames of composite Wannier functions with exponentially decaying tails, including the existence of frames consisting of exponential functions with certain decay rates, as well as the existence of frames with additional properties like orthogonality and symmetry. We also provide examples and counterexamples to illustrate our findings. \n",
      "\n",
      "Overall, our research sheds light on the behavior of composite Wannier functions with exponential decay tails and their Parseval frames. Our results contribute to a better understanding of the properties and applications of frame theory in various fields, including signal processing, quantum physics, and information theory. 1 into PostgreSQL...\n",
      "Inserting test sample 17  The main purpose of this paper is to revisit the well known potentials, called stress functions, needed in order to study the parametrizations of the stress equations, respectively provided by G.B. Airy (1863) for 2-dimensional elasticity, then by E. Beltrami (1892), J.C. Maxwell (1870) and G. Morera (1892) for 3-dimensional elasticity, finally by A. Einstein (1915) for 4-dimensional elasticity, both with a variational procedure introduced by C.\n",
      "\n",
      "Lanczos (1949,1962) in order to relate potentials to Lagrange multipliers.\n",
      "\n",
      "Using the methods of Algebraic Analysis, namely mixing differential geometry with homological algebra and combining the double duality test involved with the Spencer cohomology, we shall be able to extend these results to an arbitrary situation with an arbitrary dimension n. We shall also explain why double duality is perfectly adapted to variational calculus with differential constraints as a way to eliminate the corresponding Lagrange multipliers. For example, the canonical parametrization of the stress equations is just described by the formal adjoint of the n2(n2 -- 1)/12 components of the linearized Riemann tensor considered as a linear second order differential operator but the minimum number of potentials needed in elasticity theory is equal to n(n -- 1)/2 for any minimal parametrization. Meanwhile, we can provide all the above results without even using indices for writing down explicit formulas in the way it is done in any textbook today. The example of relativistic continuum mechanics with n = 4 is provided in order to prove that it could be strictly impossible to obtain such results without using the above methods. We also revisit the possibility (Maxwell equations of electromag- netism) or the impossibility (Einstein equations of gravitation) to obtain canonical or minimal parametrizations for various other equations of physics.\n",
      "\n",
      "It is nevertheless important to notice that, when n and the algorithms presented are known, most of the calculations can be achieved by using computers for the corresponding symbolic computations. Finally, though the paper is mathematically oriented as it aims providing new insights towards the mathematical foundations of elasticity theory and mathematical physics, it is written in a rather self-contained way. 0 into PostgreSQL...\n",
      "Inserting test sample 18  This paper presents a comprehensive review and reappraisal of some of the most prominent mathematical potentials in physics, namely: Airy, Beltrami, Maxwell, Morera, Einstein, and Lanczos potentials. By collecting and analyzing the existing literature on the topic, we aim to provide a theoretical framework that captures both the physical and mathematical essence of these potentials. In particular, we focus on investigating the interrelationships and underlying structures that unite these potentials and reveal their significance in contemporary physics research.\n",
      "\n",
      "Each of the potentials under investigation plays a critical role in various areas of physics. The Airy potential, for instance, has broad applications in the description of wave propagation phenomena, while the Beltrami potential is essential in fluid mechanics and plasma physics. The Maxwell potential was historically crucial in the formulation of electromagnetic theory, and the Morera potential has been essential in studying harmonic functions and their complex analysis. The Einstein potential is a fundamental construct in general relativity, and the Lanczos potential has vast applications in fields such as quantum mechanics and statistical mechanics.\n",
      "\n",
      "We review these potentials under three main categories: historical developments, mathematical properties, and physical applications. By distinguishing and analyzing these perspectives in detail, we provide a unique and holistic viewpoint to these potentials that is often absent in the existing literature. We also explain how significant mathematical properties of these potentials, such as linearity, homogeneity, and conformality, can often translate to fundamental physical principles such as the conservation of energy and momentum, invariance under coordinate transformations, and the principle of least action.\n",
      "\n",
      "We conclude by emphasizing the current and future importance of these potentials in contemporary physics research. From formulating efficient numerical algorithms to studying cosmology and black hole physics, these potentials play a vital role in almost all areas of modern physics. We hope this review paper will inspire further interdisciplinary studies, create new insights, and spark promising research projects in the years to come. 1 into PostgreSQL...\n",
      "Inserting test sample 19  Let $(R, \\m, k)$ be a complete Cohen-Macaulay local ring. In this paper, we assign a numerical invariant, for any balanced big Cohen-Macaulay module, called $\\uh$-length. Among other results, it is proved that, for a given balanced big Cohen-Macaulay $R$-module $M$ with an $\\m$-primary cohomological annihilator, if there is a bound on the $\\uh$-length of all modules appearing in $\\CM$-support of $M$, then it is fully decomposable, i.e. it is a direct sum of finitely generated modules. While the first Brauer-Thrall conjecture fails in general by a counterexample of Dieterich dealing with multiplicities to measure the size of maximal Cohen-Macaulay modules, our formalism establishes the validity of the conjecture for complete Cohen-Macaulay local rings. In addition, the pure-semisimplicity of a subcategory of balanced big Cohen-Macaulay modules is settled. Namely, it is shown that $R$ is of finite $\\CM$-type if and only if the category of all fully decomposable balanced big Cohen-Macaulay modules is closed under kernels of epimorphisms. Finally, we examine the mentioned results in the context of Cohen-Macaulay artin algebras admitting a dualizing bimodule $\\omega$, as defined by Auslander and Reiten. It will turn out that, $\\omega$-Gorenstein projective modules with bounded $\\CM$-support are fully decomposable. In particular, a Cohen-Macaulay algebra $\\Lambda$ is of finite $\\CM$-type if and only if every $\\omega$-Gorenstein projective module is of finite $\\CM$-type, which generalizes a result of Chen for Gorenstein algebras. Our main tool in the proof of results is Gabriel-Roiter (co)measure, an invariant assigned to modules of finite length, and defined by Gabriel and Ringel. This, in fact, provides an application of the Gabriel-Roiter (co)measure in the category of maximal Cohen-Macaulay modules. 0 into PostgreSQL...\n",
      "Inserting test sample 20  The study of balanced big Cohen-Macaulay modules has been of great interest in the field of representation theory due to their intriguing properties. These modules are known for their ability to provide significant insights into the structure and behavior of algebraic varieties. In this paper, we explore the representation-theoretic properties of these modules and provide a comprehensive analysis of their various characteristics. \n",
      "\n",
      "We begin by examining the notion of balancedness in the context of Cohen-Macaulay modules. We show that the concept of balance is intimately linked to the structure of these modules and plays a critical role in their representation theory. We then delve into the study of big Cohen-Macaulay modules and analyze their behavior under a variety of conditions. \n",
      "\n",
      "Our investigation takes us to a deeper understanding of the interplay between balancedness, big Cohen-Macaulayness, and representation theory. We provide examples of these modules and investigate their properties in depth, including Hilbert functions, Betti numbers, and depth. \n",
      "\n",
      "Moreover, we examine the relationships between balanced big Cohen-Macaulay modules and other aspects of algebraic geometry. We show how these modules can provide insights into the Hilbert scheme, the moduli spaces of sheaves, and the geometry of the Grassmannian. \n",
      "\n",
      "In conclusion, our study of representation-theoretic properties of balanced big Cohen-Macaulay modules provides a significant contribution to this fascinating area of algebraic geometry. Our results shed light on the structure and behavior of these modules and provide a foundation for further research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 21  We have investigated analytically and numerically the liquid-glass transition of hard spheres for dimensions $d\\to \\infty $ in the framework of mode-coupling theory. The numerical results for the critical collective and self nonergodicity parameters $f_{c}(k;d) $ and $f_{c}^{(s)}(k;d) $ exhibit non-Gaussian $k$ -dependence even up to $d=800$. $f_{c}^{(s)}(k;d) $ and $f_{c}(k;d) $ differ for $k\\sim d^{1/2}$, but become identical on a scale $k\\sim d$, which is proven analytically. The critical packing fraction $\\phi_{c}(d) \\sim d^{2}2^{-d}$ is above the corresponding Kauzmann packing fraction $\\phi_{K}(d)$ derived by a small cage expansion. Its quadratic pre-exponential factor is different from the linear one found earlier. The numerical values for the exponent parameter and therefore the critical exponents $a$ and $b$ depend on $d$, even for the largest values of $d$. 0 into PostgreSQL...\n",
      "Inserting test sample 22  The glass transition of hard spheres in high dimensions is a subject of great interest in materials science. In this study, we investigate the behavior of dense, hard spheres in high-dimensional spaces. We use computer simulations to understand the nature of the glass transition in such systems, and compare the results with those obtained using theoretical models. Our findings suggest that the glass transition in high-dimensional systems occurs at a lower density compared to the usual three-dimensional case. Additionally, we investigate the effect of temperature on the glass transition, and find that the transition is smoother and occurs at lower temperatures in high dimensions. Our results provide new insights into the glass transition in high-dimensional systems, which could be useful in designing and developing materials with desirable properties. 1 into PostgreSQL...\n",
      "Inserting test sample 23  We further investigate, in the planar limit of N=4 supersymmetric Yang Mills theories,the high energy Regge behavior of six-point MHV scattering amplitudes.\n",
      "\n",
      "In particular, for the new Regge cut contribution found in our previous paper, we compute in the leading logarithmic approximation (LLA) the energy spectrum of the BFKL equation in the color octet channel, and we calculate explicitly the two loop corrections to the discontinuities of the amplitudes for the transitions 2 to 4 and 3 to 3. We find an explicit solution of the BFKL equation for the octet channel for arbitrary momentum transfers and investigate the intercepts of the Regge singularities in this channel. As an important result we find that the universal collinear and infrared singularities of the BDS formula are not affected by this Regge-cut contribution. Any improvement of the BDS formula should reproduce this cut to all orders in the coupling. 0 into PostgreSQL...\n",
      "Inserting test sample 24  In this paper, we investigate the Regge cut contribution of N=4 supersymmetric Yang-Mills scattering amplitudes at high energies. We utilize the formalism of multi-Regge kinematics to express the scattering amplitude of gluons in terms of complex angular momentum. By extracting the residues of the amplitude at the poles associated with the Regge cuts, we obtain the contribution to the amplitude from particles with different spins and masses. We show that this approach provides a powerful tool to study the high-energy behavior of the scattering amplitude, and allows for the computation of the leading logarithmic contribution to the amplitude at all orders in perturbation theory. Our results serve as a stepping stone toward a complete understanding of N=4 supersymmetric Yang-Mills scattering amplitudes and their behavior at high energies. 1 into PostgreSQL...\n",
      "Inserting test sample 25  Let $G$ be a graph with vertex set $V(G)$ and edge set $E(G)$, and let $d(u,w)$ denote the length of a $u-w$ geodesic in $G$. For any $v\\in V(G)$ and $e=xy\\in E(G)$, let $d(e,v)=\\min\\{d(x,v),d(y,v)\\}$. For distinct $e_1, e_2\\in E(G)$, let $R\\{e_1,e_2\\}=\\{z\\in V(G):d(z,e_1)\\neq d(z,e_2)\\}$. Kelenc et al.\n",
      "\n",
      "[Discrete Appl. Math. 251 (2018) 204-220] introduced the edge dimension of a graph: A vertex subset $S\\subseteq V(G)$ is an edge resolving set of $G$ if $|S\\cap R\\{e_1,e_2\\}|\\ge 1$ for any distinct $e_1, e_2\\in E(G)$, and the edge dimension $edim(G)$ of $G$ is the minimum cardinality among all edge resolving sets of $G$.\n",
      "\n",
      "For a real-valued function $g$ defined on $V(G)$ and for $U\\subseteq V(G)$, let $g(U)=\\sum_{s\\in U}g(s)$. Then $g:V(G)\\rightarrow[0,1]$ is an edge resolving function of $G$ if $g(R\\{e_1,e_2\\})\\ge1$ for any distinct $e_1,e_2\\in E(G)$. The fractional edge dimension $edim_f(G)$ of $G$ is $\\min\\{g(V(G)):g\\mbox{ is an edge resolving function of }G\\}$. Note that $edim_f(G)$ reduces to $edim(G)$ if the codomain of edge resolving functions is restricted to $\\{0,1\\}$.\n",
      "\n",
      "We introduce and study fractional edge dimension and obtain some general results on the edge dimension of graphs. We show that there exist two non-isomorphic graphs on the same vertex set with the same edge metric coordinates. We construct two graphs $G$ and $H$ such that $H \\subset G$ and both $edim(H)-edim(G)$ and $edim_f(H)-edim_f(G)$ can be arbitrarily large. We show that a graph $G$ with $edim(G)=2$ cannot have $K_5$ or $K_{3,3}$ as a subgraph, and we construct a non-planar graph $H$ satisfying $edim(H)=2$. It is easy to see that, for any connected graph $G$ of order $n\\ge3$, $1\\le edim_f(G) \\le \\frac{n}{2}$; we characterize graphs $G$ satisfying $edim_f(G)=1$ and examine some graph classes satisfying $edim_f(G)=\\frac{n}{2}$. We also determine the fractional edge dimension for some classes of graphs. 0 into PostgreSQL...\n",
      "Inserting test sample 26  Graph theory is a fundamental field of mathematics that deals with the study of mathematical structures called graphs. Of particular interest are the concepts of edge dimension and fractional edge dimension, which are important parameters for many applications in computer science, communication networks and social networks. In this paper, we investigate these two parameters by providing new theoretical results, algorithms and applications.\n",
      "\n",
      "First, we define the edge dimension of a graph as the smallest integer k such that the edges of the graph can be represented as the union of k matchings. This definition leads to several interesting properties, such as the fact that the edge dimension of a bipartite graph is equal to its maximum degree. We also study the complexity of computing the edge dimension of a graph, and propose a dynamic programming algorithm that solves this problem in polynomial time.\n",
      "\n",
      "Second, we introduce the concept of fractional edge dimension, which is a real-valued parameter that measures the extent to which the edges of a graph can be covered by matchings of fractional size. We prove that the fractional edge dimension of a graph is always less than or equal to its edge dimension, and show that there exist graphs whose fractional edge dimension is strictly smaller than their edge dimension.\n",
      "\n",
      "Finally, we present several applications of edge dimension and fractional edge dimension in various contexts. For example, we show how edge dimension can be used to study the complexity of network routing problems in communication networks, and how fractional edge dimension can be used to model the spread of diseases in social networks.\n",
      "\n",
      "Overall, this paper provides a comprehensive study of edge dimension and fractional edge dimension of graphs, and sheds light on their many theoretical and practical applications. Our results pave the way for future research in this important area of graph theory. 1 into PostgreSQL...\n",
      "Inserting test sample 27  I present the results of multi-component decomposition of V and R broadband images of a sample of 17 nearby galaxies, most of them hosting bars and active galactic nuclei. I use BUDDA v2.1 to produce the fits, allowing to include bars and AGN in the models. A comparison with previous results from the literature shows a fairly good agreement. It is found that the axial ratio of bars, as measured from ellipse fits, can be severely underestimated if the galaxy axisymmetric component is relatively luminous. Thus, reliable bar axial ratios can only be determined by taking into account the contributions of bulge and disc to the light distribution in the galaxy image. Through a number of tests, I show that neglecting bars when modelling barred galaxies can result in a overestimation of the bulge-to-total luminosity ratio of a factor of two.\n",
      "\n",
      "Similar effects result when bright, type 1 AGN are not considered in the models. By artificially redshifting the images, I show that the structural parameters of more distant galaxies can in general be reliably retrieved through image fitting, at least up to the point where the physical spatial resolution is ~ 1.5 Kpc. This exercise shows that disc parameters are particularly robust, but bulge parameters are prone to errors if its effective radius is small compared to the seeing radius, and might suffer from systematic effects. In this low resolution regime, the effects of ignoring bars are still present, but AGN light is smeared out. I briefly discuss the consequences of these results to studies of the structural properties of galaxies, in particular on the stellar mass budget in the local universe. With reasonable assumptions, it is possible to show that the stellar content in bars can be similar to that in classical bulges and elliptical galaxies. (Abridged) 0 into PostgreSQL...\n",
      "Inserting test sample 28  Barred galaxies and Active Galactic Nuclei (AGN) hosts have been subjects of extensive research over the years. In this study, we aim to investigate the image decomposition of these two astronomical phenomena, with a focus on their morphology, kinematics, and central regions. \n",
      "\n",
      "Our analysis is based on a sample of over 200 barred galaxies and 50 AGN hosts, whose imaging data was obtained from a number of ground and space-based surveys. We employ a range of state-of-the-art techniques such as 2D bulge-disk-bar decompositions, Principal Component Analysis, and Monte Carlo Markov Chain modeling to analyze and quantify the characteristics of these galaxies and AGN hosts. \n",
      "\n",
      "Our results demonstrate that barred galaxies typically exhibit a central bulge, a bar, and a disk, with various degrees of prominence. Furthermore, we find that these galaxies' central regions can be modeled as elliptical bulges, which can be linked to the mass of the supermassive black hole at their center. As for AGN hosts, we observe a range of morphologies, with some displaying clear evidence of a bar and/or strong tidal interactions with nearby galaxies. \n",
      "\n",
      "Our study highlights the importance of decomposing the images of barred galaxies and AGN hosts to fully understand their underlying characteristics and structure. The results can have implications for our understanding of galaxy evolution and supermassive black hole growth. Overall, this paper contributes to the ongoing effort in studying galaxy morphology and provides insights into the formation and evolution of barred galaxies and AGN hosts. 1 into PostgreSQL...\n",
      "Inserting test sample 29  We present broad-band X-ray spectroscopy of the energetic components that make up the supernova remnant (SNR) Kesteven 75 using concurrent 2017 Aug 17-20 XMM-Newton and NuSTAR observations, during which the pulsar PSR J1846-0258 is found to be in the quiescent state. The young remnant hosts a bright pulsar wind nebula powered by the highly-energetic (Edot = 8.1E36 erg/s) isolated, rotation-powered pulsar, with a spin-down age of only P/2Pdot ~ 728 yr. Its inferred magnetic field (Bs = 4.9E13 G) is the largest known for these objects, and is likely responsible for intervals of flare and burst activity, suggesting a transition between/to a magnetar state. The pulsed emission from PSR J1846-0258 is well-characterized in the 2-50 keV range by a power-law model with photon index Gamma_PSR = 1.24+/-0.09 and a 2-10 keV unabsorbed flux of (2.3+/-0.4)E-12 erg/s/cm^2). We find no evidence for an additional non-thermal component above 10 keV in the current state, as would be typical for a magnetar. Compared to the Chandra pulsar spectrum, the intrinsic pulsed fraction is 71+/-16% in 2-10 keV band. A power-law spectrum for the PWN yields Gamma_PWN = 2.03+/-0.03 in the 1-55 keV band, with no evidence of curvature in this range, and a 2-10 keV unabsorbed flux (2.13+/-0.02)E-11 erg/s/cm^2. The NuSTAR data reveal evidence for a hard X-ray component dominating the SNR spectrum above 10 keV which we attribute to a dust-scattered PWN component. We model the dynamical and radiative evolution of the Kes 75 system to estimate the birth properties of the neutron star, the energetics of its progenitor, and properties of the PWN. This suggests that the progenitor of Kes 75 was originally in a binary system which transferred most its mass to a companion before exploding. 0 into PostgreSQL...\n",
      "Inserting test sample 30  The highly magnetized pulsar PSR J1846-0258, its wind nebula and hosting supernova remnant Kes 75 have been the subject of extensive X-ray spectroscopy studies in the past decade. The pulsar is located in the Galactic plane, about 18,000 light years from Earth. It has the highest surface magnetic field strength, about 100 times greater than typical neutron stars. The pulsar's emission, which is observed across the electromagnetic spectrum, is characterized by non-thermal radiation. Its surrounding wind nebula has been studied in detail, revealing the presence of a rapidly moving jet, as well as an X-ray bright spot at the base of the jet. \n",
      "\n",
      "The supernova remnant Kes 75, which is associated with PSR J1846-0258, has also been a subject of X-ray spectroscopy studies. The remnant is non-spherical in shape and is believed to have originated from a Type Ia supernova explosion. The remnant's X-ray emission spectrum reveals the presence of a thermal and non-thermal component, indicating the presence of a shock-heated plasma and relativistic particles, respectively. The remnant also exhibits synchrotron radiation at radio wavelengths.\n",
      "\n",
      "Recent X-ray imaging and spectroscopy observations of Kes 75 have further revealed the complex interaction between the pulsar wind and the remnant's expanding shockwave. The observations show evidence of efficient particle acceleration, which is likely to play a key role in supernova remnants and the surrounding interstellar medium. These observations provide important insights into the complex physics of high-energy astrophysical phenomena.\n",
      "\n",
      "In summary, the X-ray spectroscopy of PSR J1846-0258, its wind nebula and hosting supernova remnant Kes 75 have revealed a wealth of information about the physical properties and dynamics of these objects. Further X-ray observations and theoretical modeling will be needed to fully understand the complex interaction between the pulsar wind and the supernova remnant, and its implications for the broader field of high-energy astrophysics. 1 into PostgreSQL...\n",
      "Inserting test sample 31  The first purpose of this paper is to point out a curious result announced by Macaulay on the Hilbert function of a differential module in his famous book The Algebraic Theory of Modular Systems published in 1916. Indeed, on page 78/79 of this book, Macaulay is saying the following: \" A polynomial ideal $\\mathfrak{a} \\subset k[{\\chi}\\_1$,..., ${\\chi}\\_n]=k[\\chi]$ is of the {\\it principal class} and thus {\\it unmixed} if it has rank $r$ and is generated by $r$ polynomials. Having in mind this definition, a primary ideal $\\mathfrak{q}$ with associated prime ideal $\\mathfrak{p} = rad(\\mathfrak{q})$ is such that any ideal $\\mathfrak{a}$ of the principal class with $\\mathfrak{a} \\subset \\mathfrak{q}$ determines a primary ideal of greater {\\it multiplicity} over $k$. In particular, we have $dim\\_k(k[\\chi]/({\\chi}\\_1$,...,${\\chi}\\_n)^2)=n+1$ because, passing to a system of PD equations for one unknown $y$, the parametric jets are \\{${y,y\\_1, ...,y\\_n}$\\} but any ideal $\\mathfrak{a}$ of the principal class with $\\mathfrak{a}\\subset ({\\chi}\\_1,{\\^a},{\\chi}\\_n)^2$ is contained into a {\\it simple} ideal, that is a primary ideal $\\mathfrak{q}$ such that $rad(\\mathfrak{q})=\\mathfrak{m}\\in max(k[\\chi])$ is a maximal and thus prime ideal with $dim\\_k(M)=dim\\_k(k[\\chi]/\\mathfrak{q})=2^n$ at least.\n",
      "\n",
      "Accordingly, any primary ideal $\\mathfrak{q}$ may not be a member of the primary decomposition of an unmixed ideal $\\mathfrak{a} \\subseteq \\mathfrak{q}$ of the principal class. Otherwise, $\\mathfrak{q}$ is said to be of the {\\it principal noetherian class} \". Our aim is to explain this result in a modern language and to illustrate it by providing a similar example for $n=4$. The importance of such an example is that it allows for the first time to exhibit symbols which are $2,3,4$-acyclic without being involutive. Another interest of this example is that it has properties quite similar to the ones held by the system of conformal Killing equations which are still not known. For this reason, we have put all the examples at the end of the paper and each one is presented in a rather independent way though a few among them are quite tricky.\n",
      "\n",
      "Meanwhile, the second purpose is to prove that the methods developped by Macaulay in order to study {\\it unmixed polynomial ideals} are only particular examples of new formal differential geometric techniques that have been introduced recently in order to study {\\it pure differential modules}. However these procedures are based on the formal theory of systems of ordinary differential (OD) or partial differential (PD) equations, in particular on a systematic use of the Spencer operator, and are still not acknowledged by the algebraic community. 0 into PostgreSQL...\n",
      "Inserting test sample 32  This paper investigates the properties of pure differential modules and their relationship to unmixed polynomial ideals. We begin by defining pure differential modules and exploring their basic algebraic properties. We then present a result of Macaulay which characterizes unmixed polynomial ideals in terms of their associated graded rings. Specifically, Macaulay showed that an ideal is unmixed if and only if its associated graded ring is Cohen-Macaulay.\n",
      "\n",
      "Using this result, we show that certain classes of pure differential modules are closely related to the Cohen-Macaulay property. In particular, we prove that a pure differential module is Cohen-Macaulay if and only if its associated graded ring is Cohen-Macaulay. This provides a powerful tool for studying pure differential modules and their associated polynomial ideals.\n",
      "\n",
      "We then turn our attention to a specific example of an unmixed polynomial ideal: the ideal of minors of a matrix. We show that this ideal is Cohen-Macaulay, and use this fact to derive some interesting consequences regarding the geometry of the set of singular matrices. In particular, we show that the set of singular matrices is a union of Zariski closed subsets of strictly smaller dimension. This provides a new perspective on the geometry of matrix singularities, and opens up new avenues for research.\n",
      "\n",
      "Finally, we apply our results to the study of certain special classes of algebraic varieties, known as Schubert varieties. We show that the ideal of a Schubert variety is unmixed, and hence Cohen-Macaulay. This allows us to compute the Hilbert series of Schubert varieties in terms of certain combinatorial data, known as Schubert polynomials. We also derive some interesting consequences regarding the cohomology of Schubert varieties, showing that it can be expressed in terms of the cohomology of certain Schubert cells.\n",
      "\n",
      "In summary, this paper provides a detailed study of pure differential modules and their relationship to unmixed polynomial ideals. Using a result of Macaulay, we show that certain classes of pure differential modules are closely related to the Cohen-Macaulay property. We then apply these results to the study of the geometry of singular matrices and the cohomology of Schubert varieties. This work provides a valuable contribution to the theory of algebraic geometry and opens up new avenues for research. 1 into PostgreSQL...\n",
      "Inserting test sample 33  We perform a reflection study on a new observation of the neutron star low-mass X-ray binary Aquila X-1 taken with NuSTAR during the August 2016 outburst and compare with the July 2014 outburst. The source was captured at $\\sim32\\%\\ L_{\\mathrm{Edd}}$, which is over four times more luminous than the previous observation during the 2014 outburst. Both observations exhibit a broadened Fe line profile. Through reflection modeling, we determine that the inner disk is truncated $R_{in,\\ 2016}=11_{-1}^{+2}\\ R_{g}$ (where $R_{g}=GM/c^{2}$) and $R_{in,\\ 2014}=14\\pm2\\ R_{g}$ (errors quoted at the 90% confidence level). Fiducial neutron star parameters (M$_{NS}=1.4$ M$_{\\odot}$, $R_{NS}=10$ km) give a stellar radius of $R_{NS}=4.85\\ R_{g}$; our measurements rule out a disk extending to that radius at more than the $6\\sigma$ level of confidence. We are able to place an upper limit on the magnetic field strength of $B\\leq3.0-4.5\\times10^{9}$ G at the magnetic poles, assuming that the disk is truncated at the magnetospheric radius in each case. This is consistent with previous estimates of the magnetic field strength for Aquila X-1. However, if the magnetosphere is not responsible for truncating the disk prior to the neutron star surface, we estimate a boundary layer with a maximum extent of $R_{BL,\\ 2016}\\sim10\\ R_{g}$ and $R_{BL,\\ 2014}\\sim6\\ R_{g}$. Additionally, we compare the magnetic field strength inferred from the Fe line profile of Aquila X-1 and other neutron star low-mass X-ray binaries to known accreting millisecond X-ray pulsars. 0 into PostgreSQL...\n",
      "Inserting test sample 34  In the study of neutron star low-mass X-ray binaries (LMXBs), the Aquila X-1 system is particularly intriguing because of the unusual truncation of its accretion disk. Previous studies have detailed a scenario wherein the accretion disk becomes truncated at a high state transition in the system, resulting in a sharp drop in X-ray luminosity. In this paper, we investigate the nature of the truncation behavior using a combination of observational and theoretical analyses. Our results show that the disk truncation in Aquila X-1 occurs at one-third of the Eddington limit, a finding which has significant implications for our understanding of accretion physics in LMXBs. We analyze the X-ray spectral and timing properties of the system during its high state transition, finding evidence for stable accretion disk truncation and reduced disk emission. Utilizing a simple model of the accretion disk structure, we show that the observed truncation behavior can be explained through the sustained cooling of the disk. Our analysis suggests that the truncation behavior is not anomalous, as previously thought, but rather a natural consequence of the accretion physics of LMXBs. This result provides important insight into the accretion physics of neutron star LMXBs and serves as an important stepping stone for future theoretical studies. In conclusion, our study provides new observational evidence for the truncation of accretion disks in LMXBs and highlights the need for further research into these intriguing systems. 1 into PostgreSQL...\n",
      "Inserting test sample 35  It has been observed in [Park 2014] that the physical states of the ADM formulation of 4D Einstein gravity holographically reduce and can be described by a 3D language. Obviously the approach poses the 4D covariance issue; it turns out that there are two covariance issues whose address is the main theme of the present work. Although the unphysical character of the trace piece of the fluctuation metric has been long known, it has not been taken care of in a manner suitable for the Feynman diagram computations; a proper handling of the trace piece through gauge-fixing is the key to more subtler of the covariance issues. As for the second covariance issue, a renormalization program can be carried out covariantly to any loop order at intermediate steps, thereby maintaining the 4D covariance; it is only at the final stage that one should consider the 3D physical external states. With the physical external states, the 1PI effective action reduces to 3D and renormalizability is restored just as in the entirely-3D approach of [Park 2014]. We revisit the one-loop two-point renormalization with careful attention to the trace piece of the fluctuation metric and in particular outline one-loop renormalization of the Newton's constant. 0 into PostgreSQL...\n",
      "Inserting test sample 36  In this paper, we investigate the 4D covariance of holographic quantization of Einstein gravity. Specifically, we explore the correspondence between the anti-de Sitter (AdS) supergravity and the boundary conformal field theory (CFT) under the holographic principle. Utilizing the holographic renormalization technique, we show that the AdS supergravity admits a consistent holographic description in terms of the CFT data. This correspondence is tested for a 4D pure gravity theory in AdS space, where the boundary is a 3D conformal field theory. We find that the regulated on-shell AdS gravitational action matches with a certain functional of the boundary metric, which satisfies the CFT Ward identities. In doing so, we establish the 4D covariance of the holographic quantization of Einstein gravity. Our results suggest a deeper understanding of the geometric and algebraic structures underlying the holographic duality. Furthermore, we discuss the potential implications of our findings on the study of black holes and the AdS/CFT correspondence in general. 1 into PostgreSQL...\n",
      "Inserting test sample 37  We construct a two dimensional Cellular Automata based model for the description of pedestrian dynamics. Wide range of complicated pattern formation phenomena in pedestrian dynamics are described in the model, e.g. lane formation, jams in a counterflow and egress behavior. Mean-field solution of the densely populated case and numerical solution of the sparsely populated case are provided. This model has the potential to describe more flow phenomena. 0 into PostgreSQL...\n",
      "Inserting test sample 38  This research paper proposes a cellular automata-based model for simulating pedestrian dynamics. The model takes into account collective behavior and individual characteristics of pedestrians to simulate realistic pedestrian movements. The model is developed through an iterative process of calibration and validation against empirical data. The results show that the proposed model is capable of reproducing a range of pedestrian phenomena, including pedestrian flow, lane formation, and density distributions. The model provides a useful tool for predicting pedestrian behavior in various scenarios. 1 into PostgreSQL...\n",
      "Inserting test sample 39  A search for a heavy Standard Model Higgs boson decaying via H->ZZ->llqq, where l=e,mu, is presented. The search is performed using a data set of pp collisions at sqrt(s)=7 TeV, corresponding to an integrated luminosity of 1.04 fb^-1 collected in 2011 by the ATLAS detector at the CERN LHC collider. No significant excess of events above the estimated background is found. Upper limits at 95% confidence level on the production cross section (relative to that expected from the Standard Model) of a Higgs boson with a mass in the range between 200 and 600 GeV are derived. Within this mass range, there is at present insufficient sensitivity to exclude a Standard Model Higgs boson. For a Higgs boson with a mass of 360 GeV, where the sensitivity is maximal, the observed and expected cross section upper limits are factors of 1.7 and 2.7, respectively, larger than the Standard Model prediction. 0 into PostgreSQL...\n",
      "Inserting test sample 40  This paper presents the results of a search for a heavy Standard Model Higgs boson in the channel H->ZZ->llqq using the ATLAS detector. The search was conducted at the Large Hadron Collider, where proton-proton collisions were studied at a center-of-mass energy of 13 TeV. The data collected correspond to an integrated luminosity of 36.1 fb^-1. No significant excess over the background expectation was observed in the analyzed data, which leads to the establishment of upper limits on the production cross section times branching ratio for this process. The analysis is based on the reconstruction and identification of leptons and jets produced in the final state of the selected events. This study enhances our understanding of the Higgs mechanism and provides clues for exploring beyond the Standard Model physics phenomena. 1 into PostgreSQL...\n",
      "Inserting test sample 41  Current operating systems are complex systems that were designed before today's computing environments. This makes it difficult for them to meet the scalability, heterogeneity, availability, and security challenges in current cloud and parallel computing environments. To address these problems, we propose a radically new OS design based on data-centric architecture: all operating system state should be represented uniformly as database tables, and operations on this state should be made via queries from otherwise stateless tasks. This design makes it easy to scale and evolve the OS without whole-system refactoring, inspect and debug system state, upgrade components without downtime, manage decisions using machine learning, and implement sophisticated security features. We discuss how a database OS (DBOS) can improve the programmability and performance of many of today's most important applications and propose a plan for the development of a DBOS proof of concept. 0 into PostgreSQL...\n",
      "Inserting test sample 42  DBOS is a proposed operating system that aims to enhance data-centric computing by providing a seamless experience for data management. The system operates by persistently managing data as a first-class citizen, centralizing it as a core resource. This approach allows for applications to easily interact with large and complex data sets, enabling developers to focus on business logic rather than data management. Moreover, DBOS seamlessly integrates with existing file systems on a computer, making it easier for end-users to access their data. The system also supports a highly modular architecture, allowing it to be extended with new data management capabilities as needed. In summary, DBOS prioritizes providing a powerful data management experience and enables developers to create data-rich applications without needing to worry about the complexities of data storage and access. 1 into PostgreSQL...\n",
      "Inserting test sample 43  Introduction: Individuals with chronic musculoskeletal pain show impairments in their pain-modulatory capacity. Stress-induced analgesia (SIA) is a paradigm of endogenous pain inhibition mainly tested in animals. It has not been tested in patients with chronic pain despite the important role of stress in pain modulation and the chronicity process. Methods: SIA was tested in 22 patients with chronic musculoskeletal pain and 18 healthy participants matched for age and gender. Pain thresholds, pain tolerance and suprathreshold pain sensitivity were examined before and after a cognitive stressor. Additionally, chronic stress levels, pain catastrophizing and pain characteristics were assessed as potential modulating factors. Results: Patients with chronic musculoskeletal pain compared to healthy controls showed significantly impaired SIA (F(1,37)=5.63, p=.02) for pain thresholds, but not pain tolerance (F(1,37)=0.05, p=.83) and stress-induced hyperalgesia (SIH) to suprathreshold pain ratings (F(1,37)=7.76, p=.008). Patients (r(22)=-0.50, p=.05) but not controls (r(18)=-0.39, p=.13) with high catastrophizing had low SIA as assessed by pain thresholds. In controls suprathreshold pain ratings were significantly positively correlated with catastrophizing (r(18)=0.57, p=.03) and life-time stress exposure (r(18)=0.54, p=.03). In patients neither catastrophizing (r(22)=0.21, p=.34) nor stress exposure (r(22)=0.34, p=.34) were associated with suprathreshold SIH. Discussion: Our data suggest impairments of SIA and SIH in patients with chronic musculoskeletal pain. Catastrophizing was associated with deficient SIA in the patients and higher pain ratings in controls. High life time stress also increased pain ratings in the controls. 0 into PostgreSQL...\n",
      "Inserting test sample 44  Chronic musculoskeletal pain is a common affliction that can have debilitating effects on daily life. Stress-induced analgesia (SIA) has been shown to be a potential modulator of pain in both healthy individuals and those with chronic pain conditions. The purpose of this study was to investigate the presence of SIA in patients with chronic musculoskeletal pain and healthy controls.\n",
      "\n",
      "Participants were recruited and assigned to either a stress or control group. The stress group underwent a cold pressor test and their pain thresholds were measured before and after the test. The control group underwent a non-stressful task and their pain thresholds were measured in the same manner. Results showed a significant increase in pain threshold in the stress group, but not in the control group, indicating the presence of SIA in patients with chronic musculoskeletal pain.\n",
      "\n",
      "These findings suggest that stress-induced analgesia may play a role in modulating pain in patients with chronic musculoskeletal pain. Future studies should investigate the underlying mechanisms of SIA and explore the potential therapeutic applications of this phenomenon in chronic pain conditions. Overall, these results contribute to a better understanding of the complex interplay between stress and pain perception, and may inform the development of novel pain management strategies. 1 into PostgreSQL...\n",
      "Inserting test sample 45  We study certain physically-relevant subgeometries of binary symplectic polar spaces $W(2N-1,2)$ of small rank $N$, when the points of these spaces canonically encode $N$-qubit observables. Key characteristics of a subspace of such a space $W(2N-1,2)$ are: the number of its negative lines, the distribution of types of observables, the character of the geometric hyperplane the subspace shares with the distinguished (non-singular) quadric of $W(2N-1,2)$ and the structure of its Veldkamp space. In particular, we classify and count polar subspaces of $W(2N-1,2)$ whose rank is $N-1$. $W(3,2)$ features three negative lines of the same type and its $W(1,2)$'s are of five different types. $W(5,2)$ is endowed with 90 negative lines of two types and its $W(3,2)$'s split into 13 types. 279 out of 480 $W(3,2)$'s with three negative lines are composite, i.\\,e. they all originate from the two-qubit $W(3,2)$.\n",
      "\n",
      "Given a three-qubit $W(3,2)$ and any of its geometric hyperplanes, there are three other $W(3,2)$'s possessing the same hyperplane. The same holds if a geometric hyperplane is replaced by a `planar' tricentric triad. A hyperbolic quadric of $W(5,2)$ is found to host particular sets of seven $W(3,2)$'s, each of them being uniquely tied to a Conwell heptad with respect to the quadric.\n",
      "\n",
      "There is also a particular type of $W(3,2)$'s, a representative of which features a point each line through which is negative. Finally, $W(7,2)$ is found to possess 1908 negative lines of five types and its $W(5,2)$'s fall into as many as 29 types. 1524 out of 1560 $W(5,2)$'s with 90 negative lines originate from the three-qubit $W(5,2)$. Remarkably, the difference in the number of negative lines for any two distinct types of four-qubit $W(5,2)$'s is a multiple of four. 0 into PostgreSQL...\n",
      "Inserting test sample 46  This paper presents a taxonomy of polar subspaces of multi-qubit symplectic polar spaces of small rank. Symplectic polar spaces are geometries that exhibit a rich interplay between generalized quadrangles, classical groups and their Kac-Moody analogues. These spaces have been the subject of intense study due to their many applications in areas such as quantum information theory, coding theory and finite geometries. A multi-qubit symplectic polar space is a symplectic polar space that arises from a set of qubits in quantum information theory. \n",
      "\n",
      "The taxonomy presented in this paper characterizes polar subspaces of multi-qubit symplectic polar spaces using a combination of techniques from algebraic geometry, representation theory and linear algebra. We classify polar subspaces into several families, each of which is characterized by a sequence of invariants. These invariants include the dimension of the polar subspace, the number of qubits involved, the rank of the subspaces, and other quantities related to the symplectic geometry of the spaces. \n",
      "\n",
      "We apply our classification to a number of examples and provide explicit constructions of polar subspaces for each of the families. In particular, we show that there are families of polar subspaces that exhibit interesting geometric properties such as maximal isotropy and strict maximality. We also investigate the relationship between our classification and the classification of polar spaces in the classical setting, and show that there are close connections between the two classifications. \n",
      "\n",
      "The results presented in this paper have important implications for quantum computing and cryptography. Our taxonomy of polar subspaces provides a framework for the construction of quantum error-correcting codes and other quantum information processing protocols. Moreover, the techniques developed in this paper can be applied to other areas of mathematics, such as the study of algebraic groups and their representations. Overall, this work is a significant contribution to the theory of multi-qubit symplectic polar spaces and has wide-ranging implications for the study of quantum information theory and related areas. 1 into PostgreSQL...\n",
      "Inserting test sample 47  We construct a mapping of Bell and bipartite Leggett-Garg experiments for microscopic qubits onto a gedanken experiment for macroscopic qubits based on two macroscopically distinct coherent states. This provides an unusual situation where the dichotomic measurements (and associated hidden variables) involved in the Bell tests need only discriminate between two macroscopically distinct states of a system i.e. correspond to coarse-grained measurements that do not specify values to a level of precision of order $\\sim\\hbar$. Violations of macro-realism and macroscopic local realism are predicted. We show how one may obtain consistency with a weak form of macroscopic realism (wMR): that for a system prepared in a superposition of macroscopically distinct pointer eigenstates, the outcome of the coarse-grained pointer measurement $\\hat{M}$ is predetermined. Macroscopic realism does not however hold in a deterministic fashion, where one assumes the predetermination of outcomes prior to the unitary rotations that define the choice of measurement setting in the Bell experiment. We illustrate an analogy with the Einstein-Podolsky-Rosen (EPR) argument, showing how wMR can be regarded as inconsistent with the completeness of quantum mechanics. 0 into PostgreSQL...\n",
      "Inserting test sample 48  The debate over macroscopic realism has been ongoing for many years. In this paper, we explore the dichotomy between weak and deterministic macroscopic realism. Weak macroscopic realism suggests that there is a fundamental limit to our ability to know the exact state of a macroscopic system, even in principle. Deterministic macroscopic realism, on the other hand, posits that there is a definite, knowable state for all macroscopic systems at all times. We examine the philosophical and scientific underpinnings of these two viewpoints and their implications for our understanding of the physical world. We discuss the role of quantum mechanics in the debate, as well as various experimental results that have been used to support one view or the other. Ultimately, we conclude that neither view is inherently more correct than the other, and that the nature of macroscopic reality may be more complex than either weak or deterministic macroscopic realism can fully capture. 1 into PostgreSQL...\n",
      "Inserting test sample 49  Autism spectrum disorder is a neurodevelopmental condition that includes issues with communication and social interactions. People with ASD also often have restricted interests and repetitive behaviors. In this paper we build preliminary bricks of an automated gesture imitation game that will aim at improving social interactions with teenagers with ASD. The structure of the game is presented, as well as support tools and methods for skeleton detection and imitation learning. The game shall later be implemented using an interactive robot. 0 into PostgreSQL...\n",
      "Inserting test sample 50  This paper presents the development of an automated gesture imitation game for teenagers with Autism Spectrum Disorder (ASD). The game aims to target social skills training and provide individuals with ASD a fun way to improve their nonverbal communication abilities. An initial usability study has been conducted with a small group of participants and preliminary findings show promising results for the game's potential to engage users. Further research is needed to evaluate the efficacy of this tool as a means of supporting social skills development in individuals with ASD. 1 into PostgreSQL...\n",
      "Inserting test sample 51  The heterotic string free fermionic formulation produced a large class of three generation models, with an underlying SO(10) GUT symmetry which is broken directly at the string level by Wilson lines. A common subset of boundary condition basis vectors in these models is the NAHE set, which corresponds to Z2 X Z2 orbifold of an SO(12) Narain lattice, with (h11,h21)=(27,3).\n",
      "\n",
      "Alternatively, a manifold with the same data is obtained by starting with a Z2 X Z2 orbifold at a generic point on the lattice, with (h11,h21)=(51,3), and adding a freely acting Z2 involution. The equivalence of the two constructions is proven by examining the relevant partition functions. The explicit realization of the shift that reproduces the compactification at the free fermionic point is found. It is shown that other closely related shifts reproduce the same massless spectrum, but different massive spectrum, thus demonstrating the utility of extracting information from the full partition function. A freely acting involution of the type discussed here, enables the use of Wilson lines to break the GUT symmetry and can be utilized in non-perturbative studies of the free fermionic models. 0 into PostgreSQL...\n",
      "Inserting test sample 52  In this paper, we investigate partition functions in NAHE-based free fermionic string models, which have been extensively studied in heterotic string theory. Specifically, we explore these models in the context of lattice vertex operator algebra, which has proven to be a fruitful approach to understanding their structure. We begin by presenting a brief overview of the underlying principles of free fermionic string models, and then delve into the details of NAHE-based models. We derive expressions for the partition functions of these models using the covariant lattice formalism, which allows us to simulate the models and study their properties numerically. Our results show that the partition functions exhibit non-trivial behavior that is related to the spectrum of the underlying lattice vertex operator algebra. We also consider the implications of our findings for understanding the physics of free fermionic string models more generally, and suggest directions for future research. Our findings contribute to the ongoing efforts to better understand the structure and behavior of NAHE-based free fermionic string models from the perspective of lattice vertex operator algebra. 1 into PostgreSQL...\n",
      "Inserting test sample 53  This paper discusses two aspects of current research on the Cepheid period-luminosity (P-L) relation: the derivation of mid-infrared (MIR) P-L relations and the investigation of multi-phase P-L relations. The MIR P-L relations for Cepheids are important in the James Webb Space Telescope era for the distance scale issue, as the relations have potential to derive the Hubble constant within ~ 2% accuracy - a critical constraint in precision cosmology.\n",
      "\n",
      "Consequently, we have derived the MIR P-L relations for Cepheids in the Large and Small Magellanic Clouds, using archival data from Spitzer Space Telescope.\n",
      "\n",
      "We also compared currently empirical P-L relations for Cepheids in the Magellanic Clouds to the synthetic MIR P-L relations derived from pulsational models. For the study of multi-phase P-L relations, we present convincing evidence that the Cepheid P-L relations in the Magellanic Clouds are highly dynamic quantities that vary significantly when considered as a function of pulsational phase. We found that there is a difference in P-L relations as a function of phase between the Cepheids in each of the Clouds; the most likely cause for this is the metallicity difference between the two galaxies. We also investigated the dispersion of the multi-phase P-L relations, and found that the minimum dispersion do not differ significantly from the mean light P-L dispersion. 0 into PostgreSQL...\n",
      "Inserting test sample 54  Cepheid variables have been used for decades as reliable distance indicators, thanks to the tight period-luminosity relation (PLR) they obey. This relation is traditionally calibrated in the visible and near-infrared (NIR) bands and relies on the empirical assumption that all Cepheids share the same intrinsic PLR shape and amplitude. However, recent studies have shown evidence of a dependency of the PLR on the metallicity and the pulsation period itself, which may introduce intrinsic scatter in the PLR. In this paper, we investigate the mid-infrared (MIR) PLR of Cepheids using the recently released, large sample of the Wide-field Infrared Survey Explorer (WISE) satellites. We then compare it with the NIR and visible PLRs, scrutinizing the metallicity dependence of the PLR in each band. By analyzing the PLRs of Cepheids in multi-phase, we also explore the effects of metallicity on the non-linearity of the PLR and quantify the ability of Cepheids to provide accurate distances to galaxies of different metallicities. Our results show promising potential for the MIR PLR as a distance indicator, which presents a decreased sensitivity to metallicity and potential applications to nearby and distant galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 55  It is proved that the stable commutator length of a Dehn twist in the mapping class group is positive and the tenth power of a Dehn twist about a nonseparating simple closed curve is a product of two commutators. As an application a new proof of the fact that the growth rate of a Dehn twist is linear is given. 0 into PostgreSQL...\n",
      "Inserting test sample 56  We study the stable commutator length of a Dehn twist, which measures the \"non-commutativity\" of the twist with itself in higher homotopy groups. We compute the stable commutator length of a Dehn twist in terms of intersection numbers of curves on a surface. Our results provide insights into the geometry and topology of 3-manifolds and surface bundles. 1 into PostgreSQL...\n",
      "Inserting test sample 57  The problem of the classification of the extensions of the Virasoro algebra is discussed. It is shown that all $H$-reduced $\\hat{\\cal G}_{r}$-current algebras belong to one of the following basic algebraic structures: local quadratic $W$-algebras, rational $U$-algebras, nonlocal $V$-algebras, nonlocal quadratic $WV$-algebras and rational nonlocal $UV$-algebras. The main new features of the quantum $V$-algebras and their heighest weight representations are demonstrated on the example of the quantum $V_{3}^{(1,1)}$-algebra. 0 into PostgreSQL...\n",
      "Inserting test sample 58  This paper presents a study of classical and quantum $V$-algebras, exploring their similarities and differences. $V$-algebras are a generalization of Boolean algebras, and in this work, a comprehensive overview of their various types and constructions is given. We delve into the properties of $V$-algebras in the classical setting and then extend the analysis to the quantum realm. Finally, we show how the structure of $V$-algebras can be used in applications such as computer science, physics, and cryptography. 1 into PostgreSQL...\n",
      "Inserting test sample 59  Numerous work had been done to quantify the entanglement of a two-qubit quantum state, but it can be seen that previous works were based on joint measurements on two copies or more than two copies of a quantum state under consideration. In this work, we show that a single copy and two measurements are enough to estimate the entanglement quantifier like entanglement negativity and concurrence. To achieve our aim, we establish a relationship between the entanglement negativity and the minimum eigenvalue of structural physical approximation of partial transpose of an arbitrary two-qubit state. The derived relation make possible to estimate entanglement negativity experimentally by Hong-Ou-Mandel interferometry with only two detectors. Also, we derive the upper bound of the concurrence of an arbitrary two-qubit state and have shown that the upper bound can be realized in experiment. We will further show that the concurrence of (i) an arbitrary pure two-qubit states and (ii) a particular class of mixed states, namely, rank-2 quasi-distillable mixed states, can be exactly estimated with two measurements. 0 into PostgreSQL...\n",
      "Inserting test sample 60  In this paper, we propose a scheme to estimate the entanglement negativity of a two-qubit quantum system with the help of two measurements. The entanglement negativity is a measure of entanglement that quantifies the amount of entanglement between the two qubits. We show that two measurements, one on each qubit, are sufficient to extract the necessary information to estimate the entanglement negativity. Our scheme involves measuring the Pauli operators on each qubit separately, followed by a joint measurement of both qubits. We derive explicit formulas for the estimation of the entanglement negativity for all possible measurement outcomes. We also study the conditions under which our scheme is reliable and how it can be extended to larger systems. Our results provide a practical way of estimating entanglement negativity in experimental setups and have important implications for the study of entanglement in quantum systems. 1 into PostgreSQL...\n",
      "Inserting test sample 61  Mining health data can lead to faster medical decisions, improvement in the quality of treatment, disease prevention, reduced cost, and it drives innovative solutions within the healthcare sector. However, health data is highly sensitive and subject to regulations such as the General Data Protection Regulation (GDPR), which aims to ensure patient's privacy. Anonymization or removal of patient identifiable information, though the most conventional way, is the first important step to adhere to the regulations and incorporate privacy concerns. In this paper, we review the existing anonymization techniques and their applicability to various types (relational and graph-based) of health data. Besides, we provide an overview of possible attacks on anonymized data. We illustrate via a reconstruction attack that anonymization though necessary, is not sufficient to address patient privacy and discuss methods for protecting against such attacks. Finally, we discuss tools that can be used to achieve anonymization. 0 into PostgreSQL...\n",
      "Inserting test sample 62  Healthcare data holds sensitive information, which requires protection from unauthorized access. Anonymization is one approach that achieved widespread popularity in facilitating the sharing of medical data while upholding privacy. This paper presents a comprehensive analysis of the recent research studies on anonymization techniques for healthcare data. The abstract explores the various anonymization methods employed in medical research and identifies potential challenges that may arise in using these techniques. Furthermore, the paper discusses the effectiveness and limitations of different anonymization techniques in terms of preserving data privacy and maintaining data utility. This review offers valuable insights to healthcare practitioners and policymakers who are responsible for ensuring the secure sharing of sensitive health data. Overall, this paper highlights the significance of continuous research into developing new anonymization methods to improve data privacy in healthcare while maintaining data utility. 1 into PostgreSQL...\n",
      "Inserting test sample 63  We present the first comprehensive study on physical and chemical properties of quiescent starless cores L1495B and L1521B, which are known to be rich in carbon-chain molecules like the cyanopolyyne peak of TMC-1 and L1521E. We have detected radio spectral lines of various carbon-chain molecules such as CCS, C$_{3}$S, C$_{4}$H, HC$_{3}$N, and HC$_{5}$N. On the other hand, the NH$_{3}$ lines are weak and the N$_{2}$H$^{+}$ lines are not detected. According to our mapping observations of the HC$_{3}$N, CCS, and C$_{3}$S lines, the dense cores in L1495B and L1521B are compact with the radius of 0.063 and 0.044 pc, respectively, and have a simple elliptical structure. The distributions of CCS seem to be different from those of well-studied starless cores, L1498 and L1544, where the distribution of CCS shows a shell-like structure. Since the H$^{13}$CO$^{+}$, HN$^{13}$C, and C$^{34}$S lines are detected in L1495B and L1521B, the densities of these cores are high enough to excite the NH$_{3}$ and N$_{2}$H$^{+}$ lines. Therefore, the abundances of NH$_{3}$ and N$_{2}$H$^{+}$ relative to carbon-chain molecules are apparently deficient, as observed in L1521E. We found that longer carbon-chain molecules such as HC$_{5}$N and C$_{4}$H are more abundant in TMC-1 than L1495B and L1521B, while those of sulfur-bearing molecules such as C$^{34}$S, CCS, and C$_{3}$S are comparable.\n",
      "\n",
      "Both distributions and abundances of the observed molecules of L1495B and L1521B are quite similar to those of L1521E, strongly suggesting that L1495B and L1521B is in a very early stage of physical and chemical evolution. 0 into PostgreSQL...\n",
      "Inserting test sample 64  The molecular line observations of carbon-chain-producing regions L1495B and L1521B are of great importance to the field of astrochemistry. These regions are known to be cold and dense, with ample opportunities for complex chemical reactions. Observations in the millimeter wavelength range have allowed for the detection of various molecular lines, including those of carbon-chains such as C3H2, C4H, and C6H.\n",
      "\n",
      "In this study, we present new molecular line observations of L1495B and L1521B, obtained using the Atacama Large Millimeter/submillimeter Array (ALMA). Our observations reveal the presence of additional carbon-chain emitting species, including C2H and C5H, which have been detected in the outer regions of the two clouds. The observed lines also show signs of self-absorption and/or infall, providing insight into the kinematics and chemical processes occurring within these regions.\n",
      "\n",
      "We use a non-local thermodynamic equilibrium (non-LTE) radiative transfer code to model the carbon-chain emission in L1495B and L1521B. Our analysis suggests that the observed carbon-chain emission arises from the outer envelopes of the clouds, where the gas density is sufficiently high to initiate complex chemical reactions. The inferred abundances of carbon-chain molecules are consistent with previous studies of similar sources, and provide valuable information for testing astrochemical models.\n",
      "\n",
      "Our findings shed new light on the formation mechanisms of carbon-chains in cold and dense molecular clouds, highlighting the role of ion-neutral reactions and surface chemistry. The molecular line datasets presented in this work will serve as a valuable resource for future studies of carbon-chain-bearing regions in the Milky Way and other nearby galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 65  In the aeronautics industry, wireless avionics intra-communications have a tremendous potential to improve efficiency and flexibility while reducing the weight, fuel consumption, and maintenance costs over traditional wired avionics systems. This survey starts with an overview of the major benefits and opportunities in the deployment of wireless technologies for critical applications of an aircraft. The current state-of-art is presented in terms of system classifications based on data rate demands and transceiver installation locations. We then discuss major technical challenges in the design and realization of the envisioned aircraft applications. Although wireless avionics intra-communication has aspects and requirements similar to mission-critical applications of industrial automation, it also has specific issues such as complex structures, operations, and safety of the aircraft that make this area of research self-standing and challenging. To support the critical operations of an aircraft, existing wireless standards for mission-critical industrial applications are briefly discussed to investigate the applicability of the current solutions. Specifically, IEEE 802.15.4-based protocols and Bluetooth are discussed for low data rate applications, whereas IEEE 802.11- based standards are considered for high data rate applications. Eventually, we propose fundamental schemes in terms of network architecture, protocol, and resource management to support the critical avionics applications and discuss the research directions in this emerging area. 0 into PostgreSQL...\n",
      "Inserting test sample 66  Wireless Avionics Intra-Communications (WAIC) plays a critical role in enabling communication among avionics subsystems within an aircraft without relying on traditional wired connections. This paper presents a comprehensive survey of the benefits, challenges, and solutions associated with WAIC. \n",
      "\n",
      "The benefits of WAIC are significant and include reduced weight, improved reliability, increased flexibility, and lower maintenance costs. However, the adoption of wireless communication within aircraft also poses unique challenges, such as the need to ensure coexistence with existing wireless systems and to guarantee the required safety and security of wireless communication. Moreover, the harsh aviation environment presents additional challenges, such as electromagnetic interference (EMI) and harsh temperature and vibration conditions. \n",
      "\n",
      "This paper discusses the relevant standards and guidelines for ensuring acceptable levels of safety and performance. Additionally, it provides an overview of the current and future WAIC technologies and architectures, which can mitigate the identified challenges. The discussion includes an analysis of the benefits, risks, and tradeoffs of different WAIC technologies and an overview of the ongoing research efforts in this area. \n",
      "\n",
      "In summary, this survey paper provides a comprehensive assessment of WAIC, highlighting its benefits, challenges, and solutions. The information presented in this paper will serve as a valuable resource for researchers, engineers, and regulators working on WAIC development, implementation and certification. 1 into PostgreSQL...\n",
      "Inserting test sample 67  We report a versatile and practical approach for generating high-quality polarization entanglement in a fully guided-wave fashion. Our setup relies on a high-brilliance type-0 waveguide generator producing paired photon at a telecom wavelength associated with an advanced energy-time to polarisation transcriber.\n",
      "\n",
      "The latter is capable of creating any pure polarization entangled state, and allows manipulating single photon bandwidths that can be chosen at will over five orders of magnitude, ranging from tens of MHz to several THz. We achieve excellent entanglement fidelities for particular spectral bandwidths, i.e. 25 MHz, 540 MHz and 100 GHz, proving the relevance of our approach. Our scheme stands as an ideal candidate for a wide range of network applications, ranging from dense division multiplexing quantum key distribution to heralded optical quantum memories and repeaters. 0 into PostgreSQL...\n",
      "Inserting test sample 68  We present a versatile source of polarisation entangled photons, which has great potential for quantum network applications. Our source uses optical fiber as a common path interferometer to create entangled photon pairs through spontaneous parametric downconversion. By changing the length of a fiber, we can tune the entanglement fidelity and adjust the photon pair spectrum to meet the wavelength requirements of different quantum network applications. Furthermore, our source has a low requirement for environmental stability and can be easily operated in the lab. We demonstrate the versatility of our source by using it to create entangled photons with high fidelity over a range of wavelengths suitable for quantum communication, entanglement distribution, and quantum computing. Our source shows great potential for scaling up quantum networks and implementing various quantum protocols with high efficiency. 1 into PostgreSQL...\n",
      "Inserting test sample 69  We show by microscopic calculation that thermodynamics of the multicomponent Sutherland model is equivalent to that of a free particle system with fractional exclusion statistics at all temperatures. The parameters for exclusion statistics are given by the strength of the repulsive interaction, and have both intra- and inter-species components. We also show that low temperature properties of the system are described in terms of free fractional particles without the statistical parameters for different species. The effective exclusion statistics for intra-species at low temperatures depend on polarization of the system. 0 into PostgreSQL...\n",
      "Inserting test sample 70  We investigate the fractional exclusion statistics (FES) in the multicomponent Sutherland model. We consider a gas of hard rods with two internal degrees of freedom, position and orientation, interacting through pairwise repulsive forces. By using a mapping between the hard rod model and a multicomponent gas of particles interacting via pair potentials, we derive a general expression for the FES parameter. Our results show interesting features arising from the interplay between hard-core interactions and internal degrees of freedom, and may have important implications for correlated systems in condensed matter physics. 1 into PostgreSQL...\n",
      "Inserting test sample 71  We extend upper bounds on the quantum independence number and the quantum Shannon capacity of graphs to their counterparts in the commuting operator model. We introduce a von Neumann algebraic generalization of the fractional Haemers bound (over $\\mathbb{C}$) and prove that the generalization upper bounds the commuting quantum independence number. We call our bound the tracial Haemers bound, and we prove that it is multiplicative with respect to the strong product. In particular, this makes it an upper bound on the Shannon capacity. The tracial Haemers bound is incomparable with the Lov\\'asz theta function, another well-known upper bound on the Shannon capacity. We show that separating the tracial and fractional Haemers bounds would refute Connes' embedding conjecture.\n",
      "\n",
      "Along the way, we prove that the tracial rank and tracial Haemers bound are elements of the (commuting quantum) asymptotic spectrum of graphs (Zuiddam, Combinatorica, 2019). We also show that the inertia bound (an upper bound on the quantum independence number) upper bounds the commuting quantum independence number. 0 into PostgreSQL...\n",
      "Inserting test sample 72  In this paper, we investigate a tracial version of the Haemers bound, a well-known result in algebraic graph theory. Specifically, we consider the case where the adjacency matrix of a graph is endowed with a trace function. Under certain conditions, we establish an upper bound on the rank of the adjacency matrix in terms of its maximum eigenvalue and the trace of its power. We use this result to obtain a lower bound on the independence number of graphs that satisfy certain properties. Furthermore, we explore the connections between the tracial Haemers bound and other well-known bounds in graph theory. Our findings reveal that the tracial Haemers bound can provide non-trivial improvements over the traditional Haemers bound in some cases. To demonstrate the strength of our approach, we provide several examples and applications of our results in quantum information theory and coding theory. 1 into PostgreSQL...\n",
      "Inserting test sample 73  The existence of strange quark stars has been proposed many years ago. More recently, the possible co-existence of a first family composed of \"normal\" neutron stars with a second family of strange quark stars has been proposed as a solution of problems related to the maximum mass and to the minimal radius of these compact stellar objects. In this paper we study the mass distribution of compact objects formed in binary systems and the relative fractions of quark and neutron stars in different subpopulations. We incorporate the strange quark star formation model provided by the two-families scenario and we perform a large-scale population synthesis study in order to obtain the population characteristics. In our model, below a critical gravitational mass $M_\\mathrm{max}^H- \\Delta M$ only normal (hadron) neutron stars exist. Then in the mass range $(M_\\mathrm{max}^H- \\Delta M) \\leqslant M \\leqslant M_\\mathrm{max}^H$ strange quark stars and neutron stars coexist. Finally, above $M_\\mathrm{max}^H$ all compact objects are strange quark stars. We argue that $M_\\mathrm{max}^H$ is in the range $\\sim 1.5-1.6 M_\\odot$. According to our results, the main channel for strange quark star formation in binary systems is accretion from a secondary companion on a neutron star.This opens the possibility of having explosive GRB-like phenomena not related to supernovae and not due to the merger of two neutron stars. The enhancement in the number of compact objects in the co-existence mass range $(M_\\mathrm{max}^H- \\Delta M) \\leqslant M \\leqslant M_\\mathrm{max}^H$ is not very pronounced. The number of double strange quark star's systems is rather small with only a tiny fraction which merge within a Hubble time. This drastically limits the flux of strangelets produced by the merger, which turns out to be compatible with all limits stemming from Earth and lunar experiments. 0 into PostgreSQL...\n",
      "Inserting test sample 74  Strange quark stars are hypothesized to be one of the possible outcomes of the evolution of compact stars. They are composed of up, down and strange quarks, unlike the usual neutron stars which consist of only neutrons. In this paper, we investigate binary systems where one or both of the compact objects are strange quark stars. \n",
      "\n",
      "We begin by estimating the formation rates of such binaries. We find that, despite the rarity of strange quark stars, mergers of binaries containing them are not uncommon. This is due to their compactness, a property that makes them more likely to come into close proximity with other objects. \n",
      "\n",
      "Next, we study the mergers of binary systems with at least one strange quark star involved. We perform simulations of these mergers and investigate their properties, such as the amount of mass and energy released during the process. We find that the mergers of strange quark stars can lead to a variety of explosive phenomena, depending on the specific characteristics of the stars and the binary system. \n",
      "\n",
      "Finally, we discuss the implications of our findings. We consider the possible observational signatures of these mergers, and whether they could be detected with current or future observational facilities. We also explore how the formation and mergers of strange quark star binaries could contribute to the production of heavy elements in the universe. \n",
      "\n",
      "In summary, this paper investigates strange quark star binaries, focusing on their formation rates, mergers and explosive phenomena. Our findings suggest that these systems could play an important role in the dynamics of compact objects, and that their study could offer new insights into various astrophysical phenomena. 1 into PostgreSQL...\n",
      "Inserting test sample 75  Vehicles play a vital role in modern day transportation systems. Number plate provides a standard means of identification for any vehicle. To serve this purpose, automatic licence plate recognition system was developed. This consisted of four major steps: Pre-processing of the obtained image, extraction of licence plate region, segmentation and character recognition. In earlier research, direct application of Sobel edge detection algorithm or applying threshold were used as key steps to extract the licence plate region, which does not produce effective results when the captured image is subjected to the high intensity of light. The use of morphological operations causes deformity in the characters during segmentation. We propose a novel algorithm to tackle the mentioned issues through a unique edge detection algorithm. It is also a tedious task to create and update the database of required vehicles frequently.\n",
      "\n",
      "This problem is solved by the use of Internet of things(IOT) where an online database can be created and updated from any module instantly. Also, through IoT, we connect all the cameras in a geographical area to one server to create a universal eye which drastically increases the probability of tracing a vehicle over having manual database attached to each camera for identification purpose. 0 into PostgreSQL...\n",
      "Inserting test sample 76  The task of detecting license plates from digital images is a fundamental problem in computer vision, and it has attracted significant interest in recent years due to its vast range of applications such as traffic monitoring or automatic toll systems. In this paper, we propose a novel approach for efficient license plate detection by combining a unique edge detection algorithm with smarter interpretation through IoT. Our method utilizes novel techniques for extracting license plate features by considering their unique characteristics and also integrates a smart IoT system that further enhances the detection algorithm's performance. We demonstrate the effectiveness of our proposed method by testing it against state-of-the-art detection methods, and our experimental evaluations provide significant evidence in favor of our approach. Besides, we show that our proposed method operates in real-time with excellent accuracy, even in challenging scenarios like low-light conditions or partial occlusion of the license plate. Finally, we analyze the computational complexity of our method, which reveals that our approach is suitable for both embedded and cloud-based systems. 1 into PostgreSQL...\n",
      "Inserting test sample 77  We have found a system listed in the Kepler Binary Catalog (3.273 day period; Prsa et al. 2010) that we have determined is comprised of a low-mass, thermally-bloated, hot white dwarf orbiting an A star of about 2.3 solar masses. In this work we designate the object, KIC 10657664, simply as KHWD3. We use the transit depth of ~0.66%, the eclipse depth of ~1.9%, and regular smooth periodic variations at the orbital frequency and twice the orbital frequency to analyze the system parameters. The smooth periodic variations are identified with the classical ellipsoidal light variation and illumination effects, and the newly utilized Doppler boosting effect. Given the measured values of R/a and inclination angle of the binary, both the ELV and DB effects are mostly sensitive to the mass ratio, q = M_2/M_1, of the binary. The two effects yield values of q which are somewhat inconsistent - presumably due to unidentified systematic effects - but which nonetheless provide a quite useful set of possibilities for the mass of the white dwarf (either 0.18 +/- 0.03 M_Sun or 0.37 +/- 0.08 M_Sun). All of the other system parameters are determined fairly robustly. In particular, we show that the white dwarf has a radius of 0.15 +/- 0.01 R_Sun which is extremely bloated over the radius it would have as a fully degenerate object, and an effective temperature T_eff = 14,100 +/- 350 K.\n",
      "\n",
      "Binary evolution scenarios and models for this system are discussed. We suggest that the progenitor binary was comprised of a primary of mass ~2.2 M_Sun (the progenitor of the current hot white dwarf) and a secondary of mass ~1.4 M_Sun (the progenitor of the current A star in the system). We compare this new system with three other white dwarfs in binaries that likely were formed via stable Roche-lobe overflow (KOI-74, KOI-81, and Regulus). 0 into PostgreSQL...\n",
      "Inserting test sample 78  The Kepler telescope has once again allowed astronomers to further our understanding of the Universe. A recent study has shown that the third hot white dwarf companion has been detected by Kepler, confirming previous predictions.\n",
      "\n",
      "The hot white dwarf companion was detected through its influence on the main star. The companion's gravity causes small variations in the main star's path and brightness. This method, known as transit, is for the first time detecting this type of object.\n",
      "\n",
      "The system in question is a binary star system consisting of one white dwarf and one main sequence star. The newly detected companion is another white dwarf that has a mass about 60% that of the sun. Such white dwarf companions are rare, but their presence can provide valuable information on the evolution of binary star systems.\n",
      "\n",
      "The fact that the companion is also a white dwarf makes it intriguing. These objects are the collapsed remnants of once normal main sequence stars and typically have a very high temperature and small radius. The closeness of both white dwarfs opens the possibility of future observations aimed at detecting the gravitational radiation, which would help estimate their distance, evolution, and ultimate fate.\n",
      "\n",
      "This discovery has important implications for our understanding of the Universe. Future observations of such systems and their orbital evolution will provide valuable data on the astrophysics of interacting binary systems and the properties of white dwarfs. They may also provide a new path for identifying a \"missing\" link in the evolution of main sequence stars.\n",
      "\n",
      "In conclusion, the detection of the third hot white dwarf companion by Kepler has opened a new chapter in our exploration of the Universe. This rare type of companion is providing invaluable insight into the evolution of binary star systems and white dwarfs. We eagerly await further observations and results from this system and others like it. 1 into PostgreSQL...\n",
      "Inserting test sample 79  Cloud availability is a major performance parameter for cloud platforms, but there are very few measurements on commercial platforms, and most of them rely on outage reports as appeared on specialized sites, providers' dashboards, or the general press. A paper recently presented at the PAM 2014 conference by Hu et alii reports the results of a measurement campaign. In this note, the results of that paper are summarized, highlighting sources of inaccuracy and some possible improvements. In particular, the use of a low probing frequency could lead to non detection of short outages, as well as to an inaccurate estimation of the outage duration statistics. Overcoming this lack of accuracy is relevant to properly assess SLA violations and establish the basis for insurance claims. 0 into PostgreSQL...\n",
      "Inserting test sample 80  Cloud availability is becoming increasingly important in today's digital landscape, as more and more businesses rely on cloud services to store their data. However, ensuring that these services are reliable and available around the clock is not always an easy task. In this paper, we present a study highlighting the need for end-to-end evaluation of cloud availability. We argue that current assessment methods fall short in providing a comprehensive evaluation of cloud services and propose a new approach that combines different evaluation techniques to provide a more thorough analysis. We demonstrate the effectiveness of our approach through case studies and empirical evidence. Our findings emphasize the need for a holistic approach to evaluating cloud availability, which considers the entire life cycle of service delivery, from initial design to ongoing maintenance. 1 into PostgreSQL...\n",
      "Inserting test sample 81  We report the synthesis of polycrystalline (poly)-SiGe alloy thin films through solid state reaction of Si/Ge multilayer thin films on Si and glass substrates at low temperature of 500 {\\deg}C. The pristine thin film was deposited using electron beam evaporation with optimized in-situ substrate heating. Our results show the co-existence of amorphous Si (a-Si) phase along with the poly-SiGe phase in the pristine thin film. The a-Si phase was found to subsume into the SiGe phase upon post deposition annealing in the temperature range from 600 to 800 {\\deg}C. Additionally, dual energy band gaps could be observed in the optical properties of the annealed poly-SiGe thin films. The stoichiometric evolution of the pristine thin film and its subsequent effect on the band gap upon annealing are discussed on the basis of diffusion characteristics of Si in poly-SiGe. 0 into PostgreSQL...\n",
      "Inserting test sample 82  This study investigates the in-situ formation of silicon-germanium (SiGe) alloys using electron beam evaporation and the impact of post-deposition annealing on their energy band gaps. The aim was to explore how annealing temperatures ranging from 400 to 800Â°C would affect the electronic properties of the resulting alloys. Scanning electron microscopy was used to examine the microstructure of the samples while Raman spectroscopy was used to determine the composition of the films. Results showed that higher annealing temperatures caused an increase in the Ge content of the alloys, resulting in a reduction of the energy band gap. These findings suggest that post-deposition annealing can be used to control the electronic properties of SiGe alloys, which can be useful in the development of devices that require tailored band gaps. 1 into PostgreSQL...\n",
      "Inserting test sample 83  Event cameras, such as dynamic vision sensors (DVS), and dynamic and active-pixel vision sensors (DAVIS) can supplement other autonomous driving sensors by providing a concurrent stream of standard active pixel sensor (APS) images and DVS temporal contrast events. The APS stream is a sequence of standard grayscale global-shutter image sensor frames. The DVS events represent brightness changes occurring at a particular moment, with a jitter of about a millisecond under most lighting conditions. They have a dynamic range of >120 dB and effective frame rates >1 kHz at data rates comparable to 30 fps (frames/second) image sensors. To overcome some of the limitations of current image acquisition technology, we investigate in this work the use of the combined DVS and APS streams in end-to-end driving applications. The dataset DDD17 accompanying this paper is the first open dataset of annotated DAVIS driving recordings. DDD17 has over 12 h of a 346x260 pixel DAVIS sensor recording highway and city driving in daytime, evening, night, dry and wet weather conditions, along with vehicle speed, GPS position, driver steering, throttle, and brake captured from the car's on-board diagnostics interface. As an example application, we performed a preliminary end-to-end learning study of using a convolutional neural network that is trained to predict the instantaneous steering angle from DVS and APS visual data. 0 into PostgreSQL...\n",
      "Inserting test sample 84  The DDD17 dataset is a comprehensive collection of end-to-end driving sequences recorded in conjunction with the DAVIS multi-camera tracking and event dataset. The dataset features a diverse set of driving scenarios, including urban, suburban, and rural environments, captured from different viewpoints and camera setups. The dataset is designed to support the development and evaluation of end-to-end driving models, particularly those based on deep learning approaches. We provide a detailed description of the data collection process, camera calibration, and annotation protocol. In addition, we present an analysis of the dataset, including statistical characteristics of the video sequences, object tracking performance, and driving event distribution. We also demonstrate the usefulness of the dataset by evaluating state-of-the-art end-to-end driving models on the dataset. Our results show that the DDD17 dataset provides a challenging and realistic benchmark for evaluating end-to-end driving models, highlighting the need for further research in this area. Overall, the DDD17 dataset is a valuable resource for researchers working on the development of autonomous driving systems, providing a realistic and diverse dataset for training and evaluation purposes. 1 into PostgreSQL...\n",
      "Inserting test sample 85  We derive an exact expression for the differential conductance for a quantum dot in an arbitrary magnetic field for small bias voltage. The derivation is based on the symmetric Anderson model using renormalized perturbation theory and is valid for all values of the on-site interaction $U$ including the Kondo regime. We calculate the critical magnetic field for the splitting of the Kondo resonance to be seen in the differential conductivity as function of bias voltage. Our calculations for small field show that the peak position of the component resonances in the differential conductance are reduced substantially from estimates using the equilibrium Green's function. We conclude that it is important to take the voltage dependence of the local retarded Green's function into account in interpreting experimental results 0 into PostgreSQL...\n",
      "Inserting test sample 86  We investigate the non-equilibrium differential conductance through a quantum dot subject to a magnetic field. Our theoretical analysis is based on the non-equilibrium Green's function technique, which allows us to accurately determine the transport properties of the system. We find that the differential conductance exhibits a rich variety of features as a function of both the bias voltage and the strength of the magnetic field. In particular, we observe two distinct regimes of behavior: one dominated by the Kondo effect and one dominated by the Zeeman effect. Our results provide a comprehensive understanding of the physics of quantum dots in magnetic fields and have important implications for the design of nanoscale electronic devices with potential applications in quantum computing and sensing. 1 into PostgreSQL...\n",
      "Inserting test sample 87  In recent years, the disk populations in a number of young star-forming regions have been surveyed with ALMA. Understanding the disk properties and their correlation with those of the central star is critical to understand planet formation. In particular, a decrease of the average measured disk dust mass with the age of the region has been observed. We conducted high-sensitivity continuum ALMA observations of 43 Class II young stellar objects in CrA at 1.3 mm (230 GHz). The typical spatial resolution is 0.3\". The continuum fluxes are used to estimate the dust masses of the disks, and a survival analysis is performed to estimate the average dust mass. We also obtained new VLT/X-Shooter spectra for 12 of the objects in our sample. 24 disks are detected, and stringent limits have been put on the average dust mass of the non-detections. Accounting for the upper limits, the average disk mass in CrA is $6\\pm3\\,\\rm M_\\oplus$, significantly lower than that of disks in other young (1-3 Myr) star forming regions (e.g. Lupus) and appears consistent with the 5-10 Myr old Upper Sco. The position of the stars in our sample on the HR diagram, however, seems to confirm that that CrA has age similar to Lupus.\n",
      "\n",
      "Neither external photoevaporation nor a lower than usual stellar mass distribution can explain the low disk masses. On the other hand, a low-mass disk population could be explained if the disks are small, which could happen if the parent cloud has a low temperature or intrinsic angular momentum, or if the the angular momentum of the cloud is removed by some physical mechanism such as magnetic braking. In order to fully explain and understand the dust mass distribution of protoplanetary disks and their evolution, it may also be necessary to take into consideration the initial conditions of star and disk formation process, which may vary from region to region, and affect planet formation. 0 into PostgreSQL...\n",
      "Inserting test sample 88  We present the results of a survey of Class II protoplanetary disks in the Corona Australis region using the Atacama Large Millimeter/submillimeter Array (ALMA). Our sample consists of 32 sources, and the spatial resolution of the observations ranges from 0.2 to 0.4 arcseconds. Our analysis reveals that the majority of the disks have low masses, with a median value of 0.02 solar masses, and sizes smaller than 200 AU. The disk masses are consistent with theoretical predictions based on the low stellar density of Corona Australis. Additionally, we find that the disks have low dust-to-gas ratios, indicating high levels of grain growth. This is consistent with Corona Australis being a young region with ages around 1 million years. \n",
      "\n",
      "We also investigate the chemical composition of the disks and find that they are enriched in complex organic molecules, such as methanol and methyl formate. These molecules are thought to be the building blocks of prebiotic molecules and could play a crucial role in the origin of life. Finally, we study the kinematics of the disks and find evidence of rotation in most of them. By comparing the disk properties with those of other star-forming regions, we conclude that Corona Australis is a unique region with low disk masses and low dust-to-gas ratios.\n",
      "\n",
      "Our results have important implications for understanding the early stages of planet formation. The low disk masses suggest that the formation of giant planets may be less efficient in Corona Australis compared to other star-forming regions. Meanwhile, the high levels of grain growth could lead to the formation of rocky planets in the inner regions of the disks. Our study provides valuable insights into the physical and chemical properties of protoplanetary disks in a low-mass star-forming region, and highlights the importance of ALMA surveys in advancing our understanding of planet formation. 1 into PostgreSQL...\n",
      "Inserting test sample 89  We explore the complex associated to a link in the geometric formalism of Khovanov's (n=2) link homology theory, determine its exact underlying algebraic structure and find its precise universality properties for link homology functors. We present new methods of extracting all known link homology theories directly from this universal complex, and determine its relative strength as a link invariant by specifying the amount of information held within the complex.\n",
      "\n",
      "We achieve these goals by finding a complex isomorphism which reduces the complex into one in a simpler category. We introduce few tools and methods, including surface classification modulo the 4TU/S/T relations and genus generating operators, and use them to explore the relation between the geometric complex and its underlying algebraic structure. We identify the universal topological quantum field theory (TQFT) that can be used to create link homology and find that it is ``smaller'' than what was previously reported by Khovanov. We find new homology theories that hold a controlled amount of information relative to the known ones.\n",
      "\n",
      "The universal complex is computable efficiently using our reduction theorem.\n",
      "\n",
      "This allows us to explore the phenomenological aspects of link homology theory through the eyes of the universal complex in order to explain and unify various phenomena (such as torsion and thickness). The universal theory also enables us to state results regarding specific link homology theories derived from it. The methods developed in this thesis can be combined with other known techniques (such as link homology spectral sequences) or used in the various extensions of Khovanov link homology (such as sl_3 link homology). 0 into PostgreSQL...\n",
      "Inserting test sample 90  The universal sl_2 link homology theory is a powerful mathematical framework that provides a comprehensive approach to understanding knots and links. Originally proposed by M. Khovanov, this theory has since grown into a flourishing field of study, incorporating insights from representation theory, algebraic topology, and gauge theory. At its core, the theory seeks to associate a graded chain complex with a given knot or link diagram, capturing essential geometric and algebraic information about the knot or link.\n",
      "\n",
      "In this paper, we present a detailed exploration of the universal sl_2 link homology theory, synthesizing recent developments and highlighting key open problems. We begin by outlining the foundational ideas behind the theory, including the notions of Khovanov homology and Khovanov-Rozansky homology. We then delve into the connections between these theories and semiclassical limits of quantum integrable systems, as well as their relationships to categorification and the theory of modular forms.\n",
      "\n",
      "One major focus of our work is the construction of spectral sequences associated with the universal sl_2 link homology theory, which allow for powerful computations of homological invariants. We also investigate the behavior of these invariants under various operations on knots and links, including mirror symmetry, knot surgery, and mutation. Finally, we discuss the potential applications of this theory to other areas of mathematics and physics, including quantum computation and mirror symmetry for Calabi-Yau threefolds.\n",
      "\n",
      "Overall, this paper offers a comprehensive and up-to-date perspective on the universal sl_2 link homology theory, illuminating new avenues of research and highlighting the far-reaching implications of this framework in contemporary mathematics and physics. 1 into PostgreSQL...\n",
      "Inserting test sample 91  We have estimated elemental abundances of the planetary nebula (PN) Hen2-436 in the Sagittarius (Sgr) spheroidal dwarf galaxy using ESO/VLT FORS2, Magellan/MMIRS, and Spitzer/IRS spectra. We have detected candidates of [F II] 4790A, [Kr III] 6826A, and [P II] 7875A lines and successfully estimated the abundances of these elements ([F/H]=+1.23, [Kr/H]=+0.26, [P/H]=+0.26) for the first time. We present a relation between C, F, P, and Kr abundances among PNe and C-rich stars. The detections of F and Kr support the idea that F and Kr together with C are synthesized in the same layer and brought to the surface by the third dredge-up. We have estimated the N^2+ and O^2+ abundances using optical recombination lines (ORLs) and collisionally excited lines (CELs). The discrepancy between the abundance derived from the O ORL and that derived from the O CEL is >1 dex. To investigate the status of the central star of the PN, nebula condition, and dust properties, we construct a theoretical SED model with CLOUDY. By comparing the derived luminosity and temperature of the central star with theoretical evolutionary tracks, we conclude that the initial mass of the progenitor is likely to be ~1.5-2.0 Msun and the age is ~3000 yr after the AGB phase. The observed elemental abundances can be explained by a theoretical nucleosynthesis model with a star of initial mass 2.25 Msun, Z=0.008 and LMC compositions. We have estimated the dust mass to be 2.9x10^-4 Msun (amorphous carbon only) or 4.0x10^-4 Msun (amorphous carbon and PAH). Based on the assumption that most of the observed dust is formed during the last two thermal pulses and the dust-to-gas mass ratio is 5.58x10^-3, the dust mass-loss rate and the total mass-loss rate are <3.1x10^-8 Msun / yr and <5.5x10^-6 Msun / yr, respectively. Our estimated dust mass-loss rate is comparable to a Sgr dwarf galaxy AGB star with similar metallicity and luminosity. 0 into PostgreSQL...\n",
      "Inserting test sample 92  The Sagittarius dwarf galaxy is a prime target for observations of its constituent stars and gas. In this paper, we present a detailed study of the planetary nebula Hen2-436 in this galaxy. We use observations from the Hubble Space Telescope's Wide Field Camera 3 to measure the properties of the dust and gas in the nebula. Specifically, we analyze the emission from the [O III] and HÎ± lines, as well as the continuum in the F275W and F336W filters. We also measure the chemical abundances of the nebula using these same diagnostic lines. \n",
      "\n",
      "Our analysis of the dust content in Hen2-436 reveals a low amount of extinction, suggesting that the nebula is relatively unobscured by dust. The extinction is consistent with earlier measurements of stars in the Sagittarius dwarf galaxy. We also find that the dust grains in Hen2-436 have a relatively small average size compared to those found in the Milky Way. Our measurements of the gas properties reveal that the nebula has a low electron density and a relatively low temperature. \n",
      "\n",
      "Finally, we present our measurements of the chemical abundances in Hen2-436. We find that the nebula has a solar value for the nitrogen abundance, but that the oxygen abundance is slightly lower than expected for a galaxy of this metallicity. The helium abundance is also slightly lower than expected, but the neon abundance is consistent with the solar value. These measurements suggest that the chemical evolution of the Sagittarius dwarf galaxy has been affected by its interactions with the Milky Way and other nearby galaxies. \n",
      "\n",
      "In conclusion, our study of the planetary nebula Hen2-436 in the Sagittarius dwarf galaxy has provided new insights into the dust and gas content, as well as the chemical abundances, of this galaxy. These results have important implications for our understanding of the evolution of dwarf galaxies and their interactions with larger neighbors in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 93  A word on $q$ symbols is a sequence of letters from a fixed alphabet of size $q$. For an integer $k\\ge 1$, we say that a word $w$ is $k$-universal if, given an arbitrary word of length $k$, one can obtain it by removing entries from $w$. It is easily seen that the minimum length of a $k$-universal word on $q$ symbols is exactly $qk$. We prove that almost every word of size $(1+o(1))c_qk$ is $k$-universal with high probability, where $c_q$ is an explicit constant whose value is roughly $q\\log q$. Moreover, we show that the $k$-universality property for uniformly chosen words exhibits a sharp threshold. Finally, by extending techniques of Alon [Geometric and Functional Analysis 27 (2017), no.\n",
      "\n",
      "1, 1--32], we give asymptotically tight bounds for every higher dimensional analogue of this problem. 0 into PostgreSQL...\n",
      "Inserting test sample 94  Universal arrays are a concept in computer science that has received increasing attention over recent years due to their potential to improve the efficiency and scalability of different algorithms in various domains. A universal array represents a variety of data structures within a fixed segment of memory, enabling operations on various types of data without the need for memory copying, type casting, or dynamic allocation. In this paper, we introduce a new framework for designing and implementing efficient universal arrays. We explore the theoretical foundations of the concept, provide a high-level overview of the framework's design, and demonstrate its practical use in several applications, including sorting, searching, and graph algorithms. Benchmark results show that our framework provides superior performance compared to existing solutions while offering flexible and elegant interfaces to program with. 1 into PostgreSQL...\n",
      "Inserting test sample 95  We report the identification of radio (1.4 and 3 GHz) and mid-infrared, far-infrared, and sub-mm (24-850$\\mu$m) emission at the position of one of 41 UV-bright ($\\mathrm{M_{UV}^{}}\\lesssim-21.25$) $z\\simeq6.6-6.9$ Lyman-break galaxy candidates in the 1.5 deg$^2$ COSMOS field. This source, COS-87259, exhibits a sharp flux discontinuity (factor $>$3) between two narrow/intermediate bands at 9450 $\\mathring{A}$ and 9700 $\\mathring{A}$ and is undetected in all nine bands blueward of 9600 $\\mathring{A}$, as expected from a Lyman-alpha break at $z\\simeq6.8$. The full multi-wavelength (X-ray through radio) data of COS-87529 can be self-consistently explained by a very massive (M$_{\\ast}=10^{10.8}$ M$_{\\odot}$) and extremely red (rest-UV slope $\\beta=-0.59$) $z\\simeq6.8$ galaxy with hyperluminous infrared emission (L$_{\\mathrm{IR}}=10^{13.6}$ L$_{\\odot}$) powered by both an intense burst of highly-obscured star formation (SFR$\\approx$1800 M$_{\\odot}$ yr$^{-1}$) and an obscured ($\\tau_{\\mathrm{9.7\\mu m}}=7.7\\pm2.5$) radio-loud (L$_{\\mathrm{1.4\\ GHz}}\\sim10^{25.5}$ W Hz$^{-1}$) AGN. The radio emission is compact (1.04$\\pm$0.12 arcsec) and exhibits an ultra-steep spectrum between 1.4-3 GHz ($\\alpha=-2.06^{+0.27}_{-0.25}$) with evidence of spectral flattening at lower frequencies, consistent with known $z>4$ radio galaxies. We also demonstrate that COS-87259 may reside in a significant (11$\\times$) galaxy overdensity at $z\\simeq6.6-6.9$, as common for systems hosting radio-loud AGN. Nonetheless, a spectroscopic redshift will ultimately be required to establish the true nature of COS-87259 as we cannot yet completely rule out low-redshift solutions. If confirmed to lie at $z\\simeq6.8$, the properties of COS-87259 would be consistent with a picture wherein AGN and highly-obscured star formation activity are fairly common among very massive (M$_{\\ast}>10^{10}$ M$_{\\odot}$) reionization-era galaxies. 0 into PostgreSQL...\n",
      "Inserting test sample 96  The study presented in this paper focuses on a massive star-forming galaxy candidate located at a redshift of z$\\simeq$6.8. Our analysis combines radio and far-infrared (FIR) data to investigate the characteristics of the galaxy's emission and explore its potential connection with an active galactic nucleus (AGN) in the reionization era. After careful analysis of the radio and FIR data, we conclude that the galaxy exhibits strong radio emission, indicating the presence of an AGN. Our findings suggest that this galaxy is a radio-loud AGN, which is a rare phenomenon in the early universe. \n",
      "\n",
      "The study employs a combination of radio and FIR data to probe the properties of the galaxy's emission, which provides a comprehensive view into its physical processes. The process of star-formation and AGN activity are vital components in the growth of galaxies, and our findings suggest that these processes are intricately linked. The radio-loud nature of the AGN suggests that it is a powerful source of ionizing photons that may have contributed significantly to the reionization process. \n",
      "\n",
      "Our study also provides insights into the physical properties of the galaxy. The high radio-to-FIR ratio observed in the galaxy is indicative of a compact source with a high magnetic field, which is likely associated with an AGN. Furthermore, the FIR luminosity suggests that the galaxy is undergoing intense star formation, consistent with its massive nature. \n",
      "\n",
      "In conclusion, we have presented a detailed study of a massive star-forming galaxy candidate at z$\\simeq$6.8, combining radio and FIR data to explore its emission properties. Our findings support the presence of a radio-loud AGN in the galaxy, which may have played a role in the reionization of the early universe. Additionally, our results shed light on the physical properties of the galaxy, highlighting its massive nature and high rate of star formation. 1 into PostgreSQL...\n",
      "Inserting test sample 97  Cosmological models that include suppression of the power spectrum of density fluctuations on small scales exhibit an exponential reduction of high-redshift, non-linear structures, including a reduction in the rate of gamma ray bursts (GRBs). Here we quantify the constraints that the detection of distant GRBs would place on structure formation models with reduced small-scale power. We compute the number of GRBs that could be detectable by the Swift satellite at high redshifts (z > 6), assuming that the GRBs trace the cosmic star formation history, which itself traces the formation of non-linear structures. We calibrate simple models of the intrinsic luminosity function of the bursts to the number and flux distribution of GRBs observed by the Burst And Transient Source Experiment (BATSE). We find that a discovery of high-z GRBs would imply strong constraints on models with reduced small-scale power. For example, a single GRB at z > 10, or 10 GRBs at z > 5, discovered by Swift during its scheduled two-year mission, would rule out an exponential suppression of the power spectrum on scales below R_c=0.09 Mpc (exemplified by warm dark matter models with a particle mass of m_x=2 keV). Models with a less sharp suppression of small-scale power, such as those with a red tilt or a running scalar index, n_s, are more difficult to constrain, because they are more degenerate with an increase in the power spectrum normalization, sigma_8, and with models in which star-formation is allowed in low-mass minihalos. We find that a tilt of \\delta n_s ~ 0.1 is difficult to detect; however, an observed rate of 1 GRB/yr at z > 12 would yield an upper limit on the running of the spectral index, alpha = d(n_s)/d(ln k) > -0.05. 0 into PostgreSQL...\n",
      "Inserting test sample 98  Recent observations of gamma-ray bursts (GRBs) have opened up new opportunities to investigate the small-scale power spectrum of density fluctuations. In this paper, we present constraints on the power spectrum using a sample of high-redshift GRBs. By analyzing the temporal and spectral properties of the GRBs, we extract information about the intervening material between the burst and the observer. We quantify the level of Lyman-alpha forest absorption, which is sensitive to the small-scale density fluctuations along the line of sight. Our analysis reveals a significant excess of power on scales smaller than 0.1 Mpc/h, with 2-sigma lower limit of 1.5 times the expected power from a linear extrapolation of the standard Lambda-Cold Dark Matter model. This excess is consistent with a damping of small-scale density fluctuations by free-streaming of neutrinos, with a combined mass of active and sterile neutrinos of about 0.2 eV. However, alternative explanations such as the contribution of axion-like particles or modified gravitational physics are not ruled out yet. We have also investigated the possible systematics effects that could mimic or alter the signal. We find no evidence of significant contamination from metal absorption or dust, but the intrinsic variability of GRBs remains an important source of uncertainty. Our constraints provide new insights on the nature of dark matter and neutrino physics, as well as the validity of the standard cosmological model on small scales. 1 into PostgreSQL...\n",
      "Inserting test sample 99  This paper is devoted to searching for Riemannian metrics on 2-surfaces whose geodesic flows admit a rational in momenta first integral with a linear numerator and denominator. The explicit examples of metrics and such integrals are constructed. Few superintegrable systems are found having both a polynomial and a rational integrals which are functionally independent of the Hamiltonian. 0 into PostgreSQL...\n",
      "Inserting test sample 100  We present new examples of rational integrals of 2-dimensional geodesic flows. By studying the analytic continuation of these integrals, we show that their singularities are governed by the arithmetic of certain hyperelliptic curves. Our results generalize previous work on the subject, and suggest further connections between the geometry of complex curves and the dynamics of geodesic flows. 1 into PostgreSQL...\n",
      "Inserting test sample 101  We present high-quality VLA images of the FR I radio galaxy 3C 31 in the frequency range 1365 to 8440 MHz with angular resolutions from 0.25 to 40 arcsec. Our new images reveal complex, well resolved filamentary substructure in the radio jets and tails. We also use these images to explore the spectral structure of 3C 31 on large and small scales. We infer the apparent magnetic field structure by correcting for Faraday rotation. Some of the intensity substructure in the jets is clearly related to structure in their apparent magnetic field: there are arcs of emission where the degree of linear polarization increases, with the apparent magnetic field parallel to the ridges of the arcs. The spectral indices are significantly steeper (0.62) within 7 arcsec of the nucleus than between 7 and 50 arcsec (0.52 - 0.57). The spectra of the jet edges are also slightly flatter than the average for their surroundings. At larger distances, the jets are clearly delimited from surrounding larger-scale emission both by their flatter radio spectra and by sharp brightness gradients. The spectral index of 0.62 in the first 7 arcsec of 3C 31's jets is very close to that found in other FR I galaxies where their jets first brighten in the radio and where X-ray synchrotron emission is most prominent. Farther from the nucleus, where the spectra flatten, X-ray emission is fainter relative to the radio. The brightest X-ray emission from FR I jets is therefore not associated with the flattest radio spectra, but with a particle-acceleration process whose characteristic energy index is 2.24. The spectral flattening with distance from the nucleus occurs where our relativistic jet models require deceleration, and the flatter-spectra at the jet edges may be associated with transverse velocity shear. (Slightly abridged) 0 into PostgreSQL...\n",
      "Inserting test sample 102  This paper presents multifrequency observations of the FR I radio galaxy 3C 31 obtained using the Karl G. Jansky Very Large Array (VLA). The distinct morphology of 3C 31 in the context of a low-luminosity nucleus is studied. The radio image at 1.5 GHz exhibits a narrow, jet-like feature that extends up to 25 kpc towards north-west. The radio counter-jet, though detected, is very weak and short. At higher frequencies, such as 8 and 15 GHz, the radio emission presents a more uniform structure with a decreased contrast between the jet and diffuse emission.\n",
      "\n",
      "We estimated the spectral index distribution of 3C 31 by comparing the 1.5 GHz, 8 GHz, and 15 GHz maps. A significant steepening of the spectral index towards the edges of the source is noticed. This, combined with a flatter spectral index in the jet region, suggests a possible gradient in the magnetic field. The evolution of the magnetic field is further investigated by examining the depolarization properties of the source. Depolarization is observed near the center of the galaxy where we suggest a thermal component is present. Meanwhile, the outer regions exhibit mild depolarization which could be caused by either turbulence or the existence of a tangled magnetic field.\n",
      "\n",
      "Overall, the multifrequency observations of 3C 31 show complex and varied structure across several scales, providing new insights into the morphology, spectrum, and magnetic properties of the source. The current result will be implemented in future studies of the magnetodynamics and plasma physics of this important object. 1 into PostgreSQL...\n",
      "Inserting test sample 103  An expression of the piano soundboard mechanical mobility (in the direction normal to the soundboard) depending on a small number of parameters and valid up to several kHz is given in this communication. Up to 1.1 kHz, our experimental and numerical investigations confirm previous results showing that the soundboard behaves like a homogeneous plate with isotropic properties and clamped boundary conditions. Therefore, according to the Skudrzyk mean-value theorem (Skudrzyk 1980), only the mass of the structure M, the modal density n(f), and the mean loss factor eta(f), are needed to express the average driving point mobility. Moreover, the expression of the envelope - resonances and antiresonances - of the mobility can be derived, according to (Langley 1994). We measured the modal loss factor and the modal density of the soundboard of an upright piano in playing condition, in an anechoic environment. The measurements could be done up to 2.5 kHz, with a novel high-resolution modal analysis technique (see the ICA companion-paper, Ege and Boutillon (2010)). Above 1.1 kHz, the change in the observed modal density together with numerical simulations confirm Berthaut's finding that the waves in the soundboard are confined between adjacent ribs (Berthaut et al. 2003).\n",
      "\n",
      "Extending the Skudrzyk and Langley approaches, we synthesize the mechanical mobility at the bridge up to 2.5 kHz. The validity of the computation for an extended spectral domain is discussed. It is also shown that the evolution of the modal density with frequency is consistent with the rise of mobility (fall of impedance) in this frequency range and that both are due to the inter-rib effect appearing when the half-wavelength becomes equal to the rib spacing.\n",
      "\n",
      "Results match previous observations by Wogram (1980), Conklin (1996), Giordano (1998), Nakamura (1983) and could be used for numerical simulations for example. This approach avoids the detailed description of the soundboard, based on a very high number of parameters. However, it can be used to predict the changes of the driving point mobility, and possibly of the sound radiation in the treble range, resulting from structural modifications. 0 into PostgreSQL...\n",
      "Inserting test sample 104  The soundboards of pianos are critical components that are responsible for converting string vibrations into audible sounds. In this research paper, we present a synthetic description of the mechanical mobility of piano soundboards and investigate the factors that affect their performance.\n",
      "\n",
      "We begin by outlining the basic principles of piano soundboard construction and the physics of sound transmission through solids. We then describe the various materials used in soundboard construction and the different techniques used to shape and support them. Our aim is to provide a comprehensive overview of the factors that contribute to soundboard mobility.\n",
      "\n",
      "We go on to examine the effects of different soundboard designs and geometries on their mechanical behavior. We also investigate the impact of various parameters such as soundboard thickness, ribbing patterns, and bridge placement on the overall sound quality of the piano.\n",
      "\n",
      "Our findings show that soundboard mobility is a critical factor in producing high-quality piano sounds. We also find that the choice of material and design significantly affect the mechanical behavior of the soundboard. Our synthetic description provides a foundation for further research aimed at improving the performance of piano soundboards.\n",
      "\n",
      "Finally, we discuss the implications of our findings for piano manufacturers and players. We suggest that the optimization of soundboard mobility should be a key consideration during piano design and manufacturing. We also highlight the potential for our research to facilitate the production of pianos with superior sound quality, and thus, enhance the playing experience for musicians and audiences alike.\n",
      "\n",
      "In conclusion, our research provides a comprehensive overview of the mechanical mobility of piano soundboards. We highlight the importance of this component in producing high-quality piano sounds, and provide insight into the factors that affect its performance. Our work has important implications for piano design and manufacturing, and represents a significant contribution to the field of acoustics. 1 into PostgreSQL...\n",
      "Inserting test sample 105  Inspirals of stellar-mass compact objects into $\\sim 10^6 M_{\\odot}$ black holes are especially interesting sources of gravitational waves for LISA. We investigate whether the emitted waveforms can be used to strongly constrain the geometry of the central massive object, and in essence check that it corresponds to a Kerr black hole (BH). For a Kerr BH, all multipole moments of the spacetime have a simple, unique relation to $M$ and $S$, the BH's mass and spin; in particular, the spacetime's mass quadrupole moment is given by $Q=- S^2/M$. Here we treat $Q$ as an additional parameter, independent of $M$ and $S$, and ask how well observation can constrain its difference from the Kerr value. This was already estimated by Ryan, but for simplified (circular, equatorial) orbits, and neglecting signal modulations due to the motion of the LISA satellites. Here we consider generic orbits and include these modulations.\n",
      "\n",
      "We use a family of approximate (post-Newtonian) waveforms, which represent the full parameter space of Inspiral sources, and exhibit the main qualitative features of true, general relativistic waveforms. We extend this parameter space to include (in an approximate manner) an arbitrary value of $Q$, and construct the Fisher information matrix for the extended parameter space. By inverting the Fisher matrix we estimate how accurately $Q$ could be extracted from LISA observations. For 1 year of coherent data from the inspiral of a $10 M_{\\odot}$ BH into rotating BHs of masses $10^{5.5} M_{\\odot}$, $10^6 M_{\\odot}$, or $10^{6.5} M_{\\odot}$, we find $\\Delta (Q/M^3) \\sim 10^{-4}$, $10^{-3}$, or $10^{-2}$, respectively (assuming total signal-to-noise ratio of 100, typical of the brightest detectable EMRIs). These results depend only weakly on the eccentricity of the orbit or the BH's spin. 0 into PostgreSQL...\n",
      "Inserting test sample 106  This study investigates off-Kerr deviations in the space-time geometry of massive black holes by analyzing data from LISA EMRI sources. We start by providing a brief overview of general relativity and the Kerr metric, which is considered the most appropriate model for the gravitational field of black holes. However, recent studies have suggested the existence of off-Kerr deviations that could arise from several physical processes, including accretion, mergers, and non-minimal couplings to matter.\n",
      "\n",
      "To test these deviations, we use LISA EMRI sources, which are binary systems composed of a compact object (such as a black hole or a neutron star) and a massive black hole. As the binary orbits around the black hole, it produces a gravitational wave signal that can be detected by the LISA observatory. By analyzing the waveform, we can extract information about the space-time geometry of the black hole, including possible off-Kerr deviations.\n",
      "\n",
      "Our analysis reveals the presence of several off-Kerr effects, including non-axisymmetry and higher-order multipole moments. These deviations could have important implications for the gravitational wave detection and parameter estimation. They could also shed light on the fundamental properties of black holes and their astrophysical environments.\n",
      "\n",
      "In conclusion, our study provides strong evidence for off-Kerr deviations in the geometry of massive black holes, which could significantly affect the gravitational wave signals produced by LISA EMRI sources. The results could have important implications for the interpretation of future gravitational wave data and the development of more accurate models for black hole physics. 1 into PostgreSQL...\n",
      "Inserting test sample 107  Semidefinite programs (SDPs) often arise in relaxations of some NP-hard problems, and if the solution of the SDP obeys certain rank constraints, the relaxation will be tight. Decomposition methods based on chordal sparsity have already been applied to speed up the solution of sparse SDPs, but methods for dealing with rank constraints are underdeveloped. This paper leverages a minimum rank completion result to decompose the rank constraint on a single large matrix into multiple rank constraints on a set of smaller matrices. The re-weighted heuristic is used as a proxy for rank, and the specific form of the heuristic preserves the sparsity pattern between iterations. Implementations of rank-minimized SDPs through interior-point and first-order algorithms are discussed. The problem of subspace clustering is used to demonstrate the computational improvement of the proposed method. 0 into PostgreSQL...\n",
      "Inserting test sample 108  This paper introduces a new method for decomposing chordal structures in rank minimized semidefinite programs and demonstrates its effectiveness in the context of subspace clustering. The proposed algorithm applies a novel relaxation to the rank minimization problem, which results in increased efficiency and convergence rates compared to existing techniques. The theoretical analysis shows that the proposed algorithm has a much lower computational complexity than previous methods, without sacrificing accuracy. Experimental results on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art methods in terms of clustering accuracy, especially in high-dimensional settings. Our proposed method provides a significant contribution to the field of subspace clustering and has broad practical applications in various domains, including computer vision, machine learning, and data mining. 1 into PostgreSQL...\n",
      "Inserting test sample 109  Several studies exhibit that the traffic load of the routers only has a small influence on their energy consumption. Hence, the power consumption in networks is strongly related to the number of active network elements, such as interfaces, line cards, base chassis,... The goal thus is to find a routing that minimizes the (weighted) number of active network elements used when routing. In this paper, we consider a simplified architecture where a connection between two routers is represented as a link joining two network interfaces. When a connection is not used, both network interfaces can be turned off. Therefore, in order to reduce power consumption, the goal is to find the routing that minimizes the number of used links while satisfying all the demands. We first define formally the problem and we model it as an integer linear program. Then, we prove that this problem is not in APX, that is there is no polynomial-time constant-factor approximation algorithm. We propose a heuristic algorithm for this problem and we also prove some negative results about basic greedy and probabilistic algorithms. Thus we present a study on specific topologies, such as trees, grids and complete graphs, that provide bounds and results useful for real topologies. We then exhibit the gain in terms of number of network interfaces (leading to a global reduction of approximately 33 MWh for a medium-sized backbone network) for a set of existing network topologies: we see that for almost all topologies more than one third of the network interfaces can be spared for usual ranges of operation. Finally, we discuss the impact of energy efficient routing on the stretch factor and on fault tolerance. 0 into PostgreSQL...\n",
      "Inserting test sample 110  Energy efficiency is a crucial aspect of network routing due to the increasing demand and scale of data networks. One approach to reducing energy consumption is by switching off network interfaces when not in use. This paper proposes a novel routing algorithm that integrates the switching-off of network interfaces in order to improve energy efficiency while maintaining quality of service (QoS) requirements.\n",
      "\n",
      "Our proposed algorithm, named Energy Efficient Routing by Switching-Off Network Interfaces (EERSO), is based on the analysis of traffic patterns and network topology. EERSO determines which network interfaces are not needed for the current traffic and switches them off accordingly. To ensure QoS requirements are met, traffic is re-routed through alternative interfaces when necessary. \n",
      "\n",
      "We conducted extensive simulations using different network topologies and traffic patterns. Our results show that EERSO achieves significant energy savings while maintaining QoS requirements. Specifically, EERSO can reduce energy consumption by up to 30% compared to traditional routing algorithms. Additionally, EERSO can handle high traffic loads and maintain QoS requirements for most traffic patterns.\n",
      "\n",
      "Our proposed algorithm can have significant environmental and economic impacts in large-scale data networks. By reducing energy consumption, data centers and network operators can save on electricity costs and reduce their carbon footprints. Furthermore, the proposed algorithm can be easily integrated into existing routing protocols, making it a practical solution for improving energy efficiency in network routing.\n",
      "\n",
      "In conclusion, our proposed algorithm, EERSO, is a novel and effective approach to improving energy efficiency in network routing. By intelligently switching off network interfaces, significant energy savings can be achieved while maintaining QoS requirements. Future work can explore further enhancements to the proposed algorithm, such as dynamic adaptation to traffic patterns and network conditions. 1 into PostgreSQL...\n",
      "Inserting test sample 111  Type Iax supernovae represent the largest class of peculiar white-dwarf supernovae. The type Iax SN~2012Z in NGC 1309 is the only white dwarf supernova with a detected progenitor system in pre-explosion observations. Deep \\textit{Hubble Space Telescope} images taken before SN~2012Z show a luminous, blue source that we have interpreted as a helium-star companion (donor) to the exploding white dwarf. We present here late-time \\textit{HST} observations taken $\\sim$1400 days after the explosion to test this model. We find the SN light curve can empirically be fit by an exponential decay model in magnitude units. The fitted asymptotic brightness is within $10\\%$ of our latest measurements and approximately twice the brightness of the pre-explosion source. The decline of the light curve is too slow to be powered by $^{56}$Co or $^{57}$Co decay: if radioactive decay is the dominate power source, it must be from longer half-life species like $^{55}$Fe. Interaction with circumstellar material may contribute to the light curve, as may shock heating of the companion star. Companion-star models underpredict the observed flux in the optical, producing most of their flux in the UV at these epochs. A radioactively-heated bound remnant, left after only a partial disruption of the white dwarf, is also capable of producing the observed excess late-time flux.\n",
      "\n",
      "Our analysis suggests that the total ejecta + remnant mass is consistent with the Chandrasekhar mass for a range of type Iax supernovae. 0 into PostgreSQL...\n",
      "Inserting test sample 112  The study of supernovae is crucial for understanding the expansion of the universe and the evolution of stars. In this study, we analyze the Hubble Space Telescope observations of SN 2012Z which exploded in the galaxy NGC 1309. We compare these observations with those taken a decade before to determine the supernova's behavior.\n",
      "\n",
      "Our results show that SN 2012Z has not disappeared and is still visible, which is surprising as most supernovae fade over time. Furthermore, our analysis shows that the brightness of SN 2012Z has increased since its explosion. This is an unusual phenomenon as supernovae typically become dimmer as they age.\n",
      "\n",
      "Our data analysis reveals that the spectral features of the supernova's light have also changed over the decade. Specifically, the absorption features due to the presence of certain elements have become broader. This result implies that the supernova is no longer moving at a uniform velocity, but rather is expanding in a non-uniform manner.\n",
      "\n",
      "Our findings are significant because they shed light on the nature of supernovae and their evolution over time. The unusual behavior of SN 2012Z challenges current models of supernova evolution and will be important in improving our understanding of these explosions. With future observations, we can continue to monitor the evolution of this unusual supernova and gain valuable insights into the workings of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 113  We consider a four-parameter family of point interactions in one dimension.\n",
      "\n",
      "This family is a generalization of the usual $\\delta$-function potential. We examine a system consisting of many particles of equal masses that are interacting pairwise through such a generalized point interaction. We follow McGuire who obtained exact solutions for the system when the interaction is the $\\delta$-function potential. We find exact bound states with the four-parameter family. For the scattering problem, however, we have not been so successful.\n",
      "\n",
      "This is because, as we point out, the condition of no diffraction that is crucial in McGuire's method is not satisfied except when the four-parameter family is essentially reduced to the $\\delta$-function potential. 0 into PostgreSQL...\n",
      "Inserting test sample 114  In this research paper, we investigate a many-body system with a four-parameter family of point interactions in one dimension. We consider the behavior of the system under different parameter values, and analyze various properties such as energy levels, scattering amplitudes, and bound states. Through our calculations, we derive explicit formulas and present numerical simulations to show the rich structure of the system. Furthermore, we develop an effective approach to deal with singularities of the point interactions, allowing for a more accurate analysis of the system. Our results suggest that the four-parameter family of point interactions leads to a wider range of physical phenomena than the commonly studied two- and three-parameter families. 1 into PostgreSQL...\n",
      "Inserting test sample 115  The Bayesian predictive density has complex representation and does not belong to any finite-dimensional statistical model except for in limited situations. In this paper, we introduce its simple approximate representation employing its projection onto a finite-dimensional exponential family. Its theoretical properties are established parallelly to those of the Bayesian predictive density when the model belongs to curved exponential families. It is also demonstrated that the projection asymptotically coincides with the plugin density with the posterior mean of the expectation parameter of the exponential family, which we refer to as the Bayes extended estimator.\n",
      "\n",
      "Information-geometric correspondence indicates that the Bayesian predictive density can be represented as the posterior mean of the infinite-dimensional exponential family. The Kullback--Leibler risk performance of the approximation is demonstrated by numerical simulations and it indicates that the posterior mean of the expectation parameter approaches the Bayesian predictive density as the dimension of the exponential family increases. It also suggests that approximation by projection onto an exponential family of reasonable size is practically advantageous with respect to risk performance and computational cost. 0 into PostgreSQL...\n",
      "Inserting test sample 116  In this paper, we propose Bayes extended estimators for curved exponential families. We introduce a general expression for the Bayes extended estimator of the natural parameter and show that it can be obtained by solving a convex optimization problem. We then focus on a specific subclass of curved exponential families called skew-normal distributions and derive the corresponding Bayes extended estimator. We compare our estimator with the maximum likelihood estimator and the Bayes estimator in terms of mean squared error and show that our estimator outperforms the other two in certain scenarios. We also provide numerical experiments to further illustrate the advantages of the proposed estimator. Furthermore, we adapt the proposed method to handle samples with missing values and show that it can be done without much difficulty. Overall, our results demonstrate the effectiveness and versatility of the proposed Bayes extended estimators for curved exponential families. 1 into PostgreSQL...\n",
      "Inserting test sample 117  We consider the high frequency Helmholtz equation with a variable refraction index $n^2(x)$ ($x \\in \\R^d$), supplemented with a given high frequency source term supported near the origin $x=0$. A small absorption parameter $\\alpha_{\\varepsilon}>0$ is added, which somehow prescribes a radiation condition at infinity for the considered Helmholtz equation. The semi-classical parameter is $\\varepsilon>0$. We let $\\eps$ and $\\a_\\eps$ go to zero {\\em simultaneaously}. We study the question whether the indirectly prescribed radiation condition at infinity is satisfied {\\em uniformly} along the asymptotic process $\\eps \\to 0$, or, in other words, whether the conveniently rescaled solution to the considered equation goes to the {\\em outgoing} solution to the natural limiting Helmholtz equation. This question has been previously studied by the first autor. It is proved that the radiation condition is indeed satisfied uniformly in $\\eps$, provided the refraction index satisfies a specific {\\em non-refocusing condition}, a condition that is first pointed out in this reference. The non-refocusing condition requires, in essence, that the rays of geometric optics naturally associated with the high-frequency Helmholtz operator, and that are sent from the origin $x=0$ at time $t=0$, should not refocus at some later time $t>0$ near the origin again.\n",
      "\n",
      "In the present text we show the {\\em optimality} of the above mentionned non-refocusing condition, in the following sense. We exhibit a refraction index which {\\em does} refocus the rays of geometric optics sent from the origin near the origin again, and, on the other hand, we completely compute the asymptotic behaviour of the solution to the associated Helmholtz equation: we show that the limiting solution {\\em does not} satisfy the natural radiation condition at infinity. More precisely, we show that the limiting solution is a {\\em perturbation} of the outgoing solution to the natural limiting Helmholtz equation, and that the perturbing term explicitly involves the contribution of the rays radiated from the origin which go back to the origin. This term is also conveniently modulated by a phase factor, which turns out to be the action along the above rays of the hamiltonian associated with the semiclassical Helmholtz equation. 0 into PostgreSQL...\n",
      "Inserting test sample 118  In this paper, we address the radiation condition at infinity for the high-frequency Helmholtz equation. Specifically, we investigate the optimality of a non-refocusing criterion that has been proposed in the literature. \n",
      "\n",
      "The concept of radiation conditions plays a fundamental role in studying the behavior of solutions to partial differential equations at infinity. In the context of the Helmholtz equation, the radiation condition requires that the solution radiates away from the source as the distance from the source tends to infinity. \n",
      "\n",
      "The traditional radiation condition for the Helmholtz equation is the Sommerfeld radiation condition, which requires that the solution behaves as an outgoing wave at infinity. However, this condition is not always applicable in practice, especially when dealing with complex geometries or high frequencies. \n",
      "\n",
      "Recently, a non-refocusing criterion for the Helmholtz equation has been proposed, which allows for a more flexible radiation condition at infinity. The criterion is based on the idea that the energy of the solution should not be concentrated in small regions, and it can provide a more accurate description of the behavior of the solution in certain situations. \n",
      "\n",
      "In this paper, we investigate the optimality of this non-refocusing criterion in terms of the decay rate of the solution at infinity. We show that the criterion is optimal in the sense that it provides the best possible decay rate for a wide class of problems. Moreover, we demonstrate the usefulness of this criterion in several examples, including scattering by a convex obstacle and multiple obstacles. \n",
      "\n",
      "Overall, our results suggest that the non-refocusing criterion is a valuable tool in studying the behavior of solutions to the high-frequency Helmholtz equation at infinity. It allows for a more flexible radiation condition, which can be particularly useful in complex geometries or high-frequency settings. 1 into PostgreSQL...\n",
      "Inserting test sample 119  We use N-body simulations to study the tidal evolution of globular clusters (GCs) in dwarf spheroidal (dSph) galaxies. Our models adopt a cosmologically motivated scenario in which the dSph is approximated by a static NFW halo with a triaxial shape. We apply our models to five GCs spanning three orders of magnitude in stellar density and two in mass, chosen to represent the properties exhibited by the five GCs of the Fornax dSph. We show that only the object representing Fornax's least dense GC (F1) can be fully disrupted by Fornax's internal tidal field--the four denser clusters survive even if their orbits decay to the centre of Fornax. For a large set of orbits and projection angles we examine the spatial and velocity distribution of stellar debris deposited during the complete disruption of an F1-like GC. Our simulations show that such debris appears as shells, isolated clumps and elongated over-densities at low surface brightness (>26 mag/arcsec^2), reminiscent of substructure observed in several MW dSphs. Such features arise from the triaxiality of the galaxy potential and do not dissolve in time. The kinematics of the debris depends strongly on the progenitor's orbit. Debris associated with box and resonant orbits does not display stream motions and may appear \"colder\"/\"hotter\" than the dSph's field population if the viewing angle is perpendicular/parallel to progenitor's orbital plane. In contrast, debris associated with loop orbits shows a rotational velocity that may be detectable out to few kpc from the galaxy centre. Chemical tagging that can distinguish GC debris from field stars may reveal whether the merger of GCs contributed to the formation of multiple stellar components observed in dSphs. 0 into PostgreSQL...\n",
      "Inserting test sample 120  Globular clusters in dwarf galaxies may experience tidal disruption due to the gravitational forces exerted by the host galaxy. In this paper, we investigate the effects of dark matter haloes with triaxial shapes on such tidal disruption events. We carry out high-resolution N-body simulations to study the dynamical evolution of globular clusters that are initially placed in the central region of dwarf galaxies with triaxial dark matter haloes.\n",
      "\n",
      "Our analysis reveals that the presence of triaxial dark matter haloes enhances the tidal forces experienced by globular clusters. This increased disruption rate is due to the asymmetric gravitational forces exerted by the triaxial dark matter halo on the globular cluster. In particular, the tidal disruption rate is found to be higher along the long and intermediate axes of the dark matter halo, as compared to the short axis.\n",
      "\n",
      "Furthermore, we find that the degree of triaxiality of the dark matter halo plays a key role in determining the disruption rate of the globular cluster. We show that the tidal disruption rate increases as the degree of triaxiality of the halo becomes more pronounced.\n",
      "\n",
      "Our results have important implications for understanding the evolution of globular clusters in dwarf galaxies. The enhanced tidal disruption rates in the presence of triaxial dark matter haloes may have significant implications for the observed distribution of globular cluster populations in dwarf galaxies. Overall, our study underscores the need to consider the effects of dark matter haloes with complex shapes when studying the dynamical evolution of globular clusters in dwarf galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 121  In combinatorial group testing problems Questioner needs to find a defective element $x\\in [n]$ by testing subsets of $[n]$. In [18] the authors introduced a new model, where each element knows the answer for those queries that contain it and each element should be able to identify the defective one. In this article we continue to investigate this kind of models with more defective elements. We also consider related models inspired by secret sharing models, where the elements should share information among them to find out the defectives. Finally the adaptive versions of the different models are also investigated. 0 into PostgreSQL...\n",
      "Inserting test sample 122  Combinatorial Group Testing (CGT) is a powerful testing technique that is used to identify a small number of defective items within a large population. However, this technique becomes less effective when the number of defectives increases. In this paper, we explore the role of conscious and controlling elements in CGT problems with more defectives. We propose a new algorithm that combines both conscious and controlling elements to increase the accuracy and efficiency of CGT in the presence of high defect rates. Our results demonstrate significant improvements in testing accuracy and reduced costs associated with defect identification. 1 into PostgreSQL...\n",
      "Inserting test sample 123  In the present contribution, we investigate first-order nonlinear systems of partial differential equations which are constituted of two parts: a system of conservation laws and non-conservative first order terms. Whereas the theory of first-order systems of conservation laws is well established and the conditions for the existence of supplementary conservation laws, and more specifically of an entropy supplementary conservation law for smooth solutions, well known, there exists so far no general extension to obtain such supplementary conservation laws when non-conservative terms are present. We propose a framework in order to extend the existing theory and show that the presence of non-conservative terms somewhat complexifies the problem since numerous combinations of the conservative and non-conservative terms can lead to a supplementary conservation law. We then identify a restricted framework in order to design and analyze physical models of complex fluid flows by means of computer algebra and thus obtain the entire ensemble of possible combination of conservative and non-conservative terms with the objective of obtaining specifically an entropy supplementary conservation law. The theory as well as developed computer algebra tool are then applied to a Baer-Nunziato two-phase flow model and to a multicomponent plasma fluid model. The first one is a first-order fluid model, with non-conservative terms impacting on the linearly degenerate field and requires a closure since there is no way to derive interfacial quantities from averaging principles and we need guidance in order to close the pressure and velocity of the interface and the thermodynamics of the mixture. The second one involves first order terms for the heavy species coupled to second order terms for the electrons, the non-conservative terms impact the genuinely nonlinear fields and the model can be rigorously derived from kinetic theory. We show how the theory allows to recover the whole spectrum of closures obtained so far in the literature for the two-phase flow system as well as conditions when one aims at extending the thermodynamics and also applies to the plasma case, where we recover the usual entropy supplementary equation, thus assessing the effectiveness and scope of the proposed theory. 0 into PostgreSQL...\n",
      "Inserting test sample 124  This paper introduces a novel approach to the modelling and analysis of complex fluid flows, which utilizes the concept of entropy as a supplementary conservation law. We apply this approach to non-linear systems of partial differential equations (PDEs) with non-conservative terms, using computer algebra techniques for efficient computation.\n",
      "\n",
      "Our approach is motivated by the need for more accurate and efficient methods for modelling complex fluid flows, which are critical to many fields ranging from engineering to environmental science. Traditional methods often rely on the assumption of conservative terms in the governing equations, which limits their usefulness for non-conservative systems.\n",
      "\n",
      "Through the use of entropy as a supplementary conservation law, we are able to model non-conservative systems more accurately and efficiently. This is achieved by adding an entropy balance equation to the system of PDEs, which provides an additional constraint on the evolution of the system. We demonstrate the effectiveness of this approach through a series of numerical experiments on a variety of complex fluid flows.\n",
      "\n",
      "The computer algebra techniques used in this study provide an efficient means for solving the resulting system of PDEs with entropy balance equation. In particular, we utilize modern symbolic computation software, which allows for automatic differentiation and simplification of the equations. This leads to significant improvements in computation time and accuracy compared to traditional numerical methods.\n",
      "\n",
      "Overall, our approach represents a promising new direction for the modelling and analysis of complex fluid flows. By incorporating the concept of entropy as a supplementary conservation law and utilizing modern computer algebra techniques, we are able to model non-conservative systems more accurately and efficiently. This has important implications for the design and understanding of complex fluid systems of practical significance. 1 into PostgreSQL...\n",
      "Inserting test sample 125  We consider N=1 supersymmetric Yang-Mills theory with fundamental matter in the large-N_c approximation in 1+1 dimensions. We add a Chern-Simons term to give the adjoint partons a mass and solve for the meson bound states. Here mesons are color-singlet states with two partons in the fundamental representation but are not necessarily bosons. We find that this theory has anomalously light meson bound states at intermediate and strong coupling. We also examine the structure functions for these states and find that they prefer to have as many partons as possible at low longitudinal momentum fraction. 0 into PostgreSQL...\n",
      "Inserting test sample 126  We investigate the spectrum of ground state mesons in (1+1)-dimensional supersymmetric quantum chromodynamics with fundamental matter. By means of a systematic perturbative analysis, we find that the masses of the lightest scalar and pseudoscalar mesons undergo anomalous renormalization, resulting in unexpectedly small values. Our calculations reveal that these anomalies can be traced back to non-perturbative effects related to dynamical chiral symmetry breaking and the ensuing generation of a mass gap. The phenomenon is of particular interest since it leads to a highly unconventional spectrum of mesons, which we characterize in detail. 1 into PostgreSQL...\n",
      "Inserting test sample 127  Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been empirically shown to be susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.\n",
      "\n",
      "In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned. Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which \\emph{reported} incidents of crime (those reported by residents) and \\emph{discovered} incidents of crime (i.e. those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest. 0 into PostgreSQL...\n",
      "Inserting test sample 128  The use of predictive policing algorithms has been growing rapidly in recent years, with the promise of identifying criminal activity before it occurs. However, this type of technology has been associated with significant concerns related to its potential bias, particularly towards minority groups. Further, there is increasing evidence that the feedback loops inherent in predictive policing algorithms may actually reinforce existing biases, creating a vicious cycle in which the system's predictions become increasingly skewed over time. In this paper, we explore the concept of runaway feedback loops in predictive policing, and the implications of such phenomenon for the fairness and effectiveness of these algorithms. Using a combination of simulation studies and real-world data analysis, we show that runaway feedback loops can lead to significant levels of bias against certain population segments, while also reducing overall accuracy. Further, we discuss potential methods for mitigating the negative effects of these feedback loops, including algorithmic adjustments, data transparency, and human oversight. Ultimately, our work highlights the urgent need for continued research aimed at identifying and addressing the underlying issues that allow runaway feedback loops to emerge in predictive policing systems. 1 into PostgreSQL...\n",
      "Inserting test sample 129  S-type AGB stars have a C/O ratio which suggests that they are transition objects between oxygen-rich M-type stars and carbon-rich C-type stars. As such, their circumstellar compositions of gas and dust are thought to be sensitive to their precise C/O ratio, and it is therefore of particular interest to examine their circumstellar properties.\n",
      "\n",
      "We present new Herschel HIFI and PACS sub-millimetre and far-infrared line observations of several molecular species towards the S-type AGB star W Aql. We use these observations, which probe a wide range of gas temperatures, to constrain the circumstellar properties of W Aql, including mass-loss rate and molecular abundances. We used radiative transfer codes to model the circumstellar dust and molecular line emission to determine circumstellar properties and molecular abundances. We assumed a spherically symmetric envelope formed by a constant mass-loss rate driven by an accelerating wind.\n",
      "\n",
      "Our model includes fully integrated H2O line cooling as part of the solution of the energy balance. We detect circumstellar molecular lines from CO, H2O, SiO, HCN, and, for the first time in an S-type AGB star, NH3. The radiative transfer calculations result in an estimated mass-loss rate for W Aql of 4.0e-6 Msol yr-1 based on the 12CO lines. The estimated 12CO/13CO ratio is 29, which is in line with ratios previously derived for S-type AGB stars. We find an H2O abundance of 1.5e-5, which is intermediate to the abundances expected for M and C stars, and an ortho/para ratio for H2O that is consistent with formation at warm temperatures. We find an HCN abundance of 3e-6, and, although no CN lines are detected using HIFI, we are able to put some constraints on the abundance, 6e-6, and distribution of CN in W Aql's circumstellar envelope using ground-based data. We find an SiO abundance of 3e-6, and an NH3 abundance of 1.7e-5, confined to a small envelope. 0 into PostgreSQL...\n",
      "Inserting test sample 130  Asymptotic Giant Branch (AGB) stars are key objects in the study of stellar evolution, owing to their crucial role in the chemical enrichment of the interstellar medium. W Aquilae is an S-type AGB star that presents an exceptionally detached circumstellar envelope and an extended molecular atmosphere. In this work, detailed modelling of the molecular line emission of W Aquilae is presented. This was performed by combining data from millimeter and submillimeter telescopes, allowing us to study a large number of molecular transitions, both in the radio and the submillimeter range. We used a well-tested and state-of-the-art radiative transfer code, which allowed us to obtain the physical conditions in the molecular envelope of the star, such as the temperature, density, and abundance of the key molecular species. A particular focus was devoted to SiO and HCN, two species that are crucial to probe the innermost regions of the molecular envelope. The modelling allowed us to derive the abundance profiles of SiO and HCN and to investigate their dependence on the stellar pulsations. Moreover, we characterized the kinematical properties of the molecular envelope, by means of an accurate study of the line profiles and their variability with time. The results obtained in this work show that W Aquilae presents a peculiar molecular structure, in which the SiO and HCN emission mainly arises from a shell-like region, located relatively far from the central star. The kinematical analysis revealed a complex velocity field, consistent with a molecular envelope that is partially detached from the central star and is subject to strong shocks and turbulence. Our study provides a detailed and comprehensive view of the molecular envelope and the kinematics of W Aquilae, and represents a significant step forward in the understanding of AGB stars and their evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 131  We study the spatio-temporal evolution of the nonlinear electrostatic oscillations in a cold magnetized electron-positron (e-p) plasma using both analytics and simulations. Using a perturbative method we demonstrate that the nonlinear solutions change significantly when a pure electrostatic mode is excited at the linear level instead of a mixed upper-hybrid and zero-frequency mode that is considered in a recent study. The pure electrostatic oscillations undergo phase mixing nonlinearly. However, the presence of the magnetic field significantly delays the phase-mixing compared to that observed in the corresponding unmagnetized plasma. Using 1D PIC simulations we then analyze the damping of the primary modes of the pure oscillations in detail and infer the dependence of the phase-mixing time on the magnetic field and the amplitude of the oscillations. The results are remarkably different from those found for the mixed upper-hybrid mode mentioned above. Exploiting the symmetry of the e-p plasma we then explain a generalized symmetry of our non-linear solutions. The symmetry allows us to construct a unique nonlinear solution up to the second order which does not show any signature of phase mixing but results in a nonlinear wave traveling at upper-hybrid frequency. Our investigations have relevance for laboratory/astrophysical e-p plasmas. 0 into PostgreSQL...\n",
      "Inserting test sample 132  Electrostatic oscillations are a fundamental phenomenon in plasma physics that can occur in a variety of different plasma environments. In this paper, we investigate the behavior of electrostatic oscillations in a cold magnetized electron-positron plasma. We find that, due to the absence of thermal pressure, these oscillations can become highly nonlinear and can lead to the development of strong electric fields. Using numerical simulations, we show that the nonlinear behavior of these oscillations can be understood in terms of the interplay between the restoring force of the plasma and the inertia of the plasma particles. We also investigate the role of magnetic fields in modifying the properties of the oscillations, and show that the presence of a magnetic field can lead to the formation of localized nonlinear structures known as solitons. Our results have potential applications in a range of fields including laboratory plasmas, astrophysics, and space science. Overall, our study provides new insights into the behavior of electrostatic oscillations in an electron-positron plasma, and highlights the importance of considering nonlinear effects in understanding plasma dynamics. 1 into PostgreSQL...\n",
      "Inserting test sample 133  Excitons in monolayer semiconductors have large optical transition dipole for strong coupling with light field. Interlayer excitons in heterobilayers, with layer separation of electron and hole components, feature large electric dipole that enables strong coupling with electric field and exciton-exciton interaction, at the cost that the optical dipole is substantially quenched (by several orders of magnitude). In this letter, we demonstrate the ability to create a new class of excitons in transition metal dichalcogenide (TMD) hetero- and homo-bilayers that combines the advantages of monolayer- and interlayer-excitons, i.e. featuring both large optical dipole and large electric dipole. These excitons consist of an electron that is well confined in an individual layer, and a hole that is well extended in both layers, realized here through the carrier-species specific layer-hybridization controlled through the interplay of rotational, translational, band offset, and valley-spin degrees of freedom. We observe different species of such layer-hybridized valley excitons in different heterobilayer and homobilayer systems, which can be utilized for realizing strongly interacting excitonic/polaritonic gases, as well as optical quantum coherent controls of bidirectional interlayer carrier transfer either with upper conversion or down conversion in energy. 0 into PostgreSQL...\n",
      "Inserting test sample 134  Van der Waals bilayers have unique optical and electronic properties, primarily due to the excitonic states they exhibit. This study explores how to tailor and control excitonic states in van der Waals bilayers through stacking configuration, band alignment, and valley-spin. By adjusting the relative orientation of the two layers, we investigated the effects of interlayer coupling on exciton binding energy and spin. Furthermore, we studied the band alignment between the layers and the delta doping necessary to achieve specific configurations. Our results indicate that excitonic states can be effectively tuned by stacking configuration and band alignment, resulting in significant shifts in energy spectra and optical absorption. Moreover, the valley-spin interactions in bilayers can be controlled by manipulating the spin-orbit coupling strength. Our findings reveal a comprehensive framework to design van der Waals heterostructures with tailored excitonic states and provide insight into how to optimize these structures for the development of future electronic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 135  Previously published packings of equal disks in an equilateral triangle have dealt with up to 21 disks. We use a new discrete-event simulation algorithm to produce packings for up to 34 disks. For each n in the range 22 =< n =< 34 we present what we believe to be the densest possible packing of n equal disks in an equilateral triangle. For these n we also list the second, often the third and sometimes the fourth best packings among those that we found. In each case, the structure of the packing implies that the minimum distance d(n) between disk centers is the root of polynomial P_n with integer coefficients. In most cases we do not explicitly compute P_n but in all cases we do compute and report d(n) to 15 significant decimal digits. Disk packings in equilateral triangles differ from those in squares or circles in that for triangles there are an infinite number of values of n for which the exact value of d(n) is known, namely, when n is of the form Delta(k) := k(k+1)/2. It has also been conjectured that d(n-1) = d(n) in this case. Based on our computations, we present conjectured optimal packings for seven other infinite classes of n, namely n = Delta(2k)+1, Delta(2k+1)+1, Delta(k+2)-2, Delta(2k+3)-3, Delta(3k+1)+2, 4*Delta(k) and 2*Delta(k+1)+2*Delta(k)-1. We also report the best packings we found for other values of n in these forms which are larger than 34, namely, n=37, 40, 42, 43, 46, 49, 56, 57, 60, 63, 67, 71, 79, 84, 92, 93, 106, 112, 121, and 254, and also for n=58, 95, 108, 175, 255, 256, 258, and 260. We say that an infinite class of packings of n disks, n=n(1), n(2),...n(k),..., is tight, if 1/d(n(k)+1) - 1/d(n(k)) is bounded away from zero as k goes to infinity. We conjecture that some of our infinite classes are tight, others are not tight, and that there are infinitely many tight classes. 0 into PostgreSQL...\n",
      "Inserting test sample 136  This paper presents a study of dense packings of equal disks within an equilateral triangle. The focus is on packings containing 22 to 34 disks, and the results extend beyond this range. The problem of dense sphere packing is one of the oldest and most fundamental topics in mathematics, and this particular case presents a challenge due to the limited space available within the triangular container. \n",
      "\n",
      "We begin our investigation by exploring the known solutions for packings within an equilateral triangle for fewer disks, which leads us to the study of packings with 22 to 34 disks. We first analyze the known solutions for these numbers of disks, before exploring new solutions that have not yet been reported. \n",
      "\n",
      "Through a combination of mathematical and computational techniques, we identify previously unknown dense packings of 25, 27, and 28 disks and improve upon the known packings for 24, 30 and 32 disks. We also present new solutions for 33 and 34 disks.\n",
      "\n",
      "To achieve these results, we use a range of methods including exhaustive search, pattern programming, and optimization techniques. These methods enable us to identify both symmetric and asymmetric packings that improve upon previously known solutions. In particular, we analyze the local configurations of the new packings and provide a detailed description of their geometric properties.\n",
      "\n",
      "Our results demonstrate that there is still much to be discovered in the problem of packing disks within an equilateral triangle. The newly discovered packings represent significant improvements upon previously known solutions and suggest that there are likely many more new packings waiting to be found. This study serves as an important contribution to the overall understanding of dense sphere packing and provides further insight into the complexities of this fundamental mathematical problem. 1 into PostgreSQL...\n",
      "Inserting test sample 137  The active region NOAA 11158 produced the first X-class flare of Solar Cycle 24, an X2.2 flare at 01:44 UT on 2011 February 15. Here we analyze SDO/HMI magnetograms covering a 12-hour interval centered at the time of this flare. We describe the spatial distributions of the photospheric magnetic changes associated with this flare, including the abrupt changes in the field vector, vertical electric current and Lorentz force vector. We also trace these parameters' temporal evolution. The abrupt magnetic changes were concentrated near the neutral line and in two neighboring sunspots. Near the neutral line, the field vectors became stronger and more horizontal during the flare and the shear increased. This was due to an increase in strength of the horizontal field components near the neutral line, most significant in the horizontal component parallel to the neutral line but the perpendicular component also increased in strength. The vertical component did not show a significant, permanent overall change at the neutral line. The increase in total flux at the neutral line was accompanied by a compensating flux decrease in the surrounding volume. In the two sunspots near the neutral line the azimuthal flux abruptly decreased during the flare but this change was permanent in only one of the spots. There was a large, abrupt, downward vertical Lorentz force change during the flare, consistent with results of past analyses and recent theoretical work. The horizontal Lorentz force acted in opposite directions along each side of neutral line, with the two sunspots at each end subject to abrupt torsional forces. The shearing forces were consistent with field contraction and decrease of shear near the neutral line, whereas the field itself became more sheared as a result of the flux collapsing towards the neutral line from the surrounding volume. 0 into PostgreSQL...\n",
      "Inserting test sample 138  This research paper presents a detailed spatio-temporal description of the abrupt changes in the photospheric magnetic and Lorentz-force vectors during the X2.2 flare that occurred on February 15, 2011. Our investigation was carried out using high-resolution observations obtained from the Helioseismic and Magnetic Imager (HMI) aboard Solar Dynamics Observatory (SDO) satellite. We employed a comprehensive statistical analysis approach to discern and examine the dynamics of the photospheric magnetic vector fields during the eruption phase and after the flare. Our findings indicate that a notable transformation of the magnetic and Lorentz-force vectors occurred in the active region before the flare onset stage.\n",
      "\n",
      "During the pre-eruptive stage, we observed an increase in the average magnetic field strength and shear, leading to the formation of a highly sheared magnetic flux rope structure that subsequently experienced a catastrophic loss of equilibrium, resulting in the X2.2 flare. Our analysis also revealed that the flare was initiated at a rapidly evolving short-scale magnetic field structure near the polarity inversion line of the active region, which underwent a series of magnetic reconnections leading to energy release in the form of an eruption.\n",
      "\n",
      "The study results indicate that the photospheric magnetic structures and their associated Lorentz-force vectors play a critical role in driving solar eruptions and, therefore, provide insights for space weather forecasting. The presented analysis techniques together with our novel methodology for detecting and quantifying the shear in the vector fields can be applied to different active regions to uncover the underlying mechanisms behind solar flares and their catastrophic effects on Earth's environment. 1 into PostgreSQL...\n",
      "Inserting test sample 139  The light curves of the pre-main-sequence star KH 15D from the years 1913--2003 can be understood if the star is a member of an eccentric binary that is encircled by a vertically thin, inclined ring of dusty gas. Eclipses occur whenever the reflex motion of a star carries it behind the circumbinary ring; the eclipses occur with period equal to the binary orbital period of 48.4 days. Features of the light curve--including the amplitude of central reversals during mid-eclipse, the phase of eclipse with respect to the binary orbit phase, the level of brightness out-of-eclipse, the depth of eclipse, and the eclipse duty cycle--are all modulated on the timescale of nodal regression of the obscuring ring, in accord with the historical data. The ring has a mean radius near 3 AU and a radial width that is likely less than this value. While the inner boundary could be shepherded by the central binary, the outer boundary may require an exterior planet to confine it against viscous spreading. The ring must be vertically warped to maintain a non-zero inclination. Thermal pressure gradients and/or ring self-gravity can readily enforce rigid precession. In coming years, as the node of the ring regresses out of our line-of-sight toward the binary, the light curve from the system should cycle approximately back through its previous behavior. Near-term observations should seek to detect a mid-infrared excess from this system; we estimate the flux densities from the ring to be 3 mJy at wavelengths of 10--100 microns. 0 into PostgreSQL...\n",
      "Inserting test sample 140  KH 15D is a unique pre-main sequence binary star embedded in a protoplanetary disk. Its system includes an eclipsing binary, which exhibits light variabilities that are not yet fully understood. The presence of a circumstellar disk, known as the circumbinary ring, contributes to the complex dynamics of the system and the light curve phenomena. \n",
      "\n",
      "In this paper, we present a comprehensive analysis of the structure and properties of the circumbinary ring in KH 15D based on our observations and simulations. Our observations suggest that the circumbinary ring has a complex morphology with an inner gap and asymmetric brightness distribution. We use numerical simulations to investigate the mechanisms that could produce such a structure and found that the most likely scenario is a dynamical interaction between the binary and the circumbinary disk.\n",
      "\n",
      "In addition, we analyze the physical characteristics of the ring, including its temperature and dust properties, to shed light on the nature and origin of the dust particles. We found that the grains in the ring are likely composed of submicron-sized silicates, and that the temperature distribution peaks at the inner edge of the ring, consistent with the dynamical interaction scenario.\n",
      "\n",
      "Our study provides new insights into the properties and dynamics of the circumbinary ring in KH 15D, and highlights the importance of investigating such systems to improve our understanding of the formation and evolution of protoplanetary disks and binary star systems. 1 into PostgreSQL...\n",
      "Inserting test sample 141  In LTE, the two inner satellite lines (ISLs) and the two outer satellite lines (OSLs) of the NH$_{3}$ (1,1) transition are each predicted to have equal intensities. However, hyperfine intensity anomalies (HIAs) are observed to be omnipresent in star formation regions, which is still not fully understood. In addressing this issue, we find that the computation method of the HIA by the ratio of the peak intensities may have defects, especially when being used to process the spectra with low velocity dispersions. Therefore we define the integrated HIAs of the ISLs (HIA$_{\\rm IS}$) and OSLs (HIA$_{\\rm OS}$) by the ratio of their redshifted to blueshifted integrated intensities and develop a procedure to calculate them. Based on this procedure, we present a systematic study of the integrated HIAs in the northern part of the Orion A MC. We find that integrated HIA$_{\\rm IS}$ and HIA$_{\\rm OS}$ are commonly present in the Orion A MC and no clear distinction is found at different locations of the MC.\n",
      "\n",
      "The medians of the integrated HIA$_{\\rm IS}$ and HIA$_{\\rm OS}$ are 0.921$\\pm$0.003 and 1.422$\\pm$0.009, respectively, which is consistent with the HIA core model and inconsistent with the CE model. Selecting those 170 positions where both integrated HIAs deviate by more than 3-$\\sigma$ from unity, most (166) are characterized by HIA$_{\\rm IS}$<1 and HIA$_{\\rm OS}$>1, which suggests that the HIA core model plays a more significant role than the CE model. The remaining four positions are consistent with the CE model. We compare the integrated HIAs with the para-NH$_{3}$ column density ($N$(para-NH$_{3}$)), kinetic temperature ($T_{\\rm K}$), total velocity dispersion ($\\sigma_{\\rm v}$), non-thermal velocity dispersion ($\\sigma_{\\rm NT}$), and the total opacity of the NH$_{3}$ (1,1) line ($\\tau_{0}$). Their correlations can not be fully explained by neither the HIA core nor the CE model. 0 into PostgreSQL...\n",
      "Inserting test sample 142  This paper presents an analysis of the ammonia (NH$_{3}$) (1,1) hyperfine intensity anomalies in the Orion A molecular cloud. We analyzed the data obtained from the Green Bank Telescope during the Green Bank Ammonia Survey (GAS), which encompassed a wide area of the Orion A cloud. Our goal is to understand the properties and characteristics of the NH$_{3}$ (1,1) line emission in this region.\n",
      "\n",
      "The NH$_{3}$ (1,1) line is an important transition in molecular spectroscopy that provides a valuable tracer of molecular gas in the interstellar medium. The hyperfine structure of this line arises from the nuclear spin of the nitrogen atom and the hydrogen atoms in the NH$_{3}$ molecule. In this study, we focused on the three components of the NH$_{3}$ (1,1) hyperfine structure, which are often used to derive the gas density and temperature.\n",
      "\n",
      "Our analysis revealed that the NH$_{3}$ (1,1) hyperfine intensity anomalies in the Orion A cloud are associated with local variations in the gas density and temperature. We found that the intensity ratios of the hyperfine components are significantly different from the expected values based on the standard assumptions of the local thermodynamic equilibrium (LTE). Instead, we observed that the intensity ratios are correlated with the local gas kinetic temperature, which can vary significantly on small spatial scales in the cloud.\n",
      "\n",
      "We discuss several physical mechanisms that could cause the observed deviations from LTE, including sub-thermal excitation of the hyperfine levels, non-thermal motions in the gas, and density gradients. Our analysis suggests that these effects are significant in the Orion A cloud and can affect the estimates of the gas properties based on the NH$_{3}$ (1,1) hyperfine structure.\n",
      "\n",
      "Overall, our study provides new insights into the properties of the NH$_{3}$ (1,1) line emission in the Orion A molecular cloud and highlights the importance of considering non-LTE effects in the interpretation of this spectral line. 1 into PostgreSQL...\n",
      "Inserting test sample 143  The introduction of Dynamic Adaptive Streaming over HTTP (DASH) helped reduce the consumption of resource in video delivery, but its client-based rate adaptation is unable to optimally use the available end-to-end network bandwidth. We consider the problem of optimizing the delivery of video content to mobile clients while meeting the constraints imposed by the available network resources. Observing the bandwidth available in the network's two main components, core network, transferring the video from the servers to edge nodes close to the client, and the edge network, which is in charge of transferring the content to the user, via wireless links, we aim to find an optimal solution by exploiting the predictability of future user requests of sequential video segments, as well as the knowledge of available infrastructural resources at the core and edge wireless networks in a given future time window. Instead of regarding the bottleneck of the end-to-end connection as our throughput, we distribute the traffic load over time and use intermediate nodes between the server and the client for buffering video content to achieve higher throughput, and ultimately significantly improve the Quality of Experience for the end user in comparison with current solutions. 0 into PostgreSQL...\n",
      "Inserting test sample 144  Dynamic Adaptive Streaming over HTTP (DASH) is a popular video streaming technology that adapts the video quality to the available network conditions. However, over wireless networks, DASH suffers from quality degradation due to the inherent instability of wireless networks. In this work, we propose a novel approach that exploits network awareness to enhance DASH performance over wireless networks. Specifically, we leverage information about wireless network conditions, such as signal strength and interference, to make more informed decisions about video quality adaptation. We present a comprehensive evaluation of our approach in a testbed consisting of multiple mobile devices and wireless access points. Our results show that our approach significantly improves video quality and reduces stalling compared to traditional DASH. Moreover, our approach is able to provide a better user experience by adapting to network conditions in real-time. Our proposed solution can be incorporated into existing DASH systems without requiring any modifications to the video player or the server, making it easy to deploy. 1 into PostgreSQL...\n",
      "Inserting test sample 145  Classical and quantum physics impose different constraints on the joint probability distributions of observed variables in a causal structure. These differences mean that certain correlations can be certified as non-classical, which has both foundational and practical importance. Rather than working with the probability distribution itself, it can instead be convenient to work with the entropies of the observed variables. In the Bell causal structure with two inputs and outputs per party, a technique that uses entropic inequalities is known that can always identify non-classical correlations. Here we consider the analogue of this technique in the generalization of this scenario to more outcomes. We identify a family of non-classical correlations in the Bell scenario with two inputs and three outputs per party whose non-classicality cannot be detected through the direct analogue of the previous technique. We also show that use of Tsallis entropy instead of Shannon entropy does not help in this case. Furthermore, we give evidence that natural extensions of the technique also do not help. More precisely, our evidence suggests that even if we allow the observed correlations to be post-processed according to a natural class of non-classicality non-generating operations, entropic inequalities for either the Shannon or Tsallis entropies cannot detect the non-classicality, and hence that entropic inequalities are generally not sufficient to detect non-classicality in the Bell causal structure. In addition, for the bipartite Bell scenario with two inputs and three outputs we find the vertex description of the polytope of non-signalling distributions that satisfy all of the CHSH-type inequalities, which is one of the main regions of investigation in this work. 0 into PostgreSQL...\n",
      "Inserting test sample 146  The Bell causal structure is a crucial framework for understanding non-classicality in quantum mechanics. Entropic inequalities are commonly used to detect such non-classicality, however, they appear to be insufficient in some cases. In this paper, we explore the limitations of entropic inequalities for detecting non-classicality in the Bell causal structure. We show that the entropic inequalities fail to capture certain aspects of non-classicality that are evident in quantum mechanics. We present a proof using a model that violates the entropic inequalities yet satisfies classical causality. Our results demonstrate the existence of non-classicality in the Bell causal structure that is not captured by entropic inequalities. \n",
      "\n",
      "Moreover, we investigate the consequences of our findings for the detection of non-classicality in real-world systems. We show that entropic inequalities may fail to detect non-classicality in experiments due to the presence of experimental imperfections such as measurement noise and decoherence. We discuss alternative methods for detecting non-classicality that are less sensitive to experimental imperfections. Our study suggests that a combination of different techniques may be necessary for detecting non-classicality in real-world systems.\n",
      "\n",
      "In conclusion, our paper reveals the insufficiency of entropic inequalities for detecting non-classicality in the Bell causal structure. Our results provide insights into the limitations of this widely used method and suggest alternative approaches for detecting non-classicality. This study contributes to the ongoing effort to deepen our understanding of non-classicality in quantum mechanics and its implications for practical applications. 1 into PostgreSQL...\n",
      "Inserting test sample 147  For a natural number $m \\ge 2$, we study $m$ layers of finite depth, horizontally infinite, viscous, and incompressible fluid bounded below by a flat rigid bottom. Adjacent layers meet at free interface regions, and the top layer is bounded above by a free boundary as well. A uniform gravitational field, normal to the rigid bottom, acts on the fluid. We assume that the fluid mass densities are strictly decreasing from bottom to top and consider the cases with and without surface tension acting on the free surfaces. In addition to these gravity-capillary effects, we allow a force to act on the bulk and external stress tensors to act on the free interface regions. Both of these additional forces are posited to be in traveling wave form: time-independent when viewed in a coordinate system moving at a constant, nontrivial velocity parallel to the lower rigid boundary. Without surface tension in the case of two dimensional fluids and with all positive surface tensions in the higher dimensional cases, we prove that for each sufficiently small force and stress tuple there exists a traveling wave solution. The existence of traveling wave solutions to the one layer configuration ($m=1$) was recently established and, to the best of our knowledge, this paper is the first construction of traveling wave solutions to the incompressible Navier-Stokes equations in the $m$-layer arrangement. 0 into PostgreSQL...\n",
      "Inserting test sample 148  This paper investigates the multilayer free boundary incompressible Navier-Stokes equations, a model for two immiscible fluid layers in a thin channel. We focus on the existence and stability of traveling wave solutions for this system, which have not been previously explored. We first derive well-posedness results for the system and show that it is exponentially stable in a suitable function space. Then, we present numerical simulations of the system, using both an adaptive spectral method and an explicit finite difference scheme. These simulations confirm the existence of traveling wave solutions for this system, and we demonstrate that they are stable over long time intervals. We also explore the dependence of the wave speed and shape on several model parameters, including the viscosity ratio and the width and depth of the channel. Our results suggest that traveling waves may play a key role in determining the long-term dynamics of this system. Overall, this paper provides new insights into the behavior of the multilayer free boundary incompressible Navier-Stokes equations and sheds light on the possible importance of traveling waves in this context. 1 into PostgreSQL...\n",
      "Inserting test sample 149  We have studied the spin dynamics in Pr$_{0.63}$Sr$_{0.37}$MnO$_3$ above and below the Curie temperature $T_C=301$ K. Three distinct new features have been observed: a softening of the magnon dispersion at the zone boundary for $T<T_C$, significant broadening of the zone boundary magnons as $T\\to T_C$, and no evidence for residual spin-wave like excitations just above $T_C$. The results are inconsistent with double exchange models that have been successfully applied to higher $T_C$ samples, indicating an evolution of the spin system with decreasing $T_C$. 0 into PostgreSQL...\n",
      "Inserting test sample 150  We investigate the behavior of zone boundary magnons in Pr0.63Sr0.37MnO3. By performing inelastic neutron scattering experiments, we observe a softening and broadening of the magnons as the temperature decreases. The observed changes are attributed to the lattice distortions induced by the Jahn-Teller effect and the electron-phonon coupling. We also find that the magnon softening is more pronounced in the vicinity of the metal-insulator transition, indicating a strong correlation between the magnon dynamics and the electronic properties of the material. These results provide insights into the interplay between lattice, charge, and spin degrees of freedom in transition metal oxides. 1 into PostgreSQL...\n",
      "Inserting test sample 151  There are two distinct regimes commonly used to model traveling waves in stratified water: continuous stratification, where the density is smooth throughout the fluid, and layer-wise continuous stratification, where the fluid consists of multiple immiscible strata. The former is the more physically accurate description, but the latter is frequently more amenable to analysis and computation. By the conservation of mass, the density is constant along the streamlines of the flow; the stratification can therefore be specified by prescribing the value of the density on each streamline. We call this the streamline density function.\n",
      "\n",
      "Our main result states that, for every smoothly stratified periodic traveling wave in a certain small-amplitude regime, there is an $L^\\infty$ neighborhood of its streamline density function such that, for any piecewise smooth streamline density function in that neighborhood, there is a corresponding traveling wave solution. Moreover, the mapping from streamline density function to wave is Lipschitz continuous in a certain function space framework. As this neighborhood includes piecewise smooth densities with arbitrarily many jump discontinues, this theorem provides a rigorous justification for the ubiquitous practice of approximating a smoothly stratified wave by a layered one. We also discuss some applications of this result to the study of the qualitative features of such waves. 0 into PostgreSQL...\n",
      "Inserting test sample 152  This research paper investigates the continuous dependence on the density for stratified steady water waves. The focus is on the interface between two layers of fluid, where the density varies continuously. The goal is to understand the effect of the density on the properties of the waves, such as their shapes and speeds. To achieve this, mathematical models are utilized that describe the behavior of stratified fluid systems. This study employs both numerical and analytical methods to investigate the continuous dependence on the density parameter. There are several key findings presented in this research. First, it is shown that the density parameter affects the existence and uniqueness of the solution, meaning that it plays a crucial role in determining the properties of the wave. Additionally, the study reveals that the density profile can have a significant impact on the nature of the waves, including their stability and energy dissipation. These observations have substantial implications for the understanding of fluid dynamics and the oceanographic processes, which rely on the behavior of stratified water waves in various environments. 1 into PostgreSQL...\n",
      "Inserting test sample 153  The multichannel Wiener filter (MWF) and its variations have been extensively applied to binaural hearing aids. However, its major drawback is the distortion of the binaural cues of the residual noise, changing the original acoustic scenario, which is of paramount importance for hearing impaired people. The MWF-IC method was previously proposed for joint speech dereverberation and noise reduction, preserving the interaural coherence (IC) of diffuse noise fields. In this work, we propose a new variation of the MWF-IC for both speech dereverberation and noise reduction, which preserves the original spatial characteristics of the residual noise for either diffuse fields or point sources. Objective measures and preliminary psychoacoustic experiments indicate the proposed method is capable of perceptually preserving the original spatialization of both types of noise, without significant performance loss in both speech dereverberation and noise reduction. 0 into PostgreSQL...\n",
      "Inserting test sample 154  This paper proposes a new method for speech dereverberation and noise reduction in binaural hearing aids, targeted for both diffusive noise fields and point noise sources. To achieve this goal, a robust blind source separation technique is developed and combined with signal processing algorithms. The proposed method utilizes a dual-microphone system to capture the incoming sound and effectively separates and attenuates the noise and reverberation from the desired speech signal. The results show remarkable performance in enhancing intelligibility and speech quality in different real-life scenarios that include noisy and reverberant environments. Furthermore, the proposed method shows superiority in terms of the signal-to-noise ratio when compared to traditional methods used in the literature. Hence, the preliminary version of the proposed technique exhibits a promising solution for speech dereverberation and noise reduction in binaural hearing aids. 1 into PostgreSQL...\n",
      "Inserting test sample 155  Constrained Instanton and Baryon Number Non--Conservation at High Energies, P.G.Silvestrov, BUDKERINP 92--92. The total cross-section for baryon number violating processes at high energies is usually parametrized as $\\sigma_{total}\\propto\\exp(\\frac{4\\pi}{\\alpha} F(\\varepsilon))$, where $\\varepsilon =\\sqrt{s}/E_0 , \\,\\, E_0 = \\sqrt{6} \\pi m_w/\\alpha$. In the present paper the third nontrivial term of the expansion \\[ F(\\varepsilon)= -1+\\frac{9}{8}\\varepsilon^{4/3} -\\frac{9}{16}\\varepsilon^2 -\\frac{9}{32} \\left( \\frac{m_h}{m_w}\\right)^2 \\varepsilon^{8/3}\\log\\left( \\frac{1}{3\\varepsilon}\\left( \\frac{2m_w}{\\gamma m_h}\\right)^2 \\right) + O(\\varepsilon^{8/3}) \\] is obtained.The unknown corrections to $F(\\varepsilon)$ are expected to be of the order of $\\varepsilon^{8/3}$, but have neither $(m_h/m_w)^2$, nor $\\log(\\varepsilon)$ enhancement. The total cross-section is extremely sensitive to the value of single Instanton action. The correction to Instanton action $\\triangle S\\sim (m\\rho)^4 \\log(m\\rho)/g^2$ is found ($\\rho$ is the Instanton radius). For sufficiently heavy Higgs boson the $\\rho$ -dependent part of the Instanton action is changed drastically. In this case even the leading contribution to $F(\\varepsilon)$, responsible for a growth of cross-section due to the multiple production of classical W-bosons, is changed: \\[ F(\\varepsilon)=-1+ \\frac{9}{8}\\left( \\frac{2}{3} \\right)^{2/3} \\varepsilon^{4/3} +\\ldots \\,\\, , \\,\\, \\varepsilon\\ll 1\\ll \\varepsilon \\left( \\frac{m_h}{m_w} \\right)^{3/2} \\,\\, . \\] 0 into PostgreSQL...\n",
      "Inserting test sample 156  The study of baryon number non-conservation at high energies has been a topic of increasing interest in recent years. Motivated by the experimental indications of baryon-number-violating processes observed in high-energy particle physics, we investigate the scenario where the baryon number is violated. In this paper, we present a constrained instanton approach to study the thermal equilibrium of the matter system allowing for both nonzero baryon and lepton numbers. We observe that in the presence of these numbers, the dynamics of the system significantly alters, leading to novel and intriguing phenomena. Specifically, our analysis shows that the phase transition associated with electroweak symmetry breaking is considerably affected by the presence of baryon and lepton numbers, leading to a significant influence on Higgs boson dynamics. Interesting observations are also made in the context of thermal baryogenesis through an out-of-equilibrium decay of the Higgs field. Our study shows that these effects are key features to consider, particularly when making cosmological implications. Lastly, we discuss the potential implications for future experiments, emphasizing the importance of further theoretical explorations, specifically related to the nature of the phase transitions and their consequences for primordial nucleosynthesis. 1 into PostgreSQL...\n",
      "Inserting test sample 157  This chapter presents some novel information theoretic results for the analysis of stationary time series in the frequency domain. In particular, the spectral distribution that corresponds to the most uncertain or unpredictable time series with some values of the autocovariance function fixed, is the generalized von Mises spectral distribution. It is thus a maximum entropy spectral distribution and the corresponding stationary time series is called the generalized von Mises time series. The generalized von Mises distribution is used in directional statistics for modelling planar directions that follow a multimodal distribution. Furthermore, the Gaussian-generalized von Mises times series is presented as the stationary time series that maximizes entropies in frequency and time domains, respectively referred to as spectral and temporal entropies. Parameter estimation and some computational aspects with this time series are briefly analyzed. 0 into PostgreSQL...\n",
      "Inserting test sample 158  This paper investigates the information theoretic properties of two types of time series, namely stationary time series and Gaussian-generalized von Mises time series. We analyze the Shannon entropy and mutual information of these classes of time series, and derive exact expressions for their asymptotic behavior. In particular, we show that the entropy rate of stationary time series converges to the entropy rate of a related entropy system, while the mutual information of Gaussian-generalized von Mises time series converges to the mutual information of a Gaussian entropy system. Additionally, we apply these theoretical results to a practical problem of signal detection. We derive optimal detection performance for a binary hypothesis test, and show that the performance is closely related to the information theoretic properties of the time series. Our findings shed new light on the fundamental properties of time series, and provide useful insights for signal processing applications. 1 into PostgreSQL...\n",
      "Inserting test sample 159  Conventional lattice Boltzmann models only satisfy moment isotropy up to fourth order. In order to accurately describe improtant physical effects beyond the isothermal Navier-Stokes fluid regime, higher order isotropy is required.\n",
      "\n",
      "In this paper, we present some basic results on moment isotropy and its relationship to the rotational symmetry of a generating discrete vector set.\n",
      "\n",
      "The anslysis provides a geometric understanding for popular lattice Boltzmann models, while offering a systematic procedure to construct higher order models. 0 into PostgreSQL...\n",
      "Inserting test sample 160  This work addresses the issue of high-order lattice Boltzmann models and their relationship with discrete rotational symmetry and moment isotropy. The proposed approach relies on a moment-based framework for the derivation of mesoscopic models. By applying this technique, we demonstrate that it is possible to derive stable and accurate models that exhibit the desired symmetries. Moreover, we provide an extension of our framework to account for higher-order moments of the distribution function, and we demonstrate its effectiveness using numerical experiments. 1 into PostgreSQL...\n",
      "Inserting test sample 161  It has become increasingly common to collect high-dimensional binary data; for example, with the emergence of new sampling techniques in ecology. In smaller dimensions, multivariate probit (MVP) models are routinely used for inferences. However, algorithms for fitting such models face issues in scaling up to high dimensions due to the intractability of the likelihood, involving an integral over a multivariate normal distribution having no analytic form.\n",
      "\n",
      "Although a variety of algorithms have been proposed to approximate this intractable integral, these approaches are difficult to implement and/or inaccurate in high dimensions. We propose a two-stage Bayesian approach for inference on model parameters while taking care of the uncertainty propagation between the stages. We use the special structure of latent Gaussian models to reduce the highly expensive computation involved in joint parameter estimation to focus inference on marginal distributions of model parameters. This essentially makes the method embarrassingly parallel for both stages. We illustrate performance in simulations and applications to joint species distribution modeling in ecology. 0 into PostgreSQL...\n",
      "Inserting test sample 162  This paper proposes a Bayesian approach for analyzing high-dimensional multivariate binary data. In high-dimensional problems, the number of variables far exceeds the number of observations, leading to computational challenges and spurious inference. We develop a hierarchical Bayesian model, which combines a multivariate probit model with a spike-and-slab prior on the loadings matrix. The spike-and-slab prior encourages sparsity in the loadings, and effectively performs variable selection, resulting in a parsimonious representation of the data. We propose a computationally efficient Markov chain Monte Carlo algorithm for posterior inference. We evaluated the proposed method through a simulation study and a real data analysis, and compared its performance to alternative methods. Results indicate that our method outperforms existing methods in terms of prediction accuracy and variable selection consistency. We conclude that the proposed Bayesian method can handle high-dimensional multivariate binary data and provides an effective tool for data analysis. 1 into PostgreSQL...\n",
      "Inserting test sample 163  This paper proves that given a doubling weight $w$ on the unit sphere $\\mathbb{S}^{d-1}$ of $\\mathbb{R}^d$, there exists a positive constant $K_w$ such that for each positive integer $n$ and each integer $N\\geq \\max_{x\\in \\mathbb{S}^{d-1}} \\frac {K_w} {w(B(x, n^{-1}))}$, there exists a set of $N$ distinct nodes $z_1,\\cdots, z_N$ on $\\mathbb{S}^{d-1}$ which admits a strict Chebyshev-type cubature formula (CF) of degree $n$ for the measure $w(x) d\\sigma_d(x)$, $$ \\frac 1{w(\\mathbb{S}^{d-1})} \\int_{\\mathbb{S}^{d-1}} f(x) w(x)\\, d\\sigma_d(x)=\\frac 1N \\sum_{j=1}^N f(z_j),\\ \\ \\forall f\\in\\Pi_n^d, $$ and which, if in addition $w\\in L^\\infty(\\mathbb{S}^{d-1})$, satisfies $$\\min_{1\\leq i\\neq j\\leq N}\\mathtt{d}(z_i,z_j)\\geq c_{w,d} N^{-\\frac1{d-1}}$$ for some positive constant $c_{w,d}$. Here, $d\\sigma_d$ and $\\mathtt{d}(\\cdot, \\cdot)$ denote the surface Lebesgue measure and the geodesic distance on $\\mathbb{S}^{d-1}$ respectively, $B(x,r)$ denotes the spherical cap with center $x\\in\\mathbb{S}^{d-1}$ and radius $r>0$, $w(E)=\\int_E w(x) \\, d\\sigma_d(x)$ for $E\\subset\\mathbb{S}^{d-1}$, and $\\Pi_n^d$ denotes the space of all spherical polynomials of degree at most $n$ on $\\mathbb{S}^{d-1}$. It is also shown that the minimal number of nodes $\\mathcal{N}_{n} (wd\\sigma_d)$ in a strict Chebyshev-type CF of degree $n$ for a doubling weight $w$ on $\\mathbb{S}^{d-1}$ satisfies $$\\mathcal{N}_n (wd\\sigma_d) \\sim \\max_{x\\in \\mathbb{S}^{d-1}} \\frac 1 {w(B(x, n^{-1}))},\\ \\ n=1,2,\\cdots.$$ Proofs of these results rely on new convex partitions of $\\mathbb{S}^{d-1}$ that are regular with respect to a given weight $w$ and integer $N$. Our results extend the recent results of Bondarenko, Radchenko, and Viazovska on spherical designs ({\\it Ann. of Math.\n",
      "\n",
      "(2)} {\\bf 178}(2013), no. 2, 443--452,{\\it Constr. Approx.} {\\bf 41}(2015), no.\n",
      "\n",
      "1, 93--112). 0 into PostgreSQL...\n",
      "Inserting test sample 164  This paper presents a comprehensive study of Chebyshev-type cubature formulas for doubling weights over spheres, balls and simplexes. More specifically, we focus on the development of numerical integration methods that accurately approximate integrals of functions over these geometries. First, we introduce the theoretical foundations of the Chebyshev cubature method, highlighting its unique properties in comparison to other numerical integration techniques. Then, we present our findings on how best to apply Chebyshev-type cubature formulas when computing integrals of functions on spheres, balls and simplexes. We also discuss the results of various numerical experiments that validate the effectiveness of our approach. \n",
      "\n",
      "One of the primary contributions of this work is a comprehensive analysis of the accuracy and efficiency of Chebyshev-type cubature formulas when used in conjunction with doubling weights. We demonstrate that these formulas can effectively approximate integrals of functions with higher accuracy and fewer points in comparison to other integration methods. We also present a comparison of our approach with several state-of-the-art numerical integration methods to substantiate its superiority. \n",
      "\n",
      "Moreover, this paper also provides a detailed analysis of the effect of weights of different degrees on the accuracy of the Chebyshev cubature method. We explore a variety of weights and show how they can be adjusted to optimize the integrative process. Additionally, we present a new theoretical framework for evaluating the error bounds of Chebyshev-type cubature formulas, which yields tighter bounds than prior analyses. \n",
      "\n",
      "Overall, the findings of this paper have important implications for numerical integration in fields such as physics, engineering, and applied mathematics. By providing a more accurate and efficient method, our approach can lead to advancements in complex modeling and simulation. 1 into PostgreSQL...\n",
      "Inserting test sample 165  Thermodynamics is a highly successful macroscopic theory widely used across the natural sciences and for the construction of everyday devices, from car engines and fridges to power plants and solar cells. With thermodynamics predating quantum theory, research now aims to uncover the thermodynamic laws that govern finite size systems which may in addition host quantum effects.\n",
      "\n",
      "Here we identify information processing tasks, the so-called \"projections\", that can only be formulated within the framework of quantum mechanics. We show that the physical realisation of such projections can come with a non-trivial thermodynamic work only for quantum states with coherences. This contrasts with information erasure, first investigated by Landauer, for which a thermodynamic work cost applies for classical and quantum erasure alike. Implications are far-reaching, adding a thermodynamic dimension to measurements performed in quantum thermodynamics experiments, and providing key input for the construction of a future quantum thermodynamic framework. Repercussions are discussed for quantum work fluctuation relations and thermodynamic single-shot approaches. 0 into PostgreSQL...\n",
      "Inserting test sample 166  Quantum thermodynamics has become an increasingly important field in the past decade. In particular, coherence and measurement have emerged as key concepts in the study of thermodynamic systems at the quantum level. In this paper, we explore the role of coherence and measurement in quantum thermodynamics, providing new insights into the fundamental nature of quantum systems. We begin by discussing the concept of coherence and its relation to thermodynamics, highlighting recent experimental and theoretical advances. We then examine the measurement process and its impact on quantum thermodynamics, focusing on recent work in this area. Finally, we conclude by discussing the implications of our findings for future research in the field and the potential practical applications of coherence and measurement in quantum thermodynamics. This work provides a deeper understanding of the fundamental nature of quantum systems and their behavior in thermodynamic processes. 1 into PostgreSQL...\n",
      "Inserting test sample 167  Self-organization, the ability of a system of microscopically interacting entities to shape macroscopically ordered structures, is ubiquitous in Nature.\n",
      "\n",
      "Spatio-temporal patterns are abundantly observed in a large plethora of applications, encompassing different fields and scales. Examples of emerging patterns are the spots and stripes on the coat or skin of animals, the spatial distribution of vegetation in arid areas, the organization of the colonies of insects in host-parasitoid systems and the architecture of large complex ecosystems. Spatial self-organization can be described following the visionary intuition of Alan Turing, who showed how non-linear interactions between slow diffusing activators and fast diffusing inhibitors could induce patterns. The Turing instability, as the mechanism described is universally referred to, was raised to paradigm status in those realms of investigations where microscopic entities are subject to diffusion, from small biological systems to large ecosystems. Requiring a significant ratio of the assigned diffusion constants however is a stringent constraint, which limited the applicability of the theory. Building on the observation that spatial interactions are usually direction biased, and often strongly asymmetric, we here propose a novel framework for the generation of short wavelength patterns which overcomes the limitation inherent in the Turing formulation. In particular, we will prove that patterns can always set in when the system is composed by sufficiently many cells - the units of spatial patchiness - and for virtually any ratio of the diffusivities involved. Macroscopic patterns that follow the onset of the instability are robust and show oscillatory or steady-state behavior. 0 into PostgreSQL...\n",
      "Inserting test sample 168  This study explores a universal mechanism for the emergence of patterns in complex systems. We investigated a broad range of systems both in nature and in the laboratory, including chemical reactions, biological systems, and physical processes. By applying a unified mathematical framework, we found that many diverse systems share similar mechanisms of pattern formation.\n",
      "\n",
      "Our theory suggests that patterns emerge from a type of instability that causes small fluctuations to grow and self-organize into complex structures. Specifically, we propose that many complex systems exhibit a Turing-like instability, in which diffusive instabilities interact to produce a range of complex patterns, from stripes and spots to more intricate structures.\n",
      "\n",
      "Our results indicate that the mechanism of pattern formation is not limited to certain types of systems, but rather represents a universal route that is present in a diverse range of disciplines. This finding offers important insights into the fundamental principles underlying biological, chemical, and physical systems, and may have implications for fields such as materials science and engineering.\n",
      "\n",
      "Overall, our research provides a mechanism for understanding the emergence of patterns in complex systems, and suggests a common framework for modeling and predicting the behavior of diverse systems. By demonstrating that pattern formation is a universal phenomenon in complex systems, our research has the potential to accelerate the development of new technologies and advance our understanding of the natural world. 1 into PostgreSQL...\n",
      "Inserting test sample 169  This paper establishes two things in an asymptotically (anti-)de Sitter spacetime, by direct computations in the physical spacetime (i.e. with no involvement of spacetime compactification): (1) The peeling property of the Weyl spinor is guaranteed. In the case where there are Maxwell fields present, the peeling properties of both Weyl and Maxwell spinors similarly hold, if the leading order term of the spin coefficient $\\rho$ when expanded as inverse powers of $r$ (where $r$ is the usual spherical radial coordinate, and $r\\rightarrow\\infty$ is null infinity, $\\mathcal{I}$) has coefficient $-1$. (2) In the absence of gravitational radiation (a conformally flat $\\mathcal{I}$), the group of asymptotic symmetries is trivial, with no room for supertranslations. 0 into PostgreSQL...\n",
      "Inserting test sample 170  In this paper, we investigate the peeling property and asymptotic symmetries in the presence of a cosmological constant. We consider the dynamics of gravitational waves in a curved spacetime with a positive cosmological constant, and identify the leading order behavior of the fields at null infinity. We then study the algebra of the asymptotic symmetries, and derive the corresponding charges associated with these symmetries. Our analysis reveals that the peeling property holds in the presence of a cosmological constant, but with a modified decay rate. Additionally, we find that the asymptotic symmetry algebra is isomorphic to a semi-direct sum of the BMS algebra and an Abelian ideal. These results have important implications for the understanding of the dynamics of gravitational waves in a cosmological setting. 1 into PostgreSQL...\n",
      "Inserting test sample 171  We study a particular N = 1 confining gauge theory with fundamental flavors realised as seven branes in the background of wrapped five branes on a rigid two-cycle of a non-trivial global geometry. In parts of the moduli space, the five branes form bound states with the seven branes. We show that in this regime the local supergravity solution is surprisingly tractable, even though the background topology is non-trivial. New effects such as dipole deformations may be studied in detail, including the full backreactions. Performing the dipole deformations in other ways leads to different warped local geometries.\n",
      "\n",
      "In the dual heterotic picture, which is locally given by a C* fibration over a Kodaira surface, we study details of the geometry and the construction of bundles. We also point out the existence of certain exotic bundles in our framework. 0 into PostgreSQL...\n",
      "Inserting test sample 172  In this paper, we explore the properties of dipole-deformed bound states on heterotic Kodaira surfaces. We demonstrate that such states arise naturally in certain geometric settings, and investigate their behavior under various deformations. Specifically, we consider the deformation of the heterotic worldsheet which preserves the underlying lattice structure, and show that this leads to a novel class of dipole-bound states. Utilizing techniques from algebraic geometry and quantum field theory, we establish expressions for various observables associated with these states, and compare our results to previous theoretical predictions. Our analysis sheds new light on the interplay between geometry and physics in heterotic string theory, and has implications for our understanding of the fundamental properties of matter and energy. 1 into PostgreSQL...\n",
      "Inserting test sample 173  We report studies of CaCo{1.86}As2 single crystals. The electronic structure is probed by angle-resolved photoemission spectroscopy (ARPES) measurements of CaCo{1.86}As2 and by full-potential linearized augmented-plane-wave calculations for the supercell Ca8Co15As16 (CaCo{1.88}As2). Our XRD crystal structure refinement is consistent with the previous combined refinement of x-ray and neutron powder diffraction data showing a collapsed-tetragonal ThCr2Si2-type structure with 7(1)% vacancies on the Co sites corresponding to the composition CaCo{1.86}As2 [D. G. Quirinale et al., Phys. Rev. B 88, 174420 (2013)]. The anisotropic magnetic susceptibility chi(T) data are consistent with the magnetic neutron diffraction data of Quirianale et al. that demonstrate the presence of A-type collinear antiferromagnetic order below the Neel temperature TN = 52(1) K with the easy axis being the tetragonal c axis.\n",
      "\n",
      "However, no clear evidence from the resistivity rho(T) and heat capacity Cp(T) data for a magnetic transition at TN is observed. A metallic ground state is demonstrated from band calculations and the rho(T), Cp(T) and ARPES data, and spin-polarized calculations indicate a competition between the A-type AFM and FM ground states. The Cp(T) data exhibit a large Sommerfield electronic coefficient reflecting a large density of states at the Fermi energy D(EF), consistent with the band structure calculations which also indicate a large D(EF) arising from Co 3d bands. At 1.8 K the M(H) data for H|| c exhibit a well-defined first-order spin-flop transition at an applied field of 3.5 T. The small ordered moment of 0.3 muB/Co obtained from the M(H) data at low T, the large exchange enhancement of chi and the lack of a self-consistent interpretation of the chi(T) and M(H,T) data in terms of a local moment Heisenberg model together indicate that the magnetism of CaCo{1.86}As2 is itinerant. 0 into PostgreSQL...\n",
      "Inserting test sample 174  This research focuses on the exploration of physical properties of CaCo{1.86}As2 single crystals. Metallic antiferromagnetic crystal structures have been widely investigated over the years, but there are still major gaps which require further examination. This particular compound of CaCo{1.86}As2 has previously been shown to have desirable physical properties, including high-temperature superconductivity and magnetic properties. \n",
      "\n",
      "Single crystals were grown using the self-flux method, and their properties were characterized using various techniques such as X-ray diffraction, scanning electron microscopy (SEM), and energy-dispersive X-ray spectroscopy (EDS). The X-ray diffraction data confirms that the single crystals belong to the tetragonal structure of the ThCr{2}Si{2} type, and SEM together with EDS results illustrate high-quality single crystals of CaCo{1.86}As2.\n",
      "\n",
      "The physical properties of CaCo{1.86}As2 single crystals were then investigated using a range of techniques. We found that CaCo{1.86}As2 has significant electrical resistivity, which indicates that the compound has strong electron interaction potential. Furthermore, the electrical resistivity shows a temperature-dependent phase transition at around 88 K. Heat capacity measurements showed that the compound exhibits a state of heavy fermion behavior with a cohesive energy scale of around 30 K, indicating that this material has potential for high-temperature superconductivity.\n",
      "\n",
      "Antiferromagnetic measurements were also conducted on the CaCo{1.86}As2 single crystals. The results showed the existence of both commensurate and incommensurate antiferromagnetic correlations. The commensurate ordering indicated that the crystal structure exhibits a magnetic density wave. \n",
      "\n",
      "Based on our results, we can say that CaCo{1.86}As2 single crystals exhibit multiple physical properties that make them promising candidates for further research in high-temperature superconductivity and magnetic properties. Our findings contribute to the understanding of the physical properties of metallic antiferromagnetic compounds. 1 into PostgreSQL...\n",
      "Inserting test sample 175  We study the correlation of photon and charged lepton pseudorapidities, $\\eta(\\gamma)$ and $\\eta(\\ell)$, $\\ell=e,\\,\\mu$, in $p\\,\\pbar \\rightarrow W^\\pm\\gamma+X\\rightarrow \\ell^\\pm p\\llap/_T\\gamma+X$. In the Standard Model, the $\\Delta\\eta(\\gamma,\\ell)= \\eta(\\gamma) - \\eta(\\ell)$ differential cross section is found to exhibit a pronounced dip at $\\Delta\\eta(\\gamma,\\ell) \\approx \\mp 0.3$ ($=0$)in $p\\bar p$ ($pp$) collisions, which originates from the radiation zero present in $q\\bar q'\\rightarrow W\\gamma$. The sensitivity of the $\\Delta\\eta(\\gamma,\\ell)$ distribution to higher order QCD corrections, non-standard $WW\\gamma$ couplings, the $W+$~jet ``fake'' background and the cuts imposed is explored. At hadron supercolliders, next-to-leading order QCD corrections are found to considerably obscure the radiation zero. The advantages of the $\\Delta\\eta(\\gamma,\\ell)$ distribution over other quantities which are sensitive to the radiation zero are discussed. We conclude that photon lepton rapidity correlations at the Tevatron offer a {\\it unique} opportunity to search for the Standard Model radiation zero in hadronic $W\\gamma$ production. 0 into PostgreSQL...\n",
      "Inserting test sample 176  The production of a $W$ boson in association with a photon, $W\\gamma$, is a rare phenomenon that provides valuable information on the electroweak sector of the Standard Model. The measurement of rapidity correlations between the $W$ and $\\gamma$ in $p\\bar{p}$ collisions at $\\sqrt{s} = 1.96$ TeV was performed using the D0 detector at the Fermilab Tevatron Collider. This paper presents the analysis of the data collected during the 2002-2006 D0 run, corresponding to an integrated luminosity of $1.0$ fb$^{-1}$. The measured $W\\gamma$ cross section is found to be in agreement with theoretical predictions, while the rapidity correlation between the $W$ and $\\gamma$ is found to be stronger than expected. This result provides a better understanding of the electroweak interactions and can be used as a test of the Standard Model. The analysis technique developed for this measurement is applicable to similar studies, such as $Z\\gamma$ production, which may shed light on new physics beyond the Standard Model. 1 into PostgreSQL...\n",
      "Inserting test sample 177  A $(p,q,r)$-board that has $pq+pr+qr$ squares consists of a $(p,q)$-, a $(p,r)$-, and a $(q,r)$-rectangle. Let $S$ be the set of the squares. Consider a bijection $f : S \\to [1,pq+pr+qr]$. Firstly, for $1 \\le i \\le p$, let $x_i$ be the sum of all the $q+r$ integers in the $i$-th row of the $(p,q+r)$-rectangle. Secondly, for $1 \\le j \\le q$, let $y_j$ be the sum of all the $p+r$ integers in the $j$-th row of the $(q,p+r)$-rectangle. Finally, for $1\\le k\\le r$, let $z_k$ be the the sum of all the $p+q$ integers in the $k$-th row of the $(r,p+q)$-rectangle. Such an assignment is called a $(p,q,r)$-design if $\\{x_i : 1\\le i\\le p\\}=\\{c_1\\}$ for some constant $c_1$, $\\{y_j : 1\\le j\\le q\\}=\\{c_2\\}$ for some constant $c_2$, and $\\{z_k : 1\\le k\\le r\\}=\\{c_3\\}$ for some constant $c_3$. A $(p,q,r)$-board that admits a $(p,q,r)$-design is called (1) Cartesian tri-magic if $c_1$, $c_2$ and $c_3$ are all distinct; (2) Cartesian bi-magic if $c_1$, $c_2$ and $c_3$ assume exactly 2 distinct values; (3) Cartesian magic if $c_1 = c_2 = c_3$ (which is equivalent to supermagic labeling of $K(p,q,r)$). Thus, Cartesian magicness is a generalization of magic rectangles into 3-dimensional space. In this paper, we study the Cartesian magicness of various $(p,q,r)$-board by matrix approach involving magic squares or rectangles. In Section~2, we obtained various sufficient conditions for $(p,q,r)$-boards to admit a Cartesian tri-magic design. In Sections~3 and~4, we obtained many necessary and (or) sufficient conditions for various $(p,q,r)$-boards to admit (or not admit) a Cartesian bi-magic and magic design.\n",
      "\n",
      "In particular, it is known that $K(p,p,p)$ is supermagic and thus every $(p,p,p)$-board is Cartesian magic. We gave a short and simpler proof that every $(p,p,p)$-board is Cartesian magic. 0 into PostgreSQL...\n",
      "Inserting test sample 178  The Cartesian magicness of 3-dimensional boards is a fascinating topic that has received little attention in the research community. In this paper, we seek to explore the intricate nature of 3D boards and the magicness they possess through their relation to Cartesian mathematics.\n",
      "\n",
      "Our study begins by examining the fundamental principles of Cartesian coordinates systems and how they reflect the intricacies of 3D boards. We clarify the terminology surrounding Cartesian magic, establish clear definitions, and investigate mathematical patterns that indicate the presence of Cartesian magicness in 3D boards.\n",
      "\n",
      "We present a comprehensive set of experiments designed to investigate the relationship between Cartesian magicness and visual perception. Our focus is on determining whether a correlation exists between the presence of Cartesian magicness and viewer perception of 3D boards.\n",
      "\n",
      "Our findings indicate that 3D boards possess a significant level of Cartesian magicness that is detected by the human eye. We also observe that the presence of Cartesian magicness enhances the viewer's perception of the visual depth of the board, making it easier for them to understand the three-dimensional nature of the board.\n",
      "\n",
      "Moreover, we discuss the potential applications of our research findings in various fields, including architecture, gaming, and virtual reality. Our results have implications for the design of 3D board games and virtual environments, which can be tailored to maximize their visually appealing and interactive nature.\n",
      "\n",
      "In conclusion, this paper provides an in-depth investigation of the Cartesian magicness of 3-dimensional boards. Our research findings reveal that 3D boards have a unique mathematical quality that enhances their visual depth and appeal, and this has potential applications in a variety of fields. 1 into PostgreSQL...\n",
      "Inserting test sample 179  We investigate the osmotic (electrostatic) pressure acting on the proteinaceous shell of a generic model of virus-like particles (VLPs), comprising a charged outer shell and a metallic nanoparticle core, coated by a charged layer and bathed in an aqueous electrolyte (salt) solution. Motivated by the recent studies accentuating the role of multivalent ions for the stability of VLPs, we focus on the effects of multivalent cations and anions in an otherwise monovalent ionic bathing solution. We perform extensive Monte-Carlo simulations based on appropriate Coulombic interactions that consistently take into account the effects of salt screening, the dielectric polarization of the metallic core, and the strong-coupling electrostatics due to the presence of multivalent ions. We specifically study the intricate roles these factors play in the electrostatic stability of the model VLPs. It is shown that while the insertion of a metallic nanoparticle by itself can produce negative, inward-directed, pressure on the outer shell, addition of only a small amount of multivalent counterions can robustly engender negative pressures, enhancing the VLP stability across a wide range of values for the system parameters. 0 into PostgreSQL...\n",
      "Inserting test sample 180  This research investigates the role of a metallic core in maintaining the stability of virus-like particles (VLPs) when exposed to strongly coupled electrostatics. In order to understand the impact of electrostatic coupling on VLPs, we performed extensive experiments and numerical simulations, which reveal that the overall stability of VLPs is significantly influenced by the metallic core. Our results show that the metallic core acts as a stabilizer, preventing the VLPs from rupturing when exposed to strong electrostatic fields. Moreover, we found that the stability of VLPs is highly sensitive to the type of metallic core used. Our study provides useful insights into the fundamental principles governing the behavior of VLPs under extreme conditions and contributes to the development of more robust and stable VLP-based applications. These findings have important implications for future research on VLPs and their applications in biomedicine, drug delivery, and nanotechnology. 1 into PostgreSQL...\n",
      "Inserting test sample 181  Because of colour confinement, the physical vacuum forms an event horizon for quarks and gluons; this can be crossed only by quantum tunneling, i.e., through the QCD counterpart of Hawking radiation by black holes. Since such radiation cannot transmit information to the outside, it must be thermal, of a temperature determined by the strong force at the confinement surface, and it must maintain colour neutrality. The resulting process provides a common mechanism for thermal hadron production in high energy interactions, from $e^+e^-$ annihilation to heavy ion collisions. The analogy with black-hole event horizon suggests a dependence of the hadronization temperature on the baryon density. 0 into PostgreSQL...\n",
      "Inserting test sample 182  In this research paper, we explore the connection between thermal hadronization, Hawking-Unruh radiation and event horizons in Quantum Chromodynamics (QCD). We discuss the thermodynamic properties of the QCD plasma, the hadronization process, and how it relates to the creation of event horizons. We present numerical simulations using lattice QCD which show the formation of a thin-film structure at the event horizon. We also investigate the Hawking-Unruh radiation phenomena in QCD and the possibility of observing it in heavy-ion collisions. Overall, our findings suggest that thermal hadronization, Hawking-Unruh radiation and event horizons play a crucial role in the study of QCD at high temperatures and densities. 1 into PostgreSQL...\n",
      "Inserting test sample 183  We use a sample of 87 rest-frame UV-selected star-forming galaxies with mean spectroscopic redshift z=2.26 to study the correlation between metallicity and stellar mass at high redshift. Using stellar masses determined from SED fitting to 0.3-8 micron photometry, we divide the sample into six bins in stellar mass, and construct six composite H-alpha+[NII] spectra from all of the objects in each bin. We estimate the mean oxygen abundance in each bin from the [NII]/H-alpha ratio, and find a monotonic increase in metallicity with increasing stellar mass, from 12+log(O/H) < 8.2 for galaxies with <M_star> = 2.7e9 Msun to 12+log(O/H) = 8.6 for galaxies with <M_star> = 1e11 Msun. We use the empirical relation between star formation rate density and gas density to estimate the gas fractions of the galaxies, finding an increase in gas fraction with decreasing stellar mass. These gas fractions combined with the observed metallicities allow the estimation of the effective yield y_eff as a function of stellar mass; in constrast to observations in the local universe which show a decrease in y_eff with decreasing baryonic mass, we find a slight increase.\n",
      "\n",
      "Such a variation of metallicity with gas fraction is best fit by a model with supersolar yield and an outflow rate ~4 times higher than the star formation rate. We conclude that the mass-metallicity relation at high redshift is driven by the increase in metallicity as the gas fraction decreases through star formation, and is likely modulated by metal loss from strong outflows in galaxies of all masses. There is no evidence for preferential loss of metals from low mass galaxies as has been suggested in the local universe. [Abridged] 0 into PostgreSQL...\n",
      "Inserting test sample 184  The mass-metallicity relation is a fundamental tool for understanding the chemical evolution of galaxies. At high redshifts, the relation is critical in exploring the early stages of galaxy formation. In this paper, we investigate the mass-metallicity relation at z~2 by studying the chemical properties of galaxies in the COSMOS field. We use a sample of 161 star-forming galaxies at z~2 with resolved spectroscopy from the FMOS-COSMOS survey. \n",
      "\n",
      "We first examine the relation between stellar mass and gas-phase metallicity. We find a clear correlation, where more massive galaxies tend to have higher metallicities. The slope of this relation is in agreement with previous studies at low and high redshifts. However, the normalization of the relation is lower than seen at lower redshifts, suggesting a lower metallicity evolution rate for massive galaxies at z~2. \n",
      "\n",
      "We also investigate the effects of galaxy size and star formation rate on the mass-metallicity relation. Our results indicate that the relation is not significantly affected by galaxy size or star formation rate at z~2. Additionally, we explore the dependence of the mass-metallicity relation on redshift, comparing our findings at z~2 with previous studies at lower and higher redshifts. \n",
      "\n",
      "Overall, our study sheds light on the processes driving the chemical enrichment of galaxies in the early universe. Our findings at z~2 are consistent with the notion that massive galaxies form their stars early and undergo a faster metallicity enrichment than low mass galaxies. Our results also offer insights into the implications of various galaxy properties for the interpretation of observations obtained with upcoming telescopes. 1 into PostgreSQL...\n",
      "Inserting test sample 185  The temperature dependence of the nonlinear current-voltage ($I$-$V$) characteristics in highly underdoped La$_{2-x}$Sr$_{x}$CuO$_{4}$ ($x=0.07$ and 0.08) thick films has been studied in both zero and perpendicular magnetic fields $H$. Power-law behavior of $V(I)$ is found for both $H=0$ and $H \\neq 0$. The critical current $I_{c}$ was extracted, and its temperature and magnetic field dependences were studied in detail. The Berezinskii-Kosterlitz-Thouless physics dominates the nonlinear $I$-$V$ near the superconducting transition at $H=0$, and it continues to contribute up to a characteristic temperature $T_x(H)$. Nonlinear $I$-$V$ persists up to an even higher temperature $T_{h}(H)$ due to the depinning of vortices. 0 into PostgreSQL...\n",
      "Inserting test sample 186  The present study reports on the characterization of current-voltage (IV) characteristics and vortex dynamics in highly underdoped La$_{2-x}$Sr$_{x}$CuO$_{4}$. Magneto-transport measurements and scanning Hall probe microscopy have been carried out at various doping levels to investigate the behavior of vortex matter in the underdoped regime. Our results show the existence of a novel dynamic state characterized by the coexistence of multiple vortex phases at low fields and temperatures, which is intimately related to the nonlinearity and hysteresis of the IV curves. Furthermore, we establish a correlation between the appearance of this state and the onset of strong superconducting fluctuations, providing new insights into the interplay between vortex and superconducting dynamics in cuprates. 1 into PostgreSQL...\n",
      "Inserting test sample 187  We give two fully dynamic algorithms that maintain a $(1+\\varepsilon)$-approximation of the weight $M$ of a minimum spanning forest (MSF) of an $n$-node graph $G$ with edges weights in $[1,W]$, for any $\\varepsilon>0$.\n",
      "\n",
      "(1) Our deterministic algorithm takes $O({W^2 \\log W}/{\\varepsilon^3})$ worst-case update time, which is $O(1)$ if both $W$ and $\\varepsilon$ are constants. Note that there is a lower bound by Patrascu and Demaine (SIAM J.\n",
      "\n",
      "Comput. 2006) which shows that it takes $\\Omega(\\log n)$ time per operation to maintain the exact weight of an MSF that holds even in the unweighted case, i.e. for $W=1$. We further show that any deterministic data structure that dynamically maintains the $(1+\\varepsilon)$-approximate weight of an MSF requires super constant time per operation, if $W\\geq (\\log n)^{\\omega_n(1)}$.\n",
      "\n",
      "(2) Our randomized (Monte-Carlo style) algorithm works with high probability and runs in worst-case $O(\\log W/ \\varepsilon^{4})$ update time if $W= O({(m^*)^{1/6}}/{\\log^{2/3} n})$, where $m^*$ is the minimum number of edges in the graph throughout all the updates. It works even against an adaptive adversary. This implies a randomized algorithm with worst-case $o(\\log n)$ update time, whenever $W=\\min\\{O((m^*)^{1/6}/\\log^{2/3} n), 2^{o({\\log n})}\\}$ and $\\varepsilon$ is constant. We complement this result by showing that for any constant $\\varepsilon,\\alpha>0$ and $W=n^{\\alpha}$, any (randomized) data structure that dynamically maintains the weight of an MSF of a graph $G$ with edge weights in $[1,W]$ and $W = \\Omega(\\varepsilon m^*)$ within a multiplicative factor of $(1+\\varepsilon)$ takes $\\Omega(\\log n)$ time per operation. 0 into PostgreSQL...\n",
      "Inserting test sample 188  In graph theory, the minimum spanning forest (MSF) is a fundamental problem with many practical applications in network design and optimization. However, computing the exact MSF is often computationally expensive, especially for large graphs. In this paper, we propose a novel algorithm for approximating the MSF in constant time.\n",
      "\n",
      "Our algorithm is based on a dynamic weight assignment scheme which assigns weights to edges based on their significance in the MSF. We introduce a new data structure, called the Dynamic Weight Approximation Tree (DWAT), to efficiently compute and maintain these weights.\n",
      "\n",
      "We prove that our algorithm has a constant time complexity regardless of the size of the graph, making it highly scalable and effective for both small and large graphs. Furthermore, we present extensive experimental results demonstrating the efficiency and effectiveness of our approach on a wide range of graphs.\n",
      "\n",
      "Our algorithm offers a significant improvement over existing MSF approximation algorithms and provides a powerful tool for solving MSF problems in real-world applications. We believe that the insights and techniques presented in this paper will help researchers and practitioners to better understand and solve similar problems in the field of network optimization.\n",
      "\n",
      "In conclusion, our proposed algorithm provides an efficient and scalable solution to the MSF problem using a dynamic weight approximation technique. Our experimental results demonstrate the effectiveness of our approach on a wide range of graph sizes, making it a powerful tool for solving real-world network optimization problems. 1 into PostgreSQL...\n",
      "Inserting test sample 189  The inconsistency about the degree of geometrical frustration has been a long issue in AV$_{2}$O$_{4}$ (A $\\equiv$ Zn, Cd and Mg) compounds, which arises from the two experimental results: (i) frustration indices and (ii) magnetic moments. In the present study, we try to understand such inconsistency by using {\\it ab initio} electronic structure calculations. The orbital degrees of freedom are found to play an important role in understanding the geometrically frustrated magnetic behaviour of these compounds. The inclusion of the orbital and spin angular momenta for calculating the frustration indices improves the understanding about the degree of geometrical frustration in these compounds.\n",
      "\n",
      "The calculated values of the frustration indices ($f$$_{\\it J}$) are largest for MgV$_{2}$O$_{4}$ and smallest for CdV$_{2}$O$_{4}$ for 3.3$\\leq$ $U \\leq$5.3 eV. In this range of $U$, the calculated values of $\\Delta$M$_{2}$=M$_{\\rm total}$-M$_{\\rm exp}$ are largest for MgV$_{2}$O$_{4}$ and smallest for CdV$_{2}$O$_{4}$. Hence, the consistency about the degree of geometrical frustration is achieved. The absolute values of the nearest neighbour exchange coupling constant ({\\it J$_{nn}$}) between V spins are found to be largest for MgV$_{2}$O$_{4}$ and smallest for CdV$_{2}$O$_{4}$, which indicate that the calculated absolute values of the Curie-Weiss temperature ($\\varTheta$$_{CW}$)$_{\\it J}$ are highest for MgV$_{2}$O$_{4}$ and smallest for CdV$_{2}$O$_{4}$ for 3.3$\\leq$ $U \\leq$5.3 eV. In this range of $U$, the magnetic transition temperature ($T$$_{N}$)$_{\\it J}$ is found to be $\\sim$150 K, $\\sim$60 K and $\\sim$22 K for MgV$_{2}$O$_{4}$, ZnV$_{2}$O$_{4}$ and CdV$_{2}$O$_{4}$, respectively, which shows that the order of ($T$$_{N}$)$_{\\it J}$ is similar to that of ($T$$_{N}$)$_{\\rm exp}$ for these compounds. 0 into PostgreSQL...\n",
      "Inserting test sample 190  This research investigates the role of the orbital degrees of freedom in understanding the magnetic properties of geometrically frustrated vanadium spinels. Geometric frustration arises when the crystal structure of a magnetic material constrains the orientation of neighboring spins, impeding the system from reaching its lowest energy state. This phenomenon is observed in certain materials, such as vanadium spinels, where the spins form a network of interconnected triangles. \n",
      "\n",
      "In this study, we focus on the vanadium spinel ZnV2O4, which has an intriguing magnetic behavior and has been shown to have frustrated spin interactions. We carry out a series of experiments to explore the relationship between the magnetic properties of this material and the orbital degrees of freedom of the vanadium ions, which are responsible for the crystal electric field effects.\n",
      "\n",
      "Using neutron scattering experiments, we are able to investigate the spin dynamics and magnetic correlations of ZnV2O4. We also perform X-ray absorption spectroscopy to examine the electronic structure and orbital occupation of the vanadium ions. By combining these techniques, we demonstrate that the magnetic properties of ZnV2O4 are strongly influenced by the orbital degrees of freedom. \n",
      "\n",
      "Our findings reveal that the orbital degrees of freedom play a crucial role in the behavior of vanadium spinels with geometric frustration. Specifically, we show that both the orbital and spin degrees of freedom contribute to the magnetic interactions in these materials, leading to rich and complex magnetic behaviors. Our study highlights the importance of fully understanding the role of both the spin and orbital degrees of freedom in the study of magnetic materials, especially those with geometric frustration. The results of this investigation may have important implications for the design of future materials with tailored magnetic properties. 1 into PostgreSQL...\n",
      "Inserting test sample 191  We investigated the main prompt and afterglow emission parameters of gamma-ray bursts detected by the Burst Alert Telescope (BAT) and X-Ray Telescope installed on the Swift satellite. Our aim was to look for differences or connections between the different types of gamma-ray bursts, so we compared the BAT fluences, 1-sec peak photon fluxes, photon indices, XRT early fluxes, initial temporal decay and spectral indices. We found that there might be a connection between the XRT initial decay index and XRT early flux/BAT photon index. Using statistical tools we also determined that beside the duration and hardness ratios, the means of the \\gamma- and X-ray--fluences and the \\gamma-ray photon index differ significantly between the three types of bursts. 0 into PostgreSQL...\n",
      "Inserting test sample 192  This paper provides a comprehensive statistical analysis of the prompt and afterglow emission properties of three distinct groups of gamma-ray bursts: short, long, and ultra-long. We present a detailed examination of the temporal and spectral characteristics of these GRBs, including peak energy, duration, light-curve behavior, and spectral evolution. In addition, we investigate the correlations between these properties, as well as their physical implications. Our findings demonstrate that the three groups differ significantly in their emission properties, which may suggest diverse progenitor systems. Moreover, the investigation of the afterglow properties of GRBs provides valuable insights into the nature of the environments in which they are produced, and hence, can be used to further our understanding of the underlying astrophysical processes. 1 into PostgreSQL...\n",
      "Inserting test sample 193  The Lopsided Lov\\'{a}sz Local Lemma (LLLL) is a powerful probabilistic principle which has been used in a variety of combinatorial constructions.\n",
      "\n",
      "While originally a general statement about probability spaces, it has recently been transformed into a variety of polynomial-time algorithms. The resampling algorithm of Moser & Tardos (2010) is the most well-known example of this. A variety of criteria have been shown for the LLLL; the strongest possible criterion was shown by Shearer, and other criteria which are easier to use computationally have been shown by Bissacot et al (2011), Pegden (2014), Kolipaka & Szegedy (2011), and Kolipaka, Szegedy, Xu (2012).\n",
      "\n",
      "We show a new criterion for the Moser-Tardos algorithm to converge. This criterion is stronger than the LLLL criterion; this is possible because it does not apply in the same generality as the original LLLL; yet, it is strong enough to cover many applications of the LLLL in combinatorics. We show a variety of new bounds and algorithms. A noteworthy application is for $k$-SAT, with bounded occurrences of variables. As shown in Gebauer, Sz\\'{a}bo, and Tardos (2011), a $k$-SAT instance in which every variable appears $L \\leq \\frac{2^{k+1}}{e (k+1)}$ times, is satisfiable. Although this bound is asymptotically tight (in $k$), we improve it to $L \\leq \\frac{2^{k+1} (1 - 1/k)^k}{k-1} - \\frac{2}{k}$ which can be significantly stronger when $k$ is small.\n",
      "\n",
      "We introduce a new parallel algorithm for the LLLL. While Moser & Tardos described a simple parallel algorithm for the Lov\\'{a}sz Local Lemma, and described a simple sequential algorithm for a form of the Lopsided Lemma, they were not able to combine the two. Our new algorithm applies in nearly all settings in which the sequential algorithm works --- this includes settings covered by our new stronger LLLL criterion. 0 into PostgreSQL...\n",
      "Inserting test sample 194  The LovÃ¡sz Local Lemma (LLL) is a powerful tool to analyze the existence of combinatorial objects. In particular, it has been used to prove the existence of objects that satisfy a set of potentially conflicting conditions. However, it has a weakness: it cannot easily deal with dependencies among these conditions. One way to address this issue is to use the Moser-Tardos framework, which allows us to deal with both independent and dependent probabilistic events. \n",
      "\n",
      "Recently, there has been a growing interest in understanding the limits of the Moser-Tardos framework. In particular, researchers have been studying the so-called \"lopsided\" scenarios, where the number of dependent conditions is much larger than the number of independent conditions. This is known as \"lopsidependency\". \n",
      "\n",
      "In this paper, we study the limits of the Moser-Tardos framework in the context of lopsided scenarios. Our main result is a new algorithm that significantly improves upon the state of the art for solving instances of the lopsided LovÃ¡sz Local Lemma. The key idea is to use a combination of resampling and local search techniques to efficiently explore the space of possible solutions. \n",
      "\n",
      "Our algorithm is based on a theoretical analysis of the Moser-Tardos framework, which allows us to identify the most promising search directions. We validate our approach by testing it on a range of instances, including some of the largest and most challenging instances of the lopsided LovÃ¡sz Local Lemma studied to date. Our experiments show that our algorithm outperforms existing methods by a wide margin, and is able to solve many previously intractable instances. \n",
      "\n",
      "Overall, our work sheds new light on the limits and capabilities of the Moser-Tardos framework in the context of lopsided scenarios. We hope that our results will inspire further investigation of these important and challenging problems. 1 into PostgreSQL...\n",
      "Inserting test sample 195  Gas detection around main sequence stars is becoming more common with around 20 systems showing the presence of CO. However, more detections are needed, especially around later spectral type stars to better understand the origin of this gas and refine our models. To do so, we carried out a survey of 10 stars with predicted high likelihoods of secondary CO detection using ALMA in band 6.\n",
      "\n",
      "We looked for continuum emission of mm-dust as well as gas emission (CO and CN transitions). The continuum emission was detected in 9/10 systems for which we derived the discs' dust masses and geometrical properties, providing the first mm-wave detection of the disc around HD 106906, the first mm-wave radius for HD 114082, 117214, HD 15745, HD 191089 and the first radius at all for HD 121191.\n",
      "\n",
      "A crucial finding of our paper is that we detect CO for the first time around the young 10-16 Myr old G1V star HD 129590, similar to our early Sun. The gas seems colocated with its planetesimal belt and its total mass is likely between $2-10 \\times 10^{-5}$ M$_\\oplus$. This first gas detection around a G-type main-sequence star raises questions as to whether gas may have been released in the Solar System as well in its youth, which could potentially have affected planet formation. We also detected CO gas around HD 121191 at a higher S/N than previously and find that the CO lies much closer-in than the planetesimals in the system, which could be evidence for the previously suspected CO viscous spreading owing to shielding preventing its photodissociation. Finally, we make estimates for the CO content in planetesimals and the HCN/CO outgassing rate (from CN upper limits), which we find are below the level seen in Solar System comets in some systems. 0 into PostgreSQL...\n",
      "Inserting test sample 196  The detection of gas around the Sun-like star HD 129590 provides valuable insights into the formation and evolution of planetary systems. In this study, we present the results of our survey of planetesimal belts around 168 stars using ALMA (Atacama Large Millimeter Array). Our observations cover a broad range of distances, from 40 to 200 AU, and are sensitive to the presence of gas at levels as low as a few percent of the observed dust flux. The data obtained were analyzed through a combination of continuum subtraction and spectral line fitting techniques, allowing us to identify gas-rich planetesimal belts beyond the limits of direct imaging.\n",
      "\n",
      "Our survey has revealed that gas is a common component of planetesimal belts around Sun-like stars, with a detection rate of approximately 20%. This finding supports the idea that gas-rich planetesimals are widespread in the early stages of planetary system evolution. Interestingly, the detection rate of gas is found to be higher in systems with warmer dust temperatures, suggesting that the presence of gas is closely linked to the properties of the host star.\n",
      "\n",
      "The detection of gas around HD 129590 is particularly significant, as it is the first time gas has been observed in a planetesimal belt beyond the Kuiper belt in our own Solar System. The gas is found to be distributed over a wide range of distances, with an estimated total mass of about 10 Earth masses. The presence of gas in this region may have important implications for planet formation models, providing a mechanism for the accretion of large bodies and the subsequent growth of planetary cores.\n",
      "\n",
      "Overall, our survey has provided important new insights into the properties and distribution of planetesimal belts around Sun-like stars, shedding new light on the processes involved in the formation and evolution of planetary systems. 1 into PostgreSQL...\n",
      "Inserting test sample 197  Mid-IR emission lines of H2 are useful probes to determine the mass of warm gas present in the surface layers of disks. Numerous observations of Herbig Ae/Be stars (HAeBes) have been performed, but only 2 detections of mid-IR H2 toward HD97048 and AB Aur have been reported. We aim at tracing the warm gas in the disks of 5 HAeBes with gas-rich environments and physical characteristics close to those of AB Aur and HD97048, to discuss whether the detections toward these 2 objects are suggestive of peculiar conditions for the gas. We search for the H2 S(1) emission line at 17.035 \\mu\\m with VISIR, and complemented by CH molecule observations with UVES. We gather the H2 measurements from the literature to put the new results in context and search for a correlation with some disk properties. None of the 5 VISIR targets shows evidence for H2 emission. From the 3sigma upper limits on the integrated line fluxes we constrain the amount of optically thin warm gas to be less than 1.4 M_Jup in the disk surface layers. There are now 20 HAeBes observed with VISIR and TEXES instruments to search for warm H2, but only two detections (HD97048 and AB Aur) were made so far. We find that the two stars with detected warm H2 show at the same time high 30/13 \\mu\\m flux ratios and large PAH line fluxes at 8.6 and 11.3 \\mu\\m compared to the bulk of observed HAeBes and have emission CO lines detected at 4.7 \\mu\\m. We detect the CH 4300.3A absorption line toward both HD97048 and AB Aur with UVES. The CH to H2 abundance ratios that this would imply if it were to arise from the same component as well as the radial velocity of the CH lines both suggest that CH arises from a surrounding envelope, while the detected H2 would reside in the disk. The two detections of the S(1) line in the disks of HD97048 and AB Aur suggest either peculiar physical conditions or a particular stage of evolution. 0 into PostgreSQL...\n",
      "Inserting test sample 198  This research investigates the distribution of warm molecular hydrogen (H2) in disks surrounding Herbig Ae/Be stars. These stars are intermediate-mass pre-main-sequence stars that have disks of gas and dust, thought to be the precursors to planetary systems. H2 plays a key role in determining the thermodynamics and chemical structure of these disks, and its distribution can give us insight into the physical processes that occur within them.\n",
      "\n",
      "We have employed an observational approach, using data from the Atacama Large Millimeter/submillimeter Array (ALMA). Our study focused on a sample of six Herbig Ae/Be stars, analyzing their H2 emission properties on a small spatial scale, ~200 AU. We used the v=1-0 S(1) line, which has an upper energy level of 17,000 K, and is an excellent tracer of warm H2.\n",
      "\n",
      "Our results showed that H2 emission was detected in all six targets, with clear spatially extended structures observed in three of them. The morphology of the emission varied significantly between targets, suggesting differences in the distribution and excitation mechanisms at work. Our study also revealed that the H2 emission is typically more extended than other molecular line tracers, such as CO and CS. This may indicate the H2 is tracing more diffuse, warmer gas than these tracers.\n",
      "\n",
      "We performed a simple modeling analysis to investigate the physical properties of the H2 emitting gas. Our results suggest that the gas is typically warm (~1000 K) and dense (~108 cm-3). We also found that the H2/H volume density ratio in the emitting gas is typically around 10-5, consistent with expected values for disk atmospheres.\n",
      "\n",
      "Overall, our findings demonstrate that warm H2 emission is ubiquitous in disks around Herbig Ae/Be stars, with a range of morphologies and kinematics. Our study provides valuable insights into the distribution and physical conditions of warm gas in these protoplanetary disks, information that is essential for advancing our understanding of the planet formation process. 1 into PostgreSQL...\n",
      "Inserting test sample 199  HII regions in the arms of spiral galaxies are indicators of recent star-forming processes. They may have been caused by the passage of the density wave or simply created by other means near the arms. The study of these regions may give us clues to clarifying the controversy over the existence of a triggering scenario, as proposed in the density wave theory. Using H$\\alpha$ direct imaging, we characterize the HII regions from a sample of three grand design galaxies: NGC5457, NGC628 and NGC6946. Broad band images in R and I were used to determine the position of the arms. The HII regions found to be associated with arms were selected for the study. The age and the star formation rate of these HII regions was obtained using measures on the H$\\alpha$ line. The distance between the current position of the selected HII regions and the position they would have if they had been created in the centre of the arm is calculated. A parameter, T, which measures whether a region was created in the arm or in the disc, is defined. With the help of the T parameter we determine that the majority of regions were formed some time after the passage of the density wave, with the regions located `behind the arm' (in the direction of the rotation of the galaxy) the zone they should have occupied had they been formed in the centre of the arm. The presence of the large number of regions created after the passage of the arm may be explained by the effect of the density wave, which helps to create the star-forming regions after its passage. There is clear evidence of triggering for NGC5457 and a co-rotation radius is proposed. A more modest triggering seems to exist for NGC628 and non significant evidence of triggering are found for NGC6946. 0 into PostgreSQL...\n",
      "Inserting test sample 200  The phenomenon of density waves and its relationship with star formation in grand design spirals has been the subject of considerable study in astronomy. In grand design spirals, spiral arms extend outward from a central nucleus, creating regions of higher and lower gas density. The resulting density waves have been shown to play a crucial role in the formation of stars. \n",
      "\n",
      "There are two predominant theories regarding the relationship between density waves and star formation. Some researchers propose that density waves trigger star formation by compressing gas clouds in the spiral arms, while others suggest that density waves facilitate star formation by dynamically cooling the interstellar medium through their shearing motion. \n",
      "\n",
      "Recent observations have provided evidence for both theories. In some regions of grand design spirals, star formation is indeed triggered by the compression of gas clouds in spiral arms. In other regions, however, the presence of density waves appears to be responsible for cooling the interstellar medium, enabling star formation to occur. \n",
      "\n",
      "The overall impact of density waves on star formation in grand design spirals is still an area of active research. Some studies have shown that grand design spirals with stronger density waves tend to have higher rates of star formation, while others have found no such correlation. Further research is needed to fully understand the complex interplay between density waves and star formation in these systems. \n",
      "\n",
      "In conclusion, density waves play a crucial role in shaping the structure and evolution of grand design spirals, including their star formation histories. Ongoing research continues to shed light on the exact mechanisms by which density waves influence star formation, and these studies will deepen our understanding of the astrophysical processes that govern the formation and evolution of galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 201  The Bloch theorem enables reduction of the eigenvalue problem of the single-particle Hamiltonian that commutes with translational group. Based on a group theory analysis we present generalization of the Bloch theorem that incorporates all additional symmetries of a crystal. The generalized Bloch theorem constrains the form of the Hamiltonian which becomes manifestly invariant under additional symmetries. In the case of isotropic interactions the generalized Bloch theorem gives a unique Hamiltonian. This Hamiltonian coincides with the Hamiltonian in the periodic gauge. In the case of anisotropic interactions the generalized Bloch theorem allows a family of Hamiltonians. Due to the continuity argument we expect that even in this case the Hamiltonian in the periodic gauge defines observables, such as Berry curvature, in the inverse space. For both cases we present examples and demonstrate that the average of the Berry curvatures of all possible Hamiltonians in the Bloch gauge is the Berry curvature in the periodic gauge. 0 into PostgreSQL...\n",
      "Inserting test sample 202  The Bloch theorem has been foundational in understanding the electronic structure of periodic systems for over 80 years. Recently, this theory has been extended to describe the topological properties of insulators and superconductors. In this paper, we present a generalized Bloch theorem that unifies the traditional Bloch theory with the powerful tools of topological characterization. This theorem allows us to identify topologically non-trivial systems, even in the presence of disorder and interactions, and to predict their robust transport properties. We apply this framework to study topological phases in various systems, including crystalline solids, amorphous materials, and photonic crystals. Our results show that the generalized Bloch theorem provides a clear and systematic approach for identifying topological phases in a wide range of physical systems, which is crucial for the development of novel topological materials and devices. 1 into PostgreSQL...\n",
      "Inserting test sample 203  This paper presents and characterizes an Open Application Repository for Federated Learning (OARF), a benchmark suite for federated machine learning systems. Previously available benchmarks for federated learning have focused mainly on synthetic datasets and use a limited number of applications. OARF mimics more realistic application scenarios with publicly available data sets as different data silos in image, text and structured data. Our characterization shows that the benchmark suite is diverse in data size, distribution, feature distribution and learning task complexity. The extensive evaluations with reference implementations show the future research opportunities for important aspects of federated learning systems. We have developed reference implementations, and evaluated the important aspects of federated learning, including model accuracy, communication cost, throughput and convergence time. Through these evaluations, we discovered some interesting findings such as federated learning can effectively increase end-to-end throughput. 0 into PostgreSQL...\n",
      "Inserting test sample 204  This paper presents the OARF benchmark suite, a set of metrics and methodologies designed to characterize federated learning systems. The suite includes tests for evaluating the performance, scalability, and robustness of federated algorithms, as well as tools for measuring the impact of different datasets and communication strategies on algorithmic performance. Using the OARF benchmark suite, we explore the implications of federated learning systems for privacy, security, and data governance. Our results show that while federated learning holds great promise for distributed machine learning, it raises several key challenges, including issues of data heterogeneity, communication overhead, and model aggregation. We conclude by discussing several promising avenues for future research in federated learning, including the development of more efficient communication protocols and the integration of privacy-preserving technologies into federated algorithms. 1 into PostgreSQL...\n",
      "Inserting test sample 205  Eigendecomposition (ED) is widely used in deep networks. However, the backpropagation of its results tends to be numerically unstable, whether using ED directly or approximating it with the Power Iteration method, particularly when dealing with large matrices. While this can be mitigated by partitioning the data in small and arbitrary groups, doing so has no theoretical basis and makes its impossible to exploit the power of ED to the full. In this paper, we introduce a numerically stable and differentiable approach to leveraging eigenvectors in deep networks. It can handle large matrices without requiring to split them. We demonstrate the better robustness of our approach over standard ED and PI for ZCA whitening, an alternative to batch normalization, and for PCA denoising, which we introduce as a new normalization strategy for deep networks, aiming to further denoise the network's features. 0 into PostgreSQL...\n",
      "Inserting test sample 206  The Backpropagation-Friendly Eigendecomposition (BFE) is a novel algorithm developed for efficient and accurate calculation of eigenvectors and eigenvalues in neural networks. In the BFE, the spectral decomposition of the Hessian matrix is diagonalized by using the forward and inverse modes of automatic differentiation. This results in significant computational savings and a more stable backpropagation, which enhances model training and optimization. We demonstrate the effectiveness of the BFE algorithm on several benchmark datasets, including image classification and natural language processing tasks. In comparison to traditional methods, the BFE algorithm achieves better accuracy and faster convergence rates. Moreover, the BFE algorithm has broad applicability as it can be easily implemented in popular deep learning frameworks. We believe the BFE algorithm has the potential to advance the field of neural network training and optimization. 1 into PostgreSQL...\n",
      "Inserting test sample 207  Using the Ashtekar-Sen variables of loop quantum gravity, a new class of exact solutions to the equations of quantum cosmology is found for gravity coupled to a scalar field, that corresponds to inflating universes. The scalar field, which has an arbitrary potential, is treated as a time variable, reducing the hamiltonian constraint to a time-dependent Schroedinger equation.\n",
      "\n",
      "When reduced to the homogeneous and isotropic case, this is solved exactly by a set of solutions that extend the Kodama state, taking into account the time dependence of the vacuum energy. Each quantum state corresponds to a classical solution of the Hamiltonian-Jacobi equation. The study of the latter shows evidence for an attractor, suggesting a universality in the phenomena of inflation. Finally, wavepackets can be constructed by superposing solutions with different ratios of kinetic to potential scalar field energy, resolving, at least in this case, the issue of normalizability of the Kodama state. 0 into PostgreSQL...\n",
      "Inserting test sample 208  The unification of quantum mechanics and general relativity, known as quantum gravity, is an area of active research in theoretical physics. This theoretical framework aims to describe gravity within the framework of quantum mechanics, and has the potential to provide insight into the nature of space and time at the quantum level. Inflation, an accelerated expansion of the universe in its early stages, is another area of research that aims to explain various cosmic mysteries. The combination of these two theories, quantum gravity and inflation, has the potential to explain many of the outstanding questions in cosmology, such as the origin of the universe and the nature of dark energy. This paper explores the current state of research into the combination of quantum gravity and inflation, highlighting recent developments in the field and the challenges that lie ahead. 1 into PostgreSQL...\n",
      "Inserting test sample 209  In the framework of the topcolor-assisted technicolor (TC2) models, we study the production of the top-pions $\\pi^{0}_{t}$, $\\pi_{t}^{\\pm}$ via the processes $ep\\to\\gamma c\\to\\pi^{0}_{t}c$ and $ep\\to\\gamma c\\to\\pi^{\\pm}_{t}b$ mediated by the anomalous top coupling $tc\\gamma$. We find that the production cross section of the process $ep\\to\\gamma c\\to\\pi^{0}_{t}c$ is very small. With reasonable values of the parameters in TC2 models, the production cross section of the process $ep\\to\\gamma c\\to\\pi^{\\pm}_{t}b$ can reach $ 1.2pb$. The charged top-pions $\\pi^{\\pm}_{t}$ might be directly observed via this process at the THERA collider based $\\gamma p$ collisions. 0 into PostgreSQL...\n",
      "Inserting test sample 210  This paper investigates the production of top-pions at the THERA collider via $\\gamma p$ collisions. Top-pions arise in models with extended technicolor and are predicted to be produced with large cross-sections in high-energy proton and photon-proton collisions. We perform simulations to study the production rate and distribution of top-pions, and their decays in this particular process. Our results show that the production rate of top-pions can be effectively enhanced by adjusting the center-of-mass energy and the photon spectrum of the collider. The study of these exotic particles can provide insight into the physics beyond the standard model and potentially lead to the discovery of new particles and interactions. 1 into PostgreSQL...\n",
      "Inserting test sample 211  Gauge/gravity duality is the conjecture that string theories have dual descriptions as gauge theories. Weakly-coupled gravity is dual to strongly-coupled gauge theories, ideal for lattice calculations. I will show precision lattice calculations that confirm large-N continuum D0-brane quantum mechanics correctly reproduces the leading-order supergravity prediction for a black hole's internal energy---the first leading-order test of the duality---and constrains stringy corrections. 0 into PostgreSQL...\n",
      "Inserting test sample 212  This paper discusses the relationship between supergravity and gauge theory. The study shows that by combining these two theories, one can obtain a unified framework for describing the behavior of particles at both low and high energies. The paper highlights the mathematical connections between these theories that allow for the prediction of the properties of subatomic particles. Finally, the results suggest that the integration of these theories can provide a deeper understanding of fundamental physical laws. 1 into PostgreSQL...\n",
      "Inserting test sample 213  The central kiloparsec region of the Andromeda galaxy is relatively gas poor, while the interstellar medium appears to be concentrated in a ring-like structure at about 10 kpc radius. The central gas depletion has been attributed to a possible head-on collision 200 Myr ago, supported by the existence of an offset inner ring of warm dust. We present new IRAM 30m radio telescope observations of the molecular gas in the central region, and the detection of CO and its isotopes $^{13}$CO(2-1) and C$^{18}$O(2-1), together with the dense gas tracers, HCN(1-0) and HCO+(1-0). A systematic study of the observed peak temperatures with non-LTE equilibrium simulations shows that the detected lines trace dense regions with n$_{H_2}$ in the range 2.5 $10^4 - 5.6 10^5$ cm$^{-3}$, while the gas is very clumpy with a beam filling factor of 0.5-2 10$^{-2}$. This is compatible with the dust mass derived from the far-infrared emission, assuming a dust-to-gas mass ratio of 0.01 with a typical clump size of 2 pc. We also show that the gas is optically thin in all lines except for $^{12}$CO(1-0) and $^{12}$CO(2-1), CO lines are close to their thermal equilibrium condition at 17.5-20 K, the molecular hydrogen density is larger than critical and HCN and HCO+ lines have a subthermal excitation temperature of 9 K with a density smaller than critical. The average $^{12}$CO/$^{13}$CO line ratio is high (~21), and close to the $^{12}$CO/C$^{18}$O ratio (~30) that was measured in the north-western region and estimated in the south-east stacking. The fact that the optically thin $^{13}$CO and C$^{18}$O lines have comparable intensities means that the secondary element $^{13}$C is depleted with respect to the primary $^{12}$C, as is expected just after a recent star formation. This suggests that there has been a recent starburst in the central region, supporting the head-on collision scenario. 0 into PostgreSQL...\n",
      "Inserting test sample 214  This paper presents a study of the dense gas in Andromeda and its potential role in tracing the galaxy's collisional past. Through observations of the inner region of Andromeda, we have found evidence of atypical properties in the gas, suggesting a complex and dynamic history.\n",
      "\n",
      "Our analysis is based on data from the Atacama Large Millimeter Array (ALMA), which allowed for high-resolution imaging of the gas distribution. We focused on the inner 2 kiloparsecs of Andromeda, studying the properties of the gas in regions of high star formation and comparing them to those in the outer regions of the galaxy.\n",
      "\n",
      "Our results show that the dense gas in the inner region of Andromeda is significantly different from that in the outer regions, with higher temperatures and column densities. We also observed a lack of kinematic alignment between the gas and the stars in the central region, which suggests that the gas has not yet settled into a stable configuration.\n",
      "\n",
      "We propose that these properties are the result of Andromeda's collisional history. Based on simulations, we suggest that the inner region of Andromeda may be the site of a recent collision between a high-density gas cloud and the galaxy. This collision could have triggered the formation of the observed star clusters in the area and injected energy into the gas, causing it to exhibit the atypical properties we observe.\n",
      "\n",
      "Overall, our study provides new insights into the complex history of Andromeda and the role of dense gas in tracing the galaxy's past collisions. Our results suggest that the inner region of Andromeda is a unique and evolving environment that may provide new opportunities for studying the formation and evolution of galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 215  Reducing a 6d fivebrane theory on a 3-manifold $Y$ gives a $q$-series 3-manifold invariant $\\widehat{Z}(Y)$. We analyse the large-$N$ behaviour of $F_K=\\widehat{Z}(M_K)$, where $M_K$ is the complement of a knot $K$ in the 3-sphere, and explore the relationship between an $a$-deformed ($a=q^N$) version of $F_{K}$ and HOMFLY-PT polynomials. On the one hand, in combination with counts of holomorphic annuli on knot complements, this gives an enumerative interpretation of $F_K$ in terms of counts of open holomorphic curves. On the other, it leads to closed form expressions for $a$-deformed $F_K$ for $(2,2p+1)$-torus knots. They suggest a further $t$-deformation based on superpolynomials, which can be used to obtain a $t$-deformation of ADO polynomials, expected to be related to categorification. Moreover, studying how $F_K$ transforms under natural geometric operations on $K$ indicates relations to quantum modularity in a new setting. 0 into PostgreSQL...\n",
      "Inserting test sample 216  The study of the $\\widehat{Z}$ system at large $N$ has far-reaching implications that extend from enumerative geometry to quantum modularity. Our research contributes to bridging the gap between these two fields through a careful study of $\\widehat{Z}$ in the large $N$ limit. We develop tools to compute curve counts in higher genus and pack them together in an efficient way. We combine these tools with the quantum Riemann-Roch theorem and show that the Gromov-Witten potential can be written as a modular form with respect to a suitable subgroup of $\\mathrm{SL}(2, \\mathbb{Z})$. Our results not only provide a deeper understanding of the algebraic geometry behind $\\widehat{Z}$ but also reveal unexpected connections to modular forms with implications in string theory. Our work highlights the beauty and richness of mathematical structures awaiting discovery at the intersection of mathematics and theoretical physics. 1 into PostgreSQL...\n",
      "Inserting test sample 217  We develop SHOPPER, a sequential probabilistic model of shopping data.\n",
      "\n",
      "SHOPPER uses interpretable components to model the forces that drive how a customer chooses products; in particular, we designed SHOPPER to capture how items interact with other items. We develop an efficient posterior inference algorithm to estimate these forces from large-scale data, and we analyze a large dataset from a major chain grocery store. We are interested in answering counterfactual queries about changes in prices. We found that SHOPPER provides accurate predictions even under price interventions, and that it helps identify complementary and substitutable pairs of products. 0 into PostgreSQL...\n",
      "Inserting test sample 218  Understanding consumer behavior is important for businesses to successfully target potential customers. In this paper, we present SHOPPER, a probabilistic model for consumer choice that incorporates both substitutes and complements. Our model takes into account individual preferences and market-level information to predict consumer purchasing behavior. Additionally, we introduce a novel probability framework for including assortments of substitute and complementary products. We evaluate our model using datasets from various real-world scenarios and find that SHOPPER outperforms existing models in accurately predicting consumer behavior. Our findings demonstrate the efficacy of our model in guiding business decision-making processes. 1 into PostgreSQL...\n",
      "Inserting test sample 219  We study the distributed average consensus problem in multi-agent systems with directed communication links that are subject to quantized information flow. The goal of distributed average consensus is for the nodes, each associated with some initial value, to obtain the average (or some value close to the average) of these initial values. In this paper, we present and analyze novel distributed averaging algorithms which operate exclusively on quantized values (specifically, the information stored, processed and exchanged between neighboring agents is subject to deterministic uniform quantization) and rely on event-driven updates (e.g., to reduce energy consumption, communication bandwidth, network congestion, and/or processor usage). We characterize the properties of the proposed distributed averaging protocols on quantized values and show that their execution, on any time-invariant and strongly connected digraph, will allow all agents to reach, in finite time, a common consensus value represented as the ratio of two quantized values that is equal to the exact average. We conclude with examples that illustrate the operation, performance, and potential advantages of the proposed algorithms. 0 into PostgreSQL...\n",
      "Inserting test sample 220  This paper presents a new distributed algorithm for average consensus problem in multi-agent systems. The proposed method tackles the issue of high communication and computation complexity associated with traditional consensus mechanisms. With the event-triggered frequency control mechanism, the number of message exchanges between agents can be significantly reduced. Besides, the quantization scheme reduces the communication cost by encoding the consensus value with a small number of bits. These methods, along with mass summation, reduce the computational burden of the algorithm. Theoretical analysis shows the convergence of the proposed algorithm under certain conditions. Simulations have also been performed to demonstrate the effectiveness of the algorithm in various settings and illustrate its superior performance over related works. The results suggest that the proposed method provides a promising solution for achieving efficient and robust distributed average consensus in large-scale multi-agent systems with limited bandwidth and computational resources. 1 into PostgreSQL...\n",
      "Inserting test sample 221  Today's most popular techniques for accurately calculating the dynamics of the reduced density operator in an open quantum system, either require, or gain great computational benefits, from representing the bath response function a(t) in the form a(t)={\\Sigma}_k^K p_k e^{O_k t} . For some of these techniques, the number of terms in the series K plays the lead role in the computational cost of the calculation, and is therefore often a limiting factor in simulating open quantum system dynamics. We present an open source MATLAB program called BATHFIT 1, whose input is any spectral distribution functions J(w) or bath response function, and whose output attempts to be the set of parameters {p_k,w_k}_k=1^K such that for a given value of K, the series {\\Sigma}_k^k p_k e^{O_k t} is as close as possible to a(t). This should allow the user to represent a(t) as accurately as possible with as few parameters as possible.\n",
      "\n",
      "The program executes non-linear least squares fitting, and for a very wide variety of spectral distribution functions, competent starting parameters are used for these fits. For most forms of J(w), these starting parameters, and the exact a(t) corresponding to the given J(w), are calculated using the recent Pade decomposition technique - therefore this program can also be used to merely implement the Pade decomposition for these spectral distribution functions; and it can also be used just to efficiently and accurately calculate a(t) for any given J(w) . The program also gives the J(w) corresponding to a given a(t), which may allow one to assess the quality (in the w-domain) of a representation of a(t) being used. Finally, the program can calculate the discretized influence functional coefficients for any J(w), and this is computed very efficiently for most forms of J(w) by implementing the recent technique published in [Quantum Physics Letters (2012) 1 (1) pg. 35]. 0 into PostgreSQL...\n",
      "Inserting test sample 222  This paper introduces an optimal representation of the bath response function and presents a fast calculation method for influence functional coefficients in open quantum systems. The method is implemented in BATHFIT 1, a software package for modeling quantum dynamics in non-equilibrium environments. The new representation of the bath response function, based on a system-bath energy decomposition, significantly reduces the numerical cost of computing influence functional coefficients. \n",
      "\n",
      "The BATHFIT 1 software package uses a hierarchical Bayesian approach to estimate the parameters of open quantum systems from a limited set of experimental data. This approach involves the calculation of influence functional coefficients, which characterize the coupling between the system and its environment. The new method presented in this paper for computing these coefficients is based on a low-rank approximation of the bath response function, and significantly reduces the computational complexity of the problem, making it feasible for larger systems and longer simulation times.\n",
      "\n",
      "The paper presents numerical experiments demonstrating the accuracy, efficiency, and scalability of the new method using a variety of model systems, including a driven harmonic oscillator, a photovoltaic device, and a quantum transport junction.\n",
      "\n",
      "Overall, the results indicate that the new representation of the bath response function and the corresponding fast calculation method for influence functional coefficients offer a significant improvement over existing methods, both in terms of accuracy and computational efficiency. The approach implemented in BATHFIT 1 has the potential to enable more accurate modeling of open quantum systems in complex, non-equilibrium environments, providing a powerful tool for understanding a wide range of physical and chemical phenomena. 1 into PostgreSQL...\n",
      "Inserting test sample 223  Using adaptive mesh-refinement cosmological hydrodynamic simulations with a physically motivated supernova feedback prescription we show that the standard cold dark matter model can account for extant observed properties of damped Lyman alpha systems (DLAs). We then examine the properties of DLA host galaxies. We find: (1) While DLA hosts roughly trace the overall population of galaxies at all redshifts, they are always gas rich. (2) The history of DLA evolution reflects primarily the evolution of the underlying cosmic density, galaxy size and galaxy interactions. With higher density and more interactions at high redshift DLAs are larger in both absolute terms and in relative terms with respect to virial radii of halos. (3) The variety of DLAs at high redshift is richer with a large contribution coming from galactic filaments, created through close galaxy interactions. The portion of gaseous disks of galaxies where most stars reside makes relatively small contribution to DLA incidence at z=3-4. (4) The vast majority of DLAs arise in halos of mass M_h=10^10-10^12 Msun at z=1.6-4. At z=3-4, 20-30% of DLA hosts are Lyman Break Galaxies (LBGs).\n",
      "\n",
      "(5) Galactic winds play an indispensable role in shaping the kinematic properties of DLAs. Specifically, the high velocity width DLAs are a mixture of those arising in high mass, high velocity dispersion halos and those arising in smaller mass systems where cold gas clouds are entrained to high velocities by galactic winds. (6) In agreement with observations, we see a weak but noticeable evolution in DLA metallicity. The metallicity distribution centers at [Z/H]=-1.5 to -1 at z=3-4, with the peak moving to [Z/H]=-0.75 at z=1.6 and [Z/H]=-0.5 by z=0. (7) The star formation rate of DLA hosts is concentrated in the range 0.3-30Msun/yr at z=3-4, gradually shifting lower to peak at ~0.5-1 Msun/yr by z=0. 0 into PostgreSQL...\n",
      "Inserting test sample 224  Damped Lyman-alpha (DLA) systems are crucial to the study of galaxy formation and evolution, as they are thought to represent the sites of future star formation. These absorbers are identified through their strong absorption of Lyman-alpha transition radiation, and are typically associated with high column density neutral hydrogen clouds. In this paper, we present a comprehensive analysis of the nature of DLA systems and their hosts, utilizing a standard cold dark matter cosmology. \n",
      "\n",
      "Our study is based on a sample of DLA systems selected from the Sloan Digital Sky Survey (SDSS) Data Release 14, which we complement with additional data from the literature. We investigate the physical properties of the DLA hosts and their connection with the larger-scale environment, as well as their impact on the observed Lyman-alpha absorption spectra.\n",
      "\n",
      "Our results indicate that DLA systems preferentially occur in galaxies with lower star formation rates (SFRs) and, as a result, lower metallicities. This trend is consistent with theoretical predictions that galaxies with lower SFRs have weaker feedback mechanisms, which allow for the buildup of large reservoirs of neutral gas. We also find that DLA systems are primarily associated with galaxies in intermediate-density environments, with little preference for either large-scale overdensities or voids. \n",
      "\n",
      "Additionally, we investigate the properties of Lyman-alpha emission in the proximity of DLA systems. Our analysis indicates that there is a correlation between the strength of the DLA absorption and the amount of Lyman-alpha emission, with stronger absorption corresponding to weaker Lyman-alpha emission. This connection suggests that the presence of a DLA system can suppress nearby star formation.\n",
      "\n",
      "Overall, our study provides a detailed picture of the nature of DLA systems and their hosts within the framework of a standard cold dark matter cosmology. We demonstrate that understanding the properties of these absorbers is crucial for our understanding of galaxy formation and evolution, and that the study of DLA systems will continue to play a significant role in shaping our understanding of the cosmos. 1 into PostgreSQL...\n",
      "Inserting test sample 225  The results of a NLTE model atmosphere analysis of 27 hydrogen-rich central stars of old planetary nebulae (PN) are reported. These stars were selected from a previous paper in this series, where we gave classifications for a total of 38 central stars. Most of the analyzed central stars fill a previously reported gap in the hydrogen-rich evolutionary sequence. Our observations imply the existence of two separated spectral evolutionary sequences for hydrogen-rich and -poor central stars/white dwarfs. This is in line with theoretical evolutionary calculations, which predict that most post-AGB stars reach the white dwarf domain with a thick hydrogen envelope of approx. 10^-4 Msun.\n",
      "\n",
      "We determine stellar masses from the comparison with evolutionary tracks and derive a mass distribution for the hydrogen-rich central stars of old PNe. The peak mass and the general shape of the distribution is in agreement with recent determinations of the white dwarf mass distribution. The properties of most analyzed stars are well explained by standard post-AGB evolution. However, for eight stars of the sample other scenarios have to be invoked.\n",
      "\n",
      "A wide spread of helium abundances is observed in the photospheres of central stars of old PNe. It is shown that a good correlation between helium abundances and luminosity is present. It is inferred that when the stars' luminosities fall below L = 300Lsun depletion starts and the helium abundance steadily decreases with decreasing luminosity. The existence of this correlation is in qualitative agreement with recent theoretical calculations of gravitational settling in the presence of a stellar wind. 0 into PostgreSQL...\n",
      "Inserting test sample 226  This study explores the abundance patterns of planetary nebulae through spectroscopic data. Specifically, it focuses on analyzing model atmospheres of old planetary nebulae. To accomplish this, a sample of planetary nebulae was observed using a spectrograph and the spectra were analyzed. The spectra were then compared to models of the nebulae's atmospheres to determine the presence of certain elements. This analysis provides insight into the history and composition of the nebulae. \n",
      "\n",
      "The modeling of the atmospheres required an accurate determination of the effective temperature, surface gravity, and chemical composition of the nebulae. A grid of models was utilized to determine the best-fit model for each nebula. Spectral line profile fitting techniques were then applied to the data to estimate the abundances of all detectable elements. \n",
      "\n",
      "Results show a diversity of abundance patterns for almost all elements observed in the planetary nebulae. For instance, oxygen was found to be variable in many nebulae, indicating that it is not always the most abundant element in old planetary nebulae. Contrarily, neon was detected in large amounts in many sample objects, demonstrating its importance in planetary nebulae. Nitrogen was also found to be abundant in some nebulae. These results provide new insights into the chemical composition of old planetary nebulae and their impact on the interstellar medium. In summary, the study provides an extensive model atmosphere analysis of old planetary nebulae and illustrates the diversity of their abundance patterns. 1 into PostgreSQL...\n",
      "Inserting test sample 227  PG1159-035 is the prototype of the PG1159 spectral class which consists of extremely hot hydrogen-deficient (pre-) white dwarfs. It is also the prototype of the GW Vir variables, which are non-radial g-mode pulsators. The study of PG1159 stars reveals insight into stellar evolution and nucleosynthesis during AGB and post-AGB phases. We perform a quantitative spectral analysis of PG1159-035 focusing on the abundance determination of trace elements. We have taken high-resolution ultraviolet spectra of PG1159-035 with the Hubble Space Telescope and the Far Ultraviolet Spectroscopic Explorer. They are analysed with non-LTE line blanketed model atmospheres. We confirm the high effective temperature with high precision (Teff=140,000+/-5000 K) and the surface gravity of logg=7. For the first time we assess the abundances of silicon, phosphorus, sulfur, and iron. Silicon is about solar. For phosphorus we find an upper limit of solar abundance. A surprisingly strong depletion of sulfur (2% solar) is discovered. Iron is not detected, suggesting an upper limit of 30% solar. This coincides with the Fe deficiency found in other PG1159 stars. We redetermine the nitrogen abundance and find it to be lower by one dex compared to previous analyses. The sulfur depletion is in contradiction with current models of AGB star intershell nucleosynthesis. The iron deficiency confirms similar results for other PG1159 stars and is explained by the conversion of iron into heavier elements by n-capture in the s-processing environment of the precursor AGB star. However, the extent of the iron depletion is stronger than predicted by evolutionary models. The relatively low nitrogen abundance compared to other pulsating PG1159 stars weakens the role of nitrogen as a distinctive feature of pulsators and non-pulsators in the GW Vir instability strip. 0 into PostgreSQL...\n",
      "Inserting test sample 228  This research presents high-resolution ultraviolet spectroscopy observation of PG1159-035 using the Hubble Space Telescope (HST) and Far-Ultraviolet Spectroscopic Explorer (FUSE). PG1159-035 is a post asymptotic giant branch (AGB) star that is continuously evolving towards becoming a planetary nebula. The observations were made to study the atmosphere of the star, as well as the properties and distribution of its elements. \n",
      "\n",
      "The HST observations covered the Near-Ultraviolet (NUV) region, while FUSE covered the Far-Ultraviolet (FUV) region. The combined data from both telescopes allowed for a wide spectral coverage, which led to the identification of multiple highly ionized lines such as N VI, O VI, Ne VIII, Ar VIII, and S VI. By analyzing the line profiles and intensities, we were able to derive important parameters of the star like its temperature, gravity, and chemical composition. \n",
      "\n",
      "The obtained spectra were compared with synthetic spectra calculated using non-LTE model atmospheres. This comparison led to the refinement of the atmospheric models for PG1159-035. The elemental abundances of the star were determined by synthesizing the spectra using a chemical abundance analysis code. The spectra showed an overabundance of Ne and Mg, while other elements like C, O, and Si were underabundant. \n",
      "\n",
      "In conclusion, the HST and FUSE observations allowed for a deeper insight into the properties of PG1159-035. The derived atmospheric parameters and chemical composition reveal that PG1159-035 is a highly evolved post-AGB star that has undergone significant nucleosynthesis and mixing processes. These observations and findings can provide crucial information for the knowledge of the formation and evolution of post-AGB stars and white dwarfs. 1 into PostgreSQL...\n",
      "Inserting test sample 229  Previous SiO maps of the innermost regions of HH212 set strong constraints on the structure and origin of this jet. They rule out a fast wide-angle wind, and tentatively favor a magneto-centrifugal disk wind launched out to 0.6 AU. We aim to assess the SiO content at the base of the HH212 jet to set an independent constraint on the location of the jet launch zone with respect to the dust sublimation radius. We present the first sub-arcsecond (0\"44x0\"96) CO map of the HH212 jet base, obtained with the IRAM Plateau de Bure Interferometer. Combining this with previous SiO(5-4) data, we infer the CO(2-1) opacity and mass-flux in the high-velocity jet and arrive at a much tighter lower limit to the SiO abundance than possible from the (optically thick) SiO emission alone. Gas-phase SiO at high velocity contains at least 10% of the elemental silicon if the jet is dusty, and at least 40% if the jet is dust-free, if CO and SiO have similar excitation temperatures. Such a high SiO content is challenging for current chemical models of both dust-free winds and dusty interstellar shocks. Updated chemical models (equatorial dust-free winds, highly magnetized dusty shocks) and observations of higher J CO lines are required to elucidate the dust content and launch radius of the HH212 high-velocity jet. 0 into PostgreSQL...\n",
      "Inserting test sample 230  This research paper presents observations of the HH212 protostellar jet, revealing an unexpectedly high abundance of silicon monoxide (SiO). Using data from high-resolution molecular line observations, we investigate the physical and chemical conditions of the jet. Our findings indicate that the jet material is significantly enriched in SiO, with abundances of up to 15 times the expected value. This excess of SiO is concentrated in the high-velocity jet spine, suggesting that SiO is preferentially ejected along the jet axis. Furthermore, the high SiO abundance appears to be correlated with other molecular tracers such as sulfur dioxide (SO2) and methanol (CH3OH), suggesting a possible connection between these molecules and the SiO enhancement mechanism. We propose that the high SiO abundance in the HH212 jet may result from the interaction between the jet material and the surrounding envelope, which could trigger chemical processes leading to SiO formation. Our results provide valuable insights into the chemical evolution of protostellar jets and their potential impact on the surrounding interstellar medium. 1 into PostgreSQL...\n",
      "Inserting test sample 231  We report a study of $\\tau$ lepton decays involving $\\Ks$ with a 669 fb$^{-1}$ data sample accumulated with the Belle detector at the KEKB asymmetric-energy $e^{+}e^{-}$ collider. The branching fractions have been measured for the $\\tauTO \\piKs \\nu_{\\tau}$, $\\KKs \\nu_{\\tau}$, $\\piKspizero \\nu_{\\tau}$, $\\KKspizero \\nu_{\\tau}$, $\\piKsKs \\nu_{\\tau}$ and $\\piKsKspizero \\nu_{\\tau}$ decays. We also provide the unfolded mass spectra for $\\tauTO \\piKspizero \\nu_{\\tau}$ and $\\tauTO \\KKspizero \\nu_{\\tau}$. 0 into PostgreSQL...\n",
      "Inserting test sample 232  The exploration of $\\tau$ lepton decays with $K_{S}^{0}$ has been studied exhaustively at Belle. This research paper presents the measurement of the branching fractions and mass spectra for $\\tau \\rightarrow K_S^{0} \\pi^{\\pm}\\nu_{\\tau}$, $\\tau \\rightarrow K_S^{0} K^{\\pm} \\pi^{\\mp}\\nu_{\\tau}$, and $\\tau \\rightarrow K_S^{0} K^{\\pm} \\pi^{\\pm}\\pi^{\\mp}\\nu_{\\tau}$. The results improve our understanding of hadronic weak decays and provide essential input for testing the Standard Model. 1 into PostgreSQL...\n",
      "Inserting test sample 233  New experimental data are collected for methyl-cyclohexane (MCH) autoignition in a heated rapid compression machine (RCM). Three mixtures of MCH/O2/N2/Ar at equivalence ratios of $\\phi$=0.5, 1.0, and 1.5 are studied and the ignition delays are measured at compressed pressure of 50 bar and for compressed temperatures in the range of 690-900 K. By keeping the fuel mole fraction in the mixture constant, the order of reactivity, in terms of inverse ignition delay, is measured to be $\\phi$=0.5 > $\\phi$=1.0 > $\\phi$=1.5, demonstrating the dependence of the ignition delay on oxygen concentration. In addition, an existing model for the combustion of MCH is updated with new reaction rates and pathways, including substantial updates to the low-temperature chemistry. The new model shows good agreement with the overall ignition delays measured in this study, as well as the ignition delays measured previously in the literature using RCMs and shock tubes. This model therefore represents a strong improvement compared to the previous version, which uniformly over-predicted the ignition delays. Chemical kinetic analyses of the updated mechanism are also conducted to help understand the fuel decomposition pathways and the reactions controlling the ignition. Combined, these results and analyses suggest that further investigation of several of the low-temperature fuel decomposition pathways is required. 0 into PostgreSQL...\n",
      "Inserting test sample 234  This paper investigates the autoignition of methylcyclohexane at high pressure through experimental and modeling approaches. The experiments were conducted in a high-pressure shock tube, which allowed the study of the ignition delay time at different pressures and temperatures. The results showed that the ignition delay time decreased with increasing temperature and pressure, highlighting the importance of pressure in the autoignition process of this fuel. On the other hand, the modeling study employed a detailed chemical kinetic mechanism to simulate the ignition process of methylcyclohexane. The model was able to predict the ignition delay time at different conditions, and the results showed good agreement with the experimental data. Moreover, the model was used to investigate the role of different chemical pathways in the autoignition process of the fuel. The results revealed that the H-atom abstraction from the methyl group is the dominant pathway in the ignition of methylcyclohexane. This work contributes to the understanding of the autoignition process of methylcyclohexane at high pressure and provides valuable information for the development of more efficient engines operating on this fuel. 1 into PostgreSQL...\n",
      "Inserting test sample 235  Motivated by the study of cosmological phase transitions, our understanding of the formation of topological defects during spontaneous symmetry-breaking and the associated non-equilibrium field theory has recently changed.\n",
      "\n",
      "Experiments have been performed in superfluid $^4$He to test the new ideas involved. In particular, it has been observed that a vortex density is seen immediately after pressure quenches from just below the $\\lambda$ transition.\n",
      "\n",
      "We discuss possible interpretations of these vortices, conclude they are consistent with our ideas of vortex formation and propose a modification of the original experiments. 0 into PostgreSQL...\n",
      "Inserting test sample 236  We investigate the presence of quench induced vortices in liquid helium-4 in the symmetry broken phase. By applying quenches, where the system is rapidly squeezed through the transition, we observed vortices in the vicinity of the quench. We show that these vortices carry significant fluid momentum and that this momentum is transferred to the mechanical motion of the trapping container. Our results suggest that quench-induced vortices can have important consequences for the dynamics of helium-4 in the symmetry-broken phase and thus need to be taken into account when interpreting experimental results. 1 into PostgreSQL...\n",
      "Inserting test sample 237  KEEN waves are nonlinear, non-stationary, self-organized asymptotic states in Vlasov plasmas outside the scope or purview of linear theory constructs such as electron plasma waves or ion acoustic waves. Nonlinear stationary mode theories such as those leading to BGK modes also do not apply. The range in velocity that is strongly perturbed by KEEN waves depends on the amplitude and duration of the ponderomotive force used to drive them. Smaller amplitude drives create highly localized structures attempting to coalesce into KEEN waves. These cases have much more chaotic and intricate time histories than strongly driven ones.\n",
      "\n",
      "The narrow range in which one must maintain adequate velocity resolution in the weakly driven cases challenges xed grid numerical schemes. What is missing there is the capability of resolving locally in velocity while maintaining a coarse grid outside the highly perturbed region of phase space. We here report on a new Semi-Lagrangian Vlasov-Poisson solver based on conservative non-uniform cubic splines in velocity that tackles this problem head on. An additional feature of our approach is the use of a new high-order time-splitting scheme which allows much longer simulations per computational e ort. This is needed for low amplitude runs which take a long time to set up KEEN waves, if they are able to do so at all. The new code's performance is compared to uniform grid simulations and the advantages quanti ed. The birth pains associated with KEEN waves which are weakly driven is captured in these simulations. These techniques allow the e cient simulation of KEEN waves in multiple dimensions which will be tackled next as well as generalizations to Vlasov-Maxwell codes which are essential to understanding the impact of KEEN waves in practice. 0 into PostgreSQL...\n",
      "Inserting test sample 238  This research paper presents simulations of Kinetic Electrostatic Electron Nonlinear (KEEN) waves utilizing high-order time-splitting techniques and variable velocity resolution grids. KEEN waves belong to a class of electrostatic waves, which are observed in the near-earth plasma environment. Numerical simulations of these waves are crucial for understanding their behavior and interaction with plasma particles. \n",
      "\n",
      "Our simulations were performed using the PIC (Particle in Cell) method, which resolves the motion of charged particles and their interactions with electromagnetic fields using numerical schemes. In order to accurately simulate KEEN waves, we used variable velocity resolution grids to resolve the evolution of these waves from the initial to the final stages. \n",
      "\n",
      "Additionally, high-order time-splitting techniques were employed to improve the accuracy and efficiency of the simulations. The improved accuracy allows for more precise predictions of the behavior of KEEN waves and helps validate the analytical predictions of their existence. \n",
      "\n",
      "The simulations were carried out in different plasma environments, ranging from linear to nonlinear, to evaluate the impact of these waves on charged particles. Our findings show that the KEEN waves, through wave-particle interactions, can induce energy transfer between different populations of electrons in a plasma, leading to their acceleration or deceleration. \n",
      "\n",
      "The results of these simulations have important implications for understanding the role of KEEN waves in various plasma environments, including space, fusion and astrophysical plasmas. The high-resolution and accurate simulations can also serve as a reference for future research on electrostatic waves in plasmas. 1 into PostgreSQL...\n",
      "Inserting test sample 239  We consider a unit speed curve $\\alpha$ in Euclidean four-dimensional space $E^4$ and denote the Frenet frame by $\\{T,N,B_1,B_2\\}$. We say that $\\alpha$ is a slant helix if its principal normal vector $N$ makes a constant angle with a fixed direction $U$. In this work we give different characterizations of such curves in terms of their curvatures. 0 into PostgreSQL...\n",
      "Inserting test sample 240  This paper investigates the properties of slant helices in the four-dimensional Euclidean space $E^4$. We study their geometric shape, torsion, and curvature. In particular, we focus on the case when a slant helix lies on a cone and determine conditions for its existence. Our findings reveal new insights into the behavior of slant helices in higher dimensions, contributing to the fields of differential geometry and mathematical physics. 1 into PostgreSQL...\n",
      "Inserting test sample 241  Keeping a memory of evolving stimuli is ubiquitous in biology, an example of which is immune memory for evolving pathogens. However, learning and memory storage for dynamic patterns still pose challenges in machine learning. Here, we introduce an analytical energy-based framework to address this problem. By accounting for the tradeoff between utility in keeping a high-affinity memory and the risk in forgetting some of the diverse stimuli, we show that a moderate tolerance for risk enables a repertoire to robustly classify evolving patterns, without much fine-tuning. Our approach offers a general guideline for learning and memory storage in systems interacting with diverse and evolving stimuli. 0 into PostgreSQL...\n",
      "Inserting test sample 242  This study investigates how individuals trade between risk and utility in devising strategies for remembering evolving patterns. The ability to effectively track patterns while balancing memory demands poses a fundamental cognitive challenge that has been largely overlooked in prior research. Using a novel experimental paradigm, we observed that participants flexibly adjusted their memory strategies to optimize performance under different levels of pattern volatility. Specifically, participants deployed more cautious yet reliable strategies under higher levels of volatility and more optimized yet riskier strategies under lower levels of volatility. These findings provide evidence of domain-general cognitive processes that adaptively balance risk and reward to achieve optimal outcomes under dynamic environmental demands. 1 into PostgreSQL...\n",
      "Inserting test sample 243  The Friedmann--Lema\\^{\\i}tre--Robertson--Walker (FLRW) solution to the Einstein-scalar field system with spatial topology $\\mathbb{S}^3$ models a universe that emanates from a singular spacelike hypersurface (the Big Bang), along which various spacetime curvature invariants blow up, only to re-collapse in a symmetric fashion in the future (the Big Crunch). In this article, we give a complete description of the maximal developments of perturbations of the FLRW data at the chronological midpoint of its evolution. We show that the perturbed solutions also exhibit curvature blowup along a pair of spacelike hypersurfaces, signifying the stability of the Big Bang and the Big Crunch.\n",
      "\n",
      "Moreover, we provide a sharp description of the asymptotic behavior of the solution up to the singularities, showing in particular that various time-rescaled solution variables converge to regular tensorfields on the singular hypersurfaces that are close to the corresponding FLRW tensorfields.\n",
      "\n",
      "Our proof crucially relies on $L^2$-type approximate monotonicity identities in the spirit of the ones we used in our joint works with Rodnianski, in which we proved similar results for nearly spatially flat solutions with spatial topology $\\mathbb{T}^3$. In the present article, we rely on new ingredients to handle nearly round spatial metrics on $\\mathbb{S}^3$, whose curvatures are order-unity near the initial data hypersurface. In particular, our proof relies on i) the construction of a globally defined spatial vectorfield frame adapted to the symmetries of a round metric on $\\mathbb{S}^3$; ii) estimates for the Lie derivatives of various geometric quantities with respect to the elements of the frame; and iii) sharp estimates for the asymptotic behavior of the FLRW solution's scale factor near the singular hypersurfaces. 0 into PostgreSQL...\n",
      "Inserting test sample 244  In this study, we investigate the maximal development of near-Friedman-LemaÃ®tre-Robertson-Walker (FLRW) data for the Einstein-scalar field system with spatial topology $\\mathbb{S}^3$. We consider a closed, homogeneous and isotropic universe and we explore the behavior of solutions of the scalar and gravitational fields in this setting. Specifically, we study the evolution of the system from initial data that closely approximates solutions of the FLRW equations to a maximal development of the fields, where no event horizons are present. \n",
      "\n",
      "We employ techniques from numerical relativity to simulate the solutions to the Einstein-scalar field system. Our results demonstrate the existence of solutions that interpolate between early-time, FLRW-like data and a maximal extension of the scalar and gravitational fields, without the formation of singularities or event horizons in the spacetime. We further examine the relationship between the strength of the initial scalar field and the properties of the final, maximal extension, and we develop a criterion for the strength of the initial data such that no horizons form during the evolution.\n",
      "\n",
      "This work provides insight into the development of strong field regimes from nearly homogeneous and isotropic initial data, as well as the formation and properties of gravitational singularities in general relativity. Future work may extend this analysis to different spatial topologies and explore the impact of matter fields on these solutions. 1 into PostgreSQL...\n",
      "Inserting test sample 245  We study the dynamic behaviour of a quantum two-level system with periodically varying parameters by solving the master equation for the density matrix. Two limiting cases are considered: multiphoton Rabi oscillations and Landau-Zener transitions. The approach is applied to the description of the dynamics of superconducting qubits. In particular, the case of the interferometer-type charge qubit with periodically varying parameters (gate voltage or magnetic flux) is investigated. The time-averaged energy level populations are calculated as funtions of the qubit's control parameters. 0 into PostgreSQL...\n",
      "Inserting test sample 246  This paper investigates the dynamic behaviour of Josephson-junction qubits. Specifically, the crossover between Rabi oscillations and Landau-Zener transitions is explored. Through simulations, we find that the qubit's transition probability oscillates with the Rabi frequency and decays exponentially as the Landau-Zener parameter increases. Our results provide insights into the design and optimization of quantum computing protocols based on these types of qubits. These findings may lead to the development of more efficient and reliable quantum computers in the future. 1 into PostgreSQL...\n",
      "Inserting test sample 247  In the standard model of particle physics, all fermions are fundamentally massless and only acquire their effective bare mass when the Higgs field condenses. Therefore, in a fundamental de Broglie-Bohm pilot-wave quantum field theory (valid before and after the Higgs condensation), position beables should be attributed to massless fermions. In our endeavour to build a pilot-wave theory of massless fermions, which would be relevant for the study of quantum non-equilibrium in the early universe, we are naturally led to Weyl spinors and to particle trajectories which give meaning to the `zig-zag' picture of the electron discussed recently by Penrose. We show that a positive-energy massive Dirac electron of given helicity can be thought of as a superposition of positive and negative energy Weyl particles of the same helicity and that a single massive Dirac electron can in principle move luminally at all times.\n",
      "\n",
      "This is however not true for the many body situation required by quantum field theory and we conclude that a more natural theory arises from attributing beable status to the positions of massless Dirac particles instead of to Weyl particles. 0 into PostgreSQL...\n",
      "Inserting test sample 248  \"The zig-zag road to reality\" explores the complex, often paradoxical nature of reality and how our understanding of it has evolved over time. We examine the various theoretical frameworks and models that have been proposed to explain the nature of reality, including classical physics, quantum mechanics, and string theory. Each of these approaches has its own unique strengths and limitations, and our analysis reveals that a more comprehensive understanding of reality requires us to view it as a kind of \"cosmic tapestry\" - a complex, interconnected web of phenomena that are simultaneously local and global, finite and infinite, particle-like and wave-like, physical and metaphysical. Along the way, we discuss the implications of these findings for our understanding of consciousness, the universe, and the role of science in human society. Ultimately, our exploration of the zig-zag road to reality underscores the importance of maintaining an open mind, embracing paradox, and engaging in ongoing dialogue and experimentation in the pursuit of truth and understanding. 1 into PostgreSQL...\n",
      "Inserting test sample 249  According to generalized Mellin derivative (Kargin), we introduce a new family of polynomials called higher order generalized geometric polynomials. We obtain some properties of them.We discuss their connections to degenerate Bernoulli and Euler polynomials. Furthermore, we find new formulas for the Carlitz's (Carlitz) and Howard's (Howard2) finite sums. Finally, we evaluate several series in closed forms, one of which has the coefficients include values of the Riemann zeta function. Moreover, we calculate some integrals in terms of generalized geometric polynomials. 0 into PostgreSQL...\n",
      "Inserting test sample 250  We introduce a novel class of polynomials, the higher order generalized geometric polynomials (HOGGP), which extends the well-known class of generalized geometric polynomials. We provide the explicit expression, properties, and some combinatorial interpretations of HOGGP. In addition, we establish several relationships between HOGGP and various types of orthogonal polynomials. Finally, we investigate their approximation properties and show that they possess good convergence properties on a wide class of functions. Our results pave the way for further investigations into the use of these polynomials in approximation theory and numerical analysis. 1 into PostgreSQL...\n",
      "Inserting test sample 251  We used the red clump stars from the photometric data of the Optical Gravitational Lensing Experiment(OGLE II) survey and the Magellanic Cloud Photometric Survey (MCPS) for both the Clouds to estimate the depth.The observed dispersion in the magnitude and colour distribution of red clump stars is used to estimate the depth, after correcting for population effects, internal reddening within the Clouds and photometric errors.The observed dispersion due to the line of sight depth ranges from 0.023 mag to 0.45 mag (a depth of 500 pc to 10.4 kpc) for LMC and from 0.025 to 0.34 magnitude(a depth of 670 pc to 9.53 kpc).The depth profile of the LMC bar indicates that it is flared.The average depth in the bar region is 4.0$\\pm$1.4 kpc. The northern disk is found to have depth(4.17$\\pm$0.97 kpc)larger than the southern part of the disk (2.63$\\pm$0.8kpc).There is no indication of depth variation between the eastern and the western disk.The average depth for the disk is 3.44$\\pm$ 1.16 kpc.In the case of SMC, the bar depth(4.90$\\pm$1.23 kpc)and the disk depth (4.23$\\pm$1.48kpc)are found to be within the standard deviations.A prominent feature in the SMC is the increase in depth near the optical center.The large dispersions estimated for the LMC bar and the northern disk suggest that the LMC either has large depth and/or different stellar populations in these regions.The halo of the LMC(using RR Lyrae stars)is found to have larger depth compared to the disk/bar,which supports the existence of an inner halo for the LMC.On the other hand, the estimated depths for the halo(RR Lyrae stars)and disk are found to be similar,for the SMC bar region. Thus,increased depth and enhanced stellar as well as HI density near the optical center suggests that the SMC may have a bulge. 0 into PostgreSQL...\n",
      "Inserting test sample 252  Depth estimation of the Large and Small Magellanic Clouds is a complex problem in astrophysics that has been investigated for decades. The Magellanic Clouds are two neighboring dwarf galaxies that orbit around our Milky Way galaxy, and they hold the key to understanding the formation and evolution of galaxies in the Universe. Accurately estimating the depth of the Magellanic Clouds can shed light on their kinematics, structure and interactions with the Milky Way and other galaxies.\n",
      "\n",
      "In this study, we present a novel method for estimating the depth of the Magellanic Clouds using photometric and astrometric data from the Gaia mission. We use a deep learning algorithm to train a neural network model to predict the distance to the Magellanic Clouds from their positions, proper motions and brightness in the sky. Our method achieves a precision of 5% for the Large Magellanic Cloud and 10% for the Small Magellanic Cloud in a range of distances up to 100 kpc, which is comparable to previous studies based on more traditional methods.\n",
      "\n",
      "We validate our results by comparing them with independent measurements from other telescopes and surveys, such as the Hubble Space Telescope and the MACHO project. Our method provides consistent estimates of the depth of the Magellanic Clouds across different regions of the sky, indicating that our method is robust and reliable.\n",
      "\n",
      "We use the estimated distances to study the 3D structure of the Magellanic Clouds and their interactions with the Milky Way. We find that the Large Magellanic Cloud is inclined with respect to the plane of the Milky Way, and it has a bow shock structure that is consistent with its motion through the Galactic halo. The Small Magellanic Cloud, on the other hand, is closer to the plane of the Milky Way and has a more chaotic motion, which suggests that it has been disrupted by the Milky Way and other nearby galaxies.\n",
      "\n",
      "In conclusion, our study presents a new method for estimating the depth of the Magellanic Clouds using machine learning techniques, which provides accurate and reliable results that can be used to study their structure and interactions with the Milky Way and other galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 253  This is the first of three papers that develop structures which are counted by a \"parabolic\" generalization of Catalan numbers. Fix a subset R of {1,..,n-1}. Consider the ordered partitions of {1,..,n} whose block sizes are determined by R. These are the \"inverses\" of (parabolic) multipermutations whose multiplicities are determined by R. The standard forms of the ordered partitions are refered to as \"R-permutations\". The notion of 312-avoidance is extended from permutations to R-permutations. Let lambda be a partition of N such that the set of column lengths in its shape is R or R union {n}. Fix an R-permutation pi. The type A Demazure character (key polynomial) in x_1, .., x_n that is indexed by lambda and pi can be described as the sum of the weight monomials for some of the semistandard Young tableau of shape lambda that are used to describe the Schur function indexed by lambda. Descriptions of these \"Demazure\" tableaux developed by the authors in earlier papers are used to prove that the set of these tableaux is convex in Z^N if and only if pi is R-312-avoiding if and only if the tableau set is the entire principal ideal generated by the key of pi. These papers were inspired by results of Reiner and Shimozono and by Postnikov and Stanley concerning coincidences between Demazure characters and flagged Schur functions. This convexity result is used in the next paper to deepen those results from the level of polynomials to the level of tableau sets. The R-parabolic Catalan number is defined to be the number of R-312-avoiding permutations. These special R-permutations are reformulated as \"R-rightmost clump deleting\" chains of subsets of {1,..,n} and as \"gapless R-tuples\"; the latter n-tuples arise in multiple contexts in these papers. 0 into PostgreSQL...\n",
      "Inserting test sample 254  This research paper explores the convexity properties of the set of tableaux arising from type A Demazure characters. These characters, known as key polynomials, play a fundamental role in the study of representation theory and symmetric functions. Our approach involves analyzing the combinatorial structure of these tableaux and characterizing their shapes.\n",
      "\n",
      "One particularly interesting aspect of our investigation concerns the parabolic Catalan numbers associated with these tableaux. These numbers are a natural extension of the classical Catalan numbers and capture the number of essentially different ways in which a particular tableaux set can be constructed.\n",
      "\n",
      "Our main result establishes the convexity of the set of tableaux for type A Demazure characters in the respect of its major index statistic. This is a noteworthy and previously unexplored result, providing new insights into the properties of these important objects. Our proof involves establishing an explicit formula for the generating function of these tableaux, which we then use to construct a polynomial that proves the convexity property.\n",
      "\n",
      "Our work represents a significant contribution to the field of symmetric functions and representation theory. The results we have obtained will have implications for many other areas of mathematics, including combinatorics, algebra, and geometry. In particular, our findings may have implications for the study of Schubert varieties and other related objects. 1 into PostgreSQL...\n",
      "Inserting test sample 255  In this paper, we extend the recent analysis of the new large $D$ limit of matrix models to the cases where the action contains arbitrary multi-trace interaction terms as well as to arbitrary correlation functions. We discuss both the cases of complex and Hermitian matrices, with $\\text{U}(N)^{2}\\times\\text{O}(D)$ and $\\text{U}(N)\\times\\text{O}(D)$ symmetries respectively. In the latter case, the new large $D$ limit is consistent for planar diagrams; at higher genera, it crucially requires the tracelessness condition. For similar reasons, the large $N$ limit of tensor models with reduced symmetries is typically inconsistent already at leading order without the tracelessness condition. We also further discuss some interesting properties of purely bosonic models pointed out recently and explain that the standard argument predicting a non-trivial IR behaviour in fermionic models \\`a la SYK does not work for bosonic models. Finally, we explain that the new large $D$ scaling is consistent with linearly realized supersymmetry. 0 into PostgreSQL...\n",
      "Inserting test sample 256  In this paper, we explore the new large $D$ limit of matrix models. Using advanced mathematical techniques, we investigate the various properties of this limit and derive important results. Specifically, we extend the previous works on the subject by providing a more detailed analysis of the behavior of matrix models in this limit. Our findings show that the large $D$ limit is crucial in understanding the complex behavior of matrix models and shedding light on many important phenomena, such as the emergence of higher genus topologies and the appearance of critical points. Furthermore, we examine the mathematical implications of these results and provide insights into the application of matrix models in other fields. Through our rigorous mathematical analysis, we contribute to the ongoing effort in understanding the fascinating properties of matrix models in the new large $D$ limit. 1 into PostgreSQL...\n",
      "Inserting test sample 257  We demonstrate the possibility of achieving the maximum possible singlet fraction using a entangled mixed two-qubit state as a resource. For this, we establish a tight upper bound on singlet fraction and show that the maximal singlet fraction obtained in \\cite{Verstraete} does not attain the obtained upper bound on the singlet fraction. Interestingly, we found that the required upper bound can in fact be achieved using local filtering operations. 0 into PostgreSQL...\n",
      "Inserting test sample 258  We derive an upper bound on the singlet fraction of two-qubit mixed entangled states using semidefinite programming. Our analysis shows that the maximum possible singlet fraction depends only on the state's fidelity with respect to the pure singlet and an explicitly computed entanglement measure. We also provide a simple criterion for determining when the bound is achieved. These results have important implications for quantum information and quantum computation protocols that rely on entangled states. 1 into PostgreSQL...\n",
      "Inserting test sample 259  L1157-B1 is the brightest shocked region of the large-scale molecular outflow, considered the prototype of chemically rich outflows, being the ideal laboratory to study how shocks affect the molecular gas. Several deuterated molecules have been previously detected with the IRAM 30m, most of them formed on grain mantles and then released into the gas phase due to the shock. We aim to observationally investigate the role of the different chemical processes at work that lead to formation the of DCN and test the predictions of the chemical models for its formation. We performed high-angular resolution observations with NOEMA of the DCN(2-1) and H13CN(2-1) lines to compute the deuterated fraction, Dfrac(HCN). We detected emission of DCN(2-1) and H13CN(2-1) arising from L1157-B1 shock. Dfrac(HCN) is ~4x10$^{-3}$ and given the uncertainties, we did not find significant variations across the bow-shock. Contrary to HDCO, whose emission delineates the region of impact between the jet and the ambient material, DCN is more widespread and not limited to the impact region. This is consistent with the idea that gas-phase chemistry is playing a major role in the deuteration of HCN in the head of the bow-shock, where HDCO is undetected as it is a product of grain-surface chemistry. The spectra of DCN and H13CN match the spectral signature of the outflow cavity walls, suggesting that their emission result from shocked gas. The analysis of the time dependent gas-grain chemical model UCL-CHEM coupled with a C-type shock model shows that the observed Dfrac(HCN) is reached during the post-shock phase, matching the dynamical timescale of the shock. Our results indicate that the presence of DCN in L1157-B1 is a combination of gas-phase chemistry that produces the widespread DCN emission, dominating in the head of the bow-shock, and sputtering from grain mantles toward the jet impact region. 0 into PostgreSQL...\n",
      "Inserting test sample 260  The L1157-B1 outflow is considered a prototypical site for investigating the formation of deuterated molecules in star-forming regions. In this work, we present new observations of DCN toward L1157-B1 obtained with the IRAM 30m telescope as part of the ASAI Large Program. The high-angular resolution images reveal that DCN is primarily associated with the jet-like outflow rather than the quiescent gas surrounding the outflow. Based on the comparison with other molecular tracers, our chemical modelling suggests that DCN traces the shocked gas in the outflow, whereas DCO+ and N2D+ mainly trace the quiescent gas. The shock chemical model suggests that the fractional abundance of DCN increases toward the shock front and peaks at a radius of ~10^16 cm from the central source. The observed DCN emission can be reproduced with a shock model that includes a pre-shock density enhancement, suggesting that the L1157-B1 outflow is likely propagating into a medium previously enriched in deuterium. Our observations and modelling highlight the power of DCN as a shock tracer in jet-like molecular outflows. They also provide important constraints on the shock conditions in the L1157-B1 outflow, and suggest that this outflow is an excellent laboratory to study deuterium chemistry and the formation of deuterated molecules in star-forming regions. 1 into PostgreSQL...\n",
      "Inserting test sample 261  Over the last few years there has been much interest in the production of hard X-rays from various targets using a kHz short pulse laser at intensities above 1014Wcm-2 (A). Most of these studies have been carried out in vacuum and very many fewer studies have been carried out in air. Recently this lack has been partially addressed with the development of femtosecond laser micromachining. Another similar although apparently unconnected field (B) deals with the channelling of high power laser beam in filaments after passage through long distances in air. This has been largely driven by the construction of a mobile terawatt laser beam (Teramobile) for atmospheric studies. The laser beams in these two cases (A and B) have very different pulse energies (mJ against J) although the filaments in (B) have similar energies to (A) and are clamped at intensities less than 1014 Wcm-2. This paper has been written to compare the production of hard X-rays in these two cases. The conclusion is interesting that a focused sub TW laser pulse in air reaches intensities sufficiently high that characteristic K and L X-rays are generated from a number of metal and non metal targets as well as a continuous bremsstrahlung spectrum. On the other hand the clamping of the multi-filaments in a 100 TW laser beam in air cannot generate hard Xrays especially when propagated over long distances. 0 into PostgreSQL...\n",
      "Inserting test sample 262  In this study, we compare the production of hard X-rays from various targets in air using a short pulse kHz laser with photon production from a high power multifilament laser beam from the same targets in air. The experiment was done to investigate the efficacy of these two methods to produce high energy photons. The experimental setup involved the use of a Nd:YLF laser with a pulse of up to 10 J and 10 ns at a 1-kHz repetition rate for the kHz laser, while a double-clad fiber laser generating 120 W at 1.07Î¼m with ~500Î¼m core diameter was used as the high power multifilament laser beam source. Four different target materials, including copper, aluminum, silver, and gold were used for this study. Our results show that the high power multifilament laser beam produced more photons compared to the kHz laser from all the tested targets. The photon flux for silver was the highest for both methods. We observed that the photon production using the high power multifilament laser beam is more homogeneous than the kHz laser. In conclusion, our results suggest that using a high power multifilament laser beam can be a more reliable and effective method for producing high energy photons from various targets in air compared to the short pulse kHz laser, which could have implications for applications in medical imaging and industrial inspection. 1 into PostgreSQL...\n",
      "Inserting test sample 263  In this work we investigate the impact of calculating non-equilibrium chemical abundances consistently with the temperature structure for the atmospheres of highly-irradiated, close-in gas giant exoplanets. Chemical kinetics models have been widely used in the literature to investigate the chemical compositions of hot Jupiter atmospheres which are expected to be driven away from chemical equilibrium via processes such as vertical mixing and photochemistry. All of these models have so far used pressure--temperature (P-T) profiles as fixed model input. This results in a decoupling of the chemistry from the radiative and thermal properties of the atmosphere, despite the fact that in nature they are intricately linked. We use a one-dimensional radiative-convective equilibrium model, ATMO, which includes a sophisticated chemistry scheme to calculate P-T profiles which are fully consistent with non-equilibrium chemical abundances, including vertical mixing and photochemistry. Our primary conclusion is that, in cases of strong chemical disequilibrium, consistent calculations can lead to differences in the P-T profile of up to 100 K compared to the P-T profile derived assuming chemical equilibrium. This temperature change can, in turn, have important consequences for the chemical abundances themselves as well as for the simulated emission spectra. In particular, we find that performing the chemical kinetics calculation consistently can reduce the overall impact of non-equilibrium chemistry on the observable emission spectrum of hot Jupiters. Simulated observations derived from non-consistent models could thus yield the wrong interpretation. We show that this behaviour is due to the non-consistent models violating the energy budget balance of the atmosphere. 0 into PostgreSQL...\n",
      "Inserting test sample 264  This study investigates the impact of consistent chemical kinetics calculations on the Pressure-Temperature (PT) profiles and emission spectra of hot Jupiters. We present a comprehensive one-dimensional atmospheric model that uses a self-consistent chemical kinetics solution coupled with radiative transfer calculations. We consider CO, CH4, and H2O as primary molecules and include more complex molecules such as NH3, HCN, and CO2 in our computations. Our results reveal significant differences in the PT profiles of the models using consistent chemical kinetics compared to those using a simplified scheme. These differences are strongly dependent on the metallicity and C/O ratio of the atmosphere. Moreover, we find that changes in the chemical composition lead to considerable variations in the emission spectra of hot Jupiters, including significant differences in prominent molecular absorption features. We show that the H2O abundance in the atmosphere significantly affects the PT profile and alters the emission spectra. We find that the contribution of minor species like NH3, HCN, and CO2 also needs to be considered for characterizing the chemical composition of hot Jupiter atmospheres. Our results have crucial implications for interpreting spectroscopic data of exoplanets, and their potential impact on our understanding of the planet formation process. 1 into PostgreSQL...\n",
      "Inserting test sample 265  Many computational models were proposed to extract temporal patterns from clinical time series for each patient and among patient group for predictive healthcare. However, the common relations among patients (e.g., share the same doctor) were rarely considered. In this paper, we represent patients and clinicians relations by bipartite graphs addressing for example from whom a patient get a diagnosis. We then solve for the top eigenvectors of the graph Laplacian, and include the eigenvectors as latent representations of the similarity between patient-clinician pairs into a time-sensitive prediction model. We conducted experiments using real-world data to predict the initiation of first-line treatment for Chronic Lymphocytic Leukemia (CLL) patients.\n",
      "\n",
      "Results show that relational similarity can improve prediction over multiple baselines, for example a 5% incremental over long-short term memory baseline in terms of area under precision-recall curve. 0 into PostgreSQL...\n",
      "Inserting test sample 266  This paper proposes a novel Graph-Augmented Time-Sensitive Model for predicting treatment initiation from clinical time series data. The proposed approach leverages patient-specific clinical observations to construct graphs that are used to capture the correlations and dependencies between different features. These graphs are then integrated into a time-sensitive deep learning model that incorporates recurrent and convolutional layers to capture temporal and spatial information. To evaluate the proposed approach, we conducted experiments on real-world electronic health records data. The results demonstrate that the proposed model achieves state-of-the-art performance on the task of predicting treatment initiation. Furthermore, we conduct sensitivity analyses to gain insights into the importance of different features and demonstrate the interpretability of the model. Overall, our approach provides a promising direction for leveraging clinical time series data to improve patient outcomes. 1 into PostgreSQL...\n",
      "Inserting test sample 267  The anomalies in the $B$-meson sector, in particular $R_{K^{(*)}}$ and $R_{D^{(*)}}$, are often interpreted as hints for physics beyond the Standard Model. To this end, leptoquarks or a heavy $Z'$ represent the most popular SM extensions which can explain the observations. However, adding these fields by hand is not very satisfactory as it does not address the big questions like a possible embedding into a unified gauge theory. On the other hand, light leptoquarks within a unified framework are challenging due to additional constraints such as lepton flavor violation. The existing accounts typically deal with this issue by providing estimates on the relevant couplings. In this letter we consider a complete model based on the $SU(4)_{\\rm C}\\otimes SU(2)_{\\rm L}\\otimes U(1)_{\\rm R}$ gauge symmetry, a subgroup of $SO(10)$, featuring both scalar and vector leptoquarks. We demonstrate that this setup has, in principle, all the potential to accommodate $R_{K^{(*)}}$ and $R_{D^{(*)}}$ while respecting bounds from other sectors usually checked in this context. However, it turns out that $K_L \\to e^{\\pm} \\mu^{\\mp}$ severely constraints not only the vector but also the scalar leptoquarks and, consequently, also the room for any sizeable deviations of $R_{K^{(*)}}$ from 1. We briefly comment on the options for extending the model in order to conform this constraint. Moreover, we present a simple criterion for all-orders proton stability within this class of models. 0 into PostgreSQL...\n",
      "Inserting test sample 268  The Standard Model of particle physics, while extremely successful, fails to account for certain experimental observations. One of these is the observed deviations from lepton universality in $B$-meson decays. One possible explanation for this is the existence of leptoquarks, particles that allow for interactions between leptons and quarks. In this paper, we present a unified leptoquark model that is consistent with the observed violations of lepton universality.\n",
      "\n",
      "We explore the experimental data within the context of our model, comparing the predictions of our theory with the precise measurements of $B$-meson decays. We find that our model is able to account for the observed deviations from lepton universality and that its predictions are in good agreement with experimental results. Furthermore, our model provides a natural explanation for why leptoquarks have not yet been observed in other experiments.\n",
      "\n",
      "We also analyze the constraints on our model from other experimental data, including constraints from collider experiments and precision measurements of other observables. We find that our model is consistent with all available data.\n",
      "\n",
      "In conclusion, we present a unified leptoquark model that provides a promising explanation for the observed deviations from lepton universality in $B$-meson decays. Our model is consistent with all available experimental data and provides a natural explanation for the absence of leptoquark signals in other experiments. Future experimental studies will help to further test our model and provide more information about the fundamental nature of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 269  We search for a direction in the sky that exhibits parity symmetry under reflections through a plane. We use the natural estimator, which compares the power in even and odd $\\ell+m$ multipoles, and apply minimal blind masking of outliers to the ILC map in order to avoid large errors in the reconstruction of multipoles. The multipoles of the cut sky are calculated both naively and by using the covariance inversion method and we estimate the significance of our results using $\\Lambda$CDM simulations. Focusing on low multipoles, $2\\leq \\ell \\leq \\ell_{\\max}$ with $\\ell_{\\max}=5,6$ or even 7, we find two perpendicular directions of even and odd parity in the map. While the even parity direction does not appear significant, the odd direction is quite significant -- at least a $3.6\\sigma$ effect. 0 into PostgreSQL...\n",
      "Inserting test sample 270  The cosmic microwave background radiation (CMB) carries the earliest imprints of the universe, providing insights into the fundamental aspects of the universe's evolution. One of the most striking properties of the CMB is the large scale parity asymmetry, commonly known as the parity anomaly. Recent cosmological surveys have provided evidence of this phenomenon and highlighted its significance in understanding the universe's origin. This study focuses on exploring the origin and nature of the parity anomaly in the CMB and its implications for cosmological models. Using state-of-the-art data analysis techniques, we investigate the statistical properties of the CMB maps and present compelling evidence in support of the existence of the parity anomaly. Our findings have significant implications for our understanding of cosmic inflation, the nature of dark energy, and the ultimate fate of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 271  We present the conditional value-at-risk (CVaR) in the context of Markov chains and Markov decision processes with reachability and mean-payoff objectives. CVaR quantifies risk by means of the expectation of the worst p-quantile. As such it can be used to design risk-averse systems. We consider not only CVaR constraints, but also introduce their conjunction with expectation constraints and quantile constraints (value-at-risk, VaR). We derive lower and upper bounds on the computational complexity of the respective decision problems and characterize the structure of the strategies in terms of memory and randomization. 0 into PostgreSQL...\n",
      "Inserting test sample 272  This paper introduces a novel technique for identifying optimal policies for Markov Decision Processes (MDPs) under risk constraints. Specifically, we propose the use of Conditional Value-at-Risk (CVaR) to address two widely studied objectives in MDPs, namely reachability and mean payoff. Our technique leverages a specialized optimization algorithm to compute an optimal policy that respects the imposed risk constraints, thus leading to more reliable decision-making. We demonstrate the effectiveness of our approach through extensive numerical experiments and comparisons with existing methods. Our results indicate that our proposed method outperforms state-of-the-art methods and enables handling of risk constraints in MDPs. 1 into PostgreSQL...\n",
      "Inserting test sample 273  Two lattice points are visible to one another if there exist no other lattice points on the line segment connecting them. In this paper we study convex lattice polygons that contain a lattice point such that all other lattice points in the polygon are visible from it. We completely classify such polygons, show that there are finitely many of lattice width greater than $2$, and computationally enumerate them. As an application of this classification, we prove new obstructions to graphs arising as skeleta of tropical plane curves. 0 into PostgreSQL...\n",
      "Inserting test sample 274  This paper investigates the properties of convex lattice polygons with all lattice points visible, meaning that every lattice point lies on the boundary or inside of the polygon and has a line of sight to the boundary. We present algorithms for constructing such polygons and prove necessary and sufficient conditions for their existence. Our study reveals interesting connections between this problem and other areas of mathematics, such as discrete geometry, combinatorics, and number theory. Our findings shed new light on the structural and geometric properties of convex lattice polygons and provide insights into their potential applications in various fields. 1 into PostgreSQL...\n",
      "Inserting test sample 275  We present a catalog of stellar age and mass estimates for a sample of 640\\,986 red giant branch (RGB) stars of the Galactic disk from the LAMOST Galactic Spectroscopic Survey (DR4). The RGB stars are distinguished from the red clump stars utilizing period spacing derived from the spectra with a machine learning method based on kernel principal component analysis (KPCA).\n",
      "\n",
      "Cross-validation suggests our method is capable of distinguishing RC from RGB stars with only 2 per cent contamination rate for stars with signal-to-noise ratio (SNR) higher than 50. The age and mass of these RGB stars are determined from their LAMOST spectra with KPCA method by taking the LAMOST - $Kepler$ giant stars having asteroseismic parameters and the LAMOST-TGAS sub-giant stars based on isochrones as training sets. Examinations suggest that the age and mass estimates of our RGB sample stars with SNR $>$ 30 have a median error of 30 per cent and 10 per cent, respectively. Stellar ages are found to exhibit positive vertical and negative radial gradients across the disk, and the age structure of the disk is strongly flared across the whole disk of $6<R<13$\\,kpc. The data set demonstrates good correlations among stellar age, [Fe/H] and [$\\alpha$/Fe]. There are two separate sequences in the [Fe/H] -- [$\\alpha$/Fe] plane: a high--$\\alpha$ sequence with stars older than $\\sim$\\,8\\,Gyr and a low--$\\alpha$ sequence composed of stars with ages covering the whole range of possible ages of stars. We also examine relations between age and kinematic parameters derived from the Gaia DR2 parallax and proper motions. Both the median value and dispersion of the orbital eccentricity are found to increase with age. The vertical angular momentum is found to fairly smoothly decrease with age from 2 to 12\\,Gyr, with a rate of about $-$50\\,kpc\\,km\\,s$^{-1}$\\,Gyr$^{-1}$. A full table of the catalog is public available online. 0 into PostgreSQL...\n",
      "Inserting test sample 276  The LAMOST Galactic Spectroscopic Survey (LGSS) provides a rich sample of Red Giant Branch (RGB) stars that can be studied to determine their ages and masses. In this work, we present a detailed analysis of the ages and masses of 0.64 million RGB stars, based on the LGSS data. We use a combination of spectroscopic and photometric measurements to obtain accurate estimates of fundamental stellar parameters such as effective temperature, surface gravity, metallicity, and reddening. \n",
      "\n",
      "We analyze the age and mass distributions of the RGB stars using a maximum likelihood method, which allows us to account for the uncertainties in the stellar parameters. Our results show that the RGB stars in the LGSS sample span a wide range of ages from 1 to 10 billion years, with a peak at around 4 billion years. The mass distribution is also broad, ranging from 0.5 to 2 solar masses, with a median value of 1.2 solar masses. We find that the age and mass distributions of the RGB stars are consistent with the predictions of stellar evolution models. \n",
      "\n",
      "We investigate the trends of age and mass with metallicity, and find that the older and more massive RGB stars are generally more metal-rich than their younger and less massive counterparts. This is consistent with the idea that metal-rich stars form later than metal-poor stars. \n",
      "\n",
      "Our study represents one of the largest and most accurate analyses of RGB stars to date, and provides valuable insights into the evolution of stars in the Milky Way galaxy. Our results will be useful for a wide range of astrophysical studies, such as galactic archaeology, stellar populations, and chemical evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 277  We analyze and model an M8.0 flare on 2005 May 13 observed by TRACE and RHESSI to determine the energy release rate from magnetic reconnection that forms and heats numerous flare loops. The flare exhibits two ribbons in UV 1600 {\\AA} emission. Analysis shows that the UV light curve at each flaring pixel rises impulsively within a few minutes, and decays slowly with a timescale >10 min. Since the lower atmosphere (transition region and chromosphere) responds to energy deposit nearly instantaneously, the rapid UV brightening is thought to reflect the energy release process in the newly formed flare loop rooted at the footpoint. We utilize spatially resolved (down to 1 arcsec) UV light curves and thick-target hard X-ray emission to construct heating functions of a few thousand flare loops anchored at UV foot points, and compute plasma evolution in these loops using the EBTEL model. The modeled coronal temperatures and densities of these flare loops are used to calculate synthetic soft X-ray spectra and light curves, which compare favorably with those observed by RHESSI and GOES/XRS. The time-dependent transition region DEM for each loop during its decay phase is also computed with a simplified model and used to calculate the optically-thin C IV line emission, which dominates the UV 1600 {\\AA} bandpass during the flare. The computed C IV line emission decays at the same rate as observed. This study presents a method to constrain heating of reconnection-formed flare loops using all available observables independently, and provides insight into the physics of energy release and plasma heating during the flare. With this method, the lower limit of the total energy used to heat the flare loops in this event is estimated to be 1.22e31 ergs, of which only 1.9e30 ergs is carried by beam-driven upflows during the impulsive phase, suggesting that the coronal plasmas are predominantly heated in situ. 0 into PostgreSQL...\n",
      "Inserting test sample 278  In this paper, we present a study on the heating rates within the reconnection-formed flare loops of the M8.0 flare that occurred on May 13, 2005. The aim of this study is to investigate the mechanism of energy release and the heating process during solar flares.\n",
      "\n",
      "We use a combination of observational data and modeling techniques to analyze the heating rates of the flare loops. The observations are obtained from the Reuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI) and the Solar Dynamics Observatory (SDO). We use a hydrodynamic model to simulate the heating rates in the flare loops.\n",
      "\n",
      "Our results indicate that the heating rates within the flare loops are highly localized and concentrated near the reconnection point. The heating is found to be intermittent and highly time-dependent. Our modeling also reveals that the heating rates are strongly correlated with the magnetic field strength and the reconnection rate.\n",
      "\n",
      "Further, we investigate the nature of the heating process within the flare loops. Our observations suggest that the heating is likely dominated by magnetic reconnection. The sudden release of magnetic energy results in an explosive release of thermal and non-thermal particles. These particles then deposit their energy in the flare loops, leading to the observed heating.\n",
      "\n",
      "Our findings have important implications for understanding the physics of solar flares and the fundamental processes that drive magnetic reconnection. The knowledge gained from our study can be used to improve our ability to predict when and how solar flares occur, and to better understand their impact on Earth's environment. 1 into PostgreSQL...\n",
      "Inserting test sample 279  Anosov representations were introduced by F. Labourie [18] for fundamental groups of closed negatively curved surfaces, and generalized by O. Guichard and A. Wienhard [19] to representations of arbitrary Gromov hyperbolic groups into real semisimple Lie groups. In this paper, we focus on Anosov representations into the identity component O0(2, n) of O(2, n) for n $\\ge$ 2. Our main result is that any Anosov representation with negative limit set as defined in [8] is the holonomy group of a spatially compact, globally hyperbolic maximal (abbrev.\n",
      "\n",
      "CGHM) conformally flat spacetime. The proof of the spatial compactness needs a particular care. The key idea is to notice that for any spacetime M , the space of lightlike geodesics of M is homeomorphic to the unit tangent bundle of a Cauchy hypersurface of M. For this purpose, we introduce the space of causal geodesics containing timelike and lightlike geodesics of anti-de Sitter space and lightlike geodesics of its conformal boundary: the Einstein spacetime. The spatial compactness is a consequence of the following theorem : Any Anosov representation acts properly discontinuously by isometries on the set of causal geodesics avoiding the limit set; besides, this action is cocompact. It is stated in a general setting by O. Guichard, F. Kassel and A. Wienhard in [11].\n",
      "\n",
      "We can see this last result as a Lorentzian analogue of the action of convex cocompact Kleinian group on the complementary of the limit set in H n. Lastly, we show that the conformally flat spacetime in our main result is the union of two conformal copies of a strongly causal AdS-spacetime with boundary which contains-when the limit set is not a topological (n -- 1)-sphere-a globally hyperbolic region having the properties of a black hole as defined in [2], [3], [4]. 0 into PostgreSQL...\n",
      "Inserting test sample 280  This paper studies the class of Anosov representations and their relation to holonomies of globally hyperbolic, spatially compact, conformally flat spacetimes. Anosov representations are a fundamental concept in the study of hyperbolic geometry, and they arise naturally in the study of certain groups such as lattices in Lie groups of higher rank. We develop the geometric tools necessary for studying Anosov representations in the context of conformal geometry, and we prove that the holonomy of a globally hyperbolic, spatially compact, conformally flat spacetime is always an Anosov representation.\n",
      "\n",
      "We also investigate the relationship between the geometry of the spacetime and the properties of the corresponding Anosov representation. Specifically, we show that the Lyapunov exponents of the representation are intimately related to the conformal structure of the spacetime, and that they can be used to classify Anosov representations in a topological sense. Moreover, we use these results to establish a link between the geometry of Anosov representations and the topology of the underlying spacetime.\n",
      "\n",
      "Another important application of this work is to the study of conformal anomalies in quantum field theory. We show that the invariance of conformal anomalies under certain transformations can be understood in terms of the Anosov property of the corresponding representations. Specifically, we prove that an Anosov representation is always associated with a conformal anomaly that is invariant under dilatations and conformal transformations.\n",
      "\n",
      "The results of this paper have important implications for a wide range of fields, including geometry, topology, and theoretical physics. Our work provides new insights into the structure of Anosov representations and their relationship to the geometry and topology of the spacetime, and it sheds light on the fundamental nature of conformal anomalies in quantum field theory. 1 into PostgreSQL...\n",
      "Inserting test sample 281  During evaporation in porous media, two types of corner films are distinguished. A continuous corner film is connected to the bulk liquid, while a discontinuous one is not. To disclose their effects on evaporation in porous media, a pore network model with both continuous and discontinuous corner films is developed, which considers the capillary and viscous forces as well as the effects of corner films on the threshold pressures of pores. The capillary valve effect induced by the sudden geometrical expansion between the small and large pores is also taken into account in the model. The developed pore network model agrees well with the evaporation experiment with a quasi 2D micro model porous medium, in terms of not only the variation of the liquid saturation in each pore but also the variation of the total evaporation rate. The pore network models that neglect the corner films or the liquid viscosity are also compared with the experiment so as to shed light on the roles of the corner films. The continuous corner films, which contribute to sustain the high evaporation rate, can be interrupted to be the discontinuous ones not only by the gas invasion into pores but also by the capillary scissors effect due to the local convex topology of the solid matrix. 0 into PostgreSQL...\n",
      "Inserting test sample 282  This study presents a pore network model to investigate the process of evaporation in porous media where continuous and discontinuous corner films are present. The model is capable of simulating the evaporation dynamics in a variety of pore geometries with different types of films. The simulations provide insights into the thickness of the corner films and their impact on the evaporation process. The results show that the presence of corner films can prolong the evaporation process and reduce the overall evaporation rate by blocking the movement of air. The model also allows for the visualization of the evaporation front and provides a better understanding of its shape and movement in the porous media. Furthermore, the effects of various factors such as temperature, humidity, and pore size distribution on the evaporation process are examined. The simulation results are compared with experimental data and the model shows good agreement with the experimental results. The findings from this study may contribute to the development of more accurate models of evaporation in porous media and provide insights into engineering applications such as enhanced oil recovery and soil remediation. 1 into PostgreSQL...\n",
      "Inserting test sample 283  The cosmic microwave background and large scale structure are complementary probes to investigate the early and late time universe. After the current accomplishment of the high accuracies of CMB measurements, accompanying precision cosmology from LSS data is emphasized. We investigate the dynamical dark energy models which can produce the same CMB angular power spectra as that of the LCDM model with less than a sub-percent level accuracy. If one adopts the dynamical DE models using the so-called Chevallier-Polarski-Linder (CPL) parametrization, w = w0 + wa(1-a), then one obtains models (w0,wa)= (-0.8,-0.767), (-0.9,-0.375), (-1.1,0.355), (-1.2,0.688) named as M8, M9, M11, and M12, respectively. The differences of the growth rate, f which is related to the redshift space distortions (RSD) between different DE models and the LCDM model are about 0.2% only at z=0. The difference of f between M8 (M9, M11, M12) and the LCDM model becomes maximum at z ~ 0.25 with -2.4 (-1.2, 1.2, 2.5)%. This is a scale-independent quantity. One can investigate the one-loop correction of the matter power spectrum of each model using the standard perturbation theory in order to probe the scale-dependent quantity in the quasi-linear regime ({\\it i.e.} k < 0.4 h/Mpc). The differences in the matter power spectra including the one-loop correction between M8 (M9, M11, M12) and the LCDM model for k= 0.4 h/Mpc scale are 1.8 (0.9, 1.2, 3.0)% at z=0, 3.0 (1.6, 1.9, 4.2)% at z=0.5, and 3.2 (1.7, 2.0, 4.5)% at z=1.0. The bigger departure from -1 of w0, the larger the difference in the power spectrum. Thus, one should use both the RSD and the quasi-linear observable in order to discriminate a viable DE model among a slew of models which are degenerated in CMB. Also we obtain the lower limit on w0 > -1.5 from the CMB acoustic peaks and this will provide the useful limitation on phantom models. 0 into PostgreSQL...\n",
      "Inserting test sample 284  The cosmic microwave background (CMB) is one of the most important tools for studying the early universe, but it has limitations when it comes to understanding dark energy. However, recent work has shown that these limits can be overcome by combining CMB data with large-scale structure (LSS) information.\n",
      "\n",
      "In this paper, we demonstrate a new method for breaking the CMB degeneracy and constraining dark energy models using LSS data. We begin by using a combination of CMB and LSS data to set limits on cosmological parameters, including the equation of state of dark energy. We then show how to break the CMB degeneracy and further constrain this model by using localized LSS data.\n",
      "\n",
      "To do this, we develop a novel hierarchical Bayesian model that includes a range of astrophysical and cosmological probes, including supernovae, baryon acoustic oscillations, and the Lyman-alpha forest. We show that a combination of these observations can effectively break the CMB degeneracy and constrain dark energy parameters.\n",
      "\n",
      "Our results show that combining CMB and LSS data can provide powerful constraints on cosmological parameters, particularly when it comes to understanding the nature of dark energy. We find that model-independent analyses of LSS data can lead to quantitatively and qualitatively different cosmological information compared to analyses using the CMB alone.\n",
      "\n",
      "Overall, our work provides a new approach for breaking the CMB degeneracy and constraining dark energy models using LSS data. We believe that this method will provide a valuable tool for future cosmological studies, and could shed new light on the nature of dark energy. 1 into PostgreSQL...\n",
      "Inserting test sample 285  Modeling of wall-bounded turbulent flows is still an open problem in classical physics, with only modest progress made in the last few decades beyond the so-called `log law', which describes only the intermediate region in wall-bounded turbulence, i.e., $30-50 y^+ \\text{ to } 0.1-0.2 R^+$ (in wall units) in a pipe of radius $R$. Here we propose a fundamentally new approach based on fractional calculus to model the {\\em entire} mean velocity profile from the wall to the centerline of the pipe. Specifically, we represent the Reynolds stresses with a non-local fractional derivative of {\\em variable order} that decays with the distance from the wall. Surprisingly, we find that this variable fractional order has a universal form for all Reynolds numbers and for three different flow types, i.e., channel flow, Couette flow, and pipe flow. We first use existing data bases from direct numerical simulations (DNS) to learn the variable fractional order function, and subsequently we test it against other DNS data and experimental measurements, including the Princeton superpipe experiments. Taken together, our findings reveal the continuous and decaying change of rate of turbulent diffusion from the wall as well as the strong non-locality of turbulent interactions that intensify away from the wall. 0 into PostgreSQL...\n",
      "Inserting test sample 286  This paper introduces a universal fractional model for wall-turbulence that can be applied to a wide range of flows over different surfaces. The model uses a fractional derivative operator to describe the behavior of the velocity fluctuations along the wall. The model is based on experimental and numerical evidence showing that the behavior of wall-turbulence in a range of flows can be characterized by fractional calculus. The model can be utilized to predict macroscopic properties of wall-turbulence such as transition, friction drag reduction, and heat transfer under various flow conditions. The model also provides a better understanding of the underlying physical mechanisms of turbulence in wall-bounded flows. The model is verified against a range of experimental and numerical datasets. The results show that the model accurately captures the key features of wall-turbulence in various flows, including the power-law scaling and spectral characteristics. In conclusion, the universal fractional model presented in this paper has the potential to provide a common language for describing wall-turbulence across different applications and can be used to develop more efficient design strategies for various turbulent flow problems. 1 into PostgreSQL...\n",
      "Inserting test sample 287  In this article, we investigate the formation and disruption of a coronal sigmoid from the active region (AR) NOAA 11909 on 07 December 2013, by analyzing multi-wavelength and multi-instrument observations. Our analysis suggests that the formation of `transient' sigmoid initiated $\\approx$1 hour before its eruption through a coupling between two twisted coronal loop systems. A comparison between coronal and photospheric images suggests that the coronal sigmoid was formed over a simple $\\beta$-type AR which also possessed dispersed magnetic field structure in the photosphere. The line-of-sight photospheric magnetograms also reveal moving magnetic features, small-scale flux cancellation events near the PIL, and overall flux cancellation during the extended pre-eruption phase which suggest the role of tether-cutting reconnection toward the build-up of the flux rope. The disruption of the sigmoid proceeded with a two-ribbon eruptive M1.2 flare (SOL2013-12-07T07:29).\n",
      "\n",
      "In radio frequencies, we observe type III and type II bursts in meter wavelengths during the impulsive phase of the flare. The successful eruption of the flux rope leads to a fast coronal mass ejection (with a linear speed of $\\approx$1085 km s -1 ) in SOHO/LASCO field-of-view. During the evolution of the flare, we clearly observe typical \"sigmoid-to-arcade\" transformation. Prior to the onset of the impulsive phase of the flare, flux rope undergoes a slow rise ($\\approx$15 km s -1 ) which subsequently transitions into a fast eruption ($\\approx$110 km s -1 ). The two-phase evolution of the flux rope shows temporal associations with the soft X-ray precursor and impulsive phase emissions of the M-class flare, respectively, thus pointing toward a feedback relationship between magnetic reconnection and early CME dynamics. 0 into PostgreSQL...\n",
      "Inserting test sample 288  This research paper presents a detailed analysis of a transient coronal sigmoid in active region NOAA 11909. We investigate the build-up phase, M-class eruptive flare, and associated fast coronal mass ejection (CME) that occurred within this active region. Our investigation employs multi-wavelength observations from the Atmospheric Imaging Assembly (AIA) and the Helioseismic and Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO), as well as data from the Global Oscillation Network Group (GONG) and the Solar and Heliospheric Observatory (SOHO). \n",
      "\n",
      "We find that the build-up phase of the transient coronal sigmoid was characterized by a continuous increase in magnetic flux, as well as the formation and rise of a flux rope within the region. This led to the occurrence of an M-class eruptive flare that was accompanied by a fast CME. Our analysis shows that the CME was associated with a filament eruption and that it had a speed of approximately 800 km/s.\n",
      "\n",
      "Furthermore, we find that the transient coronal sigmoid was associated with a complex magnetic topology, including twisted and sheared magnetic fields, which may have played a role in the build-up and eruption of the sigmoid. Our investigation also suggests that the sigmoid was located at the boundary of a large, stable coronal hole, which may have contributed to the formation and stability of the structure.\n",
      "\n",
      "Overall, our findings highlight the importance of studying the dynamics of coronal sigmoids and their associated flares and CMEs in order to better understand the mechanisms that control these phenomena. Our multi-wavelength analysis provides new insights into the physical processes that govern the evolution of coronal structures and the eruptive events that they can produce. 1 into PostgreSQL...\n",
      "Inserting test sample 289  We investigate a spectrum oligopoly market where each primary seeks to sell secondary access to its channel at multiple locations. Transmission qualities of a channel evolve randomly. Each primary needs to select a price and a set of non-interfering locations (which is an independent set in the conflict graph of the region) at which to offer its channel without knowing the transmission qualities of the channels of its competitors. We formulate the above problem as a non-cooperative game. We consider two scenarios-i) when the region is small, ii) when the region is large. In the first setting, we focus on a class of conflict graphs, known as mean valid graphs which commonly arise when the region is small. We explicitly compute a symmetric Nash equilibrium (NE); the NE is threshold type in that primaries only choose independent set whose cardinality is greater than a certain threshold. The threshold on the cardinality increases with increase in quality of the channel on sale. We show that the symmetric NE strategy profile is unique in a special class of conflict graphs (linear graph). In the second setting, we consider node symmetric conflict graphs which arises when the number of locations is large (potentially, infinite). We explicitly compute a symmetric NE that randomizes equally among the maximum independent sets at a given channel state vector. In the NE a primary only selects the maximum independent set at a given channel state vector. We show that the two symmetric NEs computed in two settings exhibit important structural difference. 0 into PostgreSQL...\n",
      "Inserting test sample 290  This research paper investigates the interaction between quality-sensitive pricing strategies and spectrum allocation in oligopolistic markets. Focusing on a duopoly market structure, we extend a previous model to account for the effect of product quality differentiation on price competition. Our analysis reveals that quality-sensitive pricing is a dominant strategy for both firms, leading to an increase in prices and a reduction in consumer surplus. We also show that strategic interactions between firms' quality and price decisions can significantly influence the market outcome, with the incumbent firm having a first-mover advantage in some scenarios. \n",
      "\n",
      "Moreover, we examine the incentives for firms to invest in quality improvement as a way to gain a competitive edge. Our results suggest that the optimal timing and level of investment depend on the trade-off between short-term profitability and long-term competitive advantage. Interestingly, we find that the quality investment decisions of the two firms are asymmetric, leading to a potential Pareto inefficiency in the market. \n",
      "\n",
      "Finally, we investigate the impact of spectrum policy on market outcomes and welfare. In particular, we explore the effect of two spectrum allocation mechanisms: a cooperative allocation and a competitive auction. Our analysis shows that cooperation can lead to higher welfare for consumers, but may also reduce the incentives for firms to invest in quality improvement. Conversely, a competitive auction can generate higher revenues for the regulator, but may also intensify price competition and harm consumer welfare. 1 into PostgreSQL...\n",
      "Inserting test sample 291  We investigate the evolution of dust formed in Population III supernovae (SNe) by considering its transport and processing by sputtering within the SN remnants (SNRs). We find that the fates of dust grains within SNRs heavily depend on their initial radii $a_{\\rm ini}$. For Type II SNRs expanding into the ambient medium with density of $n_{\\rm H,0} = 1$ cm$^{-3}$, grains of $a_{\\rm ini} < 0.05$ $\\mu$m are detained in the shocked hot gas and are completely destroyed, while grains of $a_{\\rm ini} > 0.2$ $\\mu$m are injected into the surrounding medium without being destroyed significantly. Grains with $a_{\\rm ini}$ = 0.05-0.2 $\\mu$m are finally trapped in the dense shell behind the forward shock. We show that the grains piled up in the dense shell enrich the gas up to 10$^{-6}-10^{-4}$ $Z_\\odot$, high enough to form low-mass stars with 0.1-1 $M_\\odot$. In addition, [Fe/H] in the dense shell ranges from -6 to -4.5, which is in good agreement with the ultra-metal-poor stars with [Fe/H] < -4. We suggest that newly formed dust in a Population III SN can have great impacts on the stellar mass and elemental composition of Population II.5 stars formed in the shell of the SNR. 0 into PostgreSQL...\n",
      "Inserting test sample 292  This research paper investigates the impact of newly formed dust in Population III supernova remnants on the elemental composition of Population II.5 stars. These Population III remnants are believed to be the first supernovae of the universe and their remnants still contain the newly formed dust from the supernova explosion. The abundance and composition of this dust is crucial, as it can become incorporated into subsequent generations of stars, altering their chemical makeup. In this work, we use numerical simulations of supernova explosions and the resulting dust formation to study how this newly formed dust evolves over time. We find that this dust can be effectively processed and destroyed by shocks and radiation in the remnant environment, altering its composition and abundance. We also explore how this altered dust can affect the formation and properties of Population II.5 stars, as they are believed to form in low-metallicity environments. Our results suggest that the presence of this altered dust can lead to differences in the elemental abundance patterns of Population II.5 stars compared to stars formed in higher-metallicity environments. This work has important implications for understanding the evolution of the early universe and the chemical enrichment of galaxies over cosmic time. 1 into PostgreSQL...\n",
      "Inserting test sample 293  We present new time-series CCD photometry, in the B and V bands, for the moderately metal-rich ([Fe/H] ~ -1.3) Galactic globular cluster (GC) M62 (NGC 6266). The present dataset is the largest obtained so far for this cluster, and consists of 168 images per filter, obtained with the Warsaw 1.3m telescope at the Las Campanas Observatory (LCO) and the 1.3m telescope of the Cerro Tololo Inter-American Observatory (CTIO), in two separate runs over the time span of three months. The procedure adopted to detect the variable stars was the optimal image subtraction method (ISIS v2.2), as implemented by Alard. The photometry was performed using both ISIS and DAOPHOT/ALLFRAME. We have identified 245 variable stars in the cluster fields that have been analyzed so far, of which 179 are new discoveries. Of these variables, 133 are fundamental mode RR Lyrae stars (RRab), 76 are first overtone (RRc) pulsators, 4 are type II Cepheids, 25 are long-period variables (LPV), 1 is an eclipsing binary, and 6 are not yet well classified. Such a large number of RR Lyrae stars places M62 among the top two most RR Lyrae-rich (in the sense of total number of RR Lyrae stars present) GCs known in the Galaxy, second only to M3 (NGC 5272) with a total of 230 known RR Lyrae stars. Since this study covers most but not all of the cluster area, it is not unlikely that M62 is in fact the most RR Lyrae-rich GC in the Galaxy. In like vein, we were also able to detect the largest sample of LPV's known in a Galactic GC. We analyze a variety of Oosterhoff type indicators for the cluster, and conclude that M62 is an Oosterhoff type I system. This is in good agreement with the moderately high metallicity of the cluster, in spite of its predominantly blue horizontal branch morphology -- which is more typical of Oosterhoff type II systems. We thus conclude that metallicity plays a key role in defining Oosterhoff type. [abridged] 0 into PostgreSQL...\n",
      "Inserting test sample 294  Globular clusters are compact and spherical star clusters that orbit around the Milky Way galaxy. They are interesting astronomical objects that contain a large number of stars in a relatively small region, and are therefore valuable for studying stellar populations and their evolution. In this study, we present time-series photometry of M62, also known as NGC 6266, which is one of the most RR Lyrae-rich globular clusters in the galaxy.\n",
      "\n",
      "We used the 8-meter Subaru Telescope located in Hawaii to obtain high-quality photometric data of M62 over a period of several nights. We then applied various data reduction and analysis techniques to extract the light curves of the stars in the cluster and identify their variable nature. In particular, we determined the periods and amplitudes of the RR Lyrae stars in the cluster and used them to estimate the distance to M62 based on the standard candle property of these stars.\n",
      "\n",
      "Our analysis revealed that M62 has a very large population of RR Lyrae stars, which are pulsating variables that are commonly used for tracing the ages and chemical properties of globular clusters. We identified over 150 RR Lyrae stars, which is the largest such sample in any globular cluster studied to date. We also found evidence of other types of variables, such as eclipsing binaries and SX Phoenicis stars, which add to the complexity of the stellar population in M62.\n",
      "\n",
      "Finally, we combined our results with existing data on M62 from other telescopes and derived a comprehensive picture of the cluster's properties. We found that M62 is located at a distance of about 22,500 light years from the Earth and has a metallicity slightly below solar. Its RR Lyrae stars seem to follow a distinct sequence in the period-amplitude diagram, which may indicate a spread in age or metallicity among them. Overall, our study provides a detailed and accurate characterization of M62 and highlights its unique status among globular clusters in the Milky Way. 1 into PostgreSQL...\n",
      "Inserting test sample 295  A quasi-complementary sequence set (QCSS) refers to a set of two-dimensional matrices with low non-trivial aperiodic auto- and cross- correlation sums. For multicarrier code-division multiple-access applications, the availability of large QCSSs with low correlation sums is desirable. The generalized Levenshtein bound (GLB) is a lower bound on the maximum aperiodic correlation sum of QCSSs.\n",
      "\n",
      "The bounding expression of GLB is a fractional quadratic function of a weight vector $\\mathbf{w}$ and is expressed in terms of three additional parameters associated with QCSS: the set size $K$, the number of channels $M$, and the sequence length $N$. It is known that a tighter GLB (compared to the Welch bound) is possible only if the condition $M\\geq2$ and $K\\geq \\overline{K}+1$, where $\\overline{K}$ is a certain function of $M$ and $N$, is satisfied. A challenging research problem is to determine if there exists a weight vector which gives rise to a tighter GLB for \\textit{all} (not just \\textit{some}) $K\\geq \\overline{K}+1$ and $M\\geq2$, especially for large $N$, i.e., the condition is {asymptotically} both necessary and sufficient. To achieve this, we \\textit{analytically} optimize the GLB which is (in general) non-convex as the numerator term is an indefinite quadratic function of the weight vector.\n",
      "\n",
      "Our key idea is to apply the frequency domain decomposition of the circulant matrix (in the numerator term) to convert the non-convex problem into a convex one. Following this optimization approach, we derive a new weight vector meeting the aforementioned objective and prove that it is a local minimizer of the GLB under certain conditions. 0 into PostgreSQL...\n",
      "Inserting test sample 296  In this paper, we present an asymptotically locally optimal weight vector design for a tighter correlation lower bound of quasi-complementary sequence sets. Quasi-complementary sequences have been extensively used in applications such as code-division multiple access (CDMA) communication systems and radar signal processing. The correlation lower bound of quasi-complementary sequence sets is important in evaluating their performance in these applications. \n",
      "\n",
      "Previous studies have shown that weight vector design plays a crucial role in achieving a tight correlation lower bound. However, existing methods for weight vector design are mostly heuristic and lack theoretical justification. To address this issue, we propose an asymptotically locally optimal weight vector design based on the theory of large deviations. \n",
      "\n",
      "Our proposed method not only guarantees the tightness of the correlation lower bound but also improves computational efficiency compared to existing methods. Furthermore, we demonstrate the effectiveness of the proposed method through numerical simulations. \n",
      "\n",
      "Overall, our research contributes to the development of more efficient and effective weight vector designs for quasi-complementary sequence sets, which can potentially enhance the performance of CDMA communication systems and radar signal processing. The proposed method can be extended to other sequence sets with similar properties, and opens up opportunities for further research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 297  Apollonian circle packings arise by repeatedly filling the interstices between four mutually tangent circles with further tangent circles. We observe that there exist Apollonian packings which have strong integrality properties, in which all circles in the packing have integer curvatures and rational centers such that (curvature)$\\times$(center) is an integer vector. This series of papers explain such properties. A {\\em Descartes configuration} is a set of four mutually tangent circles with disjoint interiors. We describe the space of all Descartes configurations using a coordinate system $\\sM_\\DD$ consisting of those $4 \\times 4$ real matrices $\\bW$ with $\\bW^T \\bQ_{D} \\bW = \\bQ_{W}$ where $\\bQ_D$ is the matrix of the Descartes quadratic form $Q_D= x_1^2 + x_2^2+ x_3^2 + x_4^2 -{1/2}(x_1 +x_2 +x_3 + x_4)^2$ and $\\bQ_W$ of the quadratic form $Q_W = -8x_1x_2 + 2x_3^2 + 2x_4^2$. There are natural group actions on the parameter space $\\sM_\\DD$. We observe that the Descartes configurations in each Apollonian packing form an orbit under a certain finitely generated discrete group, the {\\em Apollonian group}. This group consists of $4 \\times 4$ integer matrices, and its integrality properties lead to the integrality properties observed in some Apollonian circle packings. We introduce two more related finitely generated groups, the dual Apollonian group and the super-Apollonian group, which have nice geometrically interpretations. We show these groups are hyperbolic Coxeter groups. 0 into PostgreSQL...\n",
      "Inserting test sample 298  Apollonian Circle Packings have fascinated mathematicians since Apollonius of Perga first introduced them over two millennia ago. This paper explores the geometry and group theory behind these circle packings, beginning with an introduction to the basic concepts and definitions of circle packings. We then delve into the Apollonian group, which is a set of transformations that preserve a particular packing of circles. We provide a rigorous definition of the group and describe its most salient properties, including its connection to hyperbolic geometry and the modular group. With this foundation laid, we proceed to explore various applications of the Apollonian group, including its use in solving Diophantine equations and its connection to Soddy circles. Throughout the paper, we make use of numerous visual aids and provide ample geometric intuition to aid in understanding the underlying concepts. In particular, we use the PoincarÃ© disk model to provide a visual representation of the Apollonian group and its actions on circle packings. The paper concludes by summarizing the main results and suggesting avenues for future research in this exciting and evolving field. 1 into PostgreSQL...\n",
      "Inserting test sample 299  The interplay between band topology and magnetic order could generate a variety of time-reversal-breaking gapped topological phases with exotic topological quantization phenomena, such as quantum anomalous Hall (QAH) insulators and axion insulators (AxI). Here by combining analytic models and first-principles calculations, we predict QAH and AxI phases can be realized in thin film of an intrinsic antiferromagnetic van der Waal material Mn$_2$Bi$_2$Te$_5$. The phase transition between QAH and AxI is tuned by the layer magnetization, which would provide a promising platform for chiral superconducting phases. We further present a simple and unified continuum model that captures the magnetic topological features, and is generic for Mn$_2$Bi$_2$Te$_5$ and MnBi$_2$Te$_4$ family materials. 0 into PostgreSQL...\n",
      "Inserting test sample 300  In this paper, we investigate the intrinsic topological phases that are present in Mn$_2$Bi$_2$Te$_5$ and how they can be tuned by the layer magnetization. By using a combination of angle-dependent magnetotransport and magneto-optical measurements, we find that Mn$_2$Bi$_2$Te$_5$ exhibits a variety of topological phases with distinct quantized transport signatures. Furthermore, we observe that these topological phases can be manipulated by controlling the magnetic moment orientation of the individual layers. Our results provide new insight into the behavior of topological phases in Mn$_2$Bi$_2$Te$_5$ and suggest a promising direction for engineering topological materials for future technological applications. 1 into PostgreSQL...\n",
      "Inserting test sample 301  The exclusive omega electroproduction off the proton was studied in a large kinematical domain above the nucleon resonance region and for the highest possible photon virtuality (Q2) with the 5.75 GeV beam at CEBAF and the CLAS spectrometer. Cross sections were measured up to large values of the four-momentum transfer (-t < 2.7 GeV2) to the proton. The contributions of the interference terms sigma_TT and sigma_TL to the cross sections, as well as an analysis of the omega spin density matrix, indicate that helicity is not conserved in this process. The t-channel pi0 exchange, or more generally the exchange of the associated Regge trajectory, seems to dominate the reaction gamma* p -> omega p, even for Q2 as large as 5 GeV2. Contributions of handbag diagrams, related to Generalized Parton Distributions in the nucleon, are therefore difficult to extract for this process. Remarkably, the high-t behaviour of the cross sections is nearly Q2-independent, which may be interpreted as a coupling of the photon to a point-like object in this kinematical limit. 0 into PostgreSQL...\n",
      "Inserting test sample 302  The deeply virtual and exclusive electroproduction (DVCS) of omega mesons is a crucial process in the study of hadron structures and dynamics. This phenomenon is an important tool to probe the generalized parton distributions (GPDs) of nucleons, which encode information about the three-dimensional distribution of quarks and gluons inside hadrons. In this paper, we present a theoretical and experimental investigation of the DVCS of omega mesons using electromagnetic probes. Our analysis employs a combination of phenomenological models and QCD calculations, which allows us to study the sensitivity of the DVCS cross-sections to the different GPD parametrizations. We also discuss the effects of kinematical constraints and the role of higher-order corrections in the theoretical predictions. The experimental results, obtained from data collected at the Jefferson Lab facility, are found to be in good agreement with the theoretical expectations. Our study sheds light on the properties of omega mesons and provides new insights into the underlying dynamics of the strong interaction. 1 into PostgreSQL...\n",
      "Inserting test sample 303  A new geometry to trap neutral particles with an ac electric field using a simple electrodes structure is described. In this geometry, all electrodes are placed on a single chip plane, while particles are levitated above the chip.\n",
      "\n",
      "This provides an easy construction of the trap and a good optical access to the trap. 0 into PostgreSQL...\n",
      "Inserting test sample 304  We propose a planar electric trap for neutral particles by creating two opposite potentials on a microfabricated chip. The trap confines particles with sub-micron size, which can be applied in experiments on ultracold atoms or quantum optics. Our simulations indicate the feasibility of this approach in trapping and manipulating individualized particles on a chip. 1 into PostgreSQL...\n",
      "Inserting test sample 305  We present the problem of approximating the time-evolution operator $e^{-i\\hat{H}t}$ to error $\\epsilon$, where the Hamiltonian $\\hat{H}=(\\langle G|\\otimes\\hat{\\mathcal{I}})\\hat{U}(|G\\rangle\\otimes\\hat{\\mathcal{I}})$ is the projection of a unitary oracle $\\hat{U}$ onto the state $|G\\rangle$ created by another unitary oracle. Our algorithm solves this with a query complexity $\\mathcal{O}\\big(t+\\log({1/\\epsilon})\\big)$ to both oracles that is optimal with respect to all parameters in both the asymptotic and non-asymptotic regime, and also with low overhead, using at most two additional ancilla qubits. This approach to Hamiltonian simulation subsumes important prior art considering Hamiltonians which are $d$-sparse or a linear combination of unitaries, leading to significant improvements in space and gate complexity, such as a quadratic speed-up for precision simulations. It also motivates useful new instances, such as where $\\hat{H}$ is a density matrix. A key technical result is `qubitization', which uses the controlled version of these oracles to embed any $\\hat{H}$ in an invariant $\\text{SU}(2)$ subspace. A large class of operator functions of $\\hat{H}$ can then be computed with optimal query complexity, of which $e^{-i\\hat{H}t}$ is a special case. 0 into PostgreSQL...\n",
      "Inserting test sample 306  Hamiltonian simulation is a major problem in computational physics, where the goal is to simulate quantum mechanical systems that are described by Hamiltonians. Recently, there has been growing interest in using quantum computers to solve this problem efficiently. Qubitization is a promising approach for simulating such Hamiltonians using only a logarithmic number of quantum queries. In this paper, we present a comprehensive study of Hamiltonian simulation by qubitization, exploring the key theoretical and practical aspects of this powerful technique. We first provide a detailed overview of qubitization, including its basic principles and mathematical formulation. We then discuss how qubitization can be used to simulate a wide range of Hamiltonians, and analyze the efficiency and accuracy of this approach for different problem sizes. Finally, we present several applications of qubitization to problems in condensed matter physics, such as the simulation of spin systems and topological phases of matter. Our results demonstrate the effectiveness of qubitization for Hamiltonian simulation and suggest that this technique has significant potential for quantum computing applications in the future. 1 into PostgreSQL...\n",
      "Inserting test sample 307  As a statistical measure to quantify the topological structure of the large-scale structure in the universe, the genus number is calculated for a number of non-Gaussian distributions in which the density field is characterized by a nontrivial function of some Gaussian-distributed random numbers. As a specific example, the formulae for the lognormal and the chi-square distributions are derived and compared with the results of $N$-body simulations together with the previously known formulae for the Gaussian distribution and second-order perturbation theory. It is shown that the lognormal formula fits most of the simulation data the best. 0 into PostgreSQL...\n",
      "Inserting test sample 308  In this paper, we examine the genus statistics of the large-scale structure with non-Gaussian density fields. We present a method to calculate the genus curves for arbitrary levels in the density field based on the concept of excursion sets of the density field. We explore how the genus curves depend on the non-Gaussianity of the density field, and we study the effects of smoothing and bias on the genus curves. Our results indicate that non-Gaussianity has a significant impact on the topology of the density field, and that the genus curves can be a useful tool for characterizing non-Gaussianity in the large-scale structure. 1 into PostgreSQL...\n",
      "Inserting test sample 309  In this paper, we study the $\\mu$-ordinary locus of a Shimura variety with parahoric level structure. Under the axioms in \\cite{HR}, we show that $\\mu$-ordinary locus is a union of some maximal Ekedahl-Kottwitz-Oort-Rapoport strata introduced in \\cite{HR} and we give criteria on the density of the $\\mu$-ordinary locus. 0 into PostgreSQL...\n",
      "Inserting test sample 310  We investigate the $\\mu$-ordinary locus of a Shimura variety and its relation with the Newton stratification. We show that the $\\mu$-ordinary locus is a closed subset of the Shimura variety and its complement is a union of locally closed strata. Furthermore, we give a criterion for a point in the Shimura variety to be $\\mu$-ordinary. 1 into PostgreSQL...\n",
      "Inserting test sample 311  The size and geometry of the X-ray emitting corona in AGNs are still not well constrained. Dov\\v{c}iak & Done (2016) proposed a method based on calculations assuming a point-like lamp-post corona. To perform more self-consistent calculations of energy spectra of extended coronae, we develop monk, a Monte Carlo radiative transfer code dedicated to calculations of Comptonised spectra in the Kerr spacetime. In monk we assume Klein-Nishina scattering cross section and include all general relativistic effects. We find that for a corona located above the disc, the spectrum is not isotropic, but with harder and less luminous spectra towards observers at lower inclinations, owing to anisotropic illumination of the seed photons. This anisotropy also leads to an underestimated size of the corona if we assume the corona to be a point-like, isotropic source located on the black hole rotation axis, demonstrating the necessity of more self-consistent calculations. We also inspect the effect of motion and geometry of the corona on the emergent spectrum. Finally, we discuss the implication of anisotropic corona emission for the reflection spectrum in AGNs as well as black hole X-ray binaries (BHXRBs). We find that by assuming the corona emission to be isotropic, one may underestimate the soft excess in AGNs and the reflection continuum and iron K fluorescent line flux in BHXRBs. 0 into PostgreSQL...\n",
      "Inserting test sample 312  This paper describes a novel study aimed at constraining the size of the corona through fully relativistic calculations of spectra of extended coronae using the Monte Carlo radiative transfer code. The corona is a diffuse envelope of hot plasma that surrounds the Sun and other stars. It plays a critical role in the emission characteristics of stars and the acceleration of the solar wind. The size of the corona is an important parameter to understand the underlying physics of coronal heating as well as the dynamics of magnetic reconnection. Despite its importance, the size of the corona is difficult to measure directly due to its tenuous nature. In this study, we used the Monte Carlo radiative transfer code to simulate the emission spectra of extended coronae. We then compared these simulations to observational data to derive constraints on the size of the corona. Our results show that the size of the corona is consistent with previous estimates based on other observational techniques. These findings provide new insights into the nature of the corona and pave the way for more accurate and reliable measurements of this elusive phenomenon. 1 into PostgreSQL...\n",
      "Inserting test sample 313  The application of Deep Neural Networks for ranking in search engines may obviate the need for the extensive feature engineering common to current learning-to-rank methods. However, we show that combining simple relevance matching features like BM25 with existing Deep Neural Net models often substantially improves the accuracy of these models, indicating that they do not capture essential local relevance matching signals. We describe a novel deep Recurrent Neural Net-based model that we call Match-Tensor. The architecture of the Match-Tensor model simultaneously accounts for both local relevance matching and global topicality signals allowing for a rich interplay between them when computing the relevance of a document to a query. On a large held-out test set consisting of social media documents, we demonstrate not only that Match-Tensor outperforms BM25 and other classes of DNNs but also that it largely subsumes signals present in these models. 0 into PostgreSQL...\n",
      "Inserting test sample 314  In this paper, we propose a novel deep learning model called Match-Tensor, designed for search relevance task. The proposed model utilizes the power of tensor analysis to efficiently capture and model the complex interactions among queries, documents, and relevance signals. Match-Tensor consists of multiple processing layers that are optimized to learn discriminative representations of textual data at different granularities. Moreover, the model utilizes bidirectional attention mechanism to selectively weigh the importance of query and document words, allowing it to focus on the most relevant and contextual information. We evaluate Match-Tensor on several benchmark datasets for search relevance and show that it outperforms state-of-the-art baselines across different evaluation metrics. Our extensive experimentation results also demonstrate the effectiveness and versatility of our proposed architecture in a wide range of settings, including web search and question answering. 1 into PostgreSQL...\n",
      "Inserting test sample 315  Applications like disaster management and industrial inspection often require experts to enter contaminated places. To circumvent the need for physical presence, it is desirable to generate a fully immersive individual live teleoperation experience. However, standard video-based approaches suffer from a limited degree of immersion and situation awareness due to the restriction to the camera view, which impacts the navigation. In this paper, we present a novel VR-based practical system for immersive robot teleoperation and scene exploration. While being operated through the scene, a robot captures RGB-D data that is streamed to a SLAM-based live multi-client telepresence system.\n",
      "\n",
      "Here, a global 3D model of the already captured scene parts is reconstructed and streamed to the individual remote user clients where the rendering for e.g.\n",
      "\n",
      "head-mounted display devices (HMDs) is performed. We introduce a novel lightweight robot client component which transmits robot-specific data and enables a quick integration into existing robotic systems. This way, in contrast to first-person exploration systems, the operators can explore and navigate in the remote site completely independent of the current position and view of the capturing robot, complementing traditional input devices for teleoperation. We provide a proof-of-concept implementation and demonstrate the capabilities as well as the performance of our system regarding interactive object measurements and bandwidth-efficient data streaming and visualization.\n",
      "\n",
      "Furthermore, we show its benefits over purely video-based teleoperation in a user study revealing a higher degree of situation awareness and a more precise navigation in challenging environments. 0 into PostgreSQL...\n",
      "Inserting test sample 316  This paper presents a novel VR system that enables users to remotely control a mobile robot in a highly immersive and intuitive manner. Our approach is based on a seamless integration of live video streams, 3D reconstruction techniques, and haptic feedback to provide a realistic and responsive telepresence experience. The system consists of a mobile robot equipped with a 360-degree camera and various sensors, a VR headset, and a haptic device. By wearing the VR headset, the user can see the robot's perspective and interact with the virtual environment as if they were physically present in the robot's location. Additionally, the haptic device allows the user to feel the robot's movements and touch virtual objects with a remarkable level of realism. To evaluate the effectiveness of the proposed system, we conducted user studies with participants of varying backgrounds and levels of experience. The results show that our system significantly improves the perceived sense of presence and control compared to traditional teleoperation methods. We also demonstrate the system's potential for real-world applications such as remote inspection, exploration, and surveillance. Overall, this paper contributes to the development of immersive teleoperation systems that can enhance human capabilities and extend our reach beyond physical limitations. 1 into PostgreSQL...\n",
      "Inserting test sample 317  We have performed two-dimensional multicomponent decomposition of 144 local barred spiral galaxies using 3.6 $\\mu {\\rm m}$ images from the Spitzer Survey of Stellar Structure in Galaxies. Our model fit includes up to four components (bulge, disk, bar, and a point source) and, most importantly, takes into account disk breaks. We find that ignoring the disk break and using a single disk scale length in the model fit for Type II (down-bending) disk galaxies can lead to differences of 40% in the disk scale length, 10% in bulge-to-total luminosity ratio (B/T), and 25% in bar-to-total luminosity ratios. We find that for galaxies with B/T $\\geq$ 0.1, the break radius to bar radius, $r_{\\rm br}/R_{\\rm bar}$, varies between 1 and 3, but as a function of B/T the ratio remains roughly constant. This suggests that in bulge-dominated galaxies the disk break is likely related to the outer Lindblad Resonance (OLR) of the bar, and thus moves outwards as the bar grows. For galaxies with small bulges, B/T $<$ 0.1, $r_{\\rm br}/R_{\\rm bar}$ spans a wide range from 1 to 6. This suggests that the mechanism that produces the break in these galaxies may be different from that in galaxies with more massive bulges. Consistent with previous studies, we conclude that disk breaks in galaxies with small bulges may originate from bar resonances that may be also coupled with the spiral arms, or be related to star formation thresholds. 0 into PostgreSQL...\n",
      "Inserting test sample 318  The structure of barred galaxies has been a topic of interest among astronomers for decades. In this paper, we present the first installment of the Spitzer Survey of Stellar Structure in Galaxies (S$^4$G) which is aimed at unveiling the intricate structure present in barred galaxies at 3.6 $\\mu {\\rm m}$. In particular, we focus on disk breaks - a change in surface brightness profile that occurs at a specific radius leading to a sudden decrease in brightness beyond that point. Disk breaks have long been considered as potential indicators of the existence of bars, and their study could help shed light on the formation and evolution of barred galaxies. \n",
      "\n",
      "Using a sample of more than 230 barred galaxies, we analyze the distribution, characteristics, and origins of disk breaks. Our results show that disk breaks are a common feature found in barred galaxies, with a clear correlation between their position and the properties of the bars. Specifically, we find that the majority of disk breaks occur along circular orbits within the bars, and that they are associated with changes in the bar's ellipticity and boxiness. Furthermore, we uncover evidence for disk truncation in a subset of galaxies, indicating the presence of additional physical processes that could be responsible for shaping the structure of barred galaxies. \n",
      "\n",
      "In summary, our study provides a comprehensive analysis of disk breaks in barred galaxies using the S$^4$G dataset. Our results shed new light on the complex structure of barred galaxies, and suggest that disk breaks could act as valuable tools for understanding the formation and evolution of these systems. 1 into PostgreSQL...\n",
      "Inserting test sample 319  PSR J1713+0747 is a binary system comprising millisecond radio pulsar with a spin period of 4.57 ms, and a low-mass white dwarf (WD) companion orbiting the pulsar with a period of 67.8 days. Using the general relativistic Shapiro delay, the masses of the WD and pulsar components were previously found to be $0.28\\pm 0.03 M_{\\odot}$ and $1.3\\pm 0.2 M_{\\odot}$ (68% confidence), respectively. Standard binary evolution theory suggests that PSR J1713+0747 evolved from a low-mass X-ray binary (LMXB). Here, we test this hypothesis. We used a binary evolution code and a WD evolution code to calculate evolutionary sequences of LMXBs that could result in binary millisecond radio pulsars such as PSR J1713+0747. During the mass exchange, the mass transfer is nonconservative. Because of the thermal and viscous instabilities developing in the accretion disk, the neutron star accretes only a small part of the incoming material. We find that the progenitor of PSR J1713+0747 can be modelled as an LMXB including a donor star with mass $1.3-1.6 M_{\\odot}$ and an initial orbital period ranging from 2.40 to 4.15 days. If the cooling timescale of the WD is 8 Gyr, its present effective temperature is between 3870 and 4120 K, slightly higher than the observed value. We estimate a surface gravity of ${\\rm Log} (g) \\approx 7.38 - 7.40$. 0 into PostgreSQL...\n",
      "Inserting test sample 320  We present a detailed analysis of the binary millisecond radio pulsar PSR J1713+0747, aimed at identifying its progenitor. Using multi-wavelength observations and advanced modeling techniques, we constrain the mass and age of the progenitor and investigate the scenarios that could have led to the formation of this remarkable object. Our results suggest that the binary system likely originated from a low-mass X-ray binary, where the neutron star accreted mass from a companion star. The progenitor star was likely a low-mass main-sequence star with a mass around 1.3 solar masses and an age of about 2 billion years. The system then underwent a common envelope phase, leading to the ejection of the envelope and the formation of a tight binary of a neutron star and a white dwarf. Our findings provide important insights into the origin of binary millisecond pulsars and the evolution of compact binary systems in general. Furthermore, they shed light on the formation pathways of neutron stars and the role of binary interactions in shaping the population of compact objects in the Universe. 1 into PostgreSQL...\n",
      "Inserting test sample 321  The exponential growth of floating point power in graphics processing units (GPUs), together with their low cost, has given rise to an attractive platform upon which to deploy lattice QCD calculations. GPUs are essentially many (O(100)) core chips, that are programmed using a massively threaded environment, and so are representative of the future of high performance computing (HPC). The large ratio of raw floating point operations per second to memory bandwidth that is characteristic of GPUs necessitates that unique algorithmic design choices are made to harness their full potential. We review the progress to date in using GPUs for large scale calculations, and contrast GPUs against more traditional HPC architectures 0 into PostgreSQL...\n",
      "Inserting test sample 322  Quantum Chromodynamics (QCD) is a fundamental theory that describes the strong interaction between quarks and gluons which form protons and neutrons. Solving QCD requires extensive numerical simulations, which are computationally intensive and require large-scale supercomputing resources. Graphics Processing Units (GPUs) provide a cost-effective solution for high-performance computing, capable of accelerating QCD simulations by orders of magnitude. In this work, we investigate the implementation of QCD on GPUs, optimizing the code to take full advantage of the GPU architecture. We demonstrate that by using GPUs, the cost of QCD simulations can be significantly reduced while maintaining high accuracy and precision. Our work provides a viable solution for cost-effective QCD simulations and paves the way for further development in supercomputing technology. 1 into PostgreSQL...\n",
      "Inserting test sample 323  In this paper a new method for spotsize-measurement for singlemode optical components is presented. Based from the classical farfield-method where the measurements are made circular, the mounting of the used rotary stages and the long measurement time are great disadvantages. In this paper a new planar method is described which overcomes these problems. Based on the measurement of a singlemode fiber in accordance with ITU Recommendation G.652 the efficiency is demonstrated and discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 324  This paper presents a new Medianfield-method for the optical characterization of single mode components. The proposed method is based on numerical simulations and takes into account the effects of manufacturing tolerances and material variations. The accuracy of the method is demonstrated by comparing the simulation results with experimental measurements. The proposed method has the potential to improve the design and optimization of single mode components, which are widely used in optical communication systems. 1 into PostgreSQL...\n",
      "Inserting test sample 325  We present results from a study of the Supernova Remnant (SNR) population in a sample of six nearby galaxies (NGC 2403, NGC 3077, NGC 4214, NGC 4449, NGC 4395 and NGC 5204) based on Chandra archival data. We have detected 244 discrete X-ray sources down to a limiting flux of 10^{-15} erg/s. We identify 37 X-ray selected thermal SNRs based on their X-ray colors or spectra, 30 of which are new discoveries. In many cases the X-ray classification is confirmed based on counterparts with SNRs identified in other wavelengths. Three of the galaxies in our sample (NGC 4214, NGC 4395 and NGC 5204) are studied for the first time, resulting in the discovery of 13 thermal SNRs. We discuss the properties (luminosity, temperature, density) of the X-ray detected SNRs in the galaxies of our sample in order to address their dependence on their environment. We find that X-ray selected SNRs in irregular galaxies appear to be more luminous than those in spirals. We attribute this to the lower metalicities and therefore more massive progenitor stars of irregular galaxies or the higher local densities of the ISM. We also discuss the X-ray selected SNR populations in the context of the Star Formation Rate of their host galaxies. A comparison of the numbers of observed luminous X-ray selected SNRs with those expected based on the luminosity functions of X-ray SNRs in the MCs and M33 suggest different luminosity distributions between the SNRs in spiral and irregular galaxies with the latter tending to have flatter distributions. 0 into PostgreSQL...\n",
      "Inserting test sample 326  Supernova remnants (SNRs) are essential probes of the high-energy phenomena that occur in galaxies. In this paper, we present the first results of a multi-wavelength study of SNRs in six nearby galaxies. We use the Chandra X-ray Observatory to identify new SNRs in these galaxies and to explore their properties in detail.\n",
      "\n",
      "Our study focuses on the X-ray properties of these SNRs, which are the most sensitive probe of high-energy processes such as shocks and particle acceleration. We detect a total of 25 new X-ray selected SNRs in the six galaxies, greatly expanding the number of known SNRs in these systems. We find that the X-ray luminosity and spectral properties of the SNRs vary significantly from galaxy to galaxy, indicating that the processes responsible for their high-energy emission may be different in each system.\n",
      "\n",
      "We also compare our X-ray results with existing radio and optical data to explore the multi-wavelength properties of these SNRs. We find that the X-ray emission is often associated with bright radio and optical emission, suggesting that the SNRs are interacting with their environment and producing shocks that accelerate particles to high energies.\n",
      "\n",
      "Our study represents an important step forward in our understanding of SNRs in nearby galaxies and demonstrates the power of multi-wavelength studies in revealing the high-energy processes that occur in these systems. 1 into PostgreSQL...\n",
      "Inserting test sample 327  We report on 5 Chandra observations of the X-ray afterglow of the Gamma-Ray Burst GRB 060729 performed between 2007 March and 2008 May. In all five observations the afterglow is clearly detected. The last Chandra pointing was performed on 2008-May-04, 642 days after the burst - the latest detection of a GRB X-ray afterglow ever. A reanalysis of the Swift XRT light curve together with the three detections by Chandra in 2007 reveals a break at about 1.0 Ms after the burst with a slight steepening of the decay slope from alpha = 1.32 to 1.61. This break coincides with a significant hardening of the X-ray spectrum, consistent with a cooling break in the wind medium scenario, in which the cooling frequency of the afterglow crosses the X-ray band. The last two Chandra observations in 2007 December and 2008 May provide evidence for another break at about one year after the burst. If interpreted as a jet break, this late-time break implies a jet half opening angle of about 14 degrees for a wind medium. Alternatively, this final break may have a spectral origin, in which case no jet break has been observed and the half-opening angle of the jet of GRB 060729 must be larger than about 15 degrees for a wind medium. We compare the X-ray afterglow of GRB 060729 in a wind environment with other bright X-ray afterglows, in particular GRBs 061121 and 080319B, and discuss why the X-ray afterglow of GRB 060729 is such an exceptionally long-lasting event. 0 into PostgreSQL...\n",
      "Inserting test sample 328  This research paper presents the latest detection of an X-ray afterglow of a gamma-ray burst (GRB) using the Chandra X-ray observatory. The source of interest is GRB 060729, which was detected by the Swift observatory on July 29, 2006 and has been extensively observed at various wavelengths since then. Previous X-ray observations of this burst have shown a steep decay in the X-ray light curve, making it challenging to detect the afterglow at late times. However, with the unprecedented sensitivity and resolution of the Chandra observatory, we were able to detect the X-ray afterglow of GRB 060729 more than a decade after the burst.\n",
      "\n",
      "In this paper, we present the results of our analysis of the latest Chandra observations of GRB 060729, which were obtained in August 2017, more than 11 years after the burst. We find that the X-ray afterglow has decayed significantly compared to previous observations, but is still detectable with Chandra. Our analysis of the X-ray spectrum reveals evidence for both synchrotron and inverse Compton emission mechanisms, indicating that the afterglow is likely produced by a relativistic blast wave in the circumburst medium.\n",
      "\n",
      "Our late-time detection of the X-ray afterglow of GRB 060729 sets a new record for the latest detection of an X-ray afterglow from a GRB. This result demonstrates the power of the Chandra observatory to detect the faintest and most distant sources, and provides valuable insight into the physics of relativistic outflows from compact objects. 1 into PostgreSQL...\n",
      "Inserting test sample 329  At a Galactocentric distance of 27 kpc, Pal 13 is an old globular cluster (GC) in the outer halo. We present a chemical abundance analysis of this remote system from high-resolution spectra obtained with Keck/HIRES. Owing to the low signal-to-noise ratio of the data, our analysis is based on a coaddition of the spectra of 18 member stars. We are able to determine integrated abundance ratios for 16 species of 14 elements, of $\\alpha$-elements (Mg,Si,Ca,Ti), Fe-peak (Sc,Mn,Cr,Ni,Cu,and Zn), and n-capture elements (Y,Ba). While the mean Na abundance is found to be slightly enhanced and halo-like, our method does not allow us to probe an abundance spread that would be expected in this light element if multiple populations are present in Pal 13. We find a metal-poor mean metallicity of $-1.91\\pm0.05$ (statistical) $\\pm$ 0.22 (systematic), confirming that Pal 13 is a typical metal-poor representative of the outer halo. While there are some differences between individual $\\alpha$-elements, such as halo-like Mg and Si versus the mildly lower Ca and Ti abundances, the mean [$\\alpha$/Fe] of 0.34$\\pm$0.06 is consistent with the marginally lower $\\alpha$ component of the halo field and GC stars. We discuss our results in the context of other objects in the outer halo and consider which of these objects were likely accreted. We also discuss the properties of their progenitors. While chemically, Pal 13 is similar to Gaia-Enceladus and some of its GCs, this is not supported by its kinematic properties. Moreover, its chemodynamical similarity with NGC 5466, a progeny of the Sequoia accretion event, might indicate a common origin in this progenitor. However, the ambiguities in the full abundance space of this comparison emphasize the difficulties in unequivocally labeling a single GC as an accreted object, let alone assigning it to a single progenitor. (Abridged) 0 into PostgreSQL...\n",
      "Inserting test sample 330  Palomar 13 is a globular cluster located in the outer halo of the Milky Way. In this study, we present an abundance analysis of stars in the outermost regions of the cluster. Our analysis is based on high-resolution spectroscopic data obtained with the Keck II telescope and the HIRES spectrograph.\n",
      "\n",
      "We measured the abundances of several elements, including iron, magnesium, silicon, calcium, and titanium, in a sample of nine red giant branch stars that are located beyond the core radius of the cluster. We found that the average metallicity of the outermost stars is [Fe/H]=-1.96Â±0.05, which is consistent with previous studies of Palomar 13.\n",
      "\n",
      "In addition to the metallicity, we also measured the abundance ratios of alpha elements (Mg, Si, Ca, and Ti) relative to iron. We found that the alpha-element enhancement in the outermost stars is similar to that of the inner regions of the cluster. This suggests that the outermost stars have a similar chemical history to the inner regions, despite being located at larger distances.\n",
      "\n",
      "Our results also provide constraints on the formation and evolution of Palomar 13. The similarity in the chemical composition of the outermost stars to the inner regions of the cluster suggests that Palomar 13 has undergone a relatively quiet evolution with little mixing of gas and stars. However, the small size and low luminosity of Palomar 13 indicate that it has experienced significant mass loss throughout its evolution. The combination of these factors may have significant implications for the overall formation and evolution of the outer halo of the Milky Way.\n",
      "\n",
      "Overall, our abundance analysis of the outer halo globular cluster Palomar 13 provides new insights into the chemical properties and formation history of this faint and distant object. 1 into PostgreSQL...\n",
      "Inserting test sample 331  Motor proteins are key players in intracellular transport processes and biological motion. Theoretical modeling of these systems has been achieved by the use of step processes on one-dimensional lattices. After a comprehensive introduction to the total asymmetric exclusion process and some analytical tools, we will give a review on different lines of research attracted to the aspects of this systems. We will focus on the generic properties of a coupling between the exclusion process and Langmuir bulk kinetics that induce topological changes in the phase diagram and multi-phase coexistence. 0 into PostgreSQL...\n",
      "Inserting test sample 332  This work explores a novel class of driven lattice gas models, which can be used to simulate intracellular traffic. We demonstrate that the introduction of asymmetry in the models can result in the emergence of complex, non-equilibrium behaviors, similar to those observed in real-life traffic flow. By systematically varying system parameters, we investigate the transition between different states and identify critical points at which the behavior changes qualitatively. Our simulation results suggest that this class of models can be useful in understanding and predicting the behavior of complex systems in a range of contexts. 1 into PostgreSQL...\n",
      "Inserting test sample 333  We introduce a variant of the vertex-distinguishing edge coloring problem, where each edge is assigned a subset of colors. The label of a vertex is the union of the sets of colors on edges incident to it. In this paper we investigate the problem of finding a coloring with the minimum number of colors where every vertex receives a distinct label. Finding such a coloring generalizes several other well-known problems of vertex-distinguishing colorings in graphs.We show that for any graph (without connected component reduced to an edge or a single vertex), the minimum number of colors for which such a coloring exists can only take 3possible values depending on the order of the graph. Moreover, we provide the exact value for paths, cycles and complete binary trees. 0 into PostgreSQL...\n",
      "Inserting test sample 334  In this paper, we present a new theorem in the field of union vertex-distinguishing edge coloring that is reminiscent of Vizing's classic theorem in graph theory. We introduce a new concept called the distinguishing strength of a graph, and show that this parameter is closely related to the minimum number of colors required for a union vertex-distinguishing edge coloring. We prove that the distinguishing strength of a graph is always less than or equal to its chromatic index, and provide an algorithm for computing this parameter in linear time. Furthermore, we demonstrate the utility of our new theorem by applying it to several classical families of graphs, including complete graphs, cycles, and trees. Our results have implications for a variety of practical applications, including scheduling and resource allocation in wireless networks. 1 into PostgreSQL...\n",
      "Inserting test sample 335  We propose that the intensity changes and spectral evolution along the M87 jet can be explained by adiabatic changes to the particle momentum distribution function and the magnetic field. This is supported by the lack of any significant variation in the radio-to-optical spectral index along the jet and the moderate changes in radio brightness. Assuming a simple scaling law between magnetic field and density, we use the deprojection of a 2 cm VLA intensity map by Sparks, Biretta, & Macchetto (1996) to predict the spectral evolution along the jet. We derive limits for the magnetic field and the total pressure by comparing our results with the spatially resolved fit to spectral data by Neumann, Meisenheimer, & Roeser (1997) of a model spectrum that cuts off at approx 10^15 Hz. To explain the weakness of synchrotron cooling along the jet, the magnetic field strength must lie below the equipartition value. Although the inferred pressure in the limit of nonrelativistic bulk flow lies far above the estimated pressure of the interstellar matter in the center of M87, bulk Lorentz factors Gamma_jet in the range of 3 - 5 and inclination angles approx less than 25 deg lead to pressure estimates close to the ISM pressure. The average best fit magnetic fields we derive fall in the range of 20 - 40 microG, departing from equipartition by a factor approx 1.5 - 5. This model is consistent with the proposal by Bicknell & Begelman (1996) that the knots in the M87 jet are weak, oblique shocks. First-order Fermi acceleration will then have a minimal effect on the slope of the radio-- to-optical spectrum while possibly accounting for the X-ray spectrum. 0 into PostgreSQL...\n",
      "Inserting test sample 336  The M87 jet, powered by the supermassive black hole residing in the center of the galaxy, is one of the largest and brightest extragalactic radio sources known. In this paper, we present a comprehensive analysis of the synchrotron emission from the M87 jet, based on multi-frequency, multi-epoch data obtained with various radio telescopes. Our analysis reveals detailed structures and complex morphologies of the jet at different scales, from hundred parsecs down to sub-parsec regions close to the black hole. We identify various emission components, such as knots, ridges, and blobs, and investigate their spectral and polarization properties. The analysis of the jet morphology and kinematics provides important constraints on the jet launching mechanism and jet-environment interactions. In particular, we find evidence for the collimation of the jet by the external medium and for the acceleration of the jet to relativistic speeds. The study of the polarization properties of the emission sheds light on the magnetic field configuration and the emission mechanism. Our results suggest that the synchrotron radiation is produced by highly relativistic electrons spiraling around intense and tangled magnetic fields. We also discuss the implications of our findings for our understanding of the physics of relativistic jets and their astrophysical role. Overall, our analysis provides a comprehensive view of the synchrotron emission from the M87 jet and lays the foundation for future studies using next-generation radio facilities. 1 into PostgreSQL...\n",
      "Inserting test sample 337  Let $P$ be a planar set of $n$ sites in general position. For $k\\in\\{1,\\dots,n-1\\}$, the Voronoi diagram of order $k$ for $P$ is obtained by subdividing the plane into cells such that points in the same cell have the same set of nearest $k$ neighbors in $P$. The (nearest site) Voronoi diagram (NVD) and the farthest site Voronoi diagram (FVD) are the particular cases of $k=1$ and $k=n-1$, respectively. For any given $K\\in\\{1,\\dots,n-1\\}$, the family of all higher-order Voronoi diagrams of order $k=1,\\dots,K$ for $P$ can be computed in total time $O(nK^2+ n\\log n)$ using $O(K^2(n-K))$ space [Aggarwal et al., DCG'89; Lee, TC'82]. Moreover, NVD and FVD for $P$ can be computed in $O(n\\log n)$ time using $O(n)$ space [Preparata, Shamos, Springer'85].\n",
      "\n",
      "For $s\\in\\{1,\\dots,n\\}$, an $s$-workspace algorithm has random access to a read-only array with the sites of $P$ in arbitrary order. Additionally, the algorithm may use $O(s)$ words, of $\\Theta(\\log n)$ bits each, for reading and writing intermediate data. The output can be written only once and cannot be accessed or modified afterwards.\n",
      "\n",
      "We describe a deterministic $s$-workspace algorithm for computing NVD and FVD for $P$ that runs in $O((n^2/s)\\log s)$ time. Moreover, we generalize our $s$-workspace algorithm so that for any given $K\\in O(\\sqrt{s})$, we compute the family of all higher-order Voronoi diagrams of order $k=1,\\dots,K$ for $P$ in total expected time $O (\\frac{n^2 K^5}{s}(\\log s+K2^{O(\\log^* K)}))$ or in total deterministic time $O(\\frac{n^2 K^5}{s}(\\log s+K\\log K))$. Previously, for Voronoi diagrams, the only known $s$-workspace algorithm runs in expected time $O\\bigl((n^2/s)\\log s+n\\log s\\log^* s)$ [Korman et al., WADS'15] and only works for NVD (i.e., $k=1$). Unlike the previous algorithm, our new method is very simple and does not rely on advanced data structures or random sampling techniques. 0 into PostgreSQL...\n",
      "Inserting test sample 338  The Voronoi diagram has various applications in different fields, such as computer graphics and computational geometry. Although finding the Voronoi diagram of a set of points can be computationally expensive, recent research has shown that it can be computed with a trade-off between time and space complexities. This paper presents an improved algorithm for computing Voronoi diagrams, which balances the trade-off between time and space, particularly in high-dimensional settings.\n",
      "\n",
      "Our method is based on the partition scheme, which partitions the space into smaller cells and computes the Voronoi diagram within these cells. The main contribution of this paper is to partition the cells adaptively, based on the density of the input point set. This results in a significant improvement in run time compared to existing methods. Our proposed algorithm also improves the space overhead in some settings. Furthermore, we develop a new data structure called the \"density-dependent Voronoi diagram\", which partitions the cells in a way that is dependent on the density of the input points.\n",
      "\n",
      "We provide a thorough complexity analysis of our algorithm and compare it with state-of-the-art algorithms. Our method is shown to outperform existing methods in several settings, including high-dimensional settings where the number of dimensions is much greater than the number of points. We also provide an implementation of our algorithm and evaluate its performance on a variety of benchmark datasets.\n",
      "\n",
      "In conclusion, this paper presents an improved algorithm for computing Voronoi diagrams with a trade-off between time and space complexities. Our method is based on the partition scheme and introduces adaptive cell partitioning, resulting in significant improvement in run time and space overhead. We also propose a novel data structure, the density-dependent Voronoi diagram, which further improves the performance of our algorithm. Our algorithm outperforms state-of-the-art methods and can handle high-dimensional settings efficiently. 1 into PostgreSQL...\n",
      "Inserting test sample 339  We propose a benchmarking strategy that is robust in the presence of timer error, OS jitter and other environmental fluctuations, and is insensitive to the highly nonideal statistics produced by timing measurements. We construct a model that explains how these strongly nonideal statistics can arise from environmental fluctuations, and also justifies our proposed strategy. We implement this strategy in the BenchmarkTools Julia package, where it is used in production continuous integration (CI) pipelines for developing the Julia language and its ecosystem. 0 into PostgreSQL...\n",
      "Inserting test sample 340  Benchmarking is a crucial tool for comparing the performance of different systems, algorithms or methods. However, in noisy environments, benchmarking becomes challenging due to the presence of unexpected or uncontrollable factors. This paper proposes a robust benchmarking approach that is capable of handling and minimizing the effects of noise on performance measurements. The approach is evaluated on various practical examples, demonstrating its superiority compared to existing methods. The proposed approach enables more accurate and reliable performance evaluations, leading to improved decision-making processes in various fields. 1 into PostgreSQL...\n",
      "Inserting test sample 341  The Poland-Scheraga model describes the denaturation transition of two complementary - in particular, equally long - strands of DNA, and it has enjoyed a remarkable success both for quantitative modeling purposes and at a more theoretical level. The solvable character of the homogeneous version of the model is one of features to which its success is due. In the bio-physical literature a generalization of the model, allowing different length and non complementarity of the strands, has been considered and the solvable character extends to this substantial generalization. We present a mathematical analysis of the homogeneous generalized Poland-Scheraga model. Our approach is based on the fact that such a model is a homogeneous pinning model based on a bivariate renewal process, much like the basic Poland-Scheraga model is a pinning model based on a univariate, i.e. standard, renewal. We present a complete analysis of the free energy singularities, which include the localization-delocalization critical point and (in general) other critical points that have been only partially captured in the physical literature. We obtain also precise estimates on the path properties of the model. 0 into PostgreSQL...\n",
      "Inserting test sample 342  This paper proposes a generalized Poland-Scheraga (PS) model that incorporates randomness by introducing two-dimensional renewal processes. The model is inspired by the well-established PS denaturation model that describes the process by which a DNA molecule loses its double-stranded nature due to thermal fluctuations. We derive explicit formulas for the free energy and entropy of the system using probabilistic renewal theory and establish their properties. Furthermore, we study the thermal properties of the model using numerical simulation techniques and compare the results with the traditional PS model. Finally, we investigate the effect of various parameters, including the renewal process parameters and the nucleotide sequence, on the model's thermal stability and show that the generalized PS model is an improvement over the traditional PS model for certain circumstances. This work contributes to the field of biomolecular modeling, where understanding the denaturation process of DNA is of crucial importance. 1 into PostgreSQL...\n",
      "Inserting test sample 343  According to the no-hair theorem, astrophysical black holes are uniquely characterized by their masses and spins and are described by the Kerr metric.\n",
      "\n",
      "Several parametric spacetimes which deviate from the Kerr metric have been proposed in order to test this theorem with observations of black holes in both the electromagnetic and gravitational-wave spectra. Such metrics often contain naked singularities or closed timelike curves in the vicinity of the compact objects that can limit the applicability of the metrics to compact objects that do not spin rapidly, and generally admit only two constants of motion. The existence of a third constant, however, can facilitate the calculation of observables, because the equations of motion can be written in first-order form. In this paper, I design a Kerr-like black hole metric which is regular everywhere outside of the event horizon, possesses three independent constants of motion, and depends nonlinearly on four free functions that parameterize potential deviations from the Kerr metric. This metric is generally not a solution to the field equations of any particular gravity theory, but can be mapped to known four-dimensional black hole solutions of modified theories of gravity for suitable choices of the deviation functions. I derive expressions for the energy, angular momentum, and epicyclic frequencies of a particle on a circular equatorial orbit around the black hole and compute the location of the innermost stable circular orbit. In addition, I write the metric in a Kerr-Schild-like form, which allows for a straightforward implementation of fully relativistic magnetohydrodynamic simulations of accretion flows in this metric. The properties of this metric make it a well-suited spacetime for strong-field tests of the no-hair theorem in the electromagnetic spectrum with black holes of arbitrary spin. 0 into PostgreSQL...\n",
      "Inserting test sample 344  The study presented in this paper aims to explore the regular black hole metric and its three constants of motion. The metric, which was first introduced in the 1960s by Janis, Newman, and Winicour, has since received considerable attention in the astrophysical community due to its unique properties.\n",
      "\n",
      "In this study, we derive the equations of motion for test particles moving in the regular black hole metric, and we analyze the behavior of these equations when the particles approach the black hole event horizon. We demonstrate that the three constants of motion can be used to simplify the equations of motion, making them more manageable in practical applications.\n",
      "\n",
      "Furthermore, we investigate the relationship between the regular black hole metric and the well-known Schwarzschild metric. We show that the two metrics are equivalent in the limit of large distances from the black hole, but that significant differences arise at shorter distances.\n",
      "\n",
      "We also explore the implications of the three constants of motion for the thermodynamics of the regular black hole. Specifically, we analyze the entropy of the horizon and the Hawking temperature, and we derive expressions for these quantities in terms of the constants of motion.\n",
      "\n",
      "Overall, our study contributes to a deeper understanding of the properties and behavior of the regular black hole metric and its three constants of motion. Our results may have practical applications in fields such as astrophysics and cosmology, as well as in the development of future technologies related to gravitational physics. 1 into PostgreSQL...\n",
      "Inserting test sample 345  One of the most debated issues related to high-$T_c$ superconductivity is the symmetry of the Cooper pair or the gap function. In this report, we present numerical results regarding the gap function in strongly correlated electron systems using $t-J$ and Hubbard models in one and two dimensions. To this end, we use exact diagonalization to study the ground states of 8- and 16-site clusters consisting of single or coupled layers. We calculate a reduced two-particle density matrix in momentum space which is a measure of the gap function. We then analyze the eigenvectors of this density matrix, which display the possible Cooper pair symmetries. The eigenvector corresponding to the largest eigenvalue indicates a vanishing gap on the Fermi surface (which is in favour of odd-gap pairing) although $d_{x^2-y^2}$ symmetry is seen to be a very close contestant in many of the cases. 0 into PostgreSQL...\n",
      "Inserting test sample 346  The symmetry of the gap function in high-$T_c$ superconductors has been a topic of intense research interest for many decades. The study of the gap symmetry can provide valuable insights into the underlying mechanisms of high-$T_c$ superconductivity and shed light on the differences between conventional superconductors and their high-$T_c$ counterparts. Many experimental and theoretical studies have been conducted to investigate the gap symmetry in different high-$T_c$ materials. In this paper, we review the current understanding of the gap symmetry and its importance in the field of high-$T_c$ superconductivity. We summarize the experimental techniques used for determining the gap symmetry and discuss the theoretical models proposed to explain the observed results. Finally, we highlight the open questions and future directions for research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 347  We describe the target selection procedure by which stars are selected for 2-minute and 20-second observations by TESS. We first list the technical requirements of the TESS instrument and ground systems processing that limit the total number of target slots. We then describe algorithms used by the TESS Payload Operation Center (POC) to merge candidate targets requested by the various TESS mission elements (the Target Selection Working Group, TESS Asteroseismic Science Consortium, and Guest Investigator office). Lastly, we summarize the properties of the observed TESS targets over the two-year primary TESS mission. We find that the POC target selection algorithm results in 2.1 to 3.4 times as many observed targets as target slots allocated for each mission element. We also find that the sky distribution of observed targets is different from the sky distributions of candidate targets due to technical constraints that require a relatively even distribution of targets across the TESS fields of view. We caution researchers exploring statistical analyses of TESS planet-host stars that the population of observed targets cannot be characterized by any simple set of criteria applied to the properties of the input Candidate Target Lists. 0 into PostgreSQL...\n",
      "Inserting test sample 348  The Transiting Exoplanet Survey Satellite (TESS) mission is designed to search for exoplanets orbiting nearby bright stars. The target selection procedure for this mission involves identifying stars that are suitable for observation and analysis. The process of selecting suitable targets is crucial to the success of the mission, as it ensures that data collected by TESS will be of the highest quality and useful in studying exoplanets. The target selection procedure involves several steps, including identifying potential targets, assessing their suitability for observation, and prioritizing them based on scientific criteria. \n",
      "\n",
      "TESS uses a photometric technique to detect exoplanets by measuring the small dips in brightness of a star when a planet crosses in front of it. The target selection procedure takes into account various factors that may affect the ability to detect exoplanets, such as the brightness and stability of the target star, as well as the presence of any known planets or signals in the system. The end result is a final list of targets that maximizes the potential for exoplanet detection and scientific investigation. In summary, the TESS mission target selection procedure plays a vital role in the search for exoplanets and the understanding of our universe. 1 into PostgreSQL...\n",
      "Inserting test sample 349  We study radio-frequency (RF) wireless power transfer (WPT) using a digital radio transmitter for applications where alternative analog transmit circuits are impractical. An important paramter for assessing the viability of an RF WPT system is its end-to-end efficiency. In this regard, we present a prototype test-bed comprising a software-defined radio (SDR) transmitter and an energy harvesting receiver with a low resistive load; employing an SDR makes our research meaningful for simultaneous wireless information and power transfer (SWIPT). We analyze the effect of clipping and non-linear amplification at the SDR on multisine waveforms. Our experiments suggest that when the DC input power at the transmitter is constant, high peak-to-average power ratio (PAPR) multisine are unsuitable for RF WPT over a flat-fading channel, due to their low average radiated power. The results indicate that the end-to-end efficiency is positively correlated to the average RF power of the waveform, and that it reduces with increasing PAPR. Consequently, digital modulations such as phase-shift keying (PSK) and quadrature amplitude modeulation (QAM) yield better end-to-end efficiency than multisines. Moreover, the end-to-end efficiency of PSK and QAM signals is invariant of the transmission bit rate. An in-depth analysis of the end-to-end efficiency of WPT reveals that the transmitter efficiency is lower than the receiver efficiency. Furthermore, we study the impact of a reflecting surface on the end-to-end efficiency of WPT, and assess the transmission quality of the information signals by evaluating their error vector magnitude (EVM) for SWIPT. Overall, the experimental observations of end-to-end efficiency and EVM suggest that, while employing an SDR transmitter with fixed DC input power, a baseband quadrature PSK signal is most suitable for SWIPT at large, among PSK and QAM signals. 0 into PostgreSQL...\n",
      "Inserting test sample 350  This paper explores the optimization of RF wireless power transfer using digital radio transmitter technology. In particular, the effects of waveform shape on the efficiency of power transfer are studied. The use of digital radio transmitters offers the potential for increased efficiency and throughput as compared to traditional analog systems. The study uses simulations and experiments to explore the effects of waveform shape on power transfer efficiency. Results show that various waveform shapes can have a significant impact on the efficiency of power transfer. Specifically, waveforms with higher crest factors were found to be more efficient than those with lower crest factors. Additionally, variations in the amplitude and phase of the waveform were shown to impact the efficiency of power transfer.\n",
      "\n",
      "In addition to waveform optimization, this paper also explores the use of End-to-End (E2E) technology to improve the efficiency of RF wireless power transfer. E2E technology aims to eliminate the need for intermediate power conversion steps, resulting in a more streamlined and efficient power transfer system. Simulation and experimental results demonstrate that E2E technology can provide significant improvements in efficiency, with potential applications in areas such as electric vehicles and wireless charging.\n",
      "\n",
      "Overall, this paper sheds light on the potential benefits of using digital radio transmitters and E2E technology to optimize RF wireless power transfer. The study presents valuable insights into the optimization of waveform shape and the use of E2E technology to improve power transfer efficiency. These findings have significant implications for the design of future wireless power transfer systems. 1 into PostgreSQL...\n",
      "Inserting test sample 351  Post-starburst (or \"E+A\") galaxies are characterized by low H$\\alpha$ emission and strong Balmer absorption, suggesting a recent starburst, but little current star formation. Although many of these galaxies show evidence of recent mergers, the mechanism for ending the starburst is not yet understood.\n",
      "\n",
      "To study the fate of the molecular gas, we search for CO (1-0) and (2-1) emission with the IRAM 30m and SMT 10m telescopes in 32 nearby ($0.01<z<0.12$) post-starburst galaxies drawn from the Sloan Digital Sky Survey. We detect CO in 17 (53%). Using CO as a tracer for molecular hydrogen, and a Galactic conversion factor, we obtain molecular gas masses of $M(H_2)=10^{8.6}$-$10^{9.8} M_\\odot$ and molecular gas mass to stellar mass fractions of $\\sim10^{-2}$-$10^{-0.5}$, comparable to those of star-forming galaxies. The large amounts of molecular gas rule out complete gas consumption, expulsion, or starvation as the primary mechanism that ends the starburst in these galaxies. The upper limits on $M(H_2)$ for the 15 undetected galaxies range from $10^{7.7} M_\\odot$ to $10^{9.7} M_\\odot$, with the median more consistent with early-type galaxies than with star-forming galaxies. Upper limits on the post-starburst star formation rates (SFRs) are lower by $\\sim10\\times$ than for star-forming galaxies with the same $M(H_2)$. We also compare the molecular gas surface densities ($\\Sigma_{\\rm H_2}$) to upper limits on the SFR surface densities ($\\Sigma_{\\rm SFR}$), finding a significant offset, with lower $\\Sigma_{\\rm SFR}$ for a given $\\Sigma_{\\rm H_2}$ than is typical for star-forming galaxies. This offset from the Kennicutt-Schmidt relation suggests that post-starbursts have lower star formation efficiency, a low CO-to-H$_2$ conversion factor characteristic of ULIRGs, and/or a bottom-heavy initial mass function, although uncertainties in the rate and distribution of current star formation remain. 0 into PostgreSQL...\n",
      "Inserting test sample 352  In this study, we present the discovery of large molecular gas reservoirs in post-starburst galaxies, shedding light on the gas evolution and star formation history of these systems. Our observations were carried out using the Atacama Large Millimeter/submillimeter Array (ALMA), which enables us to probe the cold gas content of galaxies with unprecedented sensitivity and spatial resolution. We selected a sample of post-starburst galaxies based on their optical spectra, which reveal strong Balmer absorption lines and weak emission lines, suggesting a recent star formation shutdown.\n",
      "\n",
      "Our ALMA observations reveal that these galaxies host massive molecular gas reservoirs, with gas masses ranging from 10^8 to 10^10 solar masses. We find that the molecular gas fraction (i.e., the ratio of molecular gas to stellar mass) is significantly higher in post-starburst galaxies than in normal star-forming galaxies, indicating an enhanced gas content shortly after the cessation of star formation. Moreover, we find that the molecular gas reservoirs in post-starburst galaxies are more extended than those in normal galaxies, suggesting that they are more diffuse and distributed across the galaxy.\n",
      "\n",
      "Our results have important implications for our understanding of galaxy evolution and quenching mechanisms. The presence of large molecular gas reservoirs in post-starburst galaxies suggests that the star formation shutdown is not solely due to the exhaustion of the gas supply, but is also influenced by other processes such as feedback from active galactic nuclei or merger events. Furthermore, the extended nature of the gas reservoirs indicates that the gas may be more susceptible to stripping or removal from the galaxy, which can further suppress star formation.\n",
      "\n",
      "In conclusion, our study provides new insights into the gas content and evolution of post-starburst galaxies, highlighting the importance of molecular gas in understanding the quenching of star formation in these systems. Future observations with ALMA and other facilities will continue to improve our understanding of galaxy evolution and the role of molecular gas in shaping the cosmic landscape. 1 into PostgreSQL...\n",
      "Inserting test sample 353  The oxygen-order dependent emergence of superconductivity in YBa2Cu3O6+x is studied, for the first time in a comparative way, on pair samples having the same oxygen content and thermal history, but different Cu(1)Ox chain arrangements deriving from their intercalated and deintercalated nature.\n",
      "\n",
      "Structural and electronic non-equivalence of pairs samples is detected in the critical region and found to be related, on microscopic scale, to a different average chain length, which, on being experimentally determined by nuclear quadrupole resonance (NQR), sheds new light on the concept of critical chain length for hole doping efficiency. 0 into PostgreSQL...\n",
      "Inserting test sample 354  The critical chain length and the emergence of superconductivity in oxygen-equilibrated pairs of YBa2Cu3O6.30 were studied in this research. The results indicate that the sample's critical current density increased with the critical chain length. The oxygen content of samples has a significant impact on the critical chain length and its corresponding superconductivity. It was also found that the critical chain length can be maximized by increasing the oxygen concentration through oxygen annealing. These findings provide insights into the design and optimization of superconducting materials, particularly those dependent on oxygen equilibration. 1 into PostgreSQL...\n",
      "Inserting test sample 355  Aims. GJ 9827 (K2-135) has recently been found to host a tightly packed system consisting of three transiting small planets whose orbital periods of 1.2, 3.6, and 6.2 days are near the 1:3:5 ratio. GJ 9827 hosts the nearest planetary system (d = $30.32\\pm1.62$ pc) detected by Kepler and K2 . Its brightness (V = 10.35 mag) makes the star an ideal target for detailed studies of the properties of its planets.\n",
      "\n",
      "Results. We find that GJ 9827 b has a mass of $M_\\mathrm{b}=3.74^{+0.50}_{-0.48}$ $M_\\oplus$ and a radius of $R_\\mathrm{b}=1.62^{+0.17}_{-0.16}$ $R_\\oplus$, yielding a mean density of $\\rho_\\mathrm{b} = 4.81^{+1.97}_{-1.33}$ g cm$^{-3}$. GJ 9827 c has a mass of $M_\\mathrm{c}=1.47^{+0.59}_{-0.58}$ $M_\\oplus$, radius of $R_\\mathrm{c}=1.27^{+0.13}_{-0.13}$ $R_\\oplus$, and a mean density of $\\rho_\\mathrm{c}= 3.87^{+2.38}_{-1.71}$ g cm$^{-3}$. For GJ 9827 d we derive $M_\\mathrm{d}=2.38^{+0.71}_{-0.69}$ $M_\\oplus$, $R_\\mathrm{d}=2.09^{+0.22}_{-0.21}$ $R_\\oplus$, and $\\rho_\\mathrm{d}= 1.42^{+0.75}_{-0.52}$ g cm$^{-3}$.\n",
      "\n",
      "Conclusions. GJ 9827 is one of the few known transiting planetary systems for which the masses of all planets have been determined with a precision better than 30%. This system is particularly interesting because all three planets are close to the limit between super-Earths and mini-Neptunes. We also find that the planetary bulk compositions are compatible with a scenario where all three planets formed with similar core/atmosphere compositions, and we speculate that while GJ 9827 b and GJ 9827 c lost their atmospheric envelopes, GJ 9827 d maintained its atmosphere, owing to the much lower stellar irradiation. This makes GJ 9827 one of the very few systems where the dynamical evolution and the atmospheric escape can be studied in detail for all planets, helping us to understand how compact systems form and evolve. 0 into PostgreSQL...\n",
      "Inserting test sample 356  The purpose of this paper is to present a detailed analysis of the near-resonant exoplanets orbiting the star GJ 9827 (also known as K2-135), including the determination of their masses. These exoplanets have been observed to transit their host star, allowing for accurate measurements of their orbital periods and radii. \n",
      "\n",
      "Using radial velocity measurements obtained from the HARPS-N spectrograph, we have determined the masses of the three planets in the system: GJ 9827 b, c, and d. With masses of 0.02, 0.03, and 0.04 Jupiter masses respectively, the planets' densities suggest that they are primarily composed of rock and iron. \n",
      "\n",
      "We have also investigated the orbital stability of the system using numerical integrations, revealing a complex three-body resonance which drives the planets' eccentricities and mutual inclinations. The near-resonant configuration of this system, with ratios of 1:3:5 between the three planets' orbital periods, is particularly interesting due to its rarity in the population of known exoplanets. Our results provide new insights into the formation and evolution of multi-planet systems, and the occurrence rates of near-resonant architectures.\n",
      "\n",
      "Our findings have important implications for future studies of exoplanet characterization and atmospheric composition. The proximity of GJ 9827 to Earth, combined with the brightness of the host star (K=10.8 mag), make this system an attractive target for future observations with the James Webb Space Telescope and other upcoming facilities. Precise measurements of the planets' masses and atmospheric properties will allow for further investigations into the diversity of planetary compositions and the mechanisms responsible for driving their formation and evolution.\n",
      "\n",
      "In conclusion, our analysis of the near-resonant planets orbiting GJ 9827 provides new insights into the formation and dynamics of multi-planet systems. Through the combination of radial velocities and numerical integrations, we have determined the masses of the three planets and explored the stability of their orbits. Further studies of this system with the James Webb Space Telescope and other observatories will continue to shed light on the properties and diversity of exoplanets orbiting other stars beyond our own solar system. 1 into PostgreSQL...\n",
      "Inserting test sample 357  This paper presents a novel approach for multi-task learning of language understanding (LU) and dialogue state tracking (DST) in task-oriented dialogue systems. Multi-task training enables the sharing of the neural network layers responsible for encoding the user utterance for both LU and DST and improves performance while reducing the number of network parameters. In our proposed framework, DST operates on a set of candidate values for each slot that has been mentioned so far. These candidate sets are generated using LU slot annotations for the current user utterance, dialogue acts corresponding to the preceding system utterance and the dialogue state estimated for the previous turn, enabling DST to handle slots with a large or unbounded set of possible values and deal with slot values not seen during training. Furthermore, to bridge the gap between training and inference, we investigate the use of scheduled sampling on LU output for the current user utterance as well as the DST output for the preceding turn. 0 into PostgreSQL...\n",
      "Inserting test sample 358  In recent years, there has been an increasing interest in developing natural language processing systems that can simultaneously perform multiple tasks such as language understanding and dialogue state tracking. Multi-task learning, which involves training a single model to perform multiple tasks, is a promising approach to achieve this goal. In this paper, we propose a multi-task learning framework for joint language understanding and dialogue state tracking. Our framework encodes the input text using a shared hierarchical attention network, which is then used to predict both the intent of the user's utterance and the corresponding dialogue state. We evaluate our framework on a popular benchmark dataset and demonstrate that it outperforms several state-of-the-art approaches on both language understanding and dialogue state tracking tasks. Our results suggest that multi-task learning can improve the performance of natural language processing systems by leveraging shared representations across multiple tasks. 1 into PostgreSQL...\n",
      "Inserting test sample 359  We investigate the enrichment in elements produced by the slow neutron-capture process ($s$-process) in the globular clusters M4 (NGC 6121) and M22 (NGC 6656). Stars in M4 have homogeneous abundances of Fe and neutron-capture elements, but the entire cluster is enhanced in $s$-process elements (Sr, Y, Ba, Pb) relative to other clusters with a similar metallicity.\n",
      "\n",
      "In M22, two stellar groups exhibit different abundances of Fe and $s$-process elements. By subtracting the mean abundances of $s$-poor from $s$-rich stars, we derive $s$-process residuals or empirical $s$-process distributions for M4 and M22. We find that the $s$-process distribution in M22 is more weighted toward the heavy $s$-peak (Ba, La, Ce) and Pb than M4, which has been enriched mostly with light $s$-peak elements (Sr, Y, Zr). We construct simple chemical evolution models using yields from massive star models that include rotation, which dramatically increases $s$-process production at low metallicity. We show that our massive star models with rotation rates of up to 50\\% of the critical (break-up) velocity and changes to the preferred $^{17}$O($\\alpha$,$\\gamma$)$^{21}$Ne rate produce insufficient heavy $s$-elements and Pb to match the empirical distributions. For models that incorporate AGB yields, we find that intermediate-mass yields (with a $^{22}$Ne neutron source) alone do not reproduce the light-to-heavy $s$-element ratios for M4 and M22, and that a small contribution from models with a $^{13}$C pocket is required. With our assumption that $^{13}$C pockets form for initial masses below a transition range between 3.0 and 3.5 M$_\\odot$, we match the light-to-heavy s-element ratio in the s-process residual of M22 and predict a minimum enrichment timescale of between 240 and 360 Myr. Our predicted value is consistent with the 300 Myr upper limit age difference between the two groups derived from isochrone fitting. 0 into PostgreSQL...\n",
      "Inserting test sample 360  The s-process is a significant mechanism that contributes to the formation of elements heavier than iron. In globular clusters, this process has a pronounced effect on the chemical composition of stars, as it is responsible for the production of stable nuclei. M4 and M22 are two of the most significant globular clusters in our galaxy, and their study has revealed important insights into the processes that lead to the enrichment of their stars.\n",
      "\n",
      "The aim of this study is to analyze the s-process enrichment of the stars in M4 and M22 through the use of high-resolution spectroscopy. Our observations show that the s-process has had a significant impact on the chemical composition of the stars in these clusters. The heavy-element abundances we derived for our sample stars are consistent with the predictions of s-process models for low-mass AGB stars.\n",
      "\n",
      "Furthermore, our results reveal that the s-process enrichment in globular clusters is not homogeneous. In M4, the s-process enrichment is found to be higher in stars located in the cluster's core, compared to those in the outskirts. This observation suggests the existence of a metallicity gradient that has not been previously reported in this cluster. In contrast, our findings for M22 indicate that the s-process enrichment is uniform across the cluster's stars.\n",
      "\n",
      "Our study also provides new insights into the nature of the s-process in globular clusters. We found that the abundance ratios of heavy elements in our sample stars are consistent with the predictions of models that include contributions from both the main and weak s-process. This suggests that the s-process in globular clusters is complex and involves multiple nucleosynthesis mechanisms.\n",
      "\n",
      "In conclusion, our study has revealed new information about the s-process enrichment in M4 and M22. Our findings demonstrate the importance of high-resolution spectroscopy in the study of globular clusters and the s-process. This research provides valuable information for the development of models that aim to accurately predict the chemical evolution of globular clusters and the formation of heavy elements in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 361  The severity of knee osteoarthritis is graded using the 5-point Kellgren-Lawrence (KL) scale where healthy knees are assigned grade 0, and the subsequent grades 1-4 represent increasing severity of the affliction. Although several methods have been proposed in recent years to develop models that can automatically predict the KL grade from a given radiograph, most models have been developed and evaluated on datasets not sourced from India. These models fail to perform well on the radiographs of Indian patients. In this paper, we propose a novel method using convolutional neural networks to automatically grade knee radiographs on the KL scale. Our method works in two connected stages: in the first stage, an object detection model segments individual knees from the rest of the image; in the second stage, a regression model automatically grades each knee separately on the KL scale. We train our model using the publicly available Osteoarthritis Initiative (OAI) dataset and demonstrate that fine-tuning the model before evaluating it on a dataset from a private hospital significantly improves the mean absolute error from 1.09 (95% CI: 1.03-1.15) to 0.28 (95% CI: 0.25-0.32). Additionally, we compare classification and regression models built for the same task and demonstrate that regression outperforms classification. 0 into PostgreSQL...\n",
      "Inserting test sample 362  This study explores the use of convolutional neural networks (CNNs) for the automatic grading of knee osteoarthritis (OA) on the Kellgren-Lawrence (KL) scale from radiographs. Knee OA is a degenerative joint disease that affects millions of people worldwide, and accurate grading of its severity is essential for effective treatment planning. The KL scale is a widely used method for grading knee OA, which ranges from 0 (normal joint space) to 4 (severe joint space narrowing with bone deformity). However, manual grading of KL score is time-consuming and subject to inter-observer variability. CNNs have shown promise in the medical field for automated image analysis tasks, and this study aims to evaluate their effectiveness in grading knee OA on the KL scale. Using a dataset of 376 knee radiographs, a CNN model was trained and tested. The results show that the proposed CNN-based approach achieved an accuracy of 86.7% in classifying KL grades, outperforming a traditional machine learning approach. This suggests that CNNs have the potential to be a reliable and efficient tool for grading knee OA severity on the KL scale from radiographs, providing better diagnosis and treatment planning for patients with knee OA. 1 into PostgreSQL...\n",
      "Inserting test sample 363  We show that the 750 GeV diphoton excess can be explained by introducing vector-like quarks and hidden fermions charged under a hidden U(1) gauge symmetry, which has a relatively large coupling constant as well as a significant kinetic mixing with U(1)$_Y$. With the large kinetic mixing, the standard model gauge couplings unify around $10^{17}$ GeV, suggesting the grand unified theory without too rapid proton decay. Our scenario predicts events with a photon and missing transverse momentum, and its cross section is related to that for the diphoton excess through the kinetic mixing. We also discuss other possible collider signatures and cosmology, including various ways to evade constraints on exotic stable charged particles. In some cases where the 750 GeV diphoton excess is due to diaxion decays, our scenario also predicts triphoton and tetraphoton signals. 0 into PostgreSQL...\n",
      "Inserting test sample 364  The diphoton excess at the Large Hadron Collider (LHC) has recently resulted in speculation that a new particle, beyond the standard model, may exist. We explore the possibility that the diphoton excess arises from a hidden U(1) gauge symmetry with a large kinetic mixing with the electromagnetic U(1) gauge group. Such a symmetry breaking can naturally give rise to a massless gauge boson, known as the dark photon. We find that the diphoton excess can be well explained by the dark photons with a mass between 750 and 800 GeV. The model is consistent with other collider and astrophysical constraints. We propose that the diphoton excess provides a strong motivation for future searches for the dark photon at the LHC and other experiments. 1 into PostgreSQL...\n",
      "Inserting test sample 365  [Abridged] We present more than 4 years of Swift X-ray observations of the 2013 superoutburst, subsequent decline and quiescence of the WZ Sge-type dwarf nova SSS J122221.7-311525 (SSS122222) from 6 days after discovery. Only a handful of WZ Sge-type dwarf novae have been observed in X-rays, and until recently GW Lib was the only binary of this type with complete coverage of an X-ray light curve throughout a superoutburst. We collected extensive X-ray data of a second such system to understand the extent to which the unexpected properties of GW Lib are common to the WZ Sge class. We analysed the X-ray light curve and compared it with the behaviour of superhumps which were detected in the optical light curve. We also performed spectral analysis of the data. The results were compared with the properties of GW Lib, for which new X-ray observations were also obtained. SSS122222 was variable and around five times brighter in 0.3-10 keV X-rays during the superoutburst than in quiescence, mainly because of a significant strengthening of a high-energy component of the X-ray spectrum. The post-outburst decline of the X-ray flux lasted at least 500 d. The data show no evidence of the expected optically thick boundary layer in the system during the outburst. SSS122222 also exhibited a sudden X-ray flux change in the middle of the superoutburst, which occurred exactly at the time of the superhump stage transition. A similar X-ray behaviour was also detected in GW Lib. This result demonstrates a relationship between the outer disc and the white dwarf boundary layer for the first time, and suggests that models for accretion discs in high mass ratio accreting binaries are currently incomplete. The very long decline to X-ray quiescence is also in strong contrast to the expectation of low viscosity in the disc after outburst. 0 into PostgreSQL...\n",
      "Inserting test sample 366  The study presented in this paper discusses the presence of superhumps in cataclysmic variable stars and their possible link to X-ray emission. The focus of the study is SSS J122221.7-311525 and GW Lib, two cataclysmic variables that have been observed undergoing superoutbursts. These superoutbursts are characterized by a periodic variation in brightness, which can last for several days and are known as superhumps. The study aims to investigate the behavior of these superhumps and their connection to X-ray emission.\n",
      "\n",
      "The observations were conducted using the X-ray Telescope (XRT) on board the Neil Gehrels Swift Observatory, and the Optical Gravitational Lensing Experiment (OGLE) at Las Campanas Observatory. The XRT observations were made during the superoutburst of SSS J122221.7-311525 and the OGLE observations were made during the superoutburst of GW Lib. The data obtained from the observations were analyzed to determine the characteristics of the superhumps, including their period and amplitude.\n",
      "\n",
      "The results of the study show a clear correlation between the appearance of superhumps and the presence of X-ray emission in the two cataclysmic variables. The X-ray flux was found to increase during the superoutbursts, and a delay was observed between the increase in X-ray flux and the appearance of superhumps. This delay suggests that the superhumps are triggered by an increase in mass transfer rate from the secondary star to the white dwarf. \n",
      "\n",
      "Overall, this study provides new insights into the behavior of superhumps in cataclysmic variables and their relation to X-ray emission. These findings contribute to a better understanding of the dynamics of accretion disks and the processes that lead to the emission of X-rays in these systems. Further studies of superhumps and their connection to X-ray emission are needed to fully understand the underlying physical processes. 1 into PostgreSQL...\n",
      "Inserting test sample 367  I develop a novel macroeconomic epidemiological agent-based model to study the impact of the COVID-19 pandemic under varying policy scenarios. Agents differ with regard to their profession, family status and age and interact with other agents at home, work or during leisure activities. The model allows to implement and test actually used or counterfactual policies such as closing schools or the leisure industry explicitly in the model in order to explore their impact on the spread of the virus, and their economic consequences. The model is calibrated with German statistical data on time use, demography, households, firm demography, employment, company profits and wages. I set up a baseline scenario based on the German containment policies and fit the epidemiological parameters of the simulation to the observed German death curve and an estimated infection curve of the first COVID-19 wave. My model suggests that by acting one week later, the death toll of the first wave in Germany would have been 180% higher, whereas it would have been 60% lower, if the policies had been enacted a week earlier. I finally discuss two stylized fiscal policy scenarios: procyclical (zero-deficit) and anticyclical fiscal policy. In the zero-deficit scenario a vicious circle emerges, in which the economic recession spreads from the high-interaction leisure industry to the rest of the economy. Even after eliminating the virus and lifting the restrictions, the economic recovery is incomplete. Anticyclical fiscal policy on the other hand limits the economic losses and allows for a V-shaped recovery, but does not increase the number of deaths. These results suggest that an optimal response to the pandemic aiming at containment or holding out for a vaccine combines early introduction of containment measures to keep the number of infected low with expansionary fiscal policy to keep output in lower risk sectors high. 0 into PostgreSQL...\n",
      "Inserting test sample 368  The COVID-19 pandemic has affected the world in ways never before seen in modern history. Governments around the globe have sought to mitigate the spread of the virus by implementing various social distancing measures, including mandatory quarantines and lockdowns. However, these measures have had severe economic consequences. In this paper, we present COVID-Town, an integrated economic-epidemiological agent-based model that aims to simulate the effects of the pandemic on both the economy and the propagation of the virus.\n",
      "\n",
      "Our model integrates micro-level economic behaviors, such as consumer demand and firm production decisions, with epidemic diffusion dynamics, including disease transmission and individual protective measures. By doing so, we are able to capture the feedback mechanisms between those two aspects that have been crucial in shaping the pandemic outcomes.\n",
      "\n",
      "Using data from an actual U.S. small town, we demonstrate the capabilities of COVID-Town in capturing the spread of the virus and the subsequent economic fallout. Our model demonstrates that the virus has a significant negative impact on the GDP of a town, where a 10% increase in infection rate is associated with an approximately 1% decrease in GDP. While social distancing measures may help to slow down the spread of the virus, they also result in considerable short-term economic costs and unemployment.\n",
      "\n",
      "Our research highlights the importance of incorporating economic considerations in pandemic policy-making. In addition, the model presented in this paper can inform policy-makers about the potential economic consequences of different intervention strategies. Finally, by providing an open-source toolkit accessible to practitioners, COVID-Town can serve as a useful tool for policy evaluation, while also contributing to the field of agent-based modeling during pandemics. 1 into PostgreSQL...\n",
      "Inserting test sample 369  We successfully isolated the $^{17}$O NMR signals at the planar sites in La$_{1.885}$Sr$_{0.115}$CuO$_4$ ($T_{c}=30$ K) by applying an external magnetic field along the Cu-O-Cu bond direction of a single crystal. We demonstrate that charge order enhances incommensurate Cu spin fluctuations below $T_{charge} \\simeq 80$ K. 0 into PostgreSQL...\n",
      "Inserting test sample 370  We report the results of $^{17}$O NMR measurements on a charge-ordered La$_{1.885}$Sr$_{0.115}$CuO$_4$ system. Our study shows a significant splitting of the $^{17}$O line, indicating that the charge distribution in the CuO$_2$ planes is indeed ordered. The observations demonstrate a connection between charge order and the nuclear quadrupole interactions. 1 into PostgreSQL...\n",
      "Inserting test sample 371  A mixed-integer linear programming (MILP) formulation is presented for parameter estimation of the Potts model. Two algorithms are developed; the first method estimates the parameters such that the set of ground states replicate the user-prescribed data set; the second method allows the user to prescribe the ground states multiplicity. In both instances, the optimization process ensures that the bandgap is maximized. Consequently, the model parameter efficiently describes the user data for a broad range of temperatures. This is useful in the development of energy-based graph models to be simulated on Quantum annealing hardware where the exact simulation temperature is unknown. Computationally, the memory requirement in this method grows exponentially with the graph size. Therefore, this method can only be practically applied to small graphs. Such applications include learning of small generative classifiers and spin-lattice model with energy described by Ising hamiltonian. Learning large data sets poses no extra cost to this method; however, applications involving the learning of high dimensional data are out of scope. 0 into PostgreSQL...\n",
      "Inserting test sample 372  In this study, we propose a method for optimizing the bandgap of combinatorial graphs in quantum annealing, with the goal of achieving tailored ground states. The bandgap, which is the energy difference between the ground state and the first excited state, plays a crucial role in controlling the performance of quantum annealing algorithms. By introducing a novel graph construction technique, we demonstrate how to systematically adjust the bandgap to desired values. Our theoretical analysis and numerical simulations show that optimizing the bandgap can improve the performance of quantum annealing in solving certain optimization problems. Moreover, we investigate the robustness of the method to noise and imperfections, as well as its scalability to larger problem sizes. Our work provides a promising avenue for tailoring the ground state of a quantum annealer using combinatorial graphs, representing a potential solution to challenging optimization problems in various domains. 1 into PostgreSQL...\n",
      "Inserting test sample 373  A combined fit is performed to the BaBar and Belle measurements of the $\\EE \\to \\phi \\pi^+ \\pi^-$ and $\\phi \\fzero$ cross sections for center-of-mass energy between threshold and 3.0 GeV. The resonance parameters of the $\\phi(1680)$ and Y(2175) are determined. The mass is $(1681^{+10}_{-12})$ MeV/$c^2$ and the width is $(221^{+34}_{-24})$ MeV/$c^2$ for the $\\phi(1680)$, and the mass is $(2117^{+59}_{-49})$ MeV/$c^2$ and the width is $(164^{+69}_{-80})$ MeV/$c^2$ for the Y(2175). These information will shed light on the understanding of the nature of the excited $\\phi$ and $Y$ states observed in $\\EE$ annihilation. 0 into PostgreSQL...\n",
      "Inserting test sample 374  We present a combined analysis of BaBar and Belle measurements of the processes e+e- to phi pi+ pi- and phi f0(980). The dataset consists of 966 fb^-1 collected at the PEP-II B-Factory and 121.4 fb^-1 collected at the KEKB accelerator. We observe significant contributions from the intermediate states f0(980), f2'(1525), and omega pi+ pi-. Our results are consistent with isospin symmetry and provide a more precise determination of branching fractions and spin-parity assignments for the involved resonances. The combined analysis leads to a better understanding of the nature of the observed states and can contribute to future studies of the dynamics of quarkonium decay. 1 into PostgreSQL...\n",
      "Inserting test sample 375  We establish weak well-posedness for critical symmetric stable driven SDEs in R d with additive noise Z, d $\\ge$ 1. Namely, we study the case where the stable index of the driving process Z is $\\alpha$ = 1 which exactly corresponds to the order of the drift term having the coefficient b which is continuous and bounded. In particular, we cover the cylindrical case when Zt = (Z 1 t ,. .. , Z d t) and Z 1 ,. .. , Z d are independent one dimensional Cauchy processes.\n",
      "\n",
      "Our approach relies on L p-estimates for stable operators and uses perturbative arguments. 1. Statement of the problem and main results We are interested in proving well-posedness for the martingale problem associated with the following SDE: (1.1) X t = x + t 0 b(X s)ds + Z t , where (Z s) s$\\ge$0 stands for a symmetric d-dimensional stable process of order $\\alpha$ = 1 defined on some filtered probability space ($\\Omega$, F, (F t) t$\\ge$0 , P) (cf. [2] and the references therein) under the sole assumptions of continuity and boundedness on the vector valued coefficient b: (C) The drift b : R d $\\rightarrow$ R d is continuous and bounded. 1 Above, the generator L of Z writes: L$\\Phi$(x) = p.v.\n",
      "\n",
      "R d \\{0} [$\\Phi$(x + z) -- $\\Phi$(x)]$\\nu$(dz), x $\\in$ R d , $\\Phi$ $\\in$ C 2 b (R d), $\\nu$(dz) = d$\\rho$ $\\rho$ 2$\\mu$ (d$\\theta$), z = $\\rho$$\\theta$, ($\\rho$, $\\theta$) $\\in$ R * + x S d--1. (1.2) (here $\\times$, $\\times$ (or $\\times$) and | $\\times$ | denote respectively the inner product and the norm in R d). In the above equation, $\\nu$ is the L{\\'e}vy intensity measure of Z, S d--1 is the unit sphere of R d and$\\mu$ is a spherical measure on S d--1. It is well know, see e.g. [20] that the L{\\'e}vy exponent $\\Phi$ of Z writes as: (1.3) $\\Phi$($\\lambda$) = E[exp(i $\\lambda$, Z 1)] = exp -- S d--1 | $\\lambda$, $\\theta$ |$\\mu$(d$\\theta$) , $\\lambda$ $\\in$ R d , where $\\mu$ = c 1$\\mu$ , for a positive constant c 1 , is the so-called spectral measure of Z. We will assume some non-degeneracy conditions on $\\mu$. Namely we introduce assumption (ND) There exists $\\kappa$ $\\ge$ 1 s.t. (1.4) $\\forall$$\\lambda$ $\\in$ R d , $\\kappa$ --1 |$\\lambda$| $\\le$ S d--1 | $\\lambda$, $\\theta$ |$\\mu$(d$\\theta$) $\\le$ $\\kappa$|$\\lambda$|. 1 The boundedness of b is here assumed for technical simplicity. Our methodology could apply, up to suitable localization arguments, to a drift b having linear growth. 0 into PostgreSQL...\n",
      "Inserting test sample 376  We investigate the weak well-posedness of multidimensional SDEs driven by stable LÃ©vy processes with index in the range $(1,2]$. In particular, we consider the critical case where the LÃ©vy index equals $2$. Under some regularity assumptions on the coefficients, we prove that the SDE under consideration admits a unique weak solution. Moreover, we show that this solution is a Markov process with continuous paths. Our proof is based on the approximation of the SDE by a sequence of better-behaved SDEs and the application of the PDE method for the latter. We further demonstrate the necessity of our assumptions by providing examples where either uniqueness or continuity of the solution fails. In addition, we provide a comparison result for two different stability indices. Our results generalize the well-known case of Brownian motion, and we believe that they may find applications in various fields including finance, physics, and engineering. Finally, we mention some open problems such as the extension of our results to non-Markovian settings and the study of regularity properties of the solutions. We hope that our work will contribute to a better understanding of the behavior of stochastic differential equations driven by stable LÃ©vy processes, which are known to exhibit various exotic phenomena such as jumps, long-range dependence, and heavy tails. 1 into PostgreSQL...\n",
      "Inserting test sample 377  A discrete time stochastic model for a multiagent system given in terms of a large collection of interacting Markov chains is studied. The evolution of the interacting particles is described through a time inhomogeneous transition probability kernel that depends on the 'gradient' of the potential field. The particles, in turn, dynamically modify the potential field through their cumulative input. Interacting Markov processes of the above form have been suggested as models for active biological transport in response to external stimulus such as a chemical gradient. One of the basic mathematical challenges is to develop a general theory of stability for such interacting Markovian systems and for the corresponding nonlinear Markov processes that arise in the large agent limit. Such a theory would be key to a mathematical understanding of the interactive structure formation that results from the complex feedback between the agents and the potential field. It will also be a crucial ingredient in developing simulation schemes that are faithful to the underlying model over long periods of time. The goal of this work is to study qualitative properties of the above stochastic system as the number of particles (N) and the time parameter (n) approach infinity. In this regard asymptotic properties of a deterministic nonlinear dynamical system, that arises in the propagation of chaos limit of the stochastic model, play a key role. We show that under suitable conditions this dynamical system has a unique fixed point. This result allows us to study stability properties of the underlying stochastic model. We show that as N \\rightarrow \\infty, the stochastic system is well approximated by the dynamical system, uniformly over time. As a consequence, for an arbitrarily initialized system, as N\\rightarrow \\infty and n \\rightarrow \\infty, the potential field and the empirical measure of the interacting particles are shown to converge to the unique fixed point of the dynamical system. In general, simulation of such interacting Markovian systems is a computationally daunting task. We propose a particle based approximation for the dynamic potential field which allows for a numerically tractable simulation scheme. It is shown that this simulation scheme well approximates the true physical system, uniformly over an infinite time horizon. 0 into PostgreSQL...\n",
      "Inserting test sample 378  The study of interacting systems is essential to understand the behavior of a complex network of agents. In this paper, we present a model of discrete time Markovian agents that interact through a potential. The agents are characterized by their state, which can be discrete or continuous, and their transition probabilities, which depend on the current states of all the other agents in the system.\n",
      "\n",
      "We consider a potential function that measures the interaction among the agents, reflecting the cost or benefit of being in the same state as another agent. The potential function can be modeled as a pairwise interaction or as a more general function of the states of all the agents in the system.\n",
      "\n",
      "Our approach allows the study of a wide range of phenomena, from the diffusive behavior of agents in continuous spaces to the synchronization and clustering of agents in discrete spaces. We analyze the stability and convergence properties of the system for different interaction potentials and initial conditions. We also investigate the effect of different parameters, such as the number of agents, the dimension of the state space, and the strength of the interaction potential.\n",
      "\n",
      "We apply our model to several scenarios, including opinion dynamics, social learning, and epidemiology. In each case, we show how the interaction potential affects the final outcome of the system and how the behavior of the agents changes over time. For example, we demonstrate how the presence of a social influence can either speed up or slow down the convergence to consensus, depending on the strength of the interaction potential.\n",
      "\n",
      "Our results contribute to the understanding of the dynamics of interacting systems and provide a framework for the analysis of various phenomena in social, biological, and physical systems. Moreover, our model can be extended to include more complex interactions and heterogeneous agents, opening up new possibilities for future research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 379  In this paper, we extend and refine previous Tur\\'an-type results on graphs with a given circumference. Let $W_{n,k,c}$ be the graph obtained from a clique $K_{c-k+1}$ by adding $n-(c-k+1)$ isolated vertices each joined to the same $k$ vertices of the clique, and let $f(n,k,c)=e(W_{n,k,c})$. Improving a celebrated theorem of Erd\\H{o}s and Gallai, Kopylov proved that for $c<n$, any 2-connected graph $G$ on $n$ vertices with circumference $c$ has at most $\\max{f(n,2,c),f(n,\\lfloor\\frac{c}{2}\\rfloor,c)}$ edges. Recently, F\\\"uredi et al. proved a stability version of Kopylov's theorem. Their main result states that if $G$ is a 2-connected graph on $n$ vertices with circumference $c$ such that $10\\leq c<n$ and $e(G)>\\max{f(n,3,c),f(n,\\lfloor\\frac{c}{2}\\rfloor-1,c)}$, then either $G$ is a subgraph of $W_{n,2,c}$ or $W_{n,\\lfloor\\frac{c}{2}\\rfloor,c}$, or $c$ is odd and $G$ is a subgraph of a member of two well-characterized families which we define as $\\mathcal{X}_{n,c}$ and $\\mathcal{Y}_{n,c}$. We prove that if $G$ is a 2-connected graph on $n$ vertices with minimum degree at least $k$ and circumference $c$ such that $10\\leq c<n$ and $e(G)>\\max{f(n,k+1,c),f(n,\\lfloor\\frac{c}{2}\\rfloor-1,c)}$, then one of the following holds: (i) $G$ is a subgraph of $W_{n,k,c}$ or $W_{n,\\lfloor\\frac{c}{2}\\rfloor,c}$, (ii) $k=2$, $c$ is odd, and $G$ is a subgraph of a member of $\\mathcal{X}_{n,c}\\cup \\mathcal{Y}_{n,c}$, or (iii) $k\\geq 3$ and $G$ is a subgraph of the union of a clique $K_{c-k+1}$ and some cliques $K_{k+1}$'s, where any two cliques share the same two vertices.\n",
      "\n",
      "This provides a unified generalization of the above result of F\\\"uredi et al.\n",
      "\n",
      "as well as a recent result of Li et al. and independently, of F\\\"uredi et al.\n",
      "\n",
      "on non-Hamiltonian graphs. Moreover, we prove a stability result on a classical theorem of Bondy on the circumference. 0 into PostgreSQL...\n",
      "Inserting test sample 380  This paper explores stability results on the circumference of a graph. More specifically, we investigate how certain properties of a graph's circumference, such as its length and structure, affect its stability. Our main focus is on understanding the relationship between the circumference and the stability of a graph.\n",
      "\n",
      "We begin by introducing some fundamental concepts related to graphs and their circumferences. We then delve into the theory of graph stability, which involves studying the balance between the number of edges and vertices in a graph. In particular, we consider the stability of graphs that have a fixed circumference length. We explore how the stability of these graphs changes as their circumference length is varied.\n",
      "\n",
      "Our analysis reveals that there is a strong connection between the length of a graph's circumference and its stability. We show that as the circumference length grows, the stability of the graph increases as well. This result is closely linked to the notion of graph rigidity, which describes the extent to which a graph can resist deformation.\n",
      "\n",
      "However, the relationship between circumference length and stability is not always straightforward. In some cases, we find that adding edges or vertices to a graph can increase its circumference length while decreasing its stability. This counterintuitive result highlights the importance of understanding the interaction between the different properties of a graph.\n",
      "\n",
      "In addition to exploring the stability of graphs with fixed circumference length, we also examine the stability of graphs with varying circumference structures. Specifically, we investigate the stability of graphs with uniform and non-uniform circumference structures, and show how variations in structure can affect stability.\n",
      "\n",
      "Overall, our analysis provides new insights into the relationship between graph circumference and stability. Our results suggest that the stability of a graph depends not only on its size and number of edges, but also on more structural properties such as its circumference length and structure. By gaining a deeper understanding of these properties, we can develop more effective algorithms for graph analysis and optimization. 1 into PostgreSQL...\n",
      "Inserting test sample 381  We present an experimental and theoretical description of the kinetics of coalescence of two water drops on a plane solid surface. The case of partial wetting is considered. The drops are in an atmosphere of nitrogen saturated with water where they grow by condensation and eventually touch each other and coalesce. A new convex composite drop is rapidly formed that then exponentially and slowly relaxes to an equilibrium hemispherical cap. The characteristic relaxation time is proportional to the drop radius R * at final equilibrium.\n",
      "\n",
      "This relaxation time appears to be nearly 10 7 times larger than the bulk capillary relaxation time t b = R * $\\eta$/$\\sigma$, where $\\sigma$ is the gas--liquid surface tension and $\\eta$ is the liquid shear viscosity. In order to explain this extremely large relaxation time, we consider a model that involves an Arrhenius kinetic factor resulting from a liquid--vapour phase change in the vicinity of the contact line. The model results in a large relaxation time of order t b exp(L/RT) where L is the molar latent heat of vaporization, R is the gas constant and T is the temperature. We model the late time relaxation for a near spherical cap and find an exponential relaxation whose typical time scale agrees reasonably well with the experiment. 1.\n",
      "\n",
      "Introduction Fusion or coalescence between drops is a key process in a wide range of phenomena: phase transition in fluids and liquid mixtures or polymers, stability of foams and emulsions, and sintering in metallurgy (Eggers 1998), which is why the problem of coalescence has already received considerable attention. Most of the studies of this process so far have been devoted to the coalescence of two spherical drops floating in a medium. The kinetics of the process before and after the drops have touched each other is governed by the hydrodynamics inside and outside the drops and by the van der Waals forces when the drops are within mesoscopic distance from each other (Yiantsios \\& Davis 1991). The composite drop that results from the coalescence of two drops relaxes to a spherical shape within a time which is dominated by the relaxation of the flow inside and outside (Nikolayev, Beysens \\& Guenoun 1996; Nikolayev \\& Beysens 1997). There are no studies, to our knowledge, of the coalescence of two sessile drops after they touch each other. In this paper, we report a preliminary study of the dynamics and morphology of this process, in the case of hemispherical water droplets which grow slowly on a plane surface at the expense of the surrounding atmosphere, forming what is called 'dew' or 'breath figures' (Beysens et al. 1991; Beysens 1995). The drops eventually touch each other and coalesce to form an elongated composite 0 into PostgreSQL...\n",
      "Inserting test sample 382  The coalescence of sessile drops has been a focus of recent research due to its practical applications in various industries. Understanding the mechanics of coalescence is essential to designing surfaces with particular wetting properties and for controlling droplet formation and dynamics.\n",
      "\n",
      "In general, coalescence refers to the process whereby two or more drops combine to form a single drop. Sessile drops, in particular, are drops that are stationary on a surface, as opposed to sliding or rolling. The coalescence of sessile drops occurs when two or more such drops come into contact on a surface, and the forces acting on the drops cause them to merge.\n",
      "\n",
      "The factors influencing the coalescence of sessile drops are complex and dependent on a range of parameters, including surface topography, intermolecular forces, and drop size. A key parameter is the contact angle, which is the angle between the drop and the surface it is resting on. The contact angle is determined by the relative magnitudes of the various forces acting on the drop, including cohesion, adhesion, and gravitational forces.\n",
      "\n",
      "Recent research has focused on developing a detailed understanding of the factors influencing coalescence and the dynamics of the process. Experimental techniques, including high-speed imaging and atomic force microscopy, have been employed to investigate the time-dependent behavior of coalescing sessile drops.\n",
      "\n",
      "One area of particular interest is the effect of surface patterning on coalescence dynamics. For example, micro and nanoscale patterns have been shown to modify the wetting properties of surfaces, leading to changes in the contact angle and coalescence behavior.\n",
      "\n",
      "Overall, the coalescence of sessile drops is a complex process with significant practical applications. Recent research has improved our understanding of the underlying mechanics and has opened up new possibilities for designing and engineering surfaces with tailored properties. Further work in this area is necessary to fully comprehend the dynamics of coalescence and to develop novel approaches to surface engineering. 1 into PostgreSQL...\n",
      "Inserting test sample 383  Recent experiments have demonstrated strong coupling between living bacteria and light. Here we propose a scheme capable of revealing non-classical features of the bacteria (quantum discord of light-bacteria correlations) without exact modelling of the organisms and their interactions with external world. The scheme puts the bacteria in a role of mediators of quantum entanglement between otherwise non-interacting probing light modes. We then propose a plausible model of this experiment, using recently achieved parameters, demonstrating the feasibility of the scheme. Within this model we find that the steady state entanglement between the probes, which does not depend on the initial conditions, is accompanied by entanglement between the probes and bacteria, and provides independent evidence of the strong coupling between them. 0 into PostgreSQL...\n",
      "Inserting test sample 384  Photosynthesis is a fundamental process that enables the conversion of light energy into chemical energy in plants, algae, and bacteria. Over the past few decades, there has been growing evidence suggesting that quantum effects might play a crucial role in the efficiency of photosynthesis. In this work, we explore several quantum features, including coherence, superposition, and entanglement, that might contribute to the remarkable robustness and adaptability of photosynthetic organisms. By combining experimental data and theoretical models, we investigate the extent to which these features are present and relevant in different photosynthetic complexes and under various environmental conditions. Our findings shed light on the quantum underpinnings of one of the most important biological processes and open exciting avenues for biomimetic and energy-based technologies. 1 into PostgreSQL...\n",
      "Inserting test sample 385  We derive a tensorial formula for a fourth-order conformally invariant differential operator on conformal 4-manifolds. This operator is applied to algebraic Weyl tensor densities of a certain conformal weight, and takes its values in algebraic Weyl tensor densities of another weight. For oriented manifolds, this operator reverses duality: For example in the Riemannian case, it takes self-dual to anti-self-dual tensors and vice versa. We also examine the place that this operator occupies in known results on the classification of conformally invariant operators, and we examine some related operators. 0 into PostgreSQL...\n",
      "Inserting test sample 386  We construct a conformally invariant differential operator acting on Weyl tensor densities, which generalizes previously known conformally invariant operators on scalar densities. Our operator possesses several desirable properties, including invariance under conformal transformations, covariance under diffeomorphisms, and compatibility with the Bach tensor. We study its basic properties and derive explicit expressions for its action on arbitrary Weyl tensor densities. Moreover, we establish a composition rule for our operator, which shows that it integrates in a highly nontrivial way with the conformal Laplacian. The new differential operator has potential applications in various branches of theoretical physics. 1 into PostgreSQL...\n",
      "Inserting test sample 387  The charging of dust grains in astrophysical environments has been investigated with the assumption these grains are homogeneous spheres. However, there is evidence which suggests many grains in astrophysical environments are irregularly-shaped aggregates. Recent studies have shown that aggregates acquire higher charge-to-mass ratios due to their complex structures, which in turn may alter their subsequent dynamics and evolution. In this paper, the charging of aggregates is examined including secondary electron emission and photoemission in addition to primary plasma currents. The results show that the equilibrium charge on aggregates can differ markedly from spherical grains with the same mass, but that the charge can be estimated for a given environment based on structural characteristics of the grain. The \"small particle effect\" due to secondary electron emission is also important for determining the charge of micron-sized aggregates consisting of nano-sized particles. 0 into PostgreSQL...\n",
      "Inserting test sample 388  The charging of aggregate grains in astrophysical environments is a crucial process affecting the dynamics and properties of cosmic dust. This work presents a comprehensive study of the charging processes of dust aggregates based on a numerical model. The simulations investigate the effect of different parameters, such as the size and composition of the aggregate, the electron- and ion densities, and the plasma temperature, on the charging behavior of the aggregates. Our results suggest that the charging of aggregates strongly depends on the size and composition of the particles, leading to significant variations in their charge-to-mass ratios. Furthermore, we find that the plasma parameters also play a crucial role in the charging process, with higher electron densities and lower ion densities resulting in a higher degree of charging. These findings shed new light on the charging behavior of dust aggregates, which is of great importance in astrophysical environments. 1 into PostgreSQL...\n",
      "Inserting test sample 389  Eisenbud Popescu and Walter have constructed certain special 4-dimensional sextic hypersurfaces as Lagrangian degeneracy loci. We prove that the natural double cover of a generic EPW-sextic is a deformation of the Hilbert square of a K3-surface and that the family of such varieties is locally complete for deformations that keep the hyperplane class of type (1,1) - thus we get an example similar to that (discovered by Beauville and Donagi) of the Fano variety of lines on a cubic 4-fold. Conversely suppose that X is an irreducible symplectic 4-fold numerically equivalent to the Hilbert square of a K3-surface, that H is an ample divisor on X of square 2 for Beauville's quadratic form and that the map associated to |H| is the composition of the quotient map $X\\to Y$ for an anti-symplectic involution on X followed by an immersion of Y; then Y is an EPW-sextic and $X\\to Y$ is the natural double cover. 0 into PostgreSQL...\n",
      "Inserting test sample 390  In algebraic geometry, the study of irreducible symplectic varieties is a central topic related to their rich geometry. We establish a connection between the moduli space of complex irreducible symplectic 4-folds and Eisenbud-Popescu-Walter sextics, which play an essential role in the classification of algebraic cycles. Our work involves the analysis of the geometry of the sextics at the singular points to study their relation with the 4-folds. Using the derived categories of coherent sheaves, we show that there is a Hodge theoretical equivalence between the two structures, which provides a bridge between the spaces. We also obtain new results regarding the structure of these varieties and their deformation theory, which are critical in the context of mirror symmetry. The techniques and insights presented can be applied to different settings where geometrically interesting varieties are studied. 1 into PostgreSQL...\n",
      "Inserting test sample 391  The amount of high-energy stellar radiation reaching the surface of protoplanetary disks is essential to determine their chemistry and physical evolution. Here, we use millimetric and centimetric radio data to constrain the EUV luminosity impinging on 14 disks around young (~2-10Myr) sun-like stars.\n",
      "\n",
      "For each object we identify the long-wavelength emission in excess to the dust thermal emission, attribute that to free-free disk emission, and thereby compute an upper limit to the EUV reaching the disk. We find upper limits lower than 10$^{42}$ photons/s for all sources without jets and lower than $5 \\times 10^{40}$ photons/s for the three older sources in our sample. These latter values are low for EUV-driven photoevaporation alone to clear out protoplanetary material in the timescale inferred by observations. In addition, our EUV upper limits are too low to reproduce the [NeII] 12.81 micron luminosities from three disks with slow [NeII]-detected winds. This indicates that the [NeII] line in these sources primarily traces a mostly neutral wind where Ne is ionized by 1 keV X-ray photons, implying higher photoevaporative mass loss rates than those predicted by EUV-driven models alone. In summary, our results suggest that high-energy stellar photons other than EUV may dominate the dispersal of protoplanetary disks around sun-like stars. 0 into PostgreSQL...\n",
      "Inserting test sample 392  The extreme ultraviolet (EUV) luminosity of young stars is a crucial factor in determining the characteristics of protoplanetary disks and the resulting planetary systems. In this study, we investigate the effects of low EUV luminosities on protoplanetary disks using detailed radiation-hydrodynamical simulations. We find that lower EUV luminosities can lead to slower accretion rates onto the central star and lower disk masses. Additionally, the disk morphology can be affected, leading to more centrally concentrated dust distribution. We also find that the spectral energy distribution of the system is altered, with significant reductions in the near-UV and far-UV continuum fluxes. These results have important implications for our understanding of planetary formation and the diversity of exoplanetary systems. Our findings suggest that low EUV luminosity stars may be less likely to form giant planets, but may instead form smaller rocky planets. These results have important implications for the interpretation of observations of protoplanetary disks, and for understanding the diversity of exoplanetary systems. Overall, our study highlights the importance of considering EUV radiation in models of protoplanetary disks and planetary formation. 1 into PostgreSQL...\n",
      "Inserting test sample 393  We consider the problem of optimal bidding for virtual trading in two-settlement electricity markets. A virtual trader aims to arbitrage on the differences between day-ahead and real-time market prices; both prices, however, are random and unknown to market participants. An online learning algorithm is proposed to maximize the cumulative payoff over a finite number of trading sessions by allocating the trader's budget among his bids for K options in each session. It is shown that the proposed algorithm converges, with an almost optimal convergence rate, to the global optimal corresponding to the case when the underlying price distribution is known. The proposed algorithm is also generalized for trading strategies with a risk measure. By using both cumulative payoff and Sharpe ratio as performance metrics, evaluations were performed based on historical data spanning ten year period of NYISO and PJM markets. It was shown that the proposed strategy outperforms standard benchmarks and the S&P 500 index over the same period. 0 into PostgreSQL...\n",
      "Inserting test sample 394  This research paper proposes a novel algorithm for bidding in virtual electricity markets. These markets rely on computerized trading platforms to simulate real-world energy trading scenarios. Traders bid for energy based on market conditions, which are determined by supply and demand. The proposed algorithm uses machine learning techniques to adjust bids in real time based on market signals. Specifically, the algorithm utilizes historical market data to predict future price trends and adjusts bids accordingly. The effectiveness of the algorithm is evaluated using a simulated market environment, and the results are compared to existing bidding strategies. The findings show that the proposed algorithm outperforms traditional bidding strategies, resulting in increased profits for traders. The research aims to contribute to the advancement of virtual electricity market trading by providing a more efficient and effective algorithmic bidding strategy. 1 into PostgreSQL...\n",
      "Inserting test sample 395  Abridged: In one widely discussed model for the formation of nuclear star clusters (NSCs), massive globular clusters spiral into the center of a galaxy and merge to form the nucleus. It is now known that at least some NSCs coexist with supermassive black holes (SBHs); this is the case, for instance, in the Milky Way (MW). In this paper, we investigate how the presence of a SMBH at the center of the MW impacts the merger hypothesis for the formation of its NSC.\n",
      "\n",
      "Starting from a model consisting of a low-density nuclear stellar disk and the SMBH, we use N-body simulations to follow the successive inspiral and merger of globular clusters. The clusters are started on circular orbits of radius 20 pc, and their initial masses and radii are set up in such a way as to be consistent with the galactic tidal field at that radius. The total accumulated mass by ~10 clusters is about 1.5x10^7 Solar masses. Each cluster is disrupted by the SMBH at a distance of roughly one parsec. The density profile that results after the final inspiral event is characterized by a core of roughly this radius, and an envelope with density that falls off as 1/r^2. These properties are similar to those of the MW NSC, with the exception of the core size, which in the MW is a little smaller. But by continuing the evolution of the model after the final inspiral event, we find that the core shrinks substantially via gravitational encounters in a time (when scaled to the MW) of 10 Gyr as the stellar distribution evolves toward a Bahcall-Wolf cusp. We also show that the luminosity function of the MW NSC is consistent with the hypothesis that a large fraction of the mass comes from (~10Gyr) old stars, brought in by globular clusters. We conclude that a model in which a large fraction of the mass of the MW NSC arose from infalling globular clusters is consistent with existing observational constraints. 0 into PostgreSQL...\n",
      "Inserting test sample 396  The Milky Way nuclear star cluster is a unique system of stars located at the innermost region of our galaxy. It holds a significant impact on the dynamical evolution of the Milky Way and is a key component in understanding galactic formation and evolution. In this study, we investigate the dissipationless formation and evolution of the Milky Way nuclear star cluster through cosmological simulations.\n",
      "\n",
      "Our simulations considered the effects of dark matter, gas, and stars. We initialized our models with cosmological initial conditions and evolved them using a state-of-the-art numerical scheme that includes realistic treatment of stellar evolution and feedback processes.\n",
      "\n",
      "We find that the Milky Way nuclear star cluster emerges naturally as a result of the concentration of stars from the inner galaxy in the central gravitational potential well. The nuclear star cluster is found to form in the early stages of galaxy formation and experiences a long period of quiet evolution afterward.\n",
      "\n",
      "Our results show that the dissolution of the nuclear star cluster is unlikely to occur in the future of the Milky Way because of the high central concentration of stars and their largely coherent motions. We also find that dynamical friction plays an important role in shaping the final structure of the nuclear star cluster, leading to the formation of a dense, compact core surrounded by a low-density halo.\n",
      "\n",
      "Our findings are in good agreement with the observations of the Milky Way nuclear star cluster. They provide insight into the formation and evolution of galactic nuclei and inform our understanding of the dynamics of galaxies on a cosmological scale.\n",
      "\n",
      "In conclusion, in this study, we have investigated the dissipationless formation and evolution of the Milky Way nuclear star cluster using cosmological simulations. Our results shed light on the inner workings of the Milky Way and provide a unique opportunity to study the dynamics of galactic nuclei. 1 into PostgreSQL...\n",
      "Inserting test sample 397  We revisit the fundamental problem of learning Axis-Aligned-Rectangles over a finite grid $X^d\\subseteq{\\mathbb{R}}^d$ with differential privacy. Existing results show that the sample complexity of this problem is at most $\\min\\left\\{ d{\\cdot}\\log|X| \\;,\\; d^{1.5}{\\cdot}\\left(\\log^*|X| \\right)^{1.5}\\right\\}$.\n",
      "\n",
      "That is, existing constructions either require sample complexity that grows linearly with $\\log|X|$, or else it grows super linearly with the dimension $d$. We present a novel algorithm that reduces the sample complexity to only $\\tilde{O}\\left\\{d{\\cdot}\\left(\\log^*|X|\\right)^{1.5}\\right\\}$, attaining a dimensionality optimal dependency without requiring the sample complexity to grow with $\\log|X|$.The technique used in order to attain this improvement involves the deletion of \"exposed\" data-points on the go, in a fashion designed to avoid the cost of the adaptive composition theorems. The core of this technique may be of individual interest, introducing a new method for constructing statistically-efficient private algorithms. 0 into PostgreSQL...\n",
      "Inserting test sample 398  In this paper, we investigate the sample complexity of privately learning axis-aligned rectangles. Specifically, we explore the tradeoff between the number of samples required for learning and the amount of privacy guaranteed. Our study centers around two models of privacy, namely differential privacy and RÃ©nyi differential privacy. We begin by presenting a lower bound on the sample complexity of private learning of axis-aligned rectangles under differential privacy. Next, we consider a specific class of learning algorithms and obtain an upper bound on their sample complexity under RÃ©nyi differential privacy. We provide a tight characterization of the sample complexity for private learning using RÃ©nyi differential privacy, and demonstrate its superiority over differential privacy under certain conditions. Our results demonstrate the subtle interplay between sample complexity and privacy, and contribute to the ongoing research on privacy-preserving machine learning. 1 into PostgreSQL...\n",
      "Inserting test sample 399  Science students must deal with the errors inherent to all physical measurements and be conscious of the need to expressvthem as a best estimate and a range of uncertainty. Errors are routinely classified as statistical or systematic. Although statistical errors are usually dealt with in the first years of science studies, the typical approaches are based on manually performing repetitive observations. Our work proposes a set of laboratory experiments to teach error and uncertainties based on data recorded with the sensors available in many mobile devices. The main aspects addressed are the physical meaning of the mean value and standard deviation, and the interpretation of histograms and distributions. The normality of the fluctuations is analyzed qualitatively comparing histograms with normal curves and quantitatively comparing the number of observations in intervals to the number expected according to a normal distribution and also performing a Chi-squared test. We show that the distribution usually follows a normal distribution, however, when the sensor is placed on top of a loudspeaker playing a pure tone significant differences with a normal distribution are observed. As applications to every day situations we discuss the intensity of the fluctuations in different situations, such as placing the device on a table or holding it with the hands in different ways. Other activities are focused on the smoothness of a road quantified in terms of the fluctuations registered by the accelerometer. The present proposal contributes to gaining a deep insight into modern technologies and statistical errors and, finally, motivating and encouraging engineering and science students. 0 into PostgreSQL...\n",
      "Inserting test sample 400  As mobile devices increasingly permeate our daily lives, educators are tasked with finding new ways to leverage their ubiquity for educational purposes. In this study, we explore the potential of mobile-device sensors to teach students about error analysis, a fundamental concept in science and engineering. Our approach is grounded in the belief that hands-on, experiential learning is the most effective way to engage students and build their understanding of complex topics.\n",
      "\n",
      "To achieve this goal, we developed a mobile application that uses the sensors in smartphones and tablets to measure physical quantities such as acceleration, rotation, and sound intensity. The app provides a series of experiments in which users collect data and analyze the sources of error in their measurements. Through this process, students learn about the principles of measurement uncertainty, error propagation, and statistical analysis.\n",
      "\n",
      "We conducted a pilot study with a group of high school students and our results show that the app was effective in improving their understanding of error analysis, as measured by pre- and post-test scores. Furthermore, students reported high levels of engagement with the app, and many expressed interest in pursuing further studies in science and engineering.\n",
      "\n",
      "Overall, our findings suggest that mobile-device sensors have significant potential as a tool for science education, particularly in the area of error analysis. With further development and refinement, this approach could help educators engage the next generation of STEM (science, technology, engineering, and mathematics) professionals and equip them with the skills they need to succeed in an increasingly complex and data-driven world. 1 into PostgreSQL...\n",
      "Inserting test sample 401  In this paper we introduce geometric crystals and unipotent crystals which are algebro-geometric analogues of Kashiwara's crystal bases. Given a reductive group G, let I be the set of vertices of the Dynkin diagram of G and T be the maximal torus of G. The structure of a geometric G-crystal on an algebraic variety X consists of a rational morphism \\gamma:X-->T and a compatible family e_i:G_m\\times X-->X, i\\in I of rational actions of the multiplicative group G_m satisfying certain braid-like relations. Such a structure induces a rational action of W on X. Quite surprisingly, many interesting rational actions of the group W come from geometric crystals. Also all the known examples of the action of W which appear in the construction of gamma-functions for the representations of ^LG in the recent work by A. Braverman and D. Kazhdan come from geometric crystals. There are many examples of positive geometric crystals on (G_m)^l, i.e., those geometric crystals for which the actions e_i and the morphism \\gamma are given by positive rational expressions. To each positive geometric crystal X we associate a Kashiwara's crystal corresponding to the Langlands dual group ^LG. An emergence of ^LG in the \"crystal world\" was observed earlier by G. Lusztig. Another application of geometric crystals is a construction of trivialization which is an W-equivariant isomorhism X-->\\gamma^{-1}(e) \\times T for any geometric SL_n-crystal. Unipotent crystals are geometric analogues of normal Kashiwara crystals. They form a strict monoidal category. To any unipotent crystal built on a variety X we associate a certain gometric crystal. 0 into PostgreSQL...\n",
      "Inserting test sample 402  This paper examines the properties and relationships between geometric and unipotent crystals, which are important mathematical objects in the study of algebraic geometry and representation theory. Geometric crystals are typically associated with root systems and play a central role in the theory of quivers, especially in relation to Schur-Weyl duality and the geometric Langlands program. Unipotent crystals, on the other hand, arise naturally in the representation theory of algebraic groups and can be used to study Kazhdan-Lusztig polynomials and the cohomology of flag varieties. Despite their differing origins and applications, it has recently been discovered that there is a deep connection between geometric and unipotent crystals, which is still not fully understood. \n",
      "\n",
      "In this paper, we explore this connection in detail, focusing on the cases where the root system is of type A and type C. We show that there is a canonical bijection between certain subsets of unipotent crystals and certain subcrystals of geometric crystals, providing a new perspective on the relationship between these objects. As an application of this bijection, we give a criterion for a certain crystal to be self-dual, which sheds light on a long-standing conjecture in the theory of crystal bases. \n",
      "\n",
      "Our methods involve a careful analysis of the combinatorics of crystals, making use of various tools such as Littlewood-Richardson coefficients, Weyl groups, and tensor products of representations. We also provide explicit examples to illustrate our results and conjectures for future research. Overall, this paper contributes to a deeper understanding of the structure and interplay of geometric and unipotent crystals, and highlights the rich connections between algebraic geometry, representation theory, and combinatorics. 1 into PostgreSQL...\n",
      "Inserting test sample 403  We present high-precision multi-band photometry for the globular cluster (GC) M2. We combine the analysis of the photometric data obtained from the Hubble Space Telescope UV Legacy Survey of Galactic GCs GO-13297, with chemical abundances by Yong et al.(2014), and compare the photometry with models in order to analyze the multiple stellar sequences we identified in the color-magnitude diagram (CMD). We find three main stellar components, composed of metal-poor, metal-intermediate, and metal-rich stars (hereafter referred to as population A, B, and C, respectively). The components A and B include stars with different $s$-process element abundances. They host six sub-populations with different light-element abundances, and exhibit an internal variation in helium up to Delta Y~0.07 dex. In contrast with M22, another cluster characterized by the presence of populations with different metallicities, M2 contains a third stellar component, C, which shows neither evidence for sub-populations nor an internal spread in light-elements. Population C does not exhibit the typical photometric signatures that are associated with abundance variations of light elements produced by hydrogen burning at hot temperatures.\n",
      "\n",
      "We compare M2 with other GCs with intrinsic heavy-element variations and conclude that M2 resembles M22, but it includes an additional stellar component that makes it more similar to the central region of the Sagittarius galaxy, which hosts a GC (M54) and the nucleus of the Sagittarius galaxy itself. 0 into PostgreSQL...\n",
      "Inserting test sample 404  The Hubble Space Telescope UV Legacy Survey of Galactic Globular Clusters has proven to be a valuable resource for researchers. In this paper, we present an analysis of NGC7089 (M2) using this survey. We find that this cluster is composed of seven distinct stellar populations, each with unique characteristics. These populations can be differentiated based on their magnesium and calcium content, as well as their position on the horizontal branch. Our analysis suggests that NGC7089 (M2) has a complex star formation history, with multiple episodes of star formation occurring over a period of billions of years. The properties of these different populations provide insight into the formation and evolution of globular clusters, and may help to shed light on the early formation of our galaxy. Additionally, we compare our results to other well-studied globular clusters and find that NGC7089 (M2) is a unique object in several respects. Overall, our study highlights the power of the Hubble Space Telescope UV Legacy Survey in helping us to understand the complex nature of globular clusters and their role in galaxy formation. 1 into PostgreSQL...\n",
      "Inserting test sample 405  The angular size of a star is a critical factor in determining its basic properties. Direct measurement of stellar angular diameters is difficult: at interstellar distances stars are generally too small to resolve by any individual imaging telescope. This fundamental limitation can be overcome by studying the diffraction pattern in the shadow cast when an asteroid occults a star, but only when the photometric uncertainty is smaller than the noise added by atmospheric scintillation. Atmospheric Cherenkov telescopes used for particle astrophysics observations have not generally been exploited for optical astronomy due to the modest optical quality of the mirror surface.\n",
      "\n",
      "However, their large mirror area makes them well suited for such high-time-resolution precision photometry measurements. Here we report two occultations of stars observed by the VERITAS Cherenkov telescopes with millisecond sampling, from which we are able to provide a direct measurement of the occulted stars' angular diameter at the $\\leq0.1$ milliarcsecond scale.\n",
      "\n",
      "This is a resolution never achieved before with optical measurements and represents an order of magnitude improvement over the equivalent lunar occultation method. We compare the resulting stellar radius with empirically derived estimates from temperature and brightness measurements, confirming the latter can be biased for stars with ambiguous stellar classifications. 0 into PostgreSQL...\n",
      "Inserting test sample 406  This research paper describes measurements of the angular diameters of five stars using the VERITAS (Very Energetic Radiation Imaging Telescope Array System) Cherenkov telescopes. The technique of differential tacking was used, where the stars were tracked simultaneously with a nearby reference star. Observed images of the stars were then precisely analyzed for changes in shape due to their apparent size. The angular diameters measured range from 0.24 to 0.74 milliarcseconds - the smallest diameters ever measured directly. The results are consistent with previous estimates for three of the stars, yet for two stars, the measured diameters are smaller than previous estimates. The observations demonstrate the power of the Cherenkov technique for making accurate, sub-milliarcsecond measurements of stellar angular sizes, and support the use of Cherenkov telescopes in future astronomical studies. Potential applications include the investigation of the structure and composition of stars, as well as the evaluation of current models of stellar evolution. Further exploration and refinement of this technique will likely lead to even more precise measurements of a wider variety of astronomical objects. 1 into PostgreSQL...\n",
      "Inserting test sample 407  Studies of $e^+e^- \\to D_s^+ \\overline{D}{}^{(*)0}K^-$ and the $P$-wave charmed-strange mesons are performed based on an $e^+e^-$ collision data sample corresponding to an integrated luminosity of 567 pb$^{-1}$ collected with the BESIII detector at $\\sqrt{s}= 4.600$ GeV. The processes of $e^+e^-\\to D_s^+ \\overline{D}{*}^{0} K^-$ and $D_s^+ \\overline{D}{}^{0} K^-$ are observed for the first time and are found to be dominated by the modes $D_s^+ D_{s1}(2536)^-$ and $D_s^+ D^*_{s2}(2573)^-$, respectively. The Born cross sections are measured to be $\\sigma^{B}(e^+e^-\\to D_s^+ \\overline{D}{*}^{0} K^-) = (10.1\\pm2.3\\pm0.8) pb$ and $\\sigma^{B}(e^+e^-\\to D_s^+ \\overline{D}{}^{0} K^-) = (19.4\\pm2.3\\pm1.6) pb$, and the products of Born cross section and the decay branching fraction are measured to be $\\sigma^{B}(e^+e^-\\to D_s^+D_{s1}(2536)^- + c.c.)\\cdot\\mathcal{B}(D_{s1}(2536)^- \\to \\overline{D}{*}^{0} K^-) = (7.5 \\pm 1.8 \\pm 0.7) pb$ and $\\sigma^{B}(e^+e^-\\to D_s^+ D^*_{s2}(2573)^- + c.c.)\\cdot\\mathcal{B}(D^*_{s2}(2573)^- \\to \\overline{D}{}^{0} K^-) = (19.7 \\pm 2.9 \\pm 2.0) pb$. For the $D_{s1}(2536)^-$ and $D^*_{s2}(2573)^-$ mesons, the masses and widths are measured to be $M(D_{s1}(2536)^-) = (2537.7 \\pm 0.5 \\pm 3.1)~MeV/c^2,$ $ \\Gamma(D_{s1}(2536)^-)) = (1.7\\pm 1.2 \\pm 0.6)~\\rm MeV,$ and $M(D^*_{s2}(2573)^-) = (2570.7\\pm 2.0 \\pm 1.7)~MeV/c^2,$ $\\Gamma(D^*_{s2}(2573)^-) = (17.2 \\pm 3.6 \\pm 1.1)~\\rm MeV.$ The spin-parity of the $D^*_{s2}(2573)^-$ meson is determined to be $J^P=2^{+}$. In addition, the process $e^+e^-\\to D_s^+ \\overline{D}{}^{(*)0} K^-$ are searched for using the data samples taken at four (two) center-of-mass energies between 4.416 (4.527) and 4.575 GeV, and upper limits at the $90\\%$ confidence level on the cross sections are determined. 0 into PostgreSQL...\n",
      "Inserting test sample 408  This paper reports on the observation of the process $e^+e^- \\rightarrow D_s^+ \\overline{D}{}^{(*)0} K^-$ and consequent exploration of the $P$-wave $D_s$ mesons. This work is accomplished using data collected with the Belle detector at the KEKB asymmetric-energy $e^+e^-$ collider. Evidence of the $D_s^+ \\overline{D}{}^{(*)0}$ signal is observed and used to determine the resonance parameters of the $D_{s0}^*(2317)^+$. Subsequently, the branching fractions of $e^+e^- \\rightarrow D_s^+ D_{s1}^+(2536) K^-$ are also calculated, revealing the presence of $D_{s1}^+(2536)$. Additionally, an upper limit is set to the process $e^+e^- \\rightarrow D_s^+ D_{sJ}^+ (2573) K^-$ at the 90\\% confidence level. Furthermore, the $P$-wave $D_s$ system is parameterized with the FlattÃ© formalism, and a comprehensive study of the resonant substructures observed in $D_s^+ \\rightarrow \\pi^+\\pi^-\\pi^+$ is conducted. The observed substructures are found to be described well by two isotensor resonances. This is consistent with the previous experimental results but with an improved statistical significance and precision. Additionally, the $D_s^+ \\pi^+$ mass spectrum is presented, and two new resonances, namely $D_{s1}(2700)^+$ and $D_{s3}(2860)^+$, are observed. The observed resonances are interpreted as the $j=1$ and $j=3$ excited states of the $D_s$ meson, respectively.\n",
      "\n",
      "To conclude, this work presents an observation of the process $e^+e^- \\rightarrow D_s^+ \\overline{D}{}^{(*)0} K^-$ and explores the $P$-wave $D_s$ mesons. It also presents a comprehensive study of the resonant substructures observed in the $D_s^+ \\rightarrow \\pi^+\\pi^-\\pi^+$ decays, parameterizing the $P$-wave $D_s$ system with the FlattÃ© formalism. Two new resonances are also observed in the $D_s^+ \\rightarrow \\pi^+\\pi^-\\pi^+$ and $D_s^+ \\pi^+$ mass spectra and interpreted as excited states of the $D_s$ meson. 1 into PostgreSQL...\n",
      "Inserting test sample 409  We consider a wireless device-to-device (D2D) cooperative network where memory-endowed nodes store and exchange content. Each node generates random file requests following an unknown and possibly arbitrary spatio-temporal process, and a base station (BS) delivers any file that is not found at its neighbors' cache, at the expense of higher cost. We design an online learning algorithm which minimizes the aggregate delivery cost by assisting each node to decide which files to cache and which files to fetch from the BS and other devices. Our policy relies on the online gradient descent algorithm, is amenable to distributed execution, and achieves asymptotically optimal performance for any request pattern, without prior information. 0 into PostgreSQL...\n",
      "Inserting test sample 410  Cooperation among devices has become a key factor in improving the performance of wireless networks, especially in the context of device-to-device (D2D) caching. In this study, we propose a novel approach to improve D2D caching networks by introducing an active learning framework. Our proposed framework aims to optimize cooperation among devices by selecting the best policies for individual agents that coordinate with one another. Through extensive simulations, we show that our proposed approach significantly outperforms other conventional methods in terms of communication efficiency and average reward. Our results demonstrate the potential of this method to improve cooperation among agents and enhance the overall performance of D2D caching networks. 1 into PostgreSQL...\n",
      "Inserting test sample 411  The eruptive cycles of dwarf novae (DN) are thought to be due to a thermal-viscous instability in the accretion disk surrounding the white dwarf (WD). This model has long been known to imply a stress to pressure ratio \\alpha ~0.1 in outburst compared to \\alpha ~ 0.01 in quiescence. Such an enhancement in $\\alpha$ has recently been observed in simulations of turbulent transport driven by the magneto-rotational instability (MRI) when convection is present, without requiring a net magnetic flux. We independently recover this result by carrying out PLUTO MHD simulations of vertically stratified, radiative, shearing boxes with the thermodynamics and opacities appropriate to DN. The results are robust against the choice of vertical boundary conditions. The thermal equilibrium solutions found by the simulations trace the well-known S-curve in the density-temperature plane. We confirm that the high values of \\alpha ~ 0.1 occur near the tip of the hot branch of the S-curve, where convection is active. However, we also present thermally-stable simulations at lower temperatures that have standard values of \\alpha ~ 0.03 despite the presence of vigorous convection. We find no simple relationship between \\alpha and the strength of the convection, as measured by the ratio of convective to radiative flux. The cold branch is only very weakly ionized so, in the second part of this work, we studied the impact of non-ideal MHD effects on transport.\n",
      "\n",
      "We include resistivity in the simulations and find that the MRI-driven transport is quenched (\\alpha ~ 0) below the critical density at which the magnetic Reynolds number R_m \\leq 10^4. This is problematic as X-ray emission observed in quiescent systems requires ongoing accretion onto the WD. We verify that these X-rays cannot self-sustain MRI-driven turbulence by photo-ionizing the disk and discuss possible solutions to the issue of accretion in quiescence. 0 into PostgreSQL...\n",
      "Inserting test sample 412  Angular momentum transport in dwarf novae is a topic of much interest in astrophysics. In this paper, we investigate how convection and resistivity affect this transport phenomenon. We present a detailed theoretical model using the Shakura-Sunyaev equations that incorporates both convection and resistivity as key factors. Through simulations, we find that convection plays a significant role in transporting angular momentum, particularly in the outermost layers of the accretion disk. Our findings suggest that convection can lead to significant angular momentum transport, even in the absence of turbulence. Additionally, we find that resistivity can significantly impact the transport of angular momentum through the disk. In particular, we find that resistivity leads to a reduction in the angular momentum flux density and an increase in the radial velocity of the disk. This effect is most pronounced in the innermost layers of the disk. \n",
      "\n",
      "Furthermore, we study the impact of convection and resistivity on the observed properties of dwarf novae outbursts. Our results show that the presence of convection and resistivity can lead to significant differences in the outburst characteristics, such as the duration and magnitude of the outburst. Finally, we compare our theoretical model with observational data of dwarf novae and find that our model is largely consistent with the available observations. However, we also note a few discrepancies, which may be due to the simplifications in our model. \n",
      "\n",
      "Overall, our study highlights the importance of considering convection and resistivity in our models of angular momentum transport in dwarf novae. Furthermore, our study provides a useful framework for understanding the relationship between these transport phenomena and the observed properties of dwarf novae outbursts. Our findings have important implications for our understanding of accretion disks and the evolution of compact binary systems. 1 into PostgreSQL...\n",
      "Inserting test sample 413  Many problems in nonlinear analysis and optimization, among them variational inequalities and minimization of convex functions, can be reduced to finding zeros (namely, roots) of set-valued operators. Hence numerous algorithms have been devised in order to achieve this task. A lot of these algorithms are inexact in the sense that they allow perturbations to appear during the iterative process, and hence they enable one to better deal with noise and computational errors, as well as superiorization. For many years a certain fundamental question has remained open regarding many of these known inexact algorithmic schemes in various finite and infinite dimensional settings, namely whether there exist sequences satisfying these inexact schemes when errors appear. We provide a positive answer to this question. Our results also show that various theorems discussing the convergence of these inexact schemes have a genuine merit beyond the exact case. As a by-product we solve the standard and the strongly implicit inexact resolvent inclusion problems, introduce a promising class of functions (fully Legendre functions), establish continuous dependence (stability) properties of the solution of the inexact resolvent inclusion problem and continuity properties of the protoresolvent, and generalize the notion of strong monotonicity. 0 into PostgreSQL...\n",
      "Inserting test sample 414  Inexact resolvent inclusion problems arise frequently in nonlinear analysis and optimization, and efficient solutions for these problems are imperative for numerical methods to guarantee convergence. In this paper, we propose a unified framework for solving inexact resolvent inclusion problems based on the concepts of metric subregularity and prox-regularity. Under this framework, we develop several algorithms that solve the inexact resolvent inclusion problems, with applications to various nonlinear analysis and optimization problems. Our algorithms are proven to be convergent under mild assumptions, and numerical experiments demonstrate their effectiveness and efficiency. In particular, we present an application of our framework to the computation of Godunov's viscosity solutions of Hamilton-Jacobi equations. Our framework allows for the use of various types of inexact information, which makes it highly versatile in practical computational settings. Overall, this paper presents a novel and promising approach for the solution of inexact resolvent inclusion problems, and highlights its usefulness in nonlinear analysis and optimization. 1 into PostgreSQL...\n",
      "Inserting test sample 415  Despite progress in quantum Hamiltonian complexity, little is known about the computational complexity of quantum physics at the thermodynamic limit. Even defining the problem is not straight forward. We study the complexity of estimating the ground energy of a fixed, translationally invariant Hamiltonian in the thermodynamic limit, to within a given precision; the number of bits $n$ for the precision is the sole input to the problem. The complexity of this problem captures how difficult it is for the physicist to measure or compute another digit in the approximation of a physical quantity in the thermodynamic limit. We show that this problem is contained in $\\mbox{FEXP}^{\\mbox{QMA-EXP}}$ and is hard for $\\mbox{FEXP}^{\\mbox{NEXP}}$. This means that the problem is doubly exponentially hard in the size of the input. As an ingredient in our construction, we study the problem of computing the ground energy of translationally invariant finite 1D chains. A single Hamiltonian term, which is a fixed parameter of the problem, is applied to every pair of particles in a finite chain. The length of the chain is the sole input to the problem and the task is to compute an approximation of the ground energy. No thresholds are provided as in the standard formulation of the local Hamiltonian problem. We show that this problem is contained in $\\mbox{FP}^{\\mbox{QMA-EXP}}$ and is hard for $\\mbox{FP}^{\\mbox{NEXP}}$. Our techniques employ a circular clock in which the ground energy is calibrated by the length of the cycle. This requires more precise expressions for the ground states of the resulting matrices than was required for previous QMA-completeness constructions and even exact analytical bounds for the infinite case which we derive using techniques from spectral graph theory. To our knowledge, this is the first use of the circuit-to-Hamiltonian construction which shows hardness for a function class. 0 into PostgreSQL...\n",
      "Inserting test sample 416  The study of Hamiltonian complexity has recently extended into the realm of statistical physics, posing new challenges and opportunities for investigating fundamental properties of physical systems. In this work, we investigate the behavior of Hamiltonian complexity in the thermodynamic limit, where the number of particles in a system becomes infinitely large. Our main result is a proof that the thermodynamic limit sets a fundamental bound on the computational complexity of simulating quantum systems with local Hamiltonians. Specifically, we show that the computational cost of simulating a locally interacting quantum system scales exponentially with the volume of the system in the thermodynamic limit. This result has far-reaching implications for our understanding of the computational power of physical systems, and provides a rigorous theoretical foundation for the study of complex quantum systems.\n",
      "\n",
      "Our analysis builds heavily on recent advances in the theory of quantum information and computational complexity, in particular the connection between complexity and entanglement. We provide a clear and intuitive explanation of this connection, and demonstrate how it can be used to yield sharp lower bounds on the computational complexity of simulating quantum systems.\n",
      "\n",
      "In addition to our main result, we also investigate the implications of our findings for the design and optimization of quantum algorithms. We show that our results provide a natural framework for understanding the limits of quantum computational power, and suggest new directions for future research.\n",
      "\n",
      "Overall, our work represents a significant advance in our understanding of the complex interplay between Hamiltonian complexity and the thermodynamic limit, with important implications for the fundamental limits of computational power in physical systems. Our results provide a rigorous theoretical foundation for the study of complex quantum systems, and suggest new avenues for exploring the frontiers of quantum information and computational complexity. 1 into PostgreSQL...\n",
      "Inserting test sample 417  The optomechanical coupling of quantum dots and flexural mechanical modes is studied in suspended nanophononic strings. The investigated devices are designed and monolithically fabricated on an (Al)GaAs heterostructure. Radio frequency elastic waves with frequencies ranging between $f$=250 MHz to 400 MHz are generated as Rayleigh surface acoustic waves on the unpatterned substrate and injected as Lamb waves in the nanophononic string. Quantum dots inside the nanophononic string exhibit a 15-fold enhanced optomechanical modulation compared to those dynamically strained by the Rayleigh surface acoustic wave.\n",
      "\n",
      "Detailed finite element simulations of the phononic mode spectrum of the nanophononic string confirm, that the observed modulation arises from valence band deformation potential coupling via shear strain. The corresponding optomechanical coupling parameter is quantified to $0.15 \\mathrm{meV nm^{-1}}$.\n",
      "\n",
      "This value exceeds that reported for vibrating nanorods by approximately one order of magnitude at 100 times higher frequencies. Using this value, a derive vertical displacements in the range of 10 nm is deduced from the experimentally observed modulation. The results represent an important step towards the creation of large scale optomechanical circuits interfacing single optically active quantum dots with optical and mechanical waves. 0 into PostgreSQL...\n",
      "Inserting test sample 418  This paper investigates the optomechanics of suspended nanophononic strings based on quantum dots. The study explores the coupling of mechanical vibrations of nanophononic strings with light confined in quantum dots. The proposed platform enables the manipulation and detection of mechanical motion down to the quantum level. The work examines the dependence of this optomechanical system on various parameters, including the size of the quantum dot, the geometry of the nanophononic string, and the intrinsic properties of the materials. The approach can lead to the development of novel quantum sensors and quantum information processing devices. The proposed nanophononic strings come with ultra-high mechanical quality factors and frequency ranges that can be tailored according to specific applications. The study includes experimental demonstration and theoretical modeling of the electric and mechanical properties of the system. Along with the numerical simulations, the work provides a comprehensive understanding of the fundamental aspects of quantum dot optomechanics in suspended nanophononic strings, offering a path towards the implementation of these systems in real-world applications. 1 into PostgreSQL...\n",
      "Inserting test sample 419  The groundbreaking detection of gravitational waves produced by the inspiralling and coalescence of the black hole (BH) binary GW150914 confirms the existence of \"heavy\" stellar-mass BHs with masses >25 Msun. Initial modelling of the system by Abbott et al. (2016a) supposes that the formation of black holes with such large masses from the evolution of single massive stars is only feasible if the wind mass-loss rates of the progenitors were greatly reduced relative to the mass-loss rates of massive stars in the Galaxy, concluding that heavy BHs must form in low-metallicity (Z < 0.25-0.5 Zsun) environments. However, strong surface magnetic fields also provide a powerful mechanism for modifying mass loss and rotation of massive stars, independent of environmental metallicity (ud-Doula & Owocki 2002; ud-Doula et al. 2008). In this paper we explore the hypothesis that some heavy BHs, with masses >25 Msun such as those inferred to compose GW150914, could be the natural end-point of evolution of magnetic massive stars in a solar-metallicity environment. Using the MESA code, we developed a new grid of single, non-rotating, solar metallicity evolutionary models for initial ZAMS masses from 40-80 Msun that include, for the first time, the quenching of the mass loss due to a realistic dipolar surface magnetic field. The new models predict TAMS masses that are significantly greater than those from equivalent non-magnetic models, reducing the total mass lost by a strongly magnetized 80 Msun star during its main sequence evolution by 20 Msun. This corresponds approximately to the mass loss reduction expected from an environment with metallicity Z = 1/30 Zsun. 0 into PostgreSQL...\n",
      "Inserting test sample 420  Massive stars are known to undergo a supernova explosion at the end of their lives, leaving behind either a neutron star or a black hole. Recently, it has been suggested that a specific class of massive stars, those with strong magnetic fields, may be more prone to forming \"heavy\" stellar-mass black holes. In this paper, we explore this hypothesis through a combination of theoretical modeling and observational data analysis.\n",
      "\n",
      "We begin by discussing the evolution of massive stars, focusing on the role of magnetic fields in shaping their inner structure and eventual fate. We then present our theoretical models for the formation of magnetic massive stars and their subsequent collapse into black holes. These models take into account a range of physical processes, including magnetohydrodynamics, turbulence, and nuclear reactions.\n",
      "\n",
      "Our models predict that magnetic massive stars are more likely to form black holes with masses exceeding 30 times that of the sun, which we refer to as \"heavy\" stellar-mass black holes. We compare these predictions to observations of black holes in binary systems, finding strong evidence that some of these black holes may indeed have formed from magnetic massive stars.\n",
      "\n",
      "Overall, our results suggest that magnetic fields play a crucial role in the formation of \"heavy\" stellar-mass black holes, offering new insights into the astrophysical processes that drive the evolution of massive stars. We conclude by discussing avenues for future research, including the need for more detailed observational data and refined theoretical models. 1 into PostgreSQL...\n",
      "Inserting test sample 421  Source localization in EEG represents a high dimensional inverse problem, which is severely ill-posed by nature. Fortunately, sparsity constraints have come into rescue as it helps solving the ill-posed problems when the signal is sparse. When the signal has a structure such as block structure, consideration of block sparsity produces better results. Knowing sparse Bayesian learning is an important member in the family of sparse recovery, and a superior choice when the projection matrix is highly coherent (which is typical the case for EEG), in this work we evaluate the performance of block sparse Bayesian learning (BSBL) method for EEG source localization. It is already accepted by the EEG community that a group of dipoles rather than a single dipole are activated during brain activities; thus, block structure is a reasonable choice for EEG. In this work we use two definitions of blocks: Brodmann areas and automated anatomical labelling (AAL), and analyze the reconstruction performance of BSBL methodology for them. A realistic head model is used for the experiment, which was obtained from segmentation of MRI images. When the number of simultaneously active blocks is 2, the BSBL produces overall localization accuracy of less than 5 mm without the presence of noise. The presence of more than 3 simultaneously active blocks and noise significantly affect the localization performance. Consideration of AAL based blocks results more accurate source localization in comparison to Brodmann area based blocks. 0 into PostgreSQL...\n",
      "Inserting test sample 422  The aim of this study is to evaluate the performance of the Brainstorm Bayesian Source Beamforming (BSBL) methodology for EEG source localization on a realistic head model. The BSBL method has shown promise in accurately localizing brain sources within EEG signals by modeling the electrical activity as a set of distributed current dipoles using a Bayesian framework. However, further validation is required before the BSBL method can be considered a viable approach in a clinical setting. In this study, we used a high-resolution head model and a simulation pipeline to generate realistic EEG data with known source locations. We then compared the accuracy of the BSBL method with two commonly used methods for EEG source localization: the MNE (Minimum Norm Estimation) and sLORETA (standardized Low Resolution Electromagnetic Tomography) methods. Our results demonstrated that the BSBL method outperformed both MNE and sLORETA methods in terms of spatial accuracy and robustness to noise. These findings suggest that the BSBL method shows great potential for future applications in EEG source localization, particularly in complex clinical cases where accuracy is crucial for determining the location of brain sources and optimizing treatment strategies. Overall, this study adds to the growing body of evidence supporting the use of BSBL methodology for EEG source localization on a realistic head model. 1 into PostgreSQL...\n",
      "Inserting test sample 423  The problem of finding maximum (or minimum) witnesses of the Boolean product of two Boolean matrices (MW for short) has a number of important applications, in particular the all-pairs lowest common ancestor (LCA) problem in directed acyclic graphs (dags). The best known upper time-bound on the MW problem for n\\times n Boolean matrices of the form O(n^{2.575}) has not been substantially improved since 2006. In order to obtain faster algorithms for this problem, we study quantum algorithms for MW and approximation algorithms for MW (in the standard computational model). Some of our quantum algorithms are input or output sensitive. Our fastest quantum algorithm for the MW problem, and consequently for the related problems, runs in time \\tilde{O}(n^{2+\\lambda/2})=\\tilde{O}(n^{2.434}), where \\lambda satisfies the equation \\omega(1, \\lambda, 1) = 1 + 1.5 \\, \\lambda and \\omega(1, \\lambda, 1) is the exponent of the multiplication of an n \\times n^{\\lambda}$ matrix by an n^{\\lambda} \\times n matrix. Next, we consider a relaxed version of the MW problem (in the standard model) asking for reporting a witness of bounded rank (the maximum witness has rank 1) for each non-zero entry of the matrix product.\n",
      "\n",
      "First, by adapting the fastest known algorithm for maximum witnesses, we obtain an algorithm for the relaxed problem that reports for each non-zero entry of the product matrix a witness of rank at most \\ell in time \\tilde{O}((n/\\ell)n^{\\omega(1,\\log_n \\ell,1)}). Then, by reducing the relaxed problem to the so called k-witness problem, we provide an algorithm that reports for each non-zero entry C[i,j] of the product matrix C a witness of rank O(\\lceil W_C(i,j)/k\\rceil ), where W_C(i,j) is the number of witnesses for C[i,j], with high probability. The algorithm runs in \\tilde{O}(n^{\\omega}k^{0.4653} +n^2k) time, where \\omega=\\omega(1,1,1). 0 into PostgreSQL...\n",
      "Inserting test sample 424  This paper presents an examination of quantum and approximation algorithms for determining the maximum witnesses of Boolean matrix products. We explore the mathematical underpinnings of this problem, focusing particularly on its relevance to computational complexity theory and algorithmic design.\n",
      "\n",
      "Our study begins by outlining the key concepts involved in the representation of Boolean matrices, as well as the fundamental principles that inform their multiplication. We then delve into the specifics of the maximum witnesses problem, considering both its formal definition and its relevance to a range of practical applications in fields such as signal processing and machine learning.\n",
      "\n",
      "The focus of the first part of our investigation is on quantum algorithms for maximum witness determination. We evaluate the relative effectiveness of several well-known approaches, assessing their strengths and weaknesses in terms of both theoretical performance and real-world implementation. We also consider the potential implications of recent advances in quantum computing hardware for the development of more efficient algorithms.\n",
      "\n",
      "The second part of our analysis deals with approximation algorithms for maximum witness computation. We explore in detail a variety of strategies, ranging from classic methods such as randomized rounding to more recent techniques developed specifically for this problem. Again, we evaluate the advantages and disadvantages of each, considering factors such as computational complexity, accuracy, and robustness.\n",
      "\n",
      "Overall, our results suggest that both quantum and approximation algorithms have important roles to play in the solution of the maximum witnesses problem. While there is no single approach that is uniformly superior across all scenarios, the insights provided by our investigation can enable researchers and practitioners to make more informed decisions when selecting an algorithm for their particular application. We hope that this paper stimulates further research along these lines, and contributes to the development of ever-more-sophisticated techniques for Boolean matrix products. 1 into PostgreSQL...\n",
      "Inserting test sample 425  After discussing the concept of DUKPT based symmetric encryption key management (e.g., for 3DES) and definition of cloud or remote wallet, the paper analyses applicability of DUKPT to different use cases like mobile banking, NFC payment using EMV contactless card and mobile based EMV card emulation, web browser based transaction and cloud or remote wallet.\n",
      "\n",
      "Cloud wallet is an emerging payment method and is gaining momentum very fast.\n",
      "\n",
      "Anticipating that the wallet product managers and security specialists may face these questions from different stakeholders, the authors have addressed applicability of DUKPT to cloud wallet use case quite elaborately. As per knowledge of the authors, this topic has been analysed and discussed for the first time. 0 into PostgreSQL...\n",
      "Inserting test sample 426  This research explores the applicability of the Derived Unique Key Per Transaction (DUKPT) key management scheme to mobile payments, specifically cloud wallets. DUKPT provides a secure means of key management and can offer benefits such as reduced fraud and improved encryption. The suitability of DUKPT for mobile payments is evaluated through a comparative analysis of existing encryption methods used in mobile payments. The results show that DUKPT is a promising approach to mobile payment security, offering numerous advantages over current methods. However, implementing DUKPT presents challenges, including the need for hardware and software updates. The study concludes by recommending further investigation into the feasibility of DUKPT adoption in mobile payment systems. 1 into PostgreSQL...\n",
      "Inserting test sample 427  We describe the long-term dynamics of sustained stratified shear flows in the laboratory. The Stratified Inclined Duct (SID) experiment sets up a two-layer exchange flow in an inclined duct connecting two reservoirs containing salt solutions of different densities. This flow is primarily characterised by two non-dimensional parameters: the tilt angle of the duct with respect to the horizontal, $\\theta$ (a few degrees at most), and the Reynolds number $Re$, an input parameter based on the density difference driving the flow. The flow can be sustained with constant forcing over arbitrarily long times and exhibits a wealth of dynamical behaviours representative of geophysically-relevant sustained stratified shear flows. Varying $\\theta$ and $Re$ leads to four qualitatively different regimes: laminar flow; mostly laminar flow with finite-amplitude, travelling Holmboe waves; spatio-temporally intermittent turbulence with substantial interfacial mixing; and sustained, vigorous interfacial turbulence (Meyer & Linden, J. Fluid Mech., vol. 753, 2014, pp.\n",
      "\n",
      "242-253). We seek to explain the scaling of the transitions between flow regimes in the two-dimensional plane of input parameters $(\\theta, Re)$. We improve upon previous studies of this problem by providing a firm physical basis and non-dimensional scaling laws that are mutually consistent and in good agreement with the empirical transition curves we inferred from 360 experiments spanning $\\theta \\in [-1^\\circ, 6^\\circ]$ and $Re \\in [300, 5000]$. To do so, we employ state-of-the-art simultaneous volumetric measurements of the density field and the three-component velocity field, and analyse these experimental data using time- and volume-averaged potential and kinetic energy budgets. We show that regime transitions are caused by ... 0 into PostgreSQL...\n",
      "Inserting test sample 428  This paper presents a study on the energetics of sustained stratified shear flows during regime transitions. We investigate the behavior of such flows using direct numerical simulations that employ a spectral element method to solve the Navier-Stokes equations in a stably stratified fluid. Our simulations reveal that the flow transitions from a laminar state to a regime featuring large-scale, quasi-two-dimensional vortices known as panels/bands, then finally to a state characterized by multiple, interacting panels. We analyze the energetics of the flow during these transitions and find that the onset of the panels significantly alters the energy distribution within the flow. Specifically, we identify an increase in potential energy associated with the formation of the panels, which is consistent with earlier experimental and theoretical studies. We further explore the persistence of this increase in potential energy to understand its role in the transition to a state characterized by interacting panels. Our results suggest that the increase in potential energy is tied to the formation of persistent near-surface structures which are believed to play an important role in the formation of the large-scale panels. Overall, our findings improve our understanding of the energetics of stratified shear flows and provide valuable insights into the mechanics of regime transitions in such flows. 1 into PostgreSQL...\n",
      "Inserting test sample 429  We present a multi-wavelength study of seven AGN at spectroscopic redshift >2.5 in the 7 Ms Chandra Deep Field South, selected to have good FIR/sub-mm detections. Our aim is to investigate the possibility that the obscuration observed in the X-rays can be produced by the interstellar medium (ISM) of the host galaxy. Based on the 7 Ms Chandra spectra, we measured obscuring column densities N$_{H, X}$ in excess of 7x10$^{22}$ cm$^{-2}$ and intrinsic X-ray luminosities L$_{X}$>10$^{44}$ erg s$^{-1}$ for our targets, as well as equivalent widths for the Fe K$\\alpha$ emission line EW>0.5-1 keV. We built the UV-to-FIR spectral energy distributions by using broad-band photometry from CANDELS and Herschel catalogs. By means of an SED decomposition technique, we derived stellar masses (M$_{*}$~10$^{11}$ Msun), IR luminosities (L$_{IR}$>10$^{12}$ Lsun), star formation rates (SFR~190-1680 Msun yr$^{-1}$) and AGN bolometric luminosities (L$_{bol}$~10$^{46}$ erg s$^{-1}$) for our sample. We used an empirically-calibrated relation between gas masses and FIR/sub-mm luminosities and derived M$_{gas}$~0.8-5.4x10$^{10}$ Msun.\n",
      "\n",
      "High-resolution (0.3-0.7'') ALMA data (when available, CANDELS data otherwise) were used to estimate the galaxy size and hence the volume enclosing most of the ISM under simple geometrical assumptions. These measurements were then combined to derive the column density associated with the ISM of the host, on the order of N$_{H, ISM}$~10$^{23-24}$ cm$^{-2}$. The comparison between the ISM column densities and those measured from the X-ray spectral analysis shows that they are similar. This suggests that, at least at high redshift, significant absorption on kpc scales by the dense ISM in the host likely adds to or substitutes that produced by circumnuclear gas on pc scales (i.e., the torus of unified models). The lack of unobscured AGN among our ISM-rich targets supports this scenario. 0 into PostgreSQL...\n",
      "Inserting test sample 430  The study of X-ray emitting active galactic nuclei (AGN) at high redshifts has proven to be a challenging field. Observations have shown that AGN with redshifts greater than z>2.5 are often associated with their host galaxies, which can obscure their X-ray emission. The origin of this obscuration is still not fully understood, but several factors have been proposed, including dust, gas, and the geometry of the AGN environment.\n",
      "\n",
      "In this study, we investigate the impact of host galaxies on the X-ray emission of AGN with z>2.5. We selected a sample of AGN with high-quality X-ray and optical data from the Chandra and Hubble Space Telescope archives, respectively. We applied a variety of analysis techniques to our sample, including spectral fitting, photometry, and modeling of the host galaxy properties.\n",
      "\n",
      "Our results reveal that the majority of our AGN sample show evidence of obscured X-ray emission. Specifically, we find that the X-ray luminosity of AGN is significantly suppressed in galaxies with high levels of star formation and high stellar mass. Furthermore, we find that the obscuration is highly dependent on the intrinsic X-ray luminosity of the AGN, with higher luminosity AGN being more heavily obscured.\n",
      "\n",
      "We explored several possible scenarios for the origin of this obscuration, and find that dust and gas in the host galaxy are likely responsible for the attenuation of X-ray emission. While the exact mechanism behind the suppression of X-ray emission in high-redshift AGN remains unclear, this study highlights the importance of studying the host galaxies of AGN to fully understand their properties and behavior.\n",
      "\n",
      "In conclusion, our study shows that the X-ray emission of AGN with z>2.5 is often obscured by their host galaxies, particularly in those with high levels of star formation and high stellar mass. Our findings have important implications for the study of AGN and their host galaxies, and provide valuable insights into the nature of high-redshift AGN. Future studies will further investigate the nature of AGN obscuration and the impact of host galaxies on their properties. 1 into PostgreSQL...\n",
      "Inserting test sample 431  We present the discovery of a pair of transiting giant planets, TOI-216b and c, using four sectors of TESS photometry. TOI-216 is a $0.87 M_{\\odot}$ dwarf orbited by two transiters with radii of $8.2 R_{\\oplus}$ and $11.3 R_{\\oplus}$, and periods of $17.01$d and $34.57$d, respectively. Anti-correlated TTVs are clearly evident indicating that the transiters orbit the same star and interact via a near 2:1 mean motion resonance. By fitting the TTVs with a dynamical model, we infer masses of $26_{-11}^{+24} M_{\\oplus}$ and $190_{-80}^{+220} M_{\\oplus}$, establishing that the objects are planetary in nature and have likely sub-Kronian and Kronian densities. TOI-216 lies close to the southern ecliptic pole and thus will be observed by TESS throughout the first year, providing an opportunity for continuous dynamical monitoring and considerable refinement of the dynamical masses presented here. TOI-216 closely resembles Kepler-9 in architecture, and we hypothesize that in such systems these Saturn-analogs failed to fully open a gap and thus migrated far deeper into the system before becoming trapped into resonance, which would imply that future detections of new analogs may also have sub-Jupiter masses. 0 into PostgreSQL...\n",
      "Inserting test sample 432  In this study, we present the discovery of a resonant pair of warm giant planets, TOI-1130 b and TOI-1130 c, based on data obtained from NASA's Transiting Exoplanet Survey Satellite (TESS). The system is located in the stellar field of view of TESS in the southern ecliptic hemisphere. We report the stellar parameters, transit light curves, and the planet masses and radii, derived from the TESS observations, as well as the ground-based follow-up radial velocity (RV) measurements. Our analysis suggests that the two planets are of sub-Jupiter mass and reside in much closer proximity than previous detections of such systems. The period ratio, which is consistent with a 2:1 orbital resonance, indicates a dynamically stable configuration that may have implications for the formation and evolution mechanisms of such systems. Furthermore, we provide insights into the atmospheric properties of the planets, which can be used to inform future observations with telescopes such as James Webb Space Telescope. Overall, this discovery strengthens the growing understanding of the diversity of exoplanetary systems and opens up new avenues for further investigation. 1 into PostgreSQL...\n",
      "Inserting test sample 433  We derive an explicit representation for the transition law of a $p$-tempered $\\alpha$-stable process of Ornstein-Uhlenbeck-type and use it to develop a methodology for simulation. Our results apply in both the univariate and multivariate cases. Special attention is given to the case where $p\\le\\alpha$, which is more complicated and requires additional care. 0 into PostgreSQL...\n",
      "Inserting test sample 434  In this study, we investigate the transition laws of $p$-tempered $\\alpha$-stable OU-processes. We derive exact expressions for the conditional moments of these processes and characterize their long-term behavior under a range of conditions. Our findings shed light on the complex dynamics of these stochastic systems and have implications for a variety of applications in physics, finance, and engineering. 1 into PostgreSQL...\n",
      "Inserting test sample 435  As the only dark matter candidate that does not invoke a new particle that survives to the present day, primordial black holes (PBHs) have drawn increasing attention recently. Up to now, various observations have strongly constrained most of the mass range for PBHs, leaving only small windows where PBHs could make up a substantial fraction of the dark matter. Here we revisit the PBH constraints for the asteroid-mass window, i.e., the mass range $3.5\\times 10^{-17}M_\\odot < m_{\\mathrm{PBH}} < 4\\times 10^{-12}M_\\odot$. We revisit 3 categories of constraints. (1) For optical microlensing, we analyze the finite source size and diffractive effects and discuss the scaling relations between the event rate, $m_{\\mathrm{PBH}}$ and the event duration. We argue that it will be difficult to push the existing optical microlensing constraints to much lower m$_{\\mathrm{PBH}}$. (2) For dynamical capture of PBHs in stars, we derive a general result on the capture rate based on phase space arguments. We argue that survival of stars does not constrain PBHs, but that disruption of stars by captured PBHs should occur and that the asteroid-mass PBH hypothesis could be constrained if we can work out the observational signature of this process. (3) For destruction of white dwarfs by PBHs that pass through the white dwarf without getting gravitationally captured, but which produce a shock that ignites carbon fusion, we perform a 1+1D hydrodynamic simulation to explore the post-shock temperature and relevant timescales, and again we find this constraint to be ineffective. In summary, we find that the asteroid-mass window remains open for PBHs to account for all the dark matter. 0 into PostgreSQL...\n",
      "Inserting test sample 436  Asteroid-mass primordial black holes (PBHs) have recently garnered attention as a possible candidate for dark matter, one of the most intriguing unsolved mysteries in astrophysics. However, observations and models constrain their contribution to a small fraction of the total dark matter abundance. In this work, we revisit those constraints using a combination of numerical simulations and observational data analysis. We study the stochastic gravitational wave signal that would be produced by the formation and merger of PBH binaries, and compare it to the current upper limits from interferometers such as LIGO/Virgo. We also consider the formation and evolution of PBH clusters and their effect on the cosmic microwave background radiation and large-scale structures in the universe. Our results show that, depending on the specific assumptions and parameters, asteroid-mass PBHs could still account for a significant fraction of the dark matter if they are produced early enough in the history of the universe. However, they would also leave distinctive signatures that could be detected by future experiments, such as microlensing surveys or pulsar timing arrays. We discuss the implications of our findings for the dark matter problem, and the prospects for using gravitational waves as a probe of the early universe. Our work demonstrates the importance of a multi-disciplinary approach to this challenging scientific puzzle, and highlights the power of new technologies and observations to shed light on the nature of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 437  In present work, we try to understand the importance of effective Coulomb interaction ($U_{ef}$) between localized electrons of V atom to understand the comparative electronic behaviour of AV$_{2}$O$_{4}$ (A=Zn, Cd and Mg) compounds. The suitable values of $d$-linearization energy ($E_{d}$) of impurity V atom for calculating the $U_{ef}$ for these compounds are found to be $\\geq$44.89 eV above the Fermi level. Corresponding to these values of $E_{d}$, the self-consistently calculated values of effective $U_{LSDA}$ ($U_{PBEsol}$) for ZnV$_{2}$O$_{4}$, MgV$_{2}$O$_{4}$ and CdV$_{2}$O$_{4}$ are $\\sim$5.73 ($\\sim$5.92), $\\sim$6.06 ($\\sim$6.22) and $\\sim$5.59 ($\\sim$5.71) eV, respectively. The calculated values of $\\frac{t}{U_{ef}}$ ($t$ is the transfer integral between neighbouring sites) increases with decreasing V-V distance from CdV$_{2}$O$_{4}$ to MgV$_{2}$O$_{4}$ to ZnV$_{2}$O$_{4}$ and are found to be consistent with experimentally reported band gap. The values of $\\frac{t}{U_{ef}}$ for ZnV$_{2}$O$_{4}$, MgV$_{2}$O$_{4}$ and CdV$_{2}$O$_{4}$ are found to be $\\sim$0.023, $\\sim$0.020 and $\\sim$0.018, respectively. Hence, CdV$_{2}$O$_{4}$ with small (large) $\\frac{t}{U_{ef}}$ (experimental band gap) as compared to ZnV$_{2}$O$_{4}$ and MgV$_{2}$O$_{4}$ is found to be in localized-electron regime, while ZnV$_{2}$O$_{4}$ and MgV$_{2}$O$_{4}$ are intermediate between localized and an itinerant-electron regime. The calculated values of lattice parameters $a_{LSDS}$ ($a_{PBEsol}$) are found to be $\\sim$1.7\\%, $\\sim$2.0\\% and $\\sim$2.4\\% ($\\sim$0.6\\%, $\\sim$0.7\\% and $\\sim$0.7\\%) smaller than $a_{exp}$ for CdV$_{2}$O$_{4}$, MgV$_{2}$O$_{4}$ and ZnV$_{2}$O$_{4}$, respectively, which indicates that the PBEsol functional predicts the lattice parameters in good agreement with the experimental data. 0 into PostgreSQL...\n",
      "Inserting test sample 438  In this study, we investigate the effective Coulomb interaction of a vanadium (V) atom and its significance in comprehending the comparative electronic behavior of vanadium spinels. We perform a self-consistent evaluation of the effective Coulomb interaction parameter (U) in V spinels using first-principles calculations and derive the U values for different V sites. Our results reveal that the U value depends strongly on the oxidation state and coordination environment of the V atoms. We find that the U value is larger for V atoms in distorted octahedral than in tetrahedral coordination environments. The spin and orbital contributions of the self-energy are found to be essential in determining the U values.\n",
      "\n",
      "To understand the comparative electronic behavior of vanadium spinels, we calculate the doping energy, formation energy, and magnetic exchange interaction of V ions in different sites of spinel structures. We find that the V ions in distorted octahedral sites have a higher doping and formation energy, indicating that these sites are more favorable for substitutional doping. We also find that the V ions in these sites have a stronger magnetic exchange interaction with the neighboring ions, resulting in a higher Curie temperature.\n",
      "\n",
      "The analysis of the density of states and the electronic band structure reveals that the effective Coulomb interaction plays a significant role in the electronic properties of the V spinels. We find that the U value affects the bandgap and the degree of covalency in the V-O bonding. Moreover, our results suggest that the V ions in distorted octahedral coordination environments have a larger contribution to the electronic conductivity and thermoelectric properties of the V spinels.\n",
      "\n",
      "In summary, this work presents a self-consistent evaluation of the effective Coulomb interaction of V atoms in spinel structures and its significance in understanding the comparative electronic behavior of vanadium spinels. Our results provide insights into the electronic properties and applications of V spinels and highlight the importance of the effective Coulomb interaction in these systems. 1 into PostgreSQL...\n",
      "Inserting test sample 439  [ABRIDGED] CONTEXT: Gaia DR2 has opened a trove of astrometric and photometric data for Galactic clusters close to the Sun. Lucky imaging has been an operational technique to measure the positions of visual binary systems for a decade and a half, enough to apply its results to the calculation of orbits of some massive multiple systems within ~1 kpc of the Sun. AIMS: We are measuring distances to Galactic stellar groups containing O stars and I start with two of them: Collinder 419 in Cygnus and NGC 2264 in Monoceros. I also aim to derive new astrometric orbits for the Aa,Ab components for the main ionizing sources for both clusters: HD 193 322 and 15 Mon, respectively. METHODS: First, I present a method that uses Gaia DR2 photometry, positions, proper motions, and parallaxes to obtain the membership and distance of a stellar group and apply it to Collinder 419 and NGC 2264. Second, I present a new code that calculates astrometric orbits by searching the whole 7-parameter orbit space and apply it to HD 193 322 Aa,Ab and 15 Mon Aa,Ab using as input literature data from the Washington Double Star Catalog (WDS) and the AstraLux measurements recently presented by Ma\\'iz Apell\\'aniz et al. (2019). RESULTS: I obtain Gaia DR2 distances of 1006+37-34 pc for Collinder 419 and 719+-16 pc for NGC 2264, with the main contribution to the uncertainties coming from the spatial covariance of the parallaxes. The two NGC 2264 subclusters are at the same distance (within the uncertainties) and they show a significant relative proper motion. The distances are shown to be robust. HD 193 322 Aa,Ab follows an eccentric (e = 0.58+0.03-0.04) orbit with a period of 44+-1 a and the three stars it contains have a total mass of 76.1+9.9-7.4 M_Sol. The orbit of 15 Mon Aa,Ab is even more eccentric (e = 0.770+0.023-0.030), with a period of 108+-12 a and a total mass of 45.1+3.6-3.3 M_Sol for its two stars. 0 into PostgreSQL...\n",
      "Inserting test sample 440  The second data release (DR2) of the Gaia space mission has proven to be an excellent tool for advancing our knowledge of the distances and motions of stars and star clusters in the Milky Way. Here, we present new results based on the Gaia DR2 parallaxes and proper motions for two young open clusters, Collinder 419 and NGC 2264. We derive cluster distances and physical parameters using a multivariate membership analysis and find them to be in good agreement with previous studies. Moreover, we identify a number of new members of these clusters and investigate their properties.\n",
      "\n",
      "In addition, we report on the discovery of two new astrometric orbits using the Gaia DR2 data for two binary systems, HD 193 322 Aa,Ab and 15 Mon Aa,Ab. For each case, we provide a full analysis of the derived astrometric solution and determine the masses of the companions using the mass-luminosity relations. Our results indicate that both systems consist of two main-sequence stars with similar masses and ages. Furthermore, we find that the orbital periods of these binaries are relatively short, indicating that they are likely to have arisen from disk fragmentation or other dynamical processes in young, dense stellar environments.\n",
      "\n",
      "Finally, we discuss the implications of our results for the overall understanding of the formation and evolution of young stars and star clusters. Our analysis demonstrates the power of the Gaia DR2 astrometry for providing accurate and precise data on the motions and properties of stars in the Milky Way, including those in young, embedded clusters. We expect that future data releases from Gaia and other astrometric missions will continue to revolutionize our understanding of the formation and evolution of stars in our Galaxy and beyond. 1 into PostgreSQL...\n",
      "Inserting test sample 441  We studied resonant laser interaction with Rb atoms confined to the interstitial cavities of a random porous glass. Due to diffusive light propagation, the effect of atomic absorption on the light scattered by the sample is almost entirely compensated by atomic fluorescence at low atomic densities. For higher densities, radiation trapping increases the probability of non-radiative decay via atom-wall collisions. A simple connection of the fluorescence/absorption yield to the sample porosity is given. 0 into PostgreSQL...\n",
      "Inserting test sample 442  We investigate optical resonance properties of rubidium (Rb) atoms inside a random porous medium. By spectroscopically probing the D2 transition of the Rb atoms, we observe strong signatures of Anderson localization, which manifests as a suppression of the usual Doppler broadening. Our experiment provides evidence of the interplay of atomic motion and disorder in porous materials, and has implications for the development of novel sensors and devices based on random media. 1 into PostgreSQL...\n",
      "Inserting test sample 443  In the last two decades about a dozen methods were invented which derive, from a series of composite spectra over the orbit, the spectra of individual components in binary and multiple systems. Reconstructed spectra can then be analyzed with the tools developed for single stars. Eventually this has created the opportunity for chemical composition studies in previously inaccessible components of binary stars, and to follow their chemical evolution, an important aspect in understanding evolution of stellar systems. First, we review new developments in techniques to separate and reconstruct individual spectra, and thereafter concentrate on some applications. In particular, we emphasize the elemental abundance studies for high-mass stars, and present our recent results in probing theoretical evolution models which include effects of rotationally induced mixing. 0 into PostgreSQL...\n",
      "Inserting test sample 444  Binary and multiple star systems are found across the universe and play a crucial role in understanding the properties of stars. In this study, an innovative method is proposed in order to reconstruct the component spectra of these systems, leveraging high-resolution spectroscopy and spectral analysis techniques. By analyzing the individual components of binary and multiple systems, we gain a better understanding of their physical properties, such as their effective temperatures, surface gravities and metallicity. This study not only develops a new method for accurately extracting the individual components of these systems, but also provides a framework for understanding the behavior of binary and multiple stars. The proposed method can be extended to other types of multiple component spectra as well, making it a valuable tool for astronomers and astrophysicists. 1 into PostgreSQL...\n",
      "Inserting test sample 445  The only allowed Higgs superpotential term at stringy tree level in the string derived Singlet Extensions of the Minimal Supersymmetric Standard Model (SEMSSM) is h S H_d H_u, which leads to an additional global U(1) symmetry in the Higgs potential. We propose the string inspired SEMSSM where the global U(1) symmetry is broken by the additional superpotential terms or supersymmetry breaking soft terms that can be obtained naturally due to the instanton effects or anomalous U(1)_A gauge symmetry. In these models, we can solve the \\mu problem and the fine-tuning problem for the lightest CP-even Higgs boson mass in the MSSM, generate the baryon asymmetry via electroweak baryogenesis, and predict the new Higgs physics which can be tested at the LHC and ILC. 0 into PostgreSQL...\n",
      "Inserting test sample 446  We investigate the possibilities of string-inspired singlet extensions of the Minimal Supersymmetric Standard Model (MSSM). These extensions involve the introduction of a singlet field to the MSSM, which can be achieved naturally in string theory. We examine the phenomenological consequences of adding the singlet field to the MSSM, including the effects on the Higgs sector, vacuum stability, and dark matter candidates. We show that the inclusion of a singlet field can lead to interesting and testable predictions that can be explored at current and future colliders. In particular, we find that the lightest neutralino can be a viable dark matter candidate in this scenario. Our results provide new insights into the phenomenology of supersymmetry with singlet fields and the implications of string theory on particle physics. 1 into PostgreSQL...\n",
      "Inserting test sample 447  Not only the apparent atomic arrangement but the charge distribution also defines the crystalline symmetry that dictates the electronic and vibrational structures. In this work, we report reversible and direction-controlled chemical doping that modifies the inversion symmetry of AB-bilayer and ABCtrilayer graphene. For the top-down and bottom-up hole injection into graphene sheets, we employed molecular adsorption of electronegative I2 and annealing-induced interfacial hole doping, respectively. The chemical breakdown of the inversion symmetry led to the mixing of the G phonons, Raman active Eg and Raman-inactive Eu modes, which was manifested as the two split G peaks, G- and G+. The broken inversion symmetry could be recovered by removing the hole dopants by simple rinsing or interfacial molecular replacement. Alternatively, the symmetry could be regained by double-side charge injection, which eliminated G- and formed an additional peak, Go, originating from the barely doped interior layer. Chemical modification of crystalline symmetry as demonstrated in the current study can be applied to other low dimensional crystals in tuning their various material properties. 0 into PostgreSQL...\n",
      "Inserting test sample 448  This study explores the reversible mixing of G-phonons in ABC trilayer graphene by way of direction-controlled chemical doping. A highly controllable method for tuning the electronic properties of graphene, chemical doping is a promising approach for manipulating phonon properties. We found that applying this technique to ABC trilayer graphene resulted in the creation of unique G-phonon mixing patterns that were sensitive to the direction of the doping. Our results confirm the essential role of interlayer coupling in the generation of these patterns. By utilizing a variety of analytical tools, including Raman spectroscopy, we were able to gain insight into the specific mechanisms driving G-phonon mixing and determine the optimal parameters for creating these patterns. These findings have important implications for the development of advanced graphene-based technologies that rely on phonon manipulation, such as tunable energy storage and phonon-based information processing. 1 into PostgreSQL...\n",
      "Inserting test sample 449  In this paper we consider categories over a commutative ring provided either with a free action or with a grading of a not necessarily finite group. We define the smash product category and the skew category and we show that these constructions agree with the usual ones for algebras. In case of the smash product for an infinite group our construction specialized for a ring agrees with M. Beattie's construction of a ring with local units in \\cite{be}. We recover in a categorical generalized setting the Duality Theorems of M. Cohen and S. Montgomery in \\cite{cm}, and we provide a unification with the results on coverings of quivers and relations by E. Green in \\cite{g}. We obtain a confirmation in a quiver and relations free categorical setting that both constructions are mutual inverses, namely the quotient of a free action category and the smash product of a graded category. Finally we describe functorial relations between the representation theories of a category and of a Galois cover of it. 0 into PostgreSQL...\n",
      "Inserting test sample 450  In this paper, we investigate the relationship between skew categories, Galois coverings, and smash products of a $k$-category. We define the notion of a Galois covering of a $k$-category and show that it creates a new $k$-category which is isomorphic to the original one. We also introduce the concept of a skew categorical structure, and describe how it can be used to construct Galois coverings. Furthermore, we show that the smash product of two $k$-categories equipped with skew categories is also a $k$-category. We explore some properties of this construction, including its relationship to tensor products, and show that it yields interesting examples of $k$-categories with nontrivial skew categorical structures. Finally, we conclude by discussing some applications of our findings to other areas of mathematics, such as algebraic topology and representation theory. 1 into PostgreSQL...\n",
      "Inserting test sample 451  We present an analytical derivation of the Sachs Wolfe effect sourced by a primordial magnetic field, generated by a causal process, such as a first order phase transition in the early universe. As for the topological defects case, we apply the general relativistic junction conditions to match the perturbation variables before and after the phase transition, in such a way that the total energy momentum tensor is conserved across the transition. We find that the relevant contribution to the magnetic Sachs Wolfe effect comes from the metric perturbations at next-to-leading order in the large scale limit. The leading order term is strongly suppressed due to the presence of free-streaming neutrinos. We derive the neutrino compensation effect and confirm that the magnetic Sachs Wolfe spectrum from a causal magnetic field behaves as l(l+1)C_l^B ~ l^2 as found in the latest numerical analyses. 0 into PostgreSQL...\n",
      "Inserting test sample 452  Inflationary magnetogenesis is a promising scenario for the generation of large-scale magnetic fields in the pre-recombination universe. We investigate the impact of a primordial magnetic field produced causally by inflation on the Sachs Wolfe (SW) effect. Specifically, we break down the contributions to the temperature anisotropy in the cosmic microwave background from the scalar, vector and tensor modes sourced by the magnetic field, under the assumption that they enter the horizon shortly after their generation. Our results show that in this case, the scalar and vector contributions to the SW effect are proportional to the magnetic field strength, while the tensor contribution is independent of it. Although the magnetic signal may remain small in this purely inflationary scenario, we argue that our results pave the way for future constraints on models that predict a larger signal from other sources. 1 into PostgreSQL...\n",
      "Inserting test sample 453  Proliferation of 5G devices and services has driven the demand for wide-scale enhancements ranging from data rate, reliability, and compatibility to sustain the ever increasing growth of the telecommunication industry. In this regard, this work investigates how machine learning technology can improve the performance of 5G cell and beam index search in practice. The cell search is an essential function for a User Equipment (UE) to be initially associated with a base station, and is also important to further maintain the wireless connection. Unlike the former generation cellular systems, the 5G UE faces with an additional challenge to detect suitable beams as well as the cell identities in the cell search procedures. Herein, we propose and implement new channel-learning schemes to enhance the performance of 5G beam index detection.\n",
      "\n",
      "The salient point lies in the use of machine learning models and softwarization for practical implementations in a system level. We develop the proposed channel-learning scheme including algorithmic procedures and corroborative system structure for efficient beam index detection. We also implement a real-time operating 5G testbed based on the off-the-shelf Software Defined Radio (SDR) platform and conduct intensive experiments with commercial 5G base stations. The experimental results indicate that the proposed channel-learning schemes outperform the conventional correlation-based scheme in real 5G channel environments. 0 into PostgreSQL...\n",
      "Inserting test sample 454  The fifth generation (5G) wireless communication network's unprecedented transmission speed and ultra-low latency can be attained through the deployment of massive Multiple Input Multiple Output (MIMO). Beamforming, a essential transmission technology in massive MIMO systems, aims to direct the beam to the target user by using the beam index to improve the signal-to-noise ratio (SNR) of the channel. A common approach to beam index detection for massive MIMO systems is the use of blind detection schemes, which do not require a priori knowledge of the channel state information (CSI). However, blind detection techniques face a challenge in accurately determining the beam index, which can significantly affect the overall spectral efficiency and network performance. This paper investigates the use of channel learning for enhancing 5G blind beam index detection. The proposed approach would integrate channel learning into conventional blind detection strategies, resulting in a more accurate identification of the beam index. Through extensive simulations and analysis, we show that the combined approach offers a significant improvement in beam index detection accuracy compared to conventional blind detection techniques. Our results demonstrate that the exploitation of channel learning can effectively enhance the performance of 5G blind beam index detection, leading to improved network performance and user experience in 5G MIMO systems. 1 into PostgreSQL...\n",
      "Inserting test sample 455  An urban tactical wireless network is considered wherein the base stations are situated on unmanned aerial vehicles (UAVs) that provide connectivity to ground assets such as vehicles located on city streets. The UAVs are assumed to be randomly deployed at a fixed height according to a two-dimensional point process. Millimeter-wave (mmWave) frequencies are used to avail of large available bandwidths and spatial isolation due to beamforming. In urban environments, mmWave signals are prone to blocking of the line-of-sight (LoS) by buildings. While reflections are possible, the desire for consistent connectivity places a strong preference on the existence of an unblocked LoS path. As such, the key performance metric considered in this paper is the connectivity probability, which is the probability of an unblocked LoS path to at least one UAV within some maximum transmission distance. By leveraging tools from stochastic geometry, the connectivity probability is characterized as a function of the city type (e.g., urban, dense urban, suburban), density of UAVs (average number of UAVs per square km), and height of the UAVs. The city streets are modeled as a Manhattan Poisson Line Process (MPLP) and the building heights are randomly distributed. The analysis first finds the connectivity probability conditioned on a particular network realization (location of the UAVs) and then removes the conditioning to uncover the distribution of the connectivity; i.e., the fraction of network realizations that will fail to meet an outage threshold. While related work has applied an MPLP to networks with a single UAV, the contributions of this paper are that it (1) considers networks of multiple UAVs, (2) characterizes the performance by a connectivity distribution, and (3) identifies the optimal altitude for the UAVs. 0 into PostgreSQL...\n",
      "Inserting test sample 456  The deployment of unmanned aerial vehicles (UAVs) in an urban environment is becoming increasingly important for a variety of applications, such as surveillance, communication, and transportation. In this paper, we investigate the optimization of a millimeter-wave UAV-to-ground network in urban deployments. \n",
      "\n",
      "The proposed network architecture consists of UAVs equipped with millimeter-wave communication modules and a ground station. The main objective of this work is to design an efficient communication system that can reliably transmit data between the UAVs and the ground station in a challenging urban environment.\n",
      "\n",
      "To achieve this objective, we first model the network using an urban environment model and a random waypoint mobility model for the UAVs. We then propose a joint optimization algorithm to optimize the UAVs' trajectories, transmit power, and communication protocols to minimize the network's delay and maximize its throughput.\n",
      "\n",
      "Our simulation results show that the proposed optimization algorithm achieves a significant improvement in network performance compared to existing methods. Specifically, the optimized network can achieve a high data rate while maintaining a low communication delay, even in challenging urban environments.\n",
      "\n",
      "In conclusion, this paper proposes a novel optimization algorithm for a millimeter-wave UAV-to-ground network in urban deployments. The algorithm considers the unique challenges of communication in urban environments and provides an efficient and reliable network architecture for various UAV applications. 1 into PostgreSQL...\n",
      "Inserting test sample 457  Molecular adsorption on surfaces is a key element for many applications, including sensing and catalysis. Non-invasive sugar sensing has been an active area of research due to its importance to diabetes care. The adsorption of sugars on a template surface study is at the heart of matter. Here, we study doped hexagonal boron nitride sheets ($h$-BNNs) as adsorbing and sensing template for glucose and glucosamine. Using first principles calculations, we find that the adsorption of glucose and glucosamine on $h$-BNNs is significantly enhanced by the substitutional doping of the sheet with Al and Ga. Including long range van der Waals corrections gives adsorption energies of about 2 eV. In addition to the charge transfer occurring between glucose and the Al/Ga-doped BN sheets, the adsorption alters the size of the band gap, allowing for optical detection of adsorption. We also find that Al-doped boron nitride sheet is better than Ga-nitride sheet to enhance the adsorption energy of glucose and glucosamine. The results of our work can be potentially utilized when designing support templates for glucose and glucosamine. 0 into PostgreSQL...\n",
      "Inserting test sample 458  The adsorption of sugars on the surfaces of Al- and Ga-doped boron nitride (BN) has been investigated using computational methods. The study aimed to understand the adsorption behavior of various sugars on the doped BN surfaces, and to analyze the effects of doping on the adsorption process. The study found that the adsorption of sugars on the doped BN surfaces was strongly dependent on the doping type and concentration. Al-doping was found to increase the surface energy of BN, resulting in stronger sugar adsorption. In contrast, Ga-doping was observed to decrease the surface energy of BN, leading to weaker sugar adsorption. The adsorption energies of the sugars were found to be in the range of -45 to -90 kJ/mol, indicating moderate to strong adsorption. The obtained results suggest that the use of doped BN surfaces can be a promising approach for the selective adsorption and separation of different types of sugars. 1 into PostgreSQL...\n",
      "Inserting test sample 459  The broad use of organic semiconductors for optoelectronic applications relies on quantitative understanding and control of their spectroscopic properties. Of paramount importance are the transport gap - the difference between ionization potential and electron affinity - and the exciton binding energy - inferred from the difference between the transport and optical absorption gaps. Transport gaps are commonly established via photoemission and inverse photoemission spectroscopy (PES/IPES). However, PES and IPES are surface-sensitive, average over a dynamic lattice, and are subject to extrinsic effects, leading to significant uncertainty in gaps. Here, we use density functional theory and many-body perturbation theory to calculate the spectroscopic properties of two prototypical organic semiconductors, pentacene and 3,4,9,10-perylene tetracarboxylic dianhydride (PTCDA), quantitatively comparing with measured PES, IPES, and optical absorption spectra. For bulk pentacene and PTCDA, the computed transport gaps are 2.4 and 3.0 eV, and optical gaps are 1.7 and 2.1 eV, respectively. Computed bulk quasiparticle spectra are in excellent agreement with surface-sensitive photoemission measurements over several eV only if the measured gap is reduced by 0.6 eV for pentacene, and 0.6-0.9 eV for PTCDA. We attribute this redshift to several physical effects, including incomplete charge screening at the surface, static and dynamical disorder, and experimental resolution. Optical gaps are in excellent agreement with experiment, with solid-state exciton binding energies of ~0.5 eV for both systems; for pentacene, the exciton is delocalized over several molecules and exhibits significant charge transfer character. Our parameter-free calculations provide new interpretation of spectroscopic properties of organic semiconductors critical to optoelectronics. 0 into PostgreSQL...\n",
      "Inserting test sample 460  The study of organic semiconductors is of great interest in the field of materials science due to their potential applications in various electronic devices. In this paper, we present a first-principles study of two organic semiconductors, pentacene and PTCDA, using quasiparticle and optical spectroscopy techniques. Our calculations were carried out using density functional theory (DFT) with the hybrid functional of Heyd, Scuseria, and Ernzerhof (HSE06). \n",
      "\n",
      "We have investigated the electronic band structure, densities of states, and quasiparticle excitation energies of both pentacene and PTCDA. The calculated energy gaps of these materials are found to be in good agreement with experimental data. Furthermore, our results indicate that pentacene exhibits stronger electron-phonon coupling in comparison to PTCDA. \n",
      "\n",
      "We have also studied the optical properties of pentacene and PTCDA using linear response theory. Our calculated absorption spectra are consistent with experimental results, and we have identified the nature of excited states contributing to the spectra. Additionally, we have investigated the effects of local and non-local exchange-correlation potentials on the optical properties of these materials. \n",
      "\n",
      "In conclusion, our study provides a comprehensive understanding of the electronic and optical properties of pentacene and PTCDA from first-principles calculations. Our results are in good agreement with experimental data and help to further elucidate the behavior of organic semiconductors. This work has implications for the development of new materials with tailored electronic and optical properties for use in advanced electronic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 461  After a short survey of some topics of interest in the study of baryon-baryon scattering, the recent Nijmegen energy dependent partial wave analysis (PWA) of the nucleon-nucleon data is reviewed. In this PWA the energy range for both pp and np is now 0 < Tlab < 350 MeV and a chi^2_{d.o.f.}=1.08 was reached. The implications for the pion-nucleon coupling constants are discussed. Comments are made with respect to recent discussions around this coupling constant in the literature. In the second part, we briefly sketch the picture of the baryon in several, more or less QCD-based, quark-models that have been rather prominent in the literature. Inspired by these pictures we constructed a new soft-core model for the nucleon-nucleon interaction and present the first results of this model in a chi^2 -fit to the new multi-energy Nijmegen PWA.\n",
      "\n",
      "With this new model we succeeded in narrowing the gap between theory and experiment at low energies. For the energies Tlab = 25-320 MeV we reached a record low chi^2_{p.d.p.} = 1.16. We finish the paper with some conclusions and an outlook describing the extension of the new model to baryon-baryon scattering. 0 into PostgreSQL...\n",
      "Inserting test sample 462  Baryons are subatomic particles that experience strong interactions and make up atomic nuclei. Understanding the interactions between baryons is crucial for comprehending the behavior of nuclear matter and astrophysical systems. This research paper reports the latest developments in the study of baryon-baryon interactions. \n",
      "\n",
      "Experimental observations have shown that baryon-baryon interactions are primarily mediated by the strong force. Theoretical models based on quantum chromodynamics (QCD) have successfully reproduced many features of these interactions, but challenges remain in developing a complete description of the relevant physics. One important problem is the treatment of multi-baryon systems, where the interactions between three or more baryons can have a complex and subtle structure.\n",
      "\n",
      "Recent advances in lattice QCD and effective field theory techniques are providing new insights into baryon-baryon interactions. In particular, lattice QCD simulations are enabling researchers to directly compute baryon-baryon scattering amplitudes from first principles. Effective field theory approaches are also proving useful in constructing models that can accurately describe baryon-baryon scattering data. These developments are helping to refine our understanding of the strong force and its role in nuclear physics and astrophysical systems. 1 into PostgreSQL...\n",
      "Inserting test sample 463  We review the status of bottom quark physics at the CDF experiment. The measurements reported are based on about 110 pb-1 of data collected at the Fermilab Tevatron pp= Collider operating at sqrt{s} = 1.8 TeV. In particular, we review results on B hadron lifetimes, measurements of the time dependence of B0 B0= oscillations, and a search for CP violation in B0 --> J/Psi K0s decays.\n",
      "\n",
      "Prospects for future B physics at CDF in the next run of the Tevatron Collider starting in the year 2000 are also given. 0 into PostgreSQL...\n",
      "Inserting test sample 464  The CDF experiment has measured the lifetimes of neutral B mesons, as well as their B0-BÂ¯0 mixing probability and asymmetry. The B meson's ability to transform into its antiparticle and back again, known as mixing, has been determined to within 1.7 standard deviations. Using this information, we extract a lower bound on the size of CP violation in the B-system and find evidence for CP violation in B0 vs. BÂ¯0 mixing. These measurements improve the constraint on parameters in the standard model and enable searches for new physics beyond the standard model. 1 into PostgreSQL...\n",
      "Inserting test sample 465  In the present paper I formulate a framework that accommodates many unambiguous discrimination problems. I show that the prior information about any type of constituent (state, channel, or observable) allows us to reformulate the discrimination among finite number of alternatives as the discrimination among finite number of average constituents. Using this framework I solve several unambiguous tasks. I present a solution to optimal unambiguous comparison of two ensembles of unknown quantum states. I consider two cases: 1) The two unknown states are arbitrary pure states of qudits. 2) Alternatively, they are coherent states of single-mode optical fields. For this case I propose simple and optimal experimental setup composed of beam-splitters and a photodetector. As a second tasks I consider an unambiguous identification (UI) of coherent states. In this task identical quantum systems are prepared in coherent states and labeled as unknown and reference states, respectively. The promise is that one reference state is the same as the unknown state and the task is to find out unambiguously which one it is. The particular choice of the reference states is unknown to us, and only the probability distribution describing this choice is known. In a general case when multiple copies of unknown and reference states are available I propose a scheme consisting of beamsplitters and photodetectors that is optimal within linear optics. UI can be considered as a search in a quantum database, whose elements are the reference states and the query is represented by the unknown state. This perspective motivated me to show that reference states can be recovered after the measurement and might be used (with reduced success rate) in subsequent UI.\n",
      "\n",
      "Moreover, I analyze the influence of noise in preparation of coherent states on the performance of the proposed setup. Another problem I address is the unambiguous comparison of a pair of unknown qudit unitary channels. I characterize all solutions and identify the optimal ones. I prove that in optimal experiments for comparison of unitary channels the entanglement is necessary. The last task I studied is the unambiguous comparison of unknown non-degenerate projective measurements. I distinguish between measurement devices with apriori labeled and unlabeled outcomes. In both cases only the difference of the measurements can be concluded unambiguously. For the labeled case I derive the optimal strategy if each unknown measurement is used only once. However, if the apparatuses are not labeled, then each measurement device must be used (at least) twice. In particular, for qubit measurement apparatuses with unlabeled outcomes I derive the optimal test state in the two-shots scenario. 0 into PostgreSQL...\n",
      "Inserting test sample 466  The concept of unambiguous measurements in quantum theory has been a subject of intense investigation over the past few decades. The basic idea behind unambiguous measurements is to determine the outcome of a quantum measurement without any ambiguity or error. In other words, it involves distinguishing non-orthogonal quantum states with certainty, without destroying these states or introducing any uncertainty in the measurement.\n",
      "\n",
      "In this paper, we present a comprehensive overview of the theory of unambiguous measurements in quantum systems. We begin by introducing the underlying mathematical framework and the associated mathematical notation used in the literature. This includes the notion of projective measurements, POVMs (positive operator valued measures), and the resulting measurement operators that play a crucial role in the theory of unambiguous measurements. We then move on to discuss the main results and findings in the field, highlighting the key theoretical and experimental advances that have been made in recent years.\n",
      "\n",
      "One central theme that emerges from our review is the importance of understanding the relationship between unambiguous measurements and other concepts of quantum measurement theory. For example, we discuss how unambiguous measurements can be used to obtain more precise information about quantum states, how they relate to other entropic measures such as the von Neumann entropy and the quantum mutual information, and how they can be used to define new measures of non-classicality in quantum systems.\n",
      "\n",
      "Another important aspect of unambiguous measurements is their relationship to other areas of quantum information science, such as quantum cryptography and quantum communication. We explore some of the key connections between these fields, providing a unified framework for understanding the role of unambiguous measurements in broader quantum information protocols.\n",
      "\n",
      "Finally, we conclude by discussing some future directions for research in the field of unambiguous measurements. This includes new theoretical developments, such as the study of multi-partite unambiguous measurements and the relationship between unambiguous measurements and quantum error correction. We also highlight some of the key experimental challenges that remain to be addressed in the quest for more precise and accurate quantum measurements.\n",
      "\n",
      "Overall, our review provides a comprehensive overview of the theory of unambiguous measurements in quantum systems, highlighting the key theoretical and experimental advances that have been made in recent years. We believe that this work will be of interest to physicists and other researchers working in the field of quantum information science, as well as to those interested in the deeper conceptual foundations of quantum theory. 1 into PostgreSQL...\n",
      "Inserting test sample 467  For the sake of recognizing and classifying textile defects, deep learning-based methods have been proposed and achieved remarkable success in single-label textile images. However, detecting multi-label defects in a textile image remains challenging due to the coexistence of multiple defects and small-size defects. To address these challenges, a multi-level, multi-attentional deep learning network (MLMA-Net) is proposed and built to 1) increase the feature representation ability to detect small-size defects; 2) generate a discriminative representation that maximizes the capability of attending the defect status, which leverages higher-resolution feature maps for multiple defects. Moreover, a multi-label object detection dataset (DHU-ML1000) in textile defect images is built to verify the performance of the proposed model. The results demonstrate that the network extracts more distinctive features and has better performance than the state-of-the-art approaches on the real-world industrial dataset. 0 into PostgreSQL...\n",
      "Inserting test sample 468  Automated detection and classification of defects in textile products hold vital importance in textile quality control systems. In this paper, we present a novel deep learning framework, called MLMA-Net, for multi-label object detection in textile defect images. MLMA-Net utilizes multi-level and multi-attentional learning mechanisms, including object-level, feature-level, and channel-level attention modules, to effectively handle the multi-label classification task. Our proposed model outperforms current state-of-the-art approaches on benchmark datasets, achieving an average precision of 92.3% on the textile defect detection task. Furthermore, we conduct extensive experiments to assess the effectiveness of different attention modules, their interpretability, and the impact of various architectural design choices. Through experimental evaluations, we demonstrate the flexibility and versatility of our proposed framework, making it applicable to other multi-label object detection tasks beyond textile defect detection. 1 into PostgreSQL...\n",
      "Inserting test sample 469  The most powerful tests of stellar models come from the brightest stars in the sky, for which complementary techniques, such as astrometry, asteroseismology, spectroscopy, and interferometry can be combined. The K2 Mission is providing a unique opportunity to obtain high-precision photometric time series for bright stars along the ecliptic. However, bright targets require a large number of pixels to capture the entirety of the stellar flux, and bandwidth restrictions limit the number and brightness of stars that can be observed. To overcome this, we have developed a new photometric technique, that we call halo photometry, to observe very bright stars using a limited number of pixels. Halo photometry is simple, fast and does not require extensive pixel allocation, and will allow us to use K2 and other photometric missions, such as TESS, to observe very bright stars for asteroseismology and to search for transiting exoplanets. We apply this method to the seven brightest stars in the Pleiades open cluster. Each star exhibits variability; six of the stars show what are most-likely slowly pulsating B-star (SPB) pulsations, with amplitudes ranging from 20 to 2000 ppm. For the star Maia, we demonstrate the utility of combining K2 photometry with spectroscopy and interferometry to show that it is not a 'Maia variable', and to establish that its variability is caused by rotational modulation of a large chemical spot on a 10 d time scale. 0 into PostgreSQL...\n",
      "Inserting test sample 470  The Pleiades star cluster has captivated astronomers for centuries due to its prominent position in the night sky and unique stellar population. In recent years, the Kepler and K2 space telescopes have provided unprecedented photometric data for cluster members, allowing for the study of variability in a way that was previously unattainable. However, due to the sensitivity limits of these missions, only the brightest members of the Pleiades were monitored for variability. In this work, we extend the analysis beyond the Kepler/K2 bright limit and explore variability in the seven brightest cluster members. Our analysis reveals that all seven stars exhibit low-amplitude variability, ranging from rotational modulation to stochastic variations. We find that the variability behavior of these seven stars is consistent with that of other cluster members previously studied with Kepler/K2, suggesting that the entirety of the Pleiades cluster shares a similar variability behavior. Furthermore, our results provide clues to the underlying physical mechanisms driving the observed variability, and present a valuable benchmark for future theoretical models. Overall, our study highlights the importance of extending variability studies beyond the Kepler/K2 bright limit, and demonstrates the power of photometric data for understanding the complex nature of young stellar clusters. 1 into PostgreSQL...\n",
      "Inserting test sample 471  The Planck full mission cosmic microwave background(CMB) temperature and E-mode polarization maps are analysed to obtain constraints on primordial non-Gaussianity(NG). Using three classes of optimal bispectrum estimators - separable template-fitting (KSW), binned, and modal - we obtain consistent values for the local, equilateral, and orthogonal bispectrum amplitudes, quoting as our final result from temperature alone fNL^local=2.5+\\-5.7, fNL^equil=-16+\\-70 and fNL^ortho=-34+\\-33(68%CL). Combining temperature and polarization data we obtain fNL^local=0.8+\\-5.0, fNL^equil=-4+\\-43 and fNL^ortho=-26+\\-21 (68%CL). The results are based on cross-validation of these estimators on simulations, are stable across component separation techniques, pass an extensive suite of tests, and are consistent with Minkowski functionals based measurements. The effect of time-domain de-glitching systematics on the bispectrum is negligible. In spite of these test outcomes we conservatively label the results including polarization data as preliminary, owing to a known mismatch of the noise model in simulations and the data. Beyond fNL estimates, we present model-independent reconstructions of the CMB bispectrum and derive constraints on early universe scenarios that generate NG, including general single-field and axion inflation, initial state modifications, parity-violating tensor bispectra, and directionally dependent vector models. We also present a wide survey of scale-dependent oscillatory bispectra, and we look for isocurvature NG. Our constraint on the local primordial trispectrum amplitude is gNL^local=(-9.0+\\-7.7)x10^4 (68%CL), and we perform an analysis of additional trispectrum shapes. The global picture is one of consistency with the premises of the LambdaCDM cosmology, namely that the structure we observe today was sourced by adiabatic, passive, Gaussian, and primordial seed perturbations.[abridged] 0 into PostgreSQL...\n",
      "Inserting test sample 472  The Planck 2015 results provide new insights into the constraints surrounding primordial non-Gaussianity in the universe. Through a thorough analysis of the cosmic microwave background radiation, we have determined that any non-Gaussianity present must be very small. Our measurements are consistent with the predictions of inflationary models, and we have found no evidence of significant deviations from this paradigm. \n",
      "\n",
      "Specifically, we have placed tight constraints on the level of non-Gaussianity using both the local and equilateral type models. Our results show that the equilateral model has a tighter constraint than the local model, though both are highly consistent. These outcomes stem from the Planck satellite's high sensitivity to the cosmic microwave background temperature fluctuations, and its meticulous observation of polarization maps.\n",
      "\n",
      "Additionally, we verified that our results are robust by employing a range of statistical tests, including jackknife tests and simulations. Furthermore, using a combination of datasets from various sources, we have confirmed and strengthened previous constraints on the level of non-Gaussianity in the universe.\n",
      "\n",
      "Overall, our findings are crucial for testing the foundation of inflationary theory, which is key to understanding the origins and evolution of the universe. While our results suggest that the level of non-Gaussianity is very small, future research may develop new methodology that could allow for the detection of more significant deviations from the inflationary paradigm. The new constraints provided by the Planck 2015 results will form the foundation and benchmark for future studies into this critical area of cosmology. 1 into PostgreSQL...\n",
      "Inserting test sample 473  Let K be a number field. We prove that its ray class group modulo p 2 (resp.\n",
      "\n",
      "8) if p > 2 (resp. p = 2) characterizes its p-rationality. Then we give two short, very fast PARI Programs (\\S \\S 3.1, 3.2) testing if K (defined by an irreducible monic polynomial) is p-rational or not. For quadratic fields we verify some densities related to Cohen-Lenstra-Martinet ones and analyse Greenberg's conjecture on the existence of p-rational fields with Galois groups (Z/2Z) t needed for the construction of some Galois representations with open image. We give examples for p = 3, t = 5 and t = 6 (\\S \\S 5.1, 5.2) and illustrate other approaches (Pitoun-Varescon, Barbulescu-Ray). We conclude about the existence of imaginary quadratic fields, p-rational for all p $\\ge$ 2 (Angelakis-Stevenhagen on the concept of \"minimal absolute abelian Galois group\") which may enlighten a conjecture of p-rationality (Hajir-Maire) giving large Iwasawa $\\mu$-invariants of some uniform prop groups. R{\\'e}sum{\\'e} Soit K un corps de nombres. Nous montrons que son corps de classes de rayon modulo p 2 (resp. 8) si p > 2 (resp. p = 2) caract{\\'e}rise sa p-rationalit{\\'e}. Puis nous donnons deux courts programmes PARI (\\S \\S 3.1, 3.2) trs rapides testant si K (d{\\'e}fini par un polyn{\\^o}me irr{\\'e}ductible uni-taire) est p-rationnel ou non. Pour les corps quadratiques nous v{\\'e}rifions certaines densit{\\'e}s en relation avec celles de Cohen-Lenstra-Martinet et nous analysons la conjecture de Greenberg sur l'existence de corps p-rationnels de groupes de Galois (Z/2Z) t n{\\'e}cessaires pour la construction de certaines repr{\\'e}sentations galoisiennes d'image ouverte. Nous donnons des exemples pour p = 3, t = 5 et t = 6 (\\S \\S 5.1, 5.2) et il-lustrons d'autres approches (Pitoun-Varescon, Barbulescu-Ray). Nous concluons sur l'existence de corps quadratiques imaginaires p-rationnels pour tout p $\\ge$ 2 (Angelakis-Stevenhagen sur le concept de \"groupe de Galois ab{\\'e}lien absolu minimal\") qui peut{\\'e}clairerpeut{\\'e}clairer une conjecture de p-rationalit{\\'e} (Hajir-Maire) donnant de grands invariants $\\mu$ d'Iwasawa relatifs {\\`a} certains prop -groupes uniformes. 0 into PostgreSQL...\n",
      "Inserting test sample 474  The study presented in this paper proposes a novel computer program for testing the p-rationality of any given number field. The concept of p-rationality is of fundamental importance in algebraic number theory, with significant implications for various areas of mathematics and theoretical physics.\n",
      "\n",
      "The key contribution of this research is the development of a software tool that can accurately and efficiently determine whether a number field is p-rational. The program is based on a series of algorithms that allow for the computation of Galois cohomology groups, which are used to test the extent to which a given number field is p-rational.\n",
      "\n",
      "Our experimental results demonstrate the effectiveness of the proposed program, which is capable of handling number fields of varying degrees and complexities. By applying this tool to a range of real-world examples, we show that it can accurately identify p-rational fields, as well as provide valuable insights into the structure and behavior of these fields.\n",
      "\n",
      "Beyond its immediate applications in number theory, our program has far-reaching implications for various fields of mathematics and physics. For instance, p-rationality is closely related to the existence of certain types of geometry in string theory, a branch of theoretical physics that seeks to reconcile quantum mechanics and general relativity. As such, a better understanding of p-rationality could help shed new light on some of the most fundamental questions in physics.\n",
      "\n",
      "In conclusion, this paper presents a new program that represents a significant advance in the field of algebraic number theory. Our results demonstrate the effectiveness of the proposed tool in accurately testing the p-rationality of any given number field, and offer new insights into the structure and behavior of these fields. We believe that this program has the potential to not only further advance our understanding of mathematics and theoretical physics, but also to inspire new areas of research in these fields. 1 into PostgreSQL...\n",
      "Inserting test sample 475  A method is described to probe high-scale physics in lower-energy experiments by employing sum rules in terms of renormalisation group invariants. The method is worked out in detail for the study of supersymmetry-breaking mechanisms in the context of the Minimal Supersymmetric Standard Model. To this end sum rules are constructed that test either specific models of supersymmetry breaking or general properties of the physics that underlies supersymmetry breaking, such as unifications and flavour-universality. 0 into PostgreSQL...\n",
      "Inserting test sample 476  The Renormalisation Group (RG) Invariants and Sum Rules are two powerful tools for exploring the physics of high energy scales. We demonstrate how these tools can be used as fast and efficient diagnostics for understanding and predicting the behaviour of physical systems. We illustrate the methods with several examples and discuss their strengths in comparison to traditional techniques. Our findings highlight the importance of these tools in the development of new models and theories in high-energy physics. 1 into PostgreSQL...\n",
      "Inserting test sample 477  In this work we provide algorithmic solutions to five fundamental problems concerning the verification, synthesis and correction of concurrent systems that can be modeled by bounded p/t-nets. We express concurrency via partial orders and assume that behavioral specifications are given via monadic second order logic. A c-partial-order is a partial order whose Hasse diagram can be covered by c paths. For a finite set T of transitions, we let P(c,T,\\phi) denote the set of all T-labelled c-partial-orders satisfying \\phi. If N=(P,T) is a p/t-net we let P(N,c) denote the set of all c-partially-ordered runs of N.\n",
      "\n",
      "A (b, r)-bounded p/t-net is a b-bounded p/t-net in which each place appears repeated at most r times. We solve the following problems: 1. Verification: given an MSO formula \\phi and a bounded p/t-net N determine whether P(N,c)\\subseteq P(c,T,\\phi), whether P(c,T,\\phi)\\subseteq P(N,c), or whether P(N,c)\\cap P(c,T,\\phi)=\\emptyset.\n",
      "\n",
      "2. Synthesis from MSO Specifications: given an MSO formula \\phi, synthesize a semantically minimal (b,r)-bounded p/t-net N satisfying P(c,T,\\phi)\\subseteq P(N, c).\n",
      "\n",
      "3. Semantically Safest Subsystem: given an MSO formula \\phi defining a set of safe partial orders, and a b-bounded p/t-net N, possibly containing unsafe behaviors, synthesize the safest (b,r)-bounded p/t-net N' whose behavior lies in between P(N,c)\\cap P(c,T,\\phi) and P(N,c).\n",
      "\n",
      "4. Behavioral Repair: given two MSO formulas \\phi and \\psi, and a b-bounded p/t-net N, synthesize a semantically minimal (b,r)-bounded p/t net N' whose behavior lies in between P(N,c) \\cap P(c,T,\\phi) and P(c,T,\\psi).\n",
      "\n",
      "5. Synthesis from Contracts: given an MSO formula \\phi^yes specifying a set of good behaviors and an MSO formula \\phi^no specifying a set of bad behaviors, synthesize a semantically minimal (b,r)-bounded p/t-net N such that P(c,T,\\phi^yes) \\subseteq P(N,c) but P(c,T,\\phi^no ) \\cap P(N,c)=\\emptyset. 0 into PostgreSQL...\n",
      "Inserting test sample 478  In recent years, the need for automated verification, synthesis and correction of concurrent systems has grown significantly. The complexity and size of such systems have made manual methods impractical and error-prone. To tackle this, researchers have developed various automated techniques, of which MSO Logic has emerged as a promising tool. \n",
      "\n",
      "MSO Logic is a logical framework that supports the formal specification and verification of concurrent systems. It enables reasoning about properties of systems that involve complex interactions among multiple components and can handle systems with a large number of states.\n",
      "\n",
      "This paper presents a novel approach for automated verification, synthesis and correction of concurrent systems based on MSO Logic. The approach combines various techniques, including model checking, synthesis, and correction, to verify and correct properties of concurrent systems. It also enables the synthesis of correct-by-construction concurrent systems, which satisfy specified requirements and constraints.\n",
      "\n",
      "To demonstrate the effectiveness of the approach, we evaluated it on a variety of case studies, including a mutual exclusion protocol, a leader-election algorithm, and a distributed algorithm for state machine replication. The results demonstrate that the approach is effective in detecting and correcting errors in concurrent systems, improving their performance, and ensuring their correct behavior.\n",
      "\n",
      "Our approach also has several advantages over existing methods. It is modular, scalable, and can handle large systems with millions of states. It can also handle systems with complex synchronization constraints and can synthesize optimized implementations that meet specified requirements.\n",
      "\n",
      "In summary, this paper presents a novel approach for automated verification, synthesis and correction of concurrent systems based on MSO Logic. The approach is effective in verifying and correcting properties of concurrent systems, improving their performance, and ensuring their correct behavior. It has several advantages over existing methods and can handle large, complex systems with millions of states. The approach has broad applications in a variety of domains, including distributed systems, communication networks, and embedded systems. 1 into PostgreSQL...\n",
      "Inserting test sample 479  Within time-dependent density functional theory, combined with the Korringa-Kohn-Rostoker Green functions, we devise a real space method to investigate spin dynamics. Our scheme enables one to deduce the Coulomb potential which assures a proper Goldstone mode is present. We illustrate with application to 3$d$ adatoms and dimers on Cu(100). 0 into PostgreSQL...\n",
      "Inserting test sample 480  We investigate the dynamical magnetic excitations of nanostructures using first-principles simulations. Our results reveal how the spin waves of nanostructures depend on their size, shape, and magnetic properties. We find that the magnonic properties of these systems can be tuned through careful design, which has important implications for future nanomagnetic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 481  Abridged. We present high-quality optical spectra for 12 powerful radio sources at low and intermediate redshifts (z < 0.7) that show evidence for a substantial UV excess. These data were taken using the WHT and VLT to determine the detailed properties of the young stellar populations (YSPs) in the host galaxies as part of a larger project to investigate evolutionary scenarios for the AGN host galaxies. The results of our spectral synthesis model fits to the spectra highlight the importance of taking into account AGN-related components (emission lines, nebular continuum, scattered light) and reddening of the stellar populations in studies of this type. It is also clear that careful examination of the fits to the spectra, as well consideration of auxilary polarimetric and imaging data, are required to avoid degeneracies in the model solutions. In 3/12 sources in our sample we find broad permitted line components, and a combination of AGN-related continuum components and an old (12.5 Gyr) stellar population provides an adequate fit to the data. In the remaining 9 sources we find strong evidence for YSPs. In contrast to some recent studies that suggest relatively old post-starburst ages for the YSPs in radio galaxies (0.3-2.5 Gyr), we deduce a wide range of ages for the YSPs in our sample objects (0.02-1.5 Gyr), with ~50% of the sample showing evidence for young YSP ages (<~0.1 Gyr) in their nuclear regions. The nuclear YSPs are often significantly reddened (0.2 < E(B-V) < 1.4) and make up a substantial fraction (~1-35%) of the total stellar mass in the regions sampled by the spectroscopic slits. Moreover, in all the cases in which we have sufficient spatial resolution we find that the UV excess is extended across the full measureable extent of the galaxy (typically 5-30 kpc), suggesting galaxy-wide starbursts. 0 into PostgreSQL...\n",
      "Inserting test sample 482  This research paper investigates the properties of young stellar populations in powerful radio galaxies at low and intermediate redshifts. We aim to shed light on the relationship between star formation and radio activity in galaxies, and to understand how the environments of these galaxies affect their evolution.\n",
      "\n",
      "Through the use of multi-wavelength data and spectroscopic observations, we have obtained accurate measurements of the star formation rates of these galaxies. Our analysis reveals that the young stellar populations in these powerful radio galaxies are rapidly evolving, with star formation rates that are up to ten times higher than those in nearby quiescent galaxies.\n",
      "\n",
      "Furthermore, our results suggest that the star formation rates in these galaxies are strongly correlated with their radio luminosities, indicating that the radio activity is closely linked with the formation of new stars. The properties of the radio jets and the interstellar medium also seem to play an important role in the star formation process, with more powerful jets and denser media associated with higher star formation rates.\n",
      "\n",
      "We have also examined the spectral properties of these galaxies and found that they are consistent with those of normal star-forming galaxies, indicating that the same physical processes are responsible for the production of radiation in both cases.\n",
      "\n",
      "Our findings have important implications for our understanding of galaxy evolution and the role of radio activity in shaping the properties of galaxies. They suggest that the radio jets and the interstellar medium play a key role in regulating star formation in these galaxies and that the properties of the young stellar populations can provide valuable insights into their evolutionary histories. 1 into PostgreSQL...\n",
      "Inserting test sample 483  In this paper, we study asynchronous federated learning (FL) in a wireless distributed learning network (WDLN). To allow each edge device to use its local data more efficiently via asynchronous FL, transmission scheduling in the WDLN for asynchronous FL should be carefully determined considering system uncertainties, such as time-varying channel and stochastic data arrivals, and the scarce radio resources in the WDLN. To address this, we propose a metric, called an effectivity score, which represents the amount of learning from asynchronous FL. We then formulate an Asynchronous Learning-aware transmission Scheduling (ALS) problem to maximize the effectivity score and develop three ALS algorithms, called ALSA-PI, BALSA, and BALSA-PO, to solve it. If the statistical information about the uncertainties is known, the problem can be optimally and efficiently solved by ALSA-PI. Even if not, it can be still optimally solved by BALSA that learns the uncertainties based on a Bayesian approach using the state information reported from devices. BALSA-PO suboptimally solves the problem, but it addresses a more restrictive WDLN in practice, where the AP can observe a limited state information compared with the information used in BALSA. We show via simulations that the models trained by our ALS algorithms achieve performances close to that by an ideal benchmark and outperform those by other state-of-the-art baseline scheduling algorithms in terms of model accuracy, training loss, learning speed, and robustness of learning. These results demonstrate that the adaptive scheduling strategy in our ALS algorithms is effective to asynchronous FL. 0 into PostgreSQL...\n",
      "Inserting test sample 484  Wireless networks have become ubiquitous in modern times, connecting a plethora of devices such as smartphones, laptops, and IoT devices. The rise of federated learning, where multiple clients collaboratively train a machine learning model, has made scheduling communication over these networks more challenging. In particular, the asynchronous nature of federated learning makes it difficult to design a transmission scheduling algorithm that balances communication efficiency, computational cost, and model quality. \n",
      "\n",
      "To tackle this challenge, this paper proposes an adaptive transmission scheduling framework for asynchronous federated learning in wireless networks. Our framework dynamically schedules data transmissions based on client performance and network conditions. We formulate the optimization problem as a convex program and propose a distributed algorithm to solve it in a federated setting. Furthermore, we analyze the theoretical performance guarantees of our approach and demonstrate its effectiveness through extensive simulations. \n",
      "\n",
      "Experimental results indicate that our approach can significantly improve communication efficiency while maintaining model quality in asynchronous federated learning. Specifically, our framework reduces the communication cost by up to 50% compared to other state-of-the-art algorithms. Moreover, the adaptive scheduling strategy enables our framework to handle nonstationary network conditions and improve the training performance even under poor network conditions. \n",
      "\n",
      "Overall, this work provides a practical and effective solution to the transmission scheduling problem in asynchronous federated learning over wireless networks. Our framework has broad applications in various areas such as mobile computing, IoT, and edge computing, where wireless communication is prevalent. 1 into PostgreSQL...\n",
      "Inserting test sample 485  We study the implications of a light tetraquark on the chiral phase transition at nonzero temperature $T$: The behavior of the chiral and four-quark condensates and the meson masses are studied in the scenario in which the resonance $f_{0}(600)$ is described as a predominantly tetraquark state. It is shown that the critical temperature is lowered and the transition softened. Interesting mixing effects between tetraquark, and quarkonium configurations take place. 0 into PostgreSQL...\n",
      "Inserting test sample 486  We investigate the properties of light tetraquark states at nonzero temperature using lattice QCD simulations. Our study reveals that the tetraquark resonance has a substantial increase in mass with temperature, consistent with the melting of bound states due to thermal effects. In addition, we observed that the spectral function for the tetraquark state exhibits a broadening at high temperatures, which could indicate the transition from a bound state to a scattering state. These findings provide important insights into the behavior of tetraquark mesons at high temperature. 1 into PostgreSQL...\n",
      "Inserting test sample 487  In a wide range of robotic applications, being able to create a 3D model of the surrounding environment is a key feature for autonomous tasks. In this research report, we present a statistical model to perform 3D reconstructions of the environment from range sensors using an occupancy grid. To do so, we take into account all the available information obtained from the sensor, considering the distances traversed by the rays in each cell and seeking to reduce reconstruction errors caused by discretization. The approach has been validated qualitatively using the KITTI dataset. 0 into PostgreSQL...\n",
      "Inserting test sample 488  This paper presents a statistical update of grid representations obtained from range sensors. This approach leverages the flexibility of stochastic processes to improve the accuracy of range measurements and produce more reliable map representations. We demonstrate the efficacy of our approach using both simulated and real-world data, showing that it outperforms traditional approaches in terms of map accuracy and computational efficiency. Our results suggest that statistical updating can be an effective way to optimize grid representations in a wide range of applications, from robotics to autonomous vehicles, and beyond. 1 into PostgreSQL...\n",
      "Inserting test sample 489  The creation of manipulated multimedia content involving human characters has reached in the last years unprecedented realism, calling for automated techniques to expose synthetically generated faces in images and videos. This work explores the analysis of spatio-temporal texture dynamics of the video signal, with the goal of characterizing and distinguishing real and fake sequences. We propose to build a binary decision on the joint analysis of multiple temporal segments and, in contrast to previous approaches, to exploit the textural dynamics of both the spatial and temporal dimensions. This is achieved through the use of Local Derivative Patterns on Three Orthogonal Planes (LDP-TOP), a compact feature representation known to be an important asset for the detection of face spoofing attacks. Experimental analyses on state-of-the-art datasets of manipulated videos show the discriminative power of such descriptors in separating real and fake sequences, and also identifying the creation method used. Linear Support Vector Machines (SVMs) are used which, despite the lower complexity, yield comparable performance to previously proposed deep models for fake content detection. 0 into PostgreSQL...\n",
      "Inserting test sample 490  The proliferation of advanced technologies that allow for the creation and manipulation of digital media has given rise to the unauthorized use of someone's identity via fake faces in video sequences. Detecting these fake faces is a challenging task and requires specialized techniques. In this paper, we propose a dynamic texture analysis method based on local binary patterns (LBP) and spatiotemporal analysis for detecting fake faces in video sequences. Our method is inspired by the fact that fake faces tend to have limited dynamic texture compared to real faces. We evaluated our approach on a publicly available dataset of fake and real faces and achieved a high accuracy rate of 96.8%. Our results demonstrate the effectiveness of our proposed method in detecting fake faces in video sequences. Our approach can be used for various applications such as enhancing security systems, detecting digital forgeries, and identifying fake media. 1 into PostgreSQL...\n",
      "Inserting test sample 491  Semaphores were introduced by Dijkstra as a tool for modeling concurrency in computer programs. In this paper we provide a formal definition of PV-programs, i.e. programs using semaphores, their state spaces and execution spaces. The main goal of this paper is to prove that every finite homotopy type may appear as a connected component of the execution space of a PV-program. 0 into PostgreSQL...\n",
      "Inserting test sample 492  The execution space of a program is critical to its performance. PV-Programs are a class of concurrent programs where several threads share a single virtual address space. In this paper, we present a formal definition of the execution space of PV-programs. We show that the size of this space is an essential factor in the scalability of PV-programs. We provide an efficient approach to estimate the size of the execution space and suggest optimizations to improve the performance of such programs. 1 into PostgreSQL...\n",
      "Inserting test sample 493  Given an $n \\times d$ matrix $A$, its Schatten-$p$ norm, $p \\geq 1$, is defined as $\\|A\\|_p = \\left (\\sum_{i=1}^{\\textrm{rank}(A)}\\sigma_i(A)^p \\right )^{1/p}$, where $\\sigma_i(A)$ is the $i$-th largest singular value of $A$.\n",
      "\n",
      "These norms have been studied in functional analysis in the context of non-commutative $\\ell_p$-spaces, and recently in data stream and linear sketching models of computation. Basic questions on the relations between these norms, such as their embeddability, are still open. Specifically, given a set of matrices $A^1, \\ldots, A^{\\operatorname{poly}(nd)} \\in \\mathbb{R}^{n \\times d}$, suppose we want to construct a linear map $L$ such that $L(A^i) \\in \\mathbb{R}^{n' \\times d'}$ for each $i$, where $n' \\leq n$ and $d' \\leq d$, and further, $\\|A^i\\|_p \\leq \\|L(A^i)\\|_q \\leq D_{p,q} \\|A^i\\|_p$ for a given approximation factor $D_{p,q}$ and real number $q \\geq 1$. Then how large do $n'$ and $d'$ need to be as a function of $D_{p,q}$?\n",
      "\n",
      "We nearly resolve this question for every $p, q \\geq 1$, for the case where $L(A^i)$ can be expressed as $R \\cdot A^i \\cdot S$, where $R$ and $S$ are arbitrary matrices that are allowed to depend on $A^1, \\ldots, A^t$, that is, $L(A^i)$ can be implemented by left and right matrix multiplication. Namely, for every $p, q \\geq 1$, we provide nearly matching upper and lower bounds on the size of $n'$ and $d'$ as a function of $D_{p,q}$. Importantly, our upper bounds are {\\it oblivious}, meaning that $R$ and $S$ do not depend on the $A^i$, while our lower bounds hold even if $R$ and $S$ depend on the $A^i$. As an application of our upper bounds, we answer a recent open question of Blasiok et al. about space-approximation trade-offs for the Schatten $1$-norm, showing in a data stream it is possible to estimate the Schatten-$1$ norm up to a factor of $D \\geq 1$ using $\\tilde{O}(\\min(n,d)^2/D^4)$ space. 0 into PostgreSQL...\n",
      "Inserting test sample 494  The notion of Schatten norms has proven to be an influential concept in functional analysis, with far-reaching implications in various fields, including machine learning and data analysis. This paper considers the embeddings of Schatten norms, which is a powerful tool that allows the approximation of high-dimensional matrices in low-dimensional spaces. We explore the theoretical aspects of these embeddings, discussing their properties and characterizing their behavior.\n",
      "\n",
      "This work also addresses the important problem of data streams, which are increasingly prevalent in many applications. In particular, we demonstrate the utility of Schatten norms and their embeddings in addressing challenges associated with data streams, such as limited storage and the need for real-time processing. We describe algorithms that enable efficient processing of high-dimensional streaming data.\n",
      "\n",
      "We apply these techniques to several real-world datasets and show that the embedding of Schatten norms can lead to significant improvements in classification accuracy compared to existing approaches. Furthermore, we demonstrate the effectiveness of these embeddings in problems related to anomaly detection and outlier identification in streaming data.\n",
      "\n",
      "Our results suggest that Schatten norm embeddings have strong potential for use in real-world data analytics applications, particularly those involving large or streaming datasets. The embeddings provide a way to represent high-dimensional data in low-dimensional space while preserving important information about the structure of the original data. This not only enables efficient storage and processing of the data, but also facilitates its analysis and interpretation.\n",
      "\n",
      "Overall, this paper presents a comprehensive study of Schatten norm embeddings and their applications to data streams, with theoretical and practical implications. The results suggest that these embeddings are a powerful tool in data analytics, with the potential to impact a broad range of applications, including those in computer vision, natural language processing, and bioinformatics. 1 into PostgreSQL...\n",
      "Inserting test sample 495  The embedding-based representation learning is commonly used in deep learning recommendation models to map the raw sparse features to dense vectors. The traditional embedding manner that assigns a uniform size to all features has two issues. First, the numerous features inevitably lead to a gigantic embedding table that causes a high memory usage cost. Second, it is likely to cause the over-fitting problem for those features that do not require too large representation capacity. Existing works that try to address the problem always cause a significant drop in recommendation performance or suffers from the limitation of unaffordable training time cost. In this paper, we proposed a novel approach, named PEP (short for Plug-in Embedding Pruning), to reduce the size of the embedding table while avoiding the drop of recommendation accuracy.\n",
      "\n",
      "PEP prunes embedding parameter where the pruning threshold(s) can be adaptively learned from data. Therefore we can automatically obtain a mixed-dimension embedding-scheme by pruning redundant parameters for each feature. PEP is a general framework that can plug in various base recommendation models.\n",
      "\n",
      "Extensive experiments demonstrate it can efficiently cut down embedding parameters and boost the base model's performance. Specifically, it achieves strong recommendation performance while reducing 97-99% parameters. As for the computation cost, PEP only brings an additional 20-30% time cost compared with base models. Codes are available at https://github.com/ssui-liu/learnable-embed-sizes-for-RecSys. 0 into PostgreSQL...\n",
      "Inserting test sample 496  Recommender systems have become an integral part of many online platforms. However, the performance of these systems is primarily dependent on the quality of their embeddings. Embedding sizes in recommender systems are often fixed to a pre-defined value before training, which can lead to suboptimal results. In this paper, we propose a learnable approach for determining the optimal embedding size for a given task. We introduce a novel architecture that employs a multi-headed attention mechanism to dynamically adapt the embedding size based on the input data. Our approach offers several advantages over traditional embedding methods, including improved accuracy, faster training times, and increased flexibility. We evaluate our method on several datasets and achieve state-of-the-art results in terms of RMSE and rank accuracy metrics. Furthermore, we conduct a thorough analysis of the impact of embedding size on the performance of recommender systems, showing that our approach consistently outperforms fixed embedding size methods across a wide range of tasks. Our work highlights the benefits of using learnable embedding sizes for recommender systems and provides a starting point for future research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 497  Manifestations of stellar activity (such as star-spots, plage/faculae, and convective flows) are well known to induce spectroscopic signals often referred to as astrophysical noise by exoplanet hunters. For example, setting an ultimate goal of detecting true Earth-analogs demands reaching radial velocity (RV) precisions of ~9 cm/s. While this is becoming technically feasible with the latest generation of highly stabilised spectrographs, it is astrophysical noise that sets the true fundamental barrier on attainable RV precisions. In this paper we parameterise the impact of solar surface magneto-convection on absorption line profiles, and extend the analysis from the solar disc centre (Paper I) to the solar limb. Off disc-centre, the plasma flows orthogonal to the granule tops begin to lie along the line-of-sight and those parallel to the granule tops are no longer completely aligned with the observer. Moreover, the granulation is corrugated and the granules can block other granules, as well as the intergranular lane components. Overall, the visible plasma flows and geometry of the corrugated surface significantly impact the resultant line profiles and induce centre-to-limb variations in shape and net position. We detail these herein, and compare to various solar observations. We find our granulation parameterisation can recreate realistic line profiles and induced radial velocity shifts, across the stellar disc, indicative of both those found in computationally heavy radiative 3D magnetohydrodynamical simulations and empirical solar observations. 0 into PostgreSQL...\n",
      "Inserting test sample 498  The study of astrophysical noise is critical for understanding the behavior of stars, which can provide insight into fundamental physics. In this paper, we focus on the role of stellar surface magneto-convection in generating noise and explore its relationship with absorption line profiles. Specifically, we develop a comprehensive center-to-limb parameterization of absorption line profiles, which we compare to observations to validate our model.\n",
      "\n",
      "Our approach involves simulations of spectral line formation in granular convective flows, which enable us to generate absorption line profiles under varying conditions. When comparing our models to observations, we find that our center-to-limb parameterization provides a significant improvement over existing models, allowing us to better understand the underlying physics.\n",
      "\n",
      "We also show that magneto-convection can generate complex absorption line profiles, which are difficult to explain using traditional models. Our analysis of these profiles sheds new light on the dynamics of stellar atmospheres, and has important implications for future studies of astrophysical noise.\n",
      "\n",
      "Overall, our results demonstrate the power of stellar surface magneto-convection in generating astrophysical noise, and illustrate the potential for new insights into fundamental physics through observations of absorption line profiles. By improving our understanding of these processes, we can gain a deeper appreciation for the complexity and richness of the universe around us. 1 into PostgreSQL...\n",
      "Inserting test sample 499  In this paper, we make a detailed discussion on the $\\eta$ and $\\eta'$-meson leading-twist light-cone distribution amplitude $\\phi_{2;\\eta^{(\\prime)}}(u,\\mu)$ by using QCD sum rules approach under the background field theory. Taking both the non-perturbative condensates up to dimension-six and NLO QCD corrections to the perturbative part, its first three moments $\\langle\\xi^n_{2;\\eta^{(\\prime)}}\\rangle|_{\\mu_0} $ with $n = (2,4,6)$ at initial scale $\\mu_0 = 1$ GeV can be determined. e.g.\n",
      "\n",
      "$\\langle\\xi_{2;\\eta}^2\\rangle|_{\\mu_0} =0.231_{-0.013}^{+0.010}$, $\\langle\\xi_{2;\\eta}^4 \\rangle|_{\\mu_0} =0.109_{-0.007}^{+0.007}$, and $\\langle\\xi_{2;\\eta}^6 \\rangle|_{\\mu_0} =0.066_{-0.006}^{+0.006}$ for $\\eta$-meson, $\\langle\\xi_{2;\\eta'}^2\\rangle|_{\\mu_0} =0.211_{-0.017}^{+0.015}$, $\\langle\\xi_{2;\\eta'}^4 \\rangle|_{\\mu_0} =0.093_{-0.009}^{+0.009}$, and $\\langle\\xi_{2;\\eta'}^6 \\rangle|_{\\mu_0} =0.054_{-0.008}^{+0.008}$ for $\\eta'$-meson. Next, we calculate $D_s\\to\\eta^{(\\prime)}$ TFFs $f^{\\eta^{(\\prime)}}_+(q^2)$ within QCD light-cone sum rules approach up to NLO level. The values at large recoil region are $f^{\\eta}_+(0) = 0.476_{-0.036}^{+0.040}$ and $f^{\\eta'}_+(0) = 0.544_{-0.042}^{+0.046}$. After extrapolating TFFs to the allowable physical regions within the series expansion, we obtain the branching fractions of the semi-leptonic decay, i.e. $D_s^+\\to\\eta^{(\\prime)}\\ell^+ \\nu_\\ell$, i.e. ${\\cal B}(D_s^+\\to\\eta^{(\\prime)} e^+\\nu_e)=2.346_{-0.331}^{+0.418}(0.792_{-0.118}^{+0.141})\\times10^{-2}$ and ${\\cal B}(D_s^+\\to\\eta^{(\\prime)} \\mu^+\\nu_\\mu)=2.320_{-0.327}^{+0.413}(0.773_{-0.115}^{+0.138})\\times10^{-2}$ for $\\ell = (e, \\mu)$ channels respectively. And in addition to that, the mixing angle for $\\eta-\\eta'$ with $\\varphi$ and ratio for the different decay channels ${\\cal R}_{\\eta'/\\eta}^\\ell$ are given, which show good agreement with the recent BESIII measurements. 0 into PostgreSQL...\n",
      "Inserting test sample 500  In this paper, we investigate the twist-2 distribution amplitude of the $\\eta^{(\\prime)}$-meson within the QCD sum rule approach. Utilizing this distribution amplitude, we then calculate the semi-leptonic decay $D_s^+ \\to \\eta^{(\\prime)}\\ell^+\\nu_\\ell$. The theoretical framework of QCD sum rule allows us to derive the distribution amplitude of the $\\eta^{(\\prime)}$-meson, which describes its internal quark-gluon structure. By considering higher order terms in the operator product expansion, we obtain a more accurate characterization of the twist-2 distribution amplitude. Our numerical analysis shows that this distribution amplitude is stable and consistent with the existing results in literature. We then apply this distribution amplitude to the study of the semi-leptonic decay of $D_s^+$ meson. We calculate the form factors and branching ratios for each decay channel and compare them with the available experimental data. Our results indicate that the $\\eta'$ channel has a larger form factor and a larger branching ratio compared to the $\\eta$ channel. This is in agreement with the previous theoretical and experimental studies. We also investigate the uncertainties in our analysis arising from various sources such as the quark condensate, the strong coupling constant, and the decay constant. Our analysis demonstrates that the QCD sum rule approach provides a useful tool for studying the internal structure of hadrons and their decays. Our results not only deepen the understanding of the $\\eta^{(\\prime)}$-meson but also contribute to the ongoing efforts in probing the nature of the strong interaction. 1 into PostgreSQL...\n",
      "Inserting test sample 501  We present results of the combined photometric and spectroscopic analysis of three detached eclipsing binaries, which secondary components are not visible or very hard to identify in the optical spectra - ASAS J052743-0359.7, ASAS J065134-2211.5, and ASAS J073507-0905.7. The first one is a known visual binary ADS 4022, and we found that it is a quadruple system, composed of two spectroscopic binaries, one of which shows eclipses. None of the systems was previously recognized as a spectroscopic binary.\n",
      "\n",
      "We collected a number of high-resolution optical and IR spectra to calculate the radial velocities (RVs) and later combined them with MITSuME and ASAS photometry. The IR spectra were crucial for secure identification of the cooler components' lines. RV measurements were done with the TODCOR technique, and RV curves modelled with our own procedure V2FIT. Light curve modelling was performed with JKTEBOP and PHOEBE codes. Temperatures and metallicities of two systems were estimated from spectra. For the ADS 4022 system we also used the archival WDS data and new SOAR observations in order to derive the orbit of the visual pair for the first time. Ages were estimated by comparing our results with PARSEC isochrones.\n",
      "\n",
      "The eclipsing pair A052743 A ($P=5.27$ d) is composed of a 1.03(6) M$_\\odot$, 1.03(2) R$_\\odot$ primary and a 0.60(2) M$_\\odot$, 0.59(2) R$_\\odot$ secondary.\n",
      "\n",
      "The components of the $P=21.57$ d non-eclipsing pair B likely have masses in between the two eclipsing components, and both pairs are on a $\\sim$188 yr orbit around their common centre of mass. The system A065134 ($P=8.22$ d) consists of a 0.956(12) M$_\\odot$, 0.997(4) R$_\\odot$ primary and a 0.674(5) M$_\\odot$, 0.690(7) R$_\\odot$ secondary. Finally, A073507 ($P=1.45$ d), which consists of a 1.452(34) M$_\\odot$, 1.635(12) R$_\\odot$ primary and a 0.808(13) M$_\\odot$, 0.819(11) R$_\\odot$ secondary, is likely a PMS system. 0 into PostgreSQL...\n",
      "Inserting test sample 502  This research paper presents a study on the orbital and physical parameters of three high-contrast eclipsing binary systems, which were detected using infrared (IR) spectroscopy and analyzed through the database of the All-Sky Automated Survey (ASAS). The target systems, ASAS J074946+2249.8, ASAS J092518+0039.7, and ASAS J140308-1552.5, are composed of a bright primary star and a faint secondary star, whose characteristics were determined via a combination of photometric and spectroscopic measurements.\n",
      "\n",
      "Through the analysis of ASAS light curves and radial velocity curves obtained from high-resolution spectra, it was found that all three systems have long orbital periods spanning from 11.2 to 23.6 days. The primary stars were identified as F- or G-type main-sequence stars with masses ranging from 1.3 to 1.6 times that of the sun, while the secondary stars were revealed to be low-mass and cool stars with masses between 0.3 and 0.4 solar masses. Additionally, the eccentricity and the temperature of each component were also estimated.\n",
      "\n",
      "The study also focused on the apparent difference in magnitude between the primary and the secondary star, which leads to high contrast eclipse events. This phenomenon is found to be strongly correlated with the color difference between the stars in the system. Furthermore, a non-detection of the secondary component in the original ASAS catalog was observed, highlighting the importance of IR spectroscopy in detecting low-mass and cool stars.\n",
      "\n",
      "The determination of orbital and physical parameters of these eclipsing binaries contributes to a better understanding of stellar evolution and structure, as well as of binary system formation and interaction. The study also validates the capability of the ASAS database to provide a wealth of information on a wide range of astronomical phenomena, particularly the study of high-contrast stars. 1 into PostgreSQL...\n",
      "Inserting test sample 503  We analytically solve the conformal bootstrap equations in the Regge limit for large N conformal field theories. For theories with a parametrically large gap, the amplitude is dominated by spin-2 exchanges and we show how the crossing equations naturally lead to the construction of AdS exchange Witten diagrams. We also show how this is encoded in the anomalous dimensions of double-trace operators of large spin and large twist. We use the chaos bound to prove that the anomalous dimensions are negative. Extending these results to correlators containing two scalars and two conserved currents, we show how to reproduce the CEMZ constraint that the three-point function between two currents and one stress tensor only contains the structure given by Einstein-Maxwell theory in AdS, up to small corrections. Finally, we consider the case where operators of unbounded spin contribute to the Regge amplitude, whose net effect is captured by summing the leading Regge trajectory. We compute the resulting anomalous dimensions and corrections to OPE coefficients in the crossed channel and use the chaos bound to show that both are negative. 0 into PostgreSQL...\n",
      "Inserting test sample 504  The study of the conformal bootstrap has led to significant advancements in understanding the behavior of quantum field theories. In this paper, we explore the properties of the conformal bootstrap in the Regge limit, which is characterized by the exchange of particles with large angular momentum. Through a detailed analysis of the Regge limit, we provide important insights into the relationship between conformal symmetry and the underlying scattering amplitudes. Our results demonstrate that conformal symmetry imposes nontrivial constraints on the Regge limit of scattering amplitudes, and we identify a number of universal features that arise in this limit. Additionally, we investigate the role of anomalous dimensions in the conformal bootstrap and explore their implications for the Regge limit. Overall, our findings provide a deeper understanding of the conformal bootstrap and its application in the Regge limit, which has important implications for a wide range of physical phenomena, from high energy scattering to black hole physics. 1 into PostgreSQL...\n",
      "Inserting test sample 505  We give sharp sectional curvature estimates for complete immersed cylindrically bounded $m$-submanifolds $\\phi:M\\to N\\times\\mathbb{R}^{\\ell}$, $n+\\ell\\leq 2m-1$ provided that either $\\phi$ is proper with the second fundamental form with certain controlled growth or $M$ has scalar curvature with strong quadratic decay. This latter gives a non-trivial extension of the Jorge-Koutrofiotis Theorem [7] 0 into PostgreSQL...\n",
      "Inserting test sample 506  In this paper, we present an estimate for the sectional curvature of submanifolds with cylindrical boundary. By analyzing the geometry of such submanifolds, we derive an upper bound for their sectional curvature. We show that this bound depends on the radius of the cylindrical boundary and the principal curvatures of the submanifold. Our results have important implications in the study of geometric inequalities and the classification of submanifolds. 1 into PostgreSQL...\n",
      "Inserting test sample 507  Observations of chemical species can provide an insight into the physical conditions of the emitting gas but it is important to understand how their abundances and excitation vary within different heating environments. C$_2$H is a molecule typically found in PDR regions of our own Galaxy but there is evidence to suggest it also traces other regions undergoing energetic processing in extragalactic environments. As part of the ALCHEMI ALMA large program, the emission of C$_2$H in the central molecular zone of the nearby starburst galaxy NGC 253 was mapped at 1.6 \" (28 pc) resolution and characterized to understand its chemical origins. Spectral modelling of the N=1-0 through N=4-3 rotational transitions of C$_2$H was used to derive the C$_2$H column densities towards the dense clouds in NGC 253. Chemical modelling, including PDR, dense cloud, and shock models were then used to investigate the chemical processes and physical conditions that are producing the molecular emission. We find high C$_2$H column densities of $\\sim 10^{15} cm^{-3}$ detected towards the dense regions of NGC 253. We further find that these column densities cannot be reproduced by assuming that the emission arises from the PDR regions at the edge of the clouds. Instead, we find that the C$_2$H abundance remains high even in the high visual extinction interior of these clouds and that this is most likely caused by a high cosmic-ray ionization rate. 0 into PostgreSQL...\n",
      "Inserting test sample 508  The molecular gas tracer C$_2$H is a key observational tool for studying the distribution and origin of molecular gas in galaxies. In this work, we present new ALMA observations of C$_2$H emission in the nearby starburst galaxy NGC 253. The observations were taken in the frame of the ALCHEMI survey. We detect C$_2$H J=3-2 toward the nucleus of NGC 253, with a peak intensity of 5 mJy/beam and a velocity-integrated intensity of 0.46 K km/s. The C$_2$H emission is spatially resolved, with a size of 2.6\" x 1.9\". We find that C$_2$H emission is associated with the molecular gas traced by CO J=3-2 and HCN J=1-0, and with the dust continuum emission. Our results suggest that C$_2$H is a reliable tracer of dense molecular gas in NGC 253. We also use the C$_2$H emission to study the kinematics of the molecular gas in NGC 253, and find evidence for a rotating disk with a velocity gradient of 30-40 km/s. The C$_2$H emission is not associated with the known outflow in NGC 253, which is traced by HCN and HCO$^+$ emission. Our results support the hypothesis that C$_2$H is formed mainly via C$^+$ + C$^+$ reactions in the diffuse ISM, rather than via the CH + CH reaction in the dense gas. 1 into PostgreSQL...\n",
      "Inserting test sample 509  Theoretical modeling of the redshift-space power spectrum of galaxies is crucially important to correctly extract cosmological information from redshift surveys. The task is complicated by the nonlinear biasing and redshift space distortion effects, which change with halo mass, and by the wide distribution of halo masses and their occupations by galaxies. One of the main modeling challenges is the existence of satellite galaxies that have both radial distribution and large virial velocities inside halos, a phenomenon known as the Finger-of-God effect. We present a model for the galaxy power spectrum of in which we decompose a given galaxy sample into central and satellite galaxies and relate different contributions to 1- and 2-halo terms in a halo model. Our primary goal is to ensure that any parameters that we introduce have physically meaningful values, and are not just fitting parameters. For the 2-halo terms we use the previously developed RSD modeling of halos in the context of distribution function and perturbation theory approach. This term needs to be multiplied by the effect of radial distances and velocities of satellites inside the halo. To this one needs to add the 1-halo terms, which are non-perturbative. We show that the real space 1-halo terms can be modeled as almost constant, with the finite extent of the satellites inside the halo inducing a small k^2R^2 term, where R is related to the size of the halo. We adopt a similar model for FoG in redshift space, ensuring that FoG velocity dispersion is related to the halo mass. For FoG k^2 type expansions do not work and FoG resummation must be used instead. We test several damping functions to model the velocity dispersion FoG effect. Applying the formalism to mock galaxies modeled after the \"CMASS\" sample of the BOSS survey, we find that our predictions for the redshift-space power spectra are accurate up to k~0.4Mpc/h. 0 into PostgreSQL...\n",
      "Inserting test sample 510  The study of galaxy clustering in redshift space has emerged as a powerful tool to probe the underlying physical processes in large-scale structure formation. In this work, we investigate the behavior of the galaxy power spectrum in redshift space by combining the perturbation theory and halo models. We evaluate the role played by the nonlinear effects, redshift-space distortions and galaxy biases in the halo model predictions. We find that the halo model predictions can be improved by the inclusion of non-perturbative effects and by a careful treatment of the bias dependence on halo mass. Moreover, we show that the contribution from redshift-space distortions has a non-trivial dependence on scale, and it can be effectively modeled using a resummation technique that takes into account the large-scale coherent motions of dark matter halos. \n",
      "\n",
      "We apply these theoretical frameworks to the analysis of the latest cosmological simulations. We compare our predictions with measurements from the BOSS and WiggleZ surveys, and we demonstrate the capability of the perturbation theory plus halo model approach to describe the observed clustering of galaxies in redshift space over a wide range of scales and redshifts. Furthermore, we explore the potential of future surveys such as Euclid and DESI to constrain cosmological parameters and the nature of dark energy through measurements of the redshift-space galaxy power spectrum.\n",
      "\n",
      "Our results provide a robust theoretical framework to interpret the upcoming large-scale surveys and to extract the cosmological information encoded in the galaxy clustering. The combination of perturbation theory and halo models offers a flexible and computationally efficient approach that can model complex physical effects and capture the non-linear evolution of structure in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 511  The observed delay-time distribution (DTD) of Type-Ia supernovae (SNe Ia) is a valuable probe of SN Ia progenitors and physics, and of the role of SNe Ia in cosmic metal enrichment. The SN Ia rate in galaxy clusters as a function of cluster redshift is an almost-direct measure of the DTD, but current estimates have been limited out to a mean redshift z=1.1, corresponding to time delays, after cluster star-formation, of over 3.2 Gyr. We analyze data from a Hubble Space Telescope monitoring project of 12 galaxy clusters at z=1.13-1.75, where we discover 29 SNe, and present their multi-band light curves. Based on the SN photometry and the apparent host galaxies, we assess cluster membership and SN type, finding 11 cases that are likely SNe Ia in cluster galaxies and 4 more cases which are possible but not certain cluster SNe Ia. We conduct simulations to estimate the SN detection efficiency, the experiment's completeness, and the photometric errors, and perform photometry of the cluster galaxies to derive the cluster stellar masses. Separating the cluster sample into high-z and low-z bins, we obtain rest-frame SN Ia rates per unit formed stellar mass of $2.2 ^{+2.6}_{-1.3}\\times 10^{-13}{\\rm yr}^{-1}{\\rm M}_\\odot^{-1}$ at a mean redshift z=1.25, and $3.5^{+6.6}_{-2.8} \\times 10^{-13}{\\rm yr}^{-1}{\\rm M}_\\odot^{-1}$ at z=1.58. Combining our results with previous cluster SN Ia rates, we fit the DTD, now down to delays of 1.5 Gyr, with a power-law dependence, $t^\\alpha$, with $\\alpha=-1.30^{+0.23}_{-0.16}$. We confirm previous indications for a Hubble-time-integrated SN Ia production efficiency that is several times higher in galaxy clusters than in the field, perhaps caused by a peculiar stellar initial mass function in clusters, or by a higher incidence of binaries that will evolve into SNe Ia. 0 into PostgreSQL...\n",
      "Inserting test sample 512  The rate at which Type-Ia supernovae occur in galaxy clusters, and the associated delay-time distribution, is a key area of interest for astrophysicists seeking to understand galaxy evolution and the distribution of dark matter across the universe. Combining observational data from many different sources ranging from X-ray to optical wavelengths, this study presents an analysis of the Type-Ia supernovae rates within galaxy clusters, out to a redshift of 1.75. \n",
      "\n",
      "The data is obtained from a variety of telescopes operating in multiple bands and at different wavelengths. We detect a total of 68 Type-Ia supernovae within the studied galaxy clusters, 38 of which are spectroscopically confirmed. The delay-time distribution derived from this data is consistent with previous studies, supporting theoretical models that suggest that Type-Ia supernovae are primarily the result of a binary star system involving a white dwarf and another star. The observed rates of Type-Ia supernovae in galaxy clusters are also found to be in agreement with models that take into account the dependence on galaxy stellar mass and redshift. \n",
      "\n",
      "The study also highlights the importance of accurate photometric calibration and the use of statistical methods to estimate the uncertainty associated with individual detections. The data obtained in this study is consistent with the prediction that Type-Ia supernovae are good probes of dark matter, with the distribution of dark matter within galaxy clusters being an important factor in the observed rates of Type-Ia supernovae. The results of this study provide valuable constraints on the possible formation scenarios for Type-Ia supernovae, as well as on the history of star formation and dark matter in galaxy clusters at redshifts up to 1.75. The data and analysis presented here will be of interest not only to astrophysicists but also to cosmologists and those investigating the evolution of the universe as a whole. 1 into PostgreSQL...\n",
      "Inserting test sample 513  In this review, I reflect on four decades of my experience in linking astronomy research and education by supervising variable-star research projects by undergraduates, and by outstanding senior high school students. I describe the evolution of my experience, the students I have supervised, the nature of their projects, the educational contexts of the projects, the need for \"best practices\", the journals in which we publish, and the special role of the American Association of Variable Star Observers (AAVSO). I then describe our recent research on pulsating red giants and related objects, including three astrophysical mysteries that we have uncovered. Finally, I suggest how my projects might be scaled up or extended by others who supervise student research. 0 into PostgreSQL...\n",
      "Inserting test sample 514  This article presents a retrospective analysis of the efforts to link variable star research with education over the past forty years. Starting with the founding of the American Association of Variable Star Observers in 1911, we review the evolution of initiatives aimed at engaging students, educators, and the general public. By examining case studies of successful programs and the challenges that still need to be addressed, we identify best practices for promoting scientific literacy and enthusiasm. We highlight the importance of creating a supportive community and establishing partnerships between professional astronomers and educators. Our analysis concludes with a call to action for the continued development of innovative, accessible, and inclusive educational resources to advance variable star research and scientific education. 1 into PostgreSQL...\n",
      "Inserting test sample 515  We measure a value for the cosmic expansion of $H(z) = 89 \\pm 23$(stat) $\\pm$ 44(syst) km s$^{-1}$ Mpc$^{-1}$ at a redshift of $z \\simeq 0.47$ based on the differential age technique. This technique, also known as cosmic chronometers, uses the age difference between two redshifts for a passively evolving population of galaxies to calculate the expansion rate of the Universe. Our measurement is based on analysis of high quality spectra of Luminous Red Galaxies (LRGs) obtained with the Southern African Large Telescope (SALT) in two narrow redshift ranges of $z \\simeq 0.40$ and $z \\simeq 0.55$ as part of an initial pilot study. Ages were estimated by fitting single stellar population models to the observed spectra. This measurement presents one of the best estimates of $H(z)$ via this method at $z\\sim0.5$ to date. 0 into PostgreSQL...\n",
      "Inserting test sample 516  We present the latest age measurements of luminous red galaxies (LRGs) obtained with the Southern African Large Telescope (SALT). By leveraging the multi-object capability of SALT, we have been able to obtain spectra of a large sample of LRGs covering a wide redshift range. Our analysis relies on the study of six different spectral features, providing a robust age estimate and enabling us to explore the evolution of these galaxies over cosmic time. Our results reveal a clear evolution in the stellar populations of LRGs, with the oldest galaxies being present at higher redshifts. We compare our findings with state-of-the-art cosmological simulations and find excellent agreement. Our work provides important insights into the formation and evolution of LRGs and their role as tracers of large-scale structure in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 517  New sum rules for $B\\to \\pi,K $ and $B\\to \\rho,K^*$ form factors are derived from the correlation functions expanded near the light-cone in terms of B-meson distribution amplitudes. The contributions of quark-antiquark and quark-antiquark-gluon components in the B meson are taken into account. Models for the B-meson three-particle distribution amplitudes are suggested, based on QCD sum rules in HQET. Employing the new light-cone sum rules we calculate the form factors at small momentum transfers, including $SU(3)_{fl}$ violation effects. The results agree with the predictions of the conventional light-cone sum rules. 0 into PostgreSQL...\n",
      "Inserting test sample 518  In this paper, we investigate the form factors of B-meson distribution amplitudes using light-cone sum rules. Our analysis utilizes the framework of soft-collinear effective theory and perturbative QCD, providing a rigorous mathematical foundation for our results. By examining the behavior of form factors in the low-energy regime, we gain insight into the structure of B-mesons and their decay properties. Our findings suggest a consistent picture with previous experimental and theoretical results. The method and technique employed in this study can also be extended to other hadronic systems, providing valuable insights into the underlying fundamental interactions of subatomic particles. 1 into PostgreSQL...\n",
      "Inserting test sample 519  Despite the phenomenal success of deep neural networks in a broad range of learning tasks, there is a lack of theory to understand the way they work. In particular, Convolutional Neural Networks (CNNs) are known to perform much better than Fully-Connected Networks (FCNs) on spatially structured data: the architectural structure of CNNs benefits from prior knowledge on the features of the data, for instance their translation invariance. The aim of this work is to understand this fact through the lens of dynamics in the loss landscape.\n",
      "\n",
      "We introduce a method that maps a CNN to its equivalent FCN (denoted as eFCN). Such an embedding enables the comparison of CNN and FCN training dynamics directly in the FCN space. We use this method to test a new training protocol, which consists in training a CNN, embedding it to FCN space at a certain ``relax time'', then resuming the training in FCN space. We observe that for all relax times, the deviation from the CNN subspace is small, and the final performance reached by the eFCN is higher than that reachable by a standard FCN of same architecture. More surprisingly, for some intermediate relax times, the eFCN outperforms the CNN it stemmed, by combining the prior information of the CNN and the expressivity of the FCN in a complementary way.\n",
      "\n",
      "The practical interest of our protocol is limited by the very large size of the highly sparse eFCN. However, it offers interesting insights into the persistence of architectural bias under stochastic gradient dynamics. It shows the existence of some rare basins in the FCN loss landscape associated with very good generalization. These can only be accessed thanks to the CNN prior, which helps navigate the landscape during the early stages of optimization. 0 into PostgreSQL...\n",
      "Inserting test sample 520  Object detection is a widely studied field which aims to identify specific objects within an image. However, as the number of object categories increases, so does the complexity of the problem. This is known as the \"needle in a haystack\" problem, where the relevant object is buried amongst a sea of irrelevant objects. In recent years, convolutional neural networks have proven to be highly effective at addressing this problem. In particular, the use of architectural bias has been shown to be beneficial in improving the accuracy of object detection. \n",
      "\n",
      "Architectural bias involves incorporating prior knowledge into the network architecture, which can help guide the network towards identifying the relevant objects. This can take the form of specific convolutional filters designed to detect certain features, or the use of skip connections to help propagate information more effectively throughout the network. \n",
      "\n",
      "In this paper, we explore the benefits of using architectural bias for detecting objects in complex scenes. We focus specifically on the detection of rare objects, which are particularly difficult to identify due to their infrequent occurrence. Through experimentation, we demonstrate that the use of architectural bias leads to significant improvements in detection accuracy for rare objects. \n",
      "\n",
      "Furthermore, we investigate the influence of various design choices on the effectiveness of architectural bias. We compare different filter sizes, depths, and skip connection configurations to determine which are most effective at boosting detection accuracy. \n",
      "\n",
      "Overall, our findings highlight the importance of incorporating prior knowledge into the network architecture, particularly for the \"needle in a haystack\" problem of object detection. The use of architectural bias can significantly improve detection accuracy, and our results provide insights into the design choices that are most effective in achieving this. 1 into PostgreSQL...\n",
      "Inserting test sample 521  If a graph has $n\\ge4k$ vertices and more than $n^2/4$ edges, then it contains a copy of $C_{2k+1}$. In 1992, Erd\\H{o}s, Faudree and Rousseau showed even more, that the number of edges that occur in a triangle is at least $2\\lfloor n/2\\rfloor+1$, and this bound is tight. They also showed that the minimum number of edges that occur in a $C_{2k+1}$ for $k\\ge2$ is at least $11n^2/144-O(n)$, and conjectured that for any $k\\ge2$, the correct lower bound should be $2n^2/9-O(n)$. Very recently, F\\\"uredi and Maleki constructed a counterexample for $k=2$ and proved asymptotically matching lower bound, namely that for any $\\varepsilon>0$ graphs with $(1+\\varepsilon)n^2/4$ edges contain at least $(2+\\sqrt{2})n^2/16 \\approx 0.2134n^2$ edges that occur in $C_5$.\n",
      "\n",
      "In this paper, we use a different approach to tackle this problem and obtain the following stronger result: Any $n$-vertex graph with at least $\\lfloor n^2/4\\rfloor+1$ edges has at least $(2+\\sqrt{2})n^2/16-O(n^{15/8})$ edges that occur in $C_5$. Next, for all $k\\ge 3$ and $n$ sufficiently large, we determine the exact minimum number of edges that occur in $C_{2k+1}$ for $n$-vertex graphs with more than $n^2/4$ edges, and show it is indeed equal to $\\lfloor\\frac{n^2}4\\rfloor+1-\\lfloor\\frac{n+4}6\\rfloor\\lfloor\\frac{n+1}6\\rfloor=2n^2/9-O(n)$.\n",
      "\n",
      "For both results, we give a structural description of the extremal configurations as well as obtain the corresponding stability results, which answer a conjecture of F\\\"uredi and Maleki.\n",
      "\n",
      "The main ingredient is a novel approach that combines the flag algebras together with ideas from finite forcibility of graph limits. This approach allowed us to keep track of the extra edge needed to guarantee an existence of a $C_{2k+1}$. Also, we establish the first application of semidefinite method in a setting, where the set of tight examples has exponential size, and arises from different constructions. 0 into PostgreSQL...\n",
      "Inserting test sample 522  The notion of cycles and their properties is a fundamental concept in graph theory. In particular, understanding the minimum number of edges required for a cycle to be considered odd is of great interest in this field. In the present research, we investigate this topic and provide a comprehensive analysis of the minimum number of edges that occur in odd cycles.\n",
      "\n",
      "To begin with, we present the necessary background on graph theory and establish important definitions. We then focus on cycles and introduce a variety of methods and algorithms for their detection. Through these methods, we prove that an odd cycle exists in a graph if and only if it is bipartite. Using this result, we are able to arrive at our main contribution: a rigorous proof for the minimum number of edges required for odd cycles.\n",
      "\n",
      "Our analysis reveals that the minimum number of edges that an odd cycle requires is three. We provide a formal proof supported by rigorous mathematical reasoning. We also demonstrate a few scenarios in which this bound is achieved, further establishing the robustness of our results. Furthermore, we offer insights into the properties of odd cycles beyond their minimum number of edges, such as their relationship with other cycles and their structural characteristics.\n",
      "\n",
      "To test the generality of our findings, we conduct experiments on a variety of graphs and observe that our results hold for the majority of these instances. We also compare our findings with those from previous studies and demonstrate the improvements that we have achieved. Our study, therefore, serves to not only broaden our understanding of odd cycles but also contribute to the ongoing refinement of established theories and methods in graph theory.\n",
      "\n",
      "In conclusion, our research investigates and establishes the minimum number of edges required for odd cycles in graphs. This work fills a critical gap in our understanding of cycles in graph theory and presents the community with a valuable tool for further research. It also serves as a testament to the power of mathematical reasoning and the significance of rigorous proof in scientific study. 1 into PostgreSQL...\n",
      "Inserting test sample 523  We derive upper bounds for the eigenvalues of the Kirchhoff Laplacian on a compact metric graph depending on the graph's genus g. These bounds can be further improved if $g = 0$, i.e. if the metric graph is planar. Our results are based on a spectral correspondence between the Kirchhoff Laplacian and a particular a certain combinatorial weighted Laplacian. In order to take advantage of this correspondence, we also prove new estimates for the eigenvalues of the weighted combinatorial Laplacians that were previously known only in the weighted case. 0 into PostgreSQL...\n",
      "Inserting test sample 524  In this paper, we obtain upper bounds for the largest eigenvalue of the Kirchhoff Laplacian on embedded metric graphs. We consider a subdomain of the graph and establish a connection between the eigenvalues of the restricted operator and those of the whole graph. We use this connection to derive upper bounds for the largest eigenvalue of the Kirchhoff Laplacian on the entire graph. Our results are applicable to a broad class of embedded metric graphs, including those with boundary and curved edges. The obtained bounds are tight for some important classes of metric graphs. 1 into PostgreSQL...\n",
      "Inserting test sample 525  The existing image feature extraction methods are primarily based on the content and structure information of images, and rarely consider the contextual semantic information. Regarding some types of images such as scenes and objects, the annotations and descriptions of them available on the web may provide reliable contextual semantic information for feature extraction. In this paper, we introduce novel semantic features of an image based on the annotations and descriptions of its similar images available on the web.\n",
      "\n",
      "Specifically, we propose a new method which consists of two consecutive steps to extract our semantic features. For each image in the training set, we initially search the top $k$ most similar images from the internet and extract their annotations/descriptions (e.g., tags or keywords). The annotation information is employed to design a filter bank for each image category and generate filter words (codebook). Finally, each image is represented by the histogram of the occurrences of filter words in all categories. We evaluate the performance of the proposed features in scene image classification on three commonly-used scene image datasets (i.e., MIT-67, Scene15 and Event8). Our method typically produces a lower feature dimension than existing feature extraction methods. Experimental results show that the proposed features generate better classification accuracies than vision based and tag based features, and comparable results to deep learning based features. 0 into PostgreSQL...\n",
      "Inserting test sample 526  Scene image classification is a challenging task that has drawn significant attention from the research community. The semantic gap problem, which arises in this task, is a major challenge. To address this challenge, tag-based semantic features have been proposed. Tag-based features exploit the rich semantic information embedded in user-generated tags, which are associated with images. In this paper, we propose a novel tag-based semantic feature extraction approach, which captures the visual and semantic information simultaneously. Firstly, the image is represented using a weighted bag-of-visual-words technique. Next, we extract high-level semantic features from the visual words, using user-generated tags as a source of semantic information. Finally, a Support Vector Machine classifier is used to classify the image. Experimental results show that our proposed method outperforms the state-of-the-art methods in terms of classification accuracy. Furthermore, we analyze the contribution of each component of the proposed method, and demonstrate the effectiveness of the tag-based semantic features. In summary, our proposed tag-based semantic feature extraction method is effective for scene image classification, and provides a promising solution to the semantic gap problem. 1 into PostgreSQL...\n",
      "Inserting test sample 527  Video understanding is one of the most challenging topics in computer vision.\n",
      "\n",
      "In this paper, a four-stage video understanding pipeline is presented to simultaneously recognize all atomic actions and the single on-going activity in a video. This pipeline uses objects and motions from the video and a graph-based knowledge representation network as prior reference. Two deep networks are trained to identify objects and motions in each video sequence associated with an action. Low Level image features are then used to identify objects of interest in that video sequence. Confidence scores are assigned to objects of interest based on their involvement in the action and to motion classes based on results from a deep neural network that classifies the on-going action in video into motion classes. Confidence scores are computed for each candidate functional unit associated with an action using a knowledge representation network, object confidences, and motion confidences. Each action is therefore associated with a functional unit and the sequence of actions is further evaluated to identify the single on-going activity in the video. The knowledge representation used in the pipeline is called the functional object-oriented network which is a graph-based network useful for encoding knowledge about manipulation tasks. Experiments are performed on a dataset of cooking videos to test the proposed algorithm with action inference and activity classification. Experiments show that using functional object oriented network improves video understanding significantly. 0 into PostgreSQL...\n",
      "Inserting test sample 528  In the field of computer vision, understanding activity videos is an important but challenging task. Traditional approaches rely on handcrafted features or low-level representations, but these often fall short in capturing complex spatiotemporal patterns. In this paper, we present a novel approach for long activity video understanding using a Functional Object-Oriented Network (FOON). \n",
      "\n",
      "FOON is a hierarchical framework that leverages both functional and object-oriented representations to learn the underlying dynamics of long activity videos. Specifically, we encode functional representations with a Convolutional Neural Network (CNN) to capture the patterns of activities and their relations with one another. Meanwhile, we use an Object-Oriented Neural Network (OON) to represent the objects and their interactions in the scene. The two networks are integrated in a hierarchical manner to form FOON, which can capture both high-level semantic concepts and low-level spatiotemporal patterns. \n",
      "\n",
      "Our approach outperforms state-of-the-art methods on two challenging datasets, Something-Something V1 and V2. Furthermore, we conduct extensive ablation studies to demonstrate the effectiveness of each component of FOON. In summary, we present a novel deep neural network for long activity video understanding, which combines functional and object-oriented representations in a hierarchical manner. Our results show that this approach can significantly improve the performance on this challenging task. 1 into PostgreSQL...\n",
      "Inserting test sample 529  The Sun is a magnetic star whose magnetism and cyclic activity is linked to the existence of an internal dynamo. We aim to understand the establishment of the solar magnetic 22-yr cycle, its associated butterfly diagram and field parity selection through numerical simulations of the solar global dynamo.\n",
      "\n",
      "Inspired by recent observations and 3D simulations that both exhibit multicellular flows in the solar convection zone, we seek to characterise the influence of various profiles of circulation on the behaviour of solar mean-field dynamo models. We are using 2-D mean field flux transport Babcock-Leighton numerical models in which we test several types of meridional flows: 1 large single cell, 2 cells in radius and 4 cells per hemisphere. We confirm that adding cells in latitude tends to speed up the dynamo cycle whereas adding cells in radius more than triples the period. We find that the cycle period in the four cells model is less sensitive to the flow speed than in the other simpler meridional circulation profiles studied. Moreover, our studies show that adding cells in radius or in latitude seems to favour the parity switching to a quadrupolar solution. According to our numerical models, the observed 22-yr cycle and dipolar parity is easily reproduced by models including multicellular meridional flows. On the contrary, the resulting butterfly diagram and phase relationship between the toroidal and poloidal fields are affected to a point where it is unlikely that such multicellular meridional flows persist for a long period of time inside the Sun, without having to reconsider the model itself. 0 into PostgreSQL...\n",
      "Inserting test sample 530  In this paper, we explore the significance of meridional flows in flux transport dynamo models. These models aim to describe the magnetic activity of the Sun, and meridional flows have been shown to play a crucial role in their accuracy. We start by discussing the underlying physics of these flows, as well as the reasons why they are important to consider. We then describe the current state of knowledge regarding their behavior and fluctuations, using both observational data and theoretical models.\n",
      "\n",
      "Next, we examine the impact of meridional flows on the generation and evolution of magnetic fields in flux transport dynamo models. Specifically, we investigate how these flows affect the transport of magnetic flux across the solar surface, as well as the formation of sunspots and other magnetic structures. Our analysis reveals that meridional flows are indeed critical to accurately model magnetic activity on the Sun, particularly when it comes to predicting long-term variations in the solar cycle.\n",
      "\n",
      "Finally, we explore some of the open questions and challenges in this field. For example, we discuss the difficulties in accurately measuring and modeling meridional flows, as well as the need for more advanced computational tools to simulate their complex behavior. Overall, our findings underscore the importance of considering meridional flows when developing and refining flux transport dynamo models, both for our understanding of the Sun and for applications in space weather prediction. 1 into PostgreSQL...\n",
      "Inserting test sample 531  The spectra of charged particles produced within the pseudorapidity window abs(eta) < 1 at sqrt(s[NN]) = 5.02 TeV are measured using 404 inverse microbarns of PbPb and 27.4 inverse picobarns of pp data collected by the CMS detector at the LHC in 2015. The spectra are presented over the transverse momentum ranges spanning 0.5 < pt < 400 GeV in pp and 0.7 < pt < 400 GeV in PbPb collisions. The corresponding nuclear modification factor, R[AA], is measured in bins of collision centrality. The R[AA] in the 5% most central collisions shows a maximal suppression by a factor of 7-8 in the pt region of 6-9 GeV. This dip is followed by an increase, which continues up to the highest pt measured, and approaches unity in the vicinity of pt = 200 GeV. The R[AA] is compared to theoretical predictions and earlier experimental results at lower collision energies. The newly measured pp spectrum is combined with the pPb spectrum previously published by the CMS Collaboration to construct the pPb nuclear modification factor, R[pA], up to 120 GeV. For pt > 20 GeV, R[pA] exhibits weak momentum dependence and shows a moderate enhancement above unity. 0 into PostgreSQL...\n",
      "Inserting test sample 532  Nuclear modification factors of charged particles are studied in collisions of lead-lead (PbPb) and proton-lead (pPb) at sqrt(s[NN]) = 5.02 TeV. The data is collected by the CMS experiment at the Large Hadron Collider (LHC). The measurements cover a range of transverse momenta (pT) and pseudorapidity (Î·) in both systems. The nuclear modification factors are calculated as ratios of the particle yields in the two collision systems to the expected yields in the absence of nuclear effects. The results show a strong suppression of charged particles at high pT and central Î· in PbPb collisions, consistent with jet quenching due to the medium formed in heavy-ion collisions. The pPb measurements show no significant sign of suppression or enhancement, indicating that initial state effects play a minor role. Comparison with previously measured nuclear modification factors in other collision systems indicates a similar suppression pattern at higher energies. The present data provide a valuable input to understand the properties of the quark-gluon plasma and the initial state effects in relativistic nuclear collisions. 1 into PostgreSQL...\n",
      "Inserting test sample 533  The Chandra Multiwavength Plane (ChaMPlane) Survey of the galactic plane incorporates serendipitous sources from selected Chandra pointings in or near the galactic plane (b < 12deg; >20 ksec; lack of bright diffuse or point sources) to measure or constrain the luminosity function of low-luminosity accretion sources in the Galaxy. The primary goal is to detect and identify accreting white dwarfs (cataclysmic variables, with space density still uncertain by a factor of >10-100), neutron stars and black holes (quiescent low mass X-ray binaries) to constrain their space densities and thus origin and evolution. Secondary objectives are to identify Be stars in high mass X-ray binaries and constrain their space densities, and to survey the H-R diagram for stellar coronal sources. A parallel optical imaging under the NOAO Long Term Survey program provides deep optical images using the Mosaic imager on the CTIO and KPNO 4-m telescopes. The 36arcmin X 36arcmin optical images (Halpha, R, V and I) cover ~5X the area of each enclosed Chandra ACIS FOV, providing an extended survey of emission line objects for comparison with Chandra.\n",
      "\n",
      "Spectroscopic followup of optical counterparts is then conducted, thus far with WIYN and Magellan. The X-ray preliminary results from both the Chandra and optical surveys will be presented, including logN-logS vs. galactic position (l,b) and optical idenifications. 0 into PostgreSQL...\n",
      "Inserting test sample 534  The Chandra Multi-wavelength Plane (ChaMPlane) Survey is a comprehensive survey of a region of the sky located at high galactic latitudes and extending over seven square degrees. The survey is focused on understanding the populations of low and high mass X-ray binaries in the Milky Way. The primary goals of the survey are to identify and catalog sources of X-ray emission using the Chandra X-ray Observatory, and to conduct follow-up observations of the most interesting sources at other wavelengths. In this paper, we present the design and initial results of the ChaMPlane survey. We describe the survey's scientific objectives, and we discuss the selection criteria for our sample of X-ray sources. We also detail the observations obtained with Chandra and with other telescopes used for follow-up observations, such as optical, infrared, and radio telescopes. We present a summary of the properties of the X-ray sources detected in the first two years of the ChaMPlane survey, including the number of sources, their X-ray spectra, and their optical and infrared counterparts. Our results demonstrate the extraordinary scientific potential of multi-wavelength surveys such as ChaMPlane for advancing our knowledge of the X-ray sky, and they lay the groundwork for future studies of this fascinating region of the Milky Way. 1 into PostgreSQL...\n",
      "Inserting test sample 535  For a given network, a backbone is an overlay network consisting of a connected dominating set with additional accessibility properties. Once a backbone is created for a network, it can be utilized for fast communication amongst the nodes of the network.\n",
      "\n",
      "The Signal-to-Interference-plus-Noise-Ratio (SINR) model has become the standard for modeling communication among devices in wireless networks. For this model, the community has pondered what the most realistic solutions for communication problems in wireless networks would look like. Such solutions would have the characteristic that they would make the least number of assumptions about the availability of information about the participating nodes. Solving problems when nothing at all is known about the network and having nodes just start participating would be ideal. However, this is quite challenging and most likely not feasible. The pragmatic approach is then to make meaningful assumptions about the available information and present efficient solutions based on this information.\n",
      "\n",
      "We present a solution for creation of backbone in the SINR model, when nodes do not have access to their physical coordinates or the coordinates of other nodes in the network. This restriction models the deployment of nodes in various situations for sensing hurricanes, cyclones, and so on, where only information about nodes prior to their deployment may be known but not their actual locations post deployment. We assume that nodes have access to knowledge of their label, the labels of nodes within their neighborhood, the range from which labels are taken $[N]$ and the total number of participating nodes $n$.\n",
      "\n",
      "We also assume that nodes wake up spontaneously. We present an efficient deterministic protocol to create a backbone with a round complexity of $O(\\Delta \\lg^2 N)$. 0 into PostgreSQL...\n",
      "Inserting test sample 536  In wireless communication networks, the received signal-to-interference-plus-noise ratio (SINR) is a critical factor that determines the quality of the communication link. In this paper, we propose a method for creating a deterministic backbone in an SINR network, without any prior knowledge of the location of the nodes. Our method is based on the idea of constructing a virtual grid on top of the network, which is used to divide the network into cells. We then use a set of rules to determine which nodes should be included in the backbone. \n",
      "\n",
      "The backbone nodes are selected based on their connectivity to the other nodes in the same cell, as well as their connectivity to the nodes in neighboring cells. The goal of our method is to create a backbone that maximizes the network's SINR coverage while minimizing the number of backbone nodes. \n",
      "\n",
      "To evaluate the performance of our method, we conducted simulations using both synthetic and real-world networks. Our results demonstrate that our method is effective in creating a backbone that provides good SINR coverage and is robust to changes in network topology and node placement. Furthermore, our method is scalable and can be applied to networks of different sizes and architectures.\n",
      "\n",
      "In conclusion, our proposed method for deterministic backbone creation in an SINR network without prior knowledge of node location provides a robust and scalable solution. Our results show that the backbone created by our method provides good SINR coverage while minimizing the number of nodes required, making it a viable option for practical wireless communication networks. 1 into PostgreSQL...\n",
      "Inserting test sample 537  Using inspiral and plunge trajectories we construct with a generalized Ori-Thorne algorithm, we use a time-domain black hole perturbation theory code to compute the corresponding gravitational waves. The last cycles of these waveforms are a superposition of Kerr quasinormal modes. In this paper, we examine how the modes' excitations vary as a function of source parameters, such as the larger black hole's spin and the geometry of the smaller body's inspiral and plunge. We find that the mixture of quasinormal modes that characterize the final gravitational waves from a coalescence is entirely determined by the spin $a$ of the larger black hole, an angle $I$ which characterizes the misalignment of the orbital plane from the black hole's spin axis, a second angle $\\theta_{\\rm fin}$ which describes the location at which the small body crosses the black hole's event horizon, and the direction sgn$(\\dot\\theta_{\\rm fin})$ of the body's final motion. If these large-mass-ratio results hold at less extreme mass ratios, then measuring multiple ringdown modes of binary black hole coalescence gravitational waves may provide important information about the source's binary properties, such as the misalignment of the orbit's angular momentum with black hole spin. This may be particularly useful for large mass binaries, for which the early inspiral waves are out of the detectors' most sensitive band. 0 into PostgreSQL...\n",
      "Inserting test sample 538  This paper investigates the mode content of late-time coalescence waveforms resulting from misaligned black hole coalescences. We employ numerical relativity simulations to study the dynamics of binary black holes with non-zero spin and arbitrary mass ratios, exploring the excitation of various quasi-normal modes. By analyzing the gravitational wave signals produced by these systems, we identify the dominant modes and compare them to those excited in aligned spin coalescences. Our results indicate that misaligned coalescences produce exciting quasi-normal mode signals that cannot be obtained via aligned coalescences. These modes are robust and remain detectable with current and future interferometric detectors. Our findings suggest that misaligned coalescences represent a valuable source of information about black hole dynamics and properties, including their spin and mass ratios. Furthermore, we explore the impact of initial spins and mass ratios on the mode content of the resulting waveforms. Our analysis provides insight into the importance of these parameters in determining the dominant modes, and highlights the potential for future studies to unveil further black hole dynamics through the observation of misaligned coalescences. 1 into PostgreSQL...\n",
      "Inserting test sample 539  Manifold learning is a hot research topic in the field of computer science. A crucial issue with current manifold learning methods is that they lack a natural quantitative measure to assess the quality of learned embeddings, which greatly limits their applications to real-world problems. In this paper, a new embedding quality assessment method for manifold learning, named as Normalization Independent Embedding Quality Assessment (NIEQA), is proposed.\n",
      "\n",
      "Compared with current assessment methods which are limited to isometric embeddings, the NIEQA method has a much larger application range due to two features. First, it is based on a new measure which can effectively evaluate how well local neighborhood geometry is preserved under normalization, hence it can be applied to both isometric and normalized embeddings. Second, it can provide both local and global evaluations to output an overall assessment.\n",
      "\n",
      "Therefore, NIEQA can serve as a natural tool in model selection and evaluation tasks for manifold learning. Experimental results on benchmark data sets validate the effectiveness of the proposed method. 0 into PostgreSQL...\n",
      "Inserting test sample 540  In the field of machine learning, manifold learning is a popular method for data analysis and dimensionality reduction. Despite the advantages of this method, assessing the quality of the embeddings produced by manifold learning algorithms remains a challenge. In this paper, we propose a novel method for embedding quality assessment that is based on exploring the topology of high-dimensional manifolds. Our approach centers around the estimation of intrinsic dimensionality and curvature, and exploits the uniqueness of the intrinsic geometry of manifolds to assess embedding quality in a robust and scalable way. Experiments on a range of large-scale datasets demonstrate the superior performance of our method compared to state-of-the-art techniques. Our proposed approach is a valuable contribution to manifold learning research that can be easily applied to various manifold learning algorithms, thereby enabling better evaluation of the quality of their respective embeddings. 1 into PostgreSQL...\n",
      "Inserting test sample 541  We present the measured Sunyaev-Zel'dovich (SZ) flux from 474 optically-selected MaxBCG clusters that fall within the Atacama Cosmology Telescope (ACT) Equatorial survey region. The ACT Equatorial region used in this analysis covers 510 square degrees and overlaps Stripe 82 of the Sloan Digital Sky Survey. We also present the measured SZ flux stacked on 52 X-ray-selected MCXC clusters that fall within the ACT Equatorial region and an ACT Southern survey region covering 455 square degrees. We find that the measured SZ flux from the X-ray-selected clusters is consistent with expectations. However, we find that the measured SZ flux from the optically-selected clusters is both significantly lower than expectations and lower than the recovered SZ flux measured by the Planck satellite. Since we find a lower recovered SZ signal than Planck, we investigate the possibility that there is a significant offset between the optically-selected brightest cluster galaxies (BCGs) and the SZ centers, to which ACT is more sensitive due to its finer resolution. Such offsets can arise due to either an intrinsic physical separation between the BCG and the center of the gas concentration or from misidentification of the cluster BCG. We find that the entire discrepancy for both ACT and Planck can be explained by assuming that the BCGs are offset from the SZ maxima with a uniform random distribution between 0 and 1.5 Mpc.\n",
      "\n",
      "Such large offsets between gas peaks and BCGs for optically-selected cluster samples seem unlikely given that we find the physical separation between BCGs and X-ray peaks for an X-ray-selected subsample of MaxBCG clusters to have a much narrower distribution that peaks within 0.2 Mpc. It is possible that other effects are lowering the ACT and Planck signals by the same amount, with offsets between BCGs and SZ peaks explaining the remaining difference between measurements. (Abridged) 0 into PostgreSQL...\n",
      "Inserting test sample 542  The Atacama Cosmology Telescope (ACT) has been used to study the relation between the optical richness of galaxy clusters and the Sunyaev-Zel'dovich (SZ) effect. The optical richness of a cluster is determined by the number of galaxies within a specific range of brightness and distance, and is often used as a proxy for the cluster mass. The SZ effect, on the other hand, is a distortion of the cosmic microwave background radiation due to the scattering of photons by free electrons in the hot gas within galaxy clusters.\n",
      "\n",
      "We present measurements of the optical richness and SZ effect for a sample of galaxy clusters detected by the ACTPol survey. We find a clear correlation between the optical richness and SZ effect, with the SZ effect increasing roughly linearly with optical richness. This result is consistent with theoretical predictions and previous observational studies with other telescopes.\n",
      "\n",
      "In addition to measuring the correlation between optical richness and SZ effect, we investigate the scatter in this relation. We find that the scatter is larger for low-mass clusters, which may be due to the fact that these systems are more strongly affected by environmental and dynamical processes that can enhance or suppress the SZ effect. We also find that the scatter is larger for clusters at higher redshifts, which may be due to the fact that these clusters are typically less well-resolved by the ACT survey and therefore have larger uncertainties in their SZ measurements.\n",
      "\n",
      "Overall, our results demonstrate the power of combining optical and SZ observations to study the properties of galaxy clusters. This technique can be used to better understand the formation and evolution of these objects, and may also have implications for studies of cosmology and the growth of large-scale structure in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 543  In this article, we study the scalar-diquark-scalar-diquark-antiquark type $udsc\\bar{c}$ pentaquark state with the QCD sum rules, the predicted mass $M_P=4.47\\pm0.11\\,\\rm{GeV}$ is in excellent agreement with the experimental data $ 4458.8 \\pm 2.9 {}^{+4.7}_{-1.1} \\mbox{ MeV}$ from the LHCb collaboration, and supports assigning the $P_{cs}(4459)$ to be the hidden-charm pentaquark state with the spin-parity $J^P={\\frac{1}{2}}^-$. We take into account the flavor $SU(3)$ mass-breaking effect, and estimate the mass spectrum of the diquark-diquark-antiquark type $udsc\\bar{c}$ pentaquark states with the strangeness $S=-1$. 0 into PostgreSQL...\n",
      "Inserting test sample 544  In this study, we conduct an analysis of the $P_{cs}(4459)$ as the hidden-charm pentaquark state using QCD sum rules. Through our investigation, we aim to understand the properties and characteristics of this state by exploring its mass, width, and residue. Our calculations demonstrate that the $P_{cs}(4459)$ possesses a mass of approximately 4.5 GeV, coupling constants consistent with those predicted by theory, and a narrow width. These findings provide insight into the structure of the hidden-charm pentaquark state and pave the way for further exploration of its role in the subatomic world. 1 into PostgreSQL...\n",
      "Inserting test sample 545  We discuss a coarse-grained approach to the computation of rare events in the context of grand canonical Monte Carlo (GCMC) simulations of self-assembly of surfactant molecules into micelles. The basic assumption is that the {\\it computational} system dynamics can be decomposed into two parts -- fast (noise) and slow (reaction coordinates) dynamics, so that the system can be described by an effective, coarse grained Fokker-Planck (FP) equation. While such an assumption may be valid in many circumstances, an explicit form of FP equation is not always available. In our computations we bypass the analytic derivation of such an effective FP equation. The effective free energy gradient and the state-dependent magnitude of the random noise, which are necessary to formulate the effective Fokker-Planck equation, are obtained from ensembles of short bursts of microscopic simulations {\\it with judiciously chosen initial conditions}. The reaction coordinate in our micelle formation problem is taken to be the size of a cluster of surfactant molecules. We test the validity of the effective FP description in this system and reconstruct a coarse-grained free energy surface in good agreement with full-scale GCMC simulations. We also show that, for very small clusters, the cluster size seizes to be a good reaction coordinate for a one-dimensional effective description. We discuss possible ways to improve the current model and to take higher-dimensional coarse-grained dynamics into account. 0 into PostgreSQL...\n",
      "Inserting test sample 546  Coarse-grained kinetic computations are a widely used technique in simulating rare events in molecular systems, such as micelle formation. In this study, we investigate the kinetics of micelle formation using a coarse-grained model, which allows us to simulate larger time and length scales than atomistic models. Our computational approach combines accelerated rare-event sampling with thermodynamic integration, enabling us to calculate accurate free-energy profiles for micelle formation. We find that the kinetics of micelle formation depend strongly on the hydrophobicity of the constituent monomers and temperature. Specifically, we observe that increasing the hydrophobicity of the monomers promotes micelle formation by lowering the free-energy barrier. Additionally, we show that at higher temperatures, micelle formation occurs more rapidly due to increased thermal energy. Finally, we compare our coarse-grained results to atomistic simulations and experimental data and find good agreement. Our findings suggest that coarse-grained kinetic computations can provide valuable insights into the mechanisms underlying rare events in molecular systems, and offer a promising avenue for designing new materials and optimizing existing ones. 1 into PostgreSQL...\n",
      "Inserting test sample 547  Display advertising is an important online advertising type where banner advertisements (shortly ad) on websites are usually measured by how many times they are viewed by online users. There are two major channels to sell ad views.\n",
      "\n",
      "They can be auctioned off in real time or be directly sold through guaranteed contracts in advance. The former is also known as real-time bidding (RTB), in which media buyers come to a common marketplace to compete for a single ad view and this inventory will be allocated to a buyer in milliseconds by an auction model. Unlike RTB, buying and selling guaranteed contracts are not usually programmatic but through private negotiations as advertisers would like to customise their requests and purchase ad views in bulk. In this paper, we propose a simple model that facilitates the automation of direct sales. In our model, a media seller puts future ad views on sale and receives buy requests sequentially over time until the future delivery period. The seller maintains a hidden yet dynamically changing reserve price in order to decide whether to accept a buy request or not. The future supply and demand are assumed to be well estimated and static, and the model's revenue management is using inventory control theory where each computed reverse price is based on the updated supply and demand, and the unsold future ad views will be auctioned off in RTB to the meet the unfulfilled demand. The model has several desirable properties. First, it is not limited to the demand arrival assumption. Second, it will not affect the current equilibrium between RTB and direct sales as there are no posted guaranteed prices. Third, the model uses the expected revenue from RTB as a lower bound for inventory control and we show that a publisher can receive expected total revenue greater than or equal to those from only RTB if she uses the computed dynamic reserves prices for direct sales. 0 into PostgreSQL...\n",
      "Inserting test sample 548  This research paper proposes a risk-aware dynamic reserve pricing strategy for programmatic guarantee (PG) in display advertising. PG is a popular method of transacting ad inventory, in which advertisers are guaranteed a certain number of impressions or clicks. In PG, a reserve price is set by the publisher, which acts as the minimum price that the advertiser is willing to pay for the inventory. However, setting a fixed reserve price may lead to inefficient allocation of ad impressions and reduced revenue for the publisher. \n",
      "\n",
      "To address this issue, we propose a dynamic reserve pricing strategy that takes into account the level of risk associated with the transaction. Our model utilizes a risk-aware approach to set the optimal reserve price for each transaction. In addition, we develop a machine learning algorithm that uses a combination of historical data and real-time bid data to predict the optimal reserve price for future transactions. \n",
      "\n",
      "The proposed approach is evaluated on a real-world dataset from a large publisher. Our results show that our dynamic reserve pricing strategy outperforms the traditional fixed reserve pricing strategy and generates higher revenues for the publisher. Additionally, our model is shown to be robust to changes in market conditions, such as changes in advertiser demand and auction pricing dynamics. \n",
      "\n",
      "We conclude that a risk-aware dynamic reserve pricing strategy is an effective approach to programmatic guarantee in display advertising. Our approach offers a more efficient allocation of ad impressions, leading to higher revenue for the publisher, while also providing a lower level of risk for both the publisher and the advertiser. Our work provides a useful framework for the optimization of reserve prices in other auction-based marketplaces, where transaction risks need to be accounted for. 1 into PostgreSQL...\n",
      "Inserting test sample 549  In 1981 Masur proved the existence of a dense geodesic in the moduli space for a Teichm\\\"uller space. We prove an analogue theorem for reduced Outer Space endowed with the Lipschitz metric. We also prove two results possibly of independent interest: we show Brun's unordered algorithm weakly converges and from this prove that the set of Perron-Frobenius eigenvectors of positive integer $m \\times m$ matrices is dense in the positive cone $\\mathbb{R}^m_+$ (these matrices will in fact be the transition matrices of positive automorphisms). We give a proof in the appendix that not every point in the boundary of Outer Space is the limit of a flow line. 0 into PostgreSQL...\n",
      "Inserting test sample 550  We study the geometry of the Outer Space of a free group, and its quotient space by the action of the group of outer automorphisms. Specifically, we focus on long and dense geodesic rays in the quotient, and we show that under certain conditions, such rays admit limits in the augmented Outer Space. We also prove that such rays support quasi-geodesic currents, which give rise to explicit cocycles spanning the quasi-isometry class of a large class of fully irreducible automorphisms of the free group. Our results provide new tools for understanding the dynamics of the Outer Space action, and offer insights into the geometry of fully irreducible automorphisms. 1 into PostgreSQL...\n",
      "Inserting test sample 551  In the singularly perturbed limit corresponding to a large diffusivity ratio between two components in a reaction-diffusion (RD) system, quasi-equilibrium spot patterns are often admitted, producing a solution that concentrates at a discrete set of points in the domain. In this paper, we derive and study the differential algebraic equation (DAE) that characterizes the slow dynamics for such spot patterns for the Brusselator RD model on the surface of a sphere.\n",
      "\n",
      "Asymptotic and numerical solutions are presented for the system governing the spot strengths, and we describe the complex bifurcation structure and demonstrate the occurrence of imperfection sensitivity due to higher order effects. Localized spot patterns can undergo a fast time instability and we derive the conditions for this phenomena, which depend on the spatial configuration of the spots and the parameters in the system. In the absence of these instabilities, our numerical solutions of the DAE system for $N = 2$ to $N = 8$ spots suggest a large basin of attraction to a small set of possible steady-state configurations. We discuss the connections between our results and the study of point vortices on the sphere, as well as the problem of determining a set of elliptic Fekete points, which correspond to globally minimizing the discrete logarithmic energy for $N$ points on the sphere. 0 into PostgreSQL...\n",
      "Inserting test sample 552  In this paper, we investigate the dynamics of reaction-diffusion systems on the sphere, focusing specifically on localized spot patterns. Through numerical simulations and analytical calculations, we demonstrate that unlike their planar counterparts, these systems exhibit unique spatio-temporal patterns that are strongly influenced by the underlying curvature of the sphere. In particular, we show that the curvature can have a stabilizing effect on spot patterns, leading to the formation of stable localized clusters. We also investigate the role of various system parameters, such as the reaction rates and diffusion coefficients, on the dynamics of spot patterns and find that they can significantly impact the stability and evolution of these patterns. Our results shed new light on the behavior of reaction-diffusion systems on non-planar geometries and provide insights into the formation and maintenance of localized spot patterns in biological systems. Overall, this work presents a comprehensive analysis of the dynamics of localized spot patterns for reaction-diffusion systems on the sphere and highlights the importance of considering the impact of geometry on the behavior of these systems. 1 into PostgreSQL...\n",
      "Inserting test sample 553  We study the phase diagram of Q-state Potts models, for Q=4 cos^2(PI/p) a Beraha number (p>2 integer), in the complex-temperature plane. The models are defined on L x N strips of the square or triangular lattice, with boundary conditions on the Potts spins that are periodic in the longitudinal (N) direction and free or fixed in the transverse (L) direction. The relevant partition functions can then be computed as sums over partition functions of an A\\_{p-1} type RSOS model, thus making contact with the theory of quantum groups. We compute the accumulation sets, as N -> infinity, of partition function zeros for p=4,5,6,infinity and L=2,3,4 and study selected features for p>6 and/or L>4. This information enables us to formulate several conjectures about the thermodynamic limit, L -> infinity, of these accumulation sets. The resulting phase diagrams are quite different from those of the generic case (irrational p). For free transverse boundary conditions, the partition function zeros are found to be dense in large parts of the complex plane, even for the Ising model (p=4). We show how this feature is modified by taking fixed transverse boundary conditions. 0 into PostgreSQL...\n",
      "Inserting test sample 554  The phase behavior of statistical lattice models has received significant attention due to its implications in condensed matter physics and materials science. In this study, we investigate the complex-temperature phase diagram of two distinct models, namely, the Potts and Restricted Solid-On-Solid (RSOS) models. A thorough analysis of the phase diagrams of these models is presented using Monte Carlo simulations and renormalization group (RG) techniques. Our results show that the Potts model exhibits a rich phase behavior, including a first-order phase transition and a continuous phase transition as the temperature is varied. On the other hand, the RSOS model presents a more conventional phase diagram with only one continuous transition. We also explore the topological properties of the system and find that the Potts model manifests winding number fluctuations at the critical point, a behavior that is not observed in the RSOS model. Our findings reveal the intriguing properties of these complex-temperature models and enhance our understanding of the phase behavior of statistical lattice models in general. 1 into PostgreSQL...\n",
      "Inserting test sample 555  AGN display an extreme range in the narrow emission line equivalent widths.\n",
      "\n",
      "Specifically, in the PG quasar sample the equivalent width of the narrow [O III] 5007 line has a range of >300, while the broad Hb line, for example, has a range of 10 only. The strength of [O III] 5007 is modulated by the covering factor, CF, of the narrow line region (NLR) gas, its density n_e, and ionization parameter U. To explore which of these factors produces the observed large range in [O III] 5007 strength, we measure the strength of the matching narrow Hb and [O III] 4363 lines, detected in 40 out of the 87 z<0.5 PG quasars in the Boroson & Green sample. The photoionization code CLOUDY is then used to infer CF, n_e, and U in each object, assuming a single uniform emitting zone.\n",
      "\n",
      "We find that the range of CF (~0.02-0.2) contributes about twice as much as the range in both n_e and U towards modulating the strength of the [O III] 5007 line. The CF is inversely correlated with luminosity, but it is not correlated with L_Edd as previously speculated. The single zone [O III] 5007 emitting region is rather compact, having R=40L_44^0.45 pc. These emission lines can also be fit with an extreme two zone model, where [O III] 4363 is mostly emitted by a dense (n_e=10^7) inner zone at R=L_44^0.5 pc, and [O III] 5007 by a low density (n_e=10^3) extended outer zone at R=750L_44^0.34 pc. Such an extended [O III] 5007 emission should be well resolved by HST imaging of luminous AGN. Further constraints on the radial gas distribution in the NLR can be obtained from the spectral shape of the IR continuum emitted by the associated dust. 0 into PostgreSQL...\n",
      "Inserting test sample 556  This paper investigates the factors that contribute to the strength of the [O III] 5007 line in active galactic nuclei (AGN), a phenomenon that provides insight into the properties and behavior of these objects. We explore the relationship between the line strength and various physical parameters of AGN, such as their luminosity, mass, and accretion rate, as well as the properties of the surrounding interstellar medium. Our analysis is based on a sample of over 200 AGN with high-quality spectroscopic data from the Sloan Digital Sky Survey. \n",
      "\n",
      "Our results show a complex interplay between the different factors that control the [O III] 5007 line strength. We find a strong correlation between the line strength and the AGN luminosity, a trend that is consistent with previous studies. However, we also observe a significant scatter around this correlation, indicating that other factors are at play. We further investigate the role of the AGN mass and accretion rate, finding weaker correlations with the line strength but with important implications for our understanding of AGN physics.\n",
      "\n",
      "In addition to these AGN properties, we analyze the impact of the interstellar medium on the [O III] 5007 line strength. We find that AGN in denser environments tend to have stronger [O III] 5007 lines, likely due to the presence of more gas and dust that can be ionized by the AGN radiation field. However, we caution that this effect may be partially mitigated by other factors, such as the AGN orientation and kinematics of the gas.\n",
      "\n",
      "Overall, our study sheds new light on the complex mechanisms that control the [O III] 5007 line strength in AGN, and highlights the importance of considering multiple physical factors in understanding these objects. Further investigations are needed to fully understand the underlying physical processes and their implications for AGN evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 557  We consider the square-lattice antiferromagnetic Heisenberg Hamiltonian extended with a single-ion axial anisotropy term as a minimal model for the multiferroic Ba2CoGe2O7. Developing a multiboson spin-wave theory, we investigate the dispersion of the spin excitations in this spin-3/2 system. As a consequence of a strong single-ion anisotropy, a stretching (longitudinal) spin-mode appears in the spectrum. The inelastic neutron scattering spectra of Zheludev et al. [Phys. Rev. B 68, 024428 (2003)] are successfully reproduced by the low energy modes in the multiboson spin-wave theory, and we anticipate the appearance of the spin stretching modes at 4meV that can be identified using the calculated dynamical spin structure factors. We expect the appearance of spin stretching modes for any S>1/2 compound where the single-ion anisotropy is significant. 0 into PostgreSQL...\n",
      "Inserting test sample 558  We present a multiboson spin-wave theory to study the magnetic properties of Ba2CoGe2O7, a spin-3/2 easy-plane NÃ©el antiferromagnet with strong single-ion anisotropy. Our analysis is based on the bilinear-biquadratic spin Hamiltonian and considering the exchange interactions within the spin lattice. Our theoretical results reveal that the system can be characterized by frustrated quantum fluctuations, which are primarily responsible for the observed low-temperature behavior. In addition, we investigate the effects of different anisotropic couplings and find that these are essential to account for the magnon excitations, providing further evidence of the unusual magnetic properties of Ba2CoGe2O7. Our study highlights the importance of considering multiboson spin-wave theory in the context of complex magnetic systems, and provides insights into the spin dynamics in materials with strong single-ion anisotropy. 1 into PostgreSQL...\n",
      "Inserting test sample 559  In this paper, we consider the second-order cone tensor eigenvalue complementarity problem (SOCTEiCP) and present three different reformulations to the model under consideration. Specifically, for the general SOCTEiCP, we first show its equivalence to a particular variational inequality under reasonable conditions. A notable benefit is that such a reformulation possibly provides an efficient way for the study of properties of the problem. Then, for the symmetric and sub-symmetric SOCTEiCPs, we reformulate them as appropriate nonlinear programming problems, which are extremely beneficial for designing reliable solvers to find solutions of the considered problem. Finally, we report some preliminary numerical results to verify our theoretical results. 0 into PostgreSQL...\n",
      "Inserting test sample 560  In this paper, we introduce a class of second-order cone eigenvalue complementarity problems for higher-order tensors. We formulate the problems as a system of nonlinear equations and solve them using a feasible interior-point method. We show that the proposed method is globally convergent and can handle larger tensor dimensions. Furthermore, we explore the use of the proposed method in tensor completion and denoising problems, and obtain promising results in comparison to existing methods. Our findings suggest that the proposed class of complementarity problems can be a useful tool in various tensor-based applications. 1 into PostgreSQL...\n",
      "Inserting test sample 561  A common failure mode of density models trained as variational autoencoders is to model the data without relying on their latent variables, rendering these variables useless. Two contributing factors, the underspecification of the model and the looseness of the variational lower bound, have been studied separately in the literature. We weave these two strands of research together, specifically the tighter bounds of Monte-Carlo objectives and constraints on the mutual information between the observable and the latent variables.\n",
      "\n",
      "Estimating the mutual information as the average Kullback-Leibler divergence between the easily available variational posterior $q(z|x)$ and the prior does not work with Monte-Carlo objectives because $q(z|x)$ is no longer a direct approximation to the model's true posterior $p(z|x)$. Hence, we construct estimators of the Kullback-Leibler divergence of the true posterior from the prior by recycling samples used in the objective, with which we train models of continuous and discrete latents at much improved rate-distortion and no posterior collapse. While alleviated, the tradeoff between modelling the data and using the latents still remains, and we urge for evaluating inference methods across a range of mutual information values. 0 into PostgreSQL...\n",
      "Inserting test sample 562  Monte Carlo methods are a popular approach for computing expectations and probabilities in various fields, including machine learning, finance, and statistics. However, these methods can become computationally intensive when dealing with high-dimensional data. In recent years, mutual information constraints have emerged as a promising approach to reduce the computational costs of Monte Carlo methods without significantly sacrificing accuracy. \n",
      "\n",
      "This paper explores the use of mutual information constraints for Monte Carlo objectives. We first provide a comprehensive introduction to mutual information and its applications. We then present a theoretical framework for incorporating mutual information constraints into Monte Carlo objectives, including a derivation of the gradient estimator for the constrained objective function. We demonstrate the effectiveness of our approach through experiments on simulated data and real-world datasets, showing that mutual information constraints can significantly reduce the variance of Monte Carlo estimators and improve efficiency in high-dimensional settings. \n",
      "\n",
      "Finally, we discuss potential applications and future directions of research in this area, including extensions to other optimization problems and deep learning models. Overall, our work provides a new perspective on using mutual information constraints for Monte Carlo objectives and opens up exciting avenues for further research in this field. 1 into PostgreSQL...\n",
      "Inserting test sample 563  Jets are rarely associated with pre-main-sequence intermediate-mass stars.\n",
      "\n",
      "Optical and near-IR observations of jet-driving sources are often hindered by the presence of a natal envelope. Jets around partly embedded sources are a useful diagnostic to constrain the geometry of the concealed protoplanetary disk. In fact, the jet-driving mechanisms are affected by both spatial anisotropies and episodic variations at the (sub-)au scale from the star. We obtained a rich set of high-contrast VLT/SPHERE observations from 0.6 micron to 2.2 micron of the young intermediate-mass star RY Tau. Given the proximity to the Sun of this source, our images have the highest spatial resolution ever obtained for an atomic jet. Optical observations in polarized light show no sign of the protoplanetary disk detected by ALMA. Instead, we observed a diffuse signal resembling a remnant envelope with an outflow cavity. The jet is detected in four spectral lines. The jet appears to be wiggling and its radial width increasing with the distance is complementary to the shape of the outflow cavity suggesting a strong jet/envelope interaction. Through the estimated tangential velocity, we revealed a possible connection between the launching time of the jet sub-structures and the stellar activity of RY Tau. RY Tau is at an intermediate stage toward the dispersal of the natal envelope. This source shows episodic increases of mass accretion/ejection similarly to other known intermediate-mass stars. The amount of observed jet wiggle is consistent with the presence of a precessing disk warp or misaligned inner disk that would be induced by an unseen planetary/sub-stellar companion at sub-/few-au scales. The high disk mass of RY Tau and of two other jet-driving intermediate-mass stars, HD163296 and MWC480, suggests that massive, full disks are more efficient at launching prominent jets. 0 into PostgreSQL...\n",
      "Inserting test sample 564  This paper presents the first imaging of RY Tau at submillimeter wavelengths, using the SPHERE ZIMPOL polarimeter and the IRDIS dual-band imager at the Very Large Telescope (VLT). The observations reveal the morphology of the system, showing a highly asymmetric envelope and a collimated jet with several emission knots. The envelope is found to be elongated along a northeast-southwest axis, with a prominent bow shock located to the southwest. The jet extends in a northwest-southeast direction, with a bright knot located close to the source. The polarization map of the system shows a complex structure, with multiple polarization vectors detected in the envelope and the jet.\n",
      "\n",
      "We use radiative transfer modeling to interpret our observations and derive physical properties of the system. The modeling shows that RY Tau is a Class I protostar with a disk and an envelope. The disk is found to have a radius of about 30 AU, and the envelope extends to a radius of about 5000 AU, with a total mass of about 0.05 solar masses. The jet is well collimated and has a velocity of about 100 km/s at the base, decreasing to about 20 km/s at the end of the observed region.\n",
      "\n",
      "The observations and modeling presented in this paper provide new insights into the formation and evolution of young stellar objects. The highly asymmetric morphology of RY Tau suggests that it may be the result of a recent merger between two protostars. The collimated jet and the polarization structure provide clues to the physical mechanisms driving the outflow from the system. Our results highlight the potential of SPHERE observations for studying the properties of protostars and their surrounding environments, and for testing theoretical models of star formation. 1 into PostgreSQL...\n",
      "Inserting test sample 565  The tremendous development of cloud computing and network technology makes it possible for multiple people with limited resources to complete a large-scale computing with the help of cloud servers. In order to protect the privacy of clients, secure multiparty computation plays an important role in the process of computing. Recently, Clementi et al[\\textcolor[rgb]{0.00,0.07,1.00}{Phys.\n",
      "\n",
      "Rev. A {\\bf 96}, 062317(2017)}] proposed a secure multiparty computation protocol using quantum resources. In their protocol, utilizing only linear classical computing and limited manipulation of quantum information, a method of computing $n-variable$ symmetric Boolean function $f(x_1, x_2, \\cdots, x_n)$ with degree 2 is proposed, and all clients can jointly compute $f(x_1, x_2, \\cdots, x_n)$ without revealing their private inputs with the help of a sever.\n",
      "\n",
      "They proposed an open problem: are there more simple nonlinear functions like the one presented by them that can be used as subroutines for larger computation protocols? We will give the answer to this question in this paper.\n",
      "\n",
      "Inspired by Clementi et al's work, we continue to explore the quantum realization of Boolean functions. First, we demonstrate a way to compute a class of $n-variable$ symmetric Boolean function $f_n^k$ by using single-particle quantum state $|0\\rangle$ and single-particle unitary operations $U_k$. Second, we show that each $n-variable$ symmetric Boolean function can be represented by the linear combination of $f_n^k(k=0,1,\\cdots,n)$ and each function $f_n^k(2\\leq k\\leq n)$ can be used to perform secure multiparty computation. Third, we propose an universal quantum implementation method for arbitrary $n-variable$ symmetric Boolean function $f(x_1, x_2, \\cdots, x_n)$. Finally, we demonstrate our secure multiparty computation protocol on IBM quantum cloud platform. 0 into PostgreSQL...\n",
      "Inserting test sample 566  Classical multiparty computation is a fundamental problem in cryptography, with applications ranging from secure multiparty computation to secure communication protocols. Despite the important progress made in the past decades, classical multiparty computation is still limited to small numbers of parties and the security of these protocols relies heavily on unproven assumptions.\n",
      "\n",
      "In this paper, we investigate the use of quantum resources to improve classical multiparty computation. In particular, we focus on the use of entanglement and quantum communication channels to enhance the security and efficiency of classical multiparty computation protocols.\n",
      "\n",
      "We first provide a formal definition of quantum-enhanced classical multiparty computation and its security properties. We then present an example protocol based on the use of GHZ states and a quantum communication channel. Our protocol enables the computation of a general function of the private inputs of multiple parties, while providing full security against any colluding subset of parties.\n",
      "\n",
      "We analyze the efficiency of our protocol and show that it outperforms classical multiparty computation protocols in terms of communication complexity and number of rounds. We also discuss the limitations of our approach and highlight directions for future research, such as the use of different types of entanglement and the investigation of the power of quantum-enhanced classical multiparty computation for tasks beyond computing general functions.\n",
      "\n",
      "Our results demonstrate that quantum resources can be successfully used to enhance classical multiparty computation, opening up new avenues for the design of secure and efficient multiparty communication protocols. This work contributes both to the field of quantum cryptography and to the broader area of classical multiparty computation. 1 into PostgreSQL...\n",
      "Inserting test sample 567  We have carried out muon spin relaxation (muSR), neutron diffraction and inelastic neutron scattering (INS) investigations on polycrystalline samples of Ce(Ru1-xFex)2Al10 (x=0, 0.3, 0.5, 0.8 and 1) to investigate the nature of the ground state (magnetic ordered versus paramagnetic) and the origin of the spin gap formation as evident from the bulk measurements in the end members. Our zero-field muSR spectra clearly reveal coherent two-frequency oscillations at low temperature in x=0, 0.3 and 0.5 samples, which confirms the long-range magnetic ordering of the Ce-moment with TN=27, 26 and 21 K respectively. On the other hand the muSR spectra of x=0.8 and x=1 down to 1.4 K and 0.045 K, respectively exhibit a temperature independent Kubo-Toyabe term confirming a paramagnetic ground state. The long-range magnetic ordering in x=0.5 below 21 K has been confirmed through the neutron diffraction study. INS measurements of x=0 clearly reveal the presence of a sharp inelastic excitation near 8 meV between 5 K and 26 K, due to an opening of a gap in the spin excitation spectrum, which transforms into a broad response at and above 30 K.\n",
      "\n",
      "Interestingly, at 4.5 K the spin gap excitation broadens in x=0.3 and exhibits two clear peaks at 8.4(3) and 12.0(5) meV in x=0.5. In the x=0.8 sample, which remains paramagnetic down to 1.2 K, there is a clear signature of a spin gap of 10-12 meV at 7 K, with a strong Q-dependent intensity. Evidence of a spin gap of 12.5(5) meV has also been found in x=1. The observation of a spin gap in the paramagnetic samples (x=0.8 and 1) is an interesting finding in this study and it challenges our understanding of the origin of the semiconducting gap in CeT2Al10 (T=Ru and Os) compounds in terms of hybridization gap opening only a small part of the Fermi surface, gapped spin waves, or a spin-dimer gap. 0 into PostgreSQL...\n",
      "Inserting test sample 568  This study investigates the competing 4f-electron dynamics in the Ce(Ru1-xFex)2Al10 compound for values of x ranging from 0 to 1. This material system exhibits an intriguing array of physical phenomena that arise from a delicate balance between localized and itinerant electron behavior. Through extensive magnetic susceptibility, transport, thermodynamic, and x-ray diffraction measurements, we reveal the evolution of the electronic ground state as a function of Fe substitution. In the absence of Fe, CeRu2Al10 is known to exhibit Kondo semiconducting behavior, characterized by a resistivity minimum and a rapid suppression of the magnetic susceptibility at low temperatures. However, through the systematic incorporation of Fe, we observe a marked deviation from this behavior. A magnetic ordering process emerges in the sample at x=0.3 as evidenced by a significant increase in the magnetic susceptibility and an associated transition to an insulating ground state. We propose that this abrupt phase transition is associated with the presence of a Ruderman-Kittel-Kasuya-Yosida (RKKY) interaction that is known to arise due to the coexistence of localized and itinerant electron behavior. This behavior is further confirmed through our analysis of specific heat measurements, which demonstrate a striking increase in the electronic entropy at low temperatures. Our results demonstrate an intricate interplay between the localized 4f moments, the itinerant electrons, and the Fe dopants in the Ce(Ru1-xFex)2Al10 system, and provide insight into how changes in chemical composition can be used to manipulate the electronic ground state of heavy fermion materials. Overall, this work highlights the intricate nature of the Kondo semiconducting ground state and underscores the need for a more comprehensive understanding of the underlying electronic properties of complex materials. 1 into PostgreSQL...\n",
      "Inserting test sample 569  The stress field at the tip of a crack of a thin plate of elastic material that is broken due to a mode III shear tearing has a universal form with a non-universal amplitude, known as the stress intensity factor, which depends on the crack length and the boundary conditions. We present in this paper exact analytic results for this stress intensity factor, thus enriching the small number of exact results that can be obtained within Linear Elastic Fracture Mechanics (LEFM). 0 into PostgreSQL...\n",
      "Inserting test sample 570  This study focuses on the Stress Intensity Factor (SIF) analysis of Mode III cracks in thin sheets, specifically in the context of fracture mechanics. The SIF is an important parameter that characterizes the severity of a crack and plays a crucial role in predicting the crack's growth and failure. In this work, analytical and numerical methods are used to determine the SIF for various crack geometries and loading conditions. The results of the study provide insight into the deformation and fracture behavior of thin sheets under Mode III loading. 1 into PostgreSQL...\n",
      "Inserting test sample 571  Many researchers have made use of the Wikipedia network for relatedness and similarity tasks. However, most approaches use only the most recent information and not historical changes in the network. We provide an analysis of entity relatedness using temporal graph-based approaches over different versions of the Wikipedia article link network and DBpedia, which is an open-source knowledge base extracted from Wikipedia. We consider creating the Wikipedia article link network as both a union and intersection of edges over multiple time points and present a novel variation of the Jaccard index to weight edges based on their transience. We evaluate our results against the KORE dataset, which was created in 2010, and show that using the 2010 Wikipedia article link network produces the strongest result, suggesting that semantic similarity is time sensitive. We then show that integrating multiple time frames in our methods can give a better overall similarity demonstrating that temporal evolution can have an important effect on entity relatedness. 0 into PostgreSQL...\n",
      "Inserting test sample 572  The vast amount of information available on the web poses a challenge to researchers seeking to understand the semantic relationships between entities. We propose a method to analyze the temporal evolution of entity relatedness by leveraging the vast knowledge sources of Wikipedia and DBpedia. Our approach involves modeling the semantic structure of Wikipedia articles using a graph-based approach and evaluating the relationships between entities. We analyze the degree to which entities become related over time, as well as the relative strength of those relationships. Our findings demonstrate that there is a gradual increase in entity-relatedness over time, meaning that entities that were previously unrelated become more closely linked. This temporal analysis of entity relatedness provides insights into the dynamic nature of the relationships between entities and their evolution over time, paving the way for new applications in the field of natural language processing and information retrieval. 1 into PostgreSQL...\n",
      "Inserting test sample 573  Using network models consisting of gap junction coupled Wang-Buszaki neurons, we demonstrate that it is possible to obtain not only synchronous activity between neurons but also a variety of constant phase shifts between 0 and \\pi.\n",
      "\n",
      "We call these phase shifts intermediate stable phaselocked states. These phase shifts can produce a large variety of wave-like activity patterns in one-dimensional chains and two-dimensional arrays of neurons, which can be studied by reducing the system of equations to a phase model. The 2\\pi periodic coupling functions of these models are characterized by prominent higher order terms in their Fourier expansion, which can be varied by changing model parameters. We study how the relative contribution of the odd and even terms affect what solutions are possible, the basin of attraction of those solutions and their stability. These models may be applicable to the spinal central pattern generators of the dogfish and also to the developing neocortex of the neonatal rat. 0 into PostgreSQL...\n",
      "Inserting test sample 574  The emergence of synchronized activity in neuronal populations through gap junction coupling is a well-established phenomenon. However, a novel aspect of this research is the discovery of antiphase waves or â€œantiwavesâ€ in chains of retina ganglion cells. These waves travel in the opposite direction to conventional waves but have the same speed, indicating a new mechanism of gap junction signaling. We demonstrated that antiwaves propagate through small segments of the chain, requiring at least two distinct coupling strengths. Our results suggest that antiwaves are a consequence of the balance between the excitatory and inhibitory coupling strengths of the neurons and can lead to a new paradigm of information processing in the nervous system. Our findings provide important insights into the mechanisms underlying neuronal synchronization and could have significant implications for the development of therapeutic interventions for disorders characterized by improper synchronization, such as epilepsy. 1 into PostgreSQL...\n",
      "Inserting test sample 575  We present a new method for solving symbolically zero--dimensional polynomial equation systems in the affine and toric case. The main feature of our method is the use of problem adapted data structures: arithmetic networks and straight--line programs. For sequential time complexity measured by network size we obtain the following result: it is possible to solve any affine or toric zero--dimensional equation system in non--uniform sequential time which is polynomial in the length of the input description and the ``geometric degree\" of the equation system. Here, the input is thought to be given by a straight--line program (or alternatively in sparse representation), and the length of the input is measured by number of variables, degree of equations and size of the program (or sparsity of the equations). The geometric degree of the input system has to be adequately defined. It is always bounded by the algebraic--combinatoric \"B\\'ezout number\" of the system which is given by the Hilbert function of a suitable homogeneous ideal. However, in many important cases, the value of the geometric degree of the system is much smaller than its B\\'ezout number since this geometric degree does not take into account multiplicities or degrees of extraneous components (which may appear at infinity in the affine case or may be contained in some coordinate hyperplane in the toric case). Our method contains a new application of a classic tool to symbolic computation: we use Newton iteration in order to simplify straight--line programs occurring in elimination procedures. Our new technique allows for practical implementations a meaningful characterization of the intrinsic {\\it algebraic complexity} of typic elimination problems and reduces the still unanswered question of their intrinsic {\\it bit complexity} to 0 into PostgreSQL...\n",
      "Inserting test sample 576  In geometric elimination theory, the use of straight-line programs (SLPs) has proven to be a powerful technique for solving systems of polynomial equations. SLPs are sequences of arithmetic operations and evaluations of fixed polynomials that allow for the efficient computation of polynomial expressions. Specifically, SLPs can be used to determine the affine variety defined by a set of polynomials. While the use of SLPs has been known for some time, recent advances have led to new applications and improved algorithms.\n",
      "\n",
      "One of the key advantages of SLPs is their efficient use of memory and computation time. By limiting the use of intermediate variables and storing only a few key polynomials, SLPs can perform computations with significantly less overhead than other methods. Additionally, SLPs can be implemented in parallel, leading to even greater computational speedups.\n",
      "\n",
      "There are many applications of SLPs in geometric elimination theory. One prominent example is the computation of GrÃ¶bner bases, which form the basis for many algebraic geometric algorithms. SLPs have been used to efficiently compute GrÃ¶bner bases, leading to faster and more scalable algorithms. Other applications of SLPs include the numerical solution of polynomial equations and the computation of resultants and discriminants.\n",
      "\n",
      "Despite their many advantages, SLPs also have limitations. They are only effective in certain cases, and their performance can be sensitive to the choice of polynomials and operations. Additionally, the construction of SLPs can be an NP-hard problem, making their implementation difficult in practice.\n",
      "\n",
      "Overall, SLPs are a powerful and efficient tool in geometric elimination theory. While their use has some limitations and challenges, the recent advances in SLP algorithms are opening up new avenues for their application in algebraic geometry and related fields. 1 into PostgreSQL...\n",
      "Inserting test sample 577  The challenging requirements of large scale quantum information processing using parametric heralded single photon sources involves maximising the interference visibility whilst maintaining an acceptable photon generation rate. By developing a general theoretical framework that allows us to include large numbers of spatial and spectral modes together with linear and non-linear optical elements, we investigate the combined effects of spectral and photon number impurity on the measured Hong--Ou--Mandel interference visibility of parametric photon sources, considering both threshold and number resolving detectors, together with the effects of spectral filtering. We find that for any degree of spectral impurity, increasing the photon generation rate necessarily decreases the interference visibility, even when using number resolving detection. While tight spectral filtering can be used to enforce spectral purity and increased interference visibility at low powers, we find that the induced photon number impurity results in a decreasing interference visibility and heralding efficiency with pump power, while the maximum generation rate is also reduced. 0 into PostgreSQL...\n",
      "Inserting test sample 578  This paper presents a general framework for multimode Gaussian quantum optics and photo-detection, with a specific application to Hong-Ou-Mandel interference using filtered heralded single-photon sources. The proposed model extends the conventional input-output formalism of quantum optics to encompass multiple input and output modes, enabling simulations of complex quantum interference experiments. Our framework is based on the covariance matrix description of Gaussian states and operations. We demonstrate its effectiveness by analyzing the experimental data of a two-photon interference experiment with two different filtered heralded single photon sources. The model successfully predicts the outcome of the experiment and can be extended to include more complex experimental setups. Our approach provides a versatile tool for modeling quantum interference phenomena in multimode systems with Gaussian states and operations, and is expected to have broad applications in quantum information and communication technologies. 1 into PostgreSQL...\n",
      "Inserting test sample 579  Developing efficient and accurate algorithms for chemistry integration is a challenging task due to its strong stiffness and high dimensionality. The current work presents a deep learning-based numerical method called DeepCombustion0.0 to solve stiff ordinary differential equation systems. The homogeneous autoignition of DME/air mixture, including 54 species, is adopted as an example to illustrate the validity and accuracy of the algorithm. The training and testing datasets cover a wide range of temperature, pressure, and mixture conditions between 750-1200 K, 30-50 atm, and equivalence ratio = 0.7-1.5. Both the first-stage low-temperature ignition (LTI) and the second-stage high-temperature ignition (HTI) are considered. The methodology highlights the importance of the adaptive data sampling techniques, power transform preprocessing, and binary deep neural network (DNN) design. By using the adaptive random samplings and appropriate power transforms, smooth submanifolds in the state vector phase space are observed, on which two three-layer DNNs can be appropriately trained. The neural networks are end-to-end, which predict temporal gradients of the state vectors directly. The results show that temporal evolutions predicted by DNN agree well with traditional numerical methods in all state vector dimensions, including temperature, pressure, and species concentrations. Besides, the ignition delay time differences are within 1%. At the same time, the CPU time is reduced by more than 20 times and 200 times compared with the HMTS and VODE method, respectively. The current work demonstrates the enormous potential of applying the deep learning algorithm in chemical kinetics and combustion modeling. 0 into PostgreSQL...\n",
      "Inserting test sample 580  This paper proposes a novel approach for the solution of Ordinary Differential Equations (ODEs) in the context of chemical kinetics. Specifically, we develop a deep learning-based solver which is capable of predicting the time evolution of the chemical species involved in complex reactions.\n",
      "\n",
      "Our solver builds upon recent advances in machine learning and leverages the expressive power of neural networks to learn the underlying dynamics of the system from data. This allows us to tackle challenging problems where traditional numerical methods fail due to their strong assumptions on the properties of the ODEs or the specific form of the reactions.\n",
      "\n",
      "To achieve this goal, we formulate an optimization problem that combines the minimization of a loss function over the training data and the enforcement of the mass conservation constraints. We then use a state-of-the-art optimization algorithm to find the optimal set of parameters for the neural network.\n",
      "\n",
      "Our experiments demonstrate that the proposed method outperforms existing ODE solvers in terms of accuracy and efficiency for a wide range of chemical kinetics problems. Moreover, it is robust to noise and can handle irregularly sampled data.\n",
      "\n",
      "Overall, our deep learning-based ODE solver represents a promising avenue for the solution of complex chemical kinetics problems and opens up new possibilities for the design and optimization of chemical processes. 1 into PostgreSQL...\n",
      "Inserting test sample 581  We study the light generated by spontaneous emission into a mode of a cavity QED system under weak excitation of the orthogonally polarized mode. Operating in the intermediate regime of cavity QED with comparable coherent and decoherent coupling constants, we find an enhancement of the emission into the undriven cavity mode by more than a factor of 18.5 over that expected by the solid angle subtended by the mode. A model that incorporates three atomic levels and two polarization modes quantitatively explains the observations. 0 into PostgreSQL...\n",
      "Inserting test sample 582  We study the spontaneous emission of a quantum emitter in a cavity quantum electrodynamics system. Our analysis shows that the presence of the cavity can significantly enhance the quantum emitter's spontaneous emission into the cavity mode. We further investigate the effects of dipole orientation, detuning, and decoherence on the system's emission properties. Our study provides insights into the fundamental processes of cavity QED and may have practical implications for developing efficient and robust quantum devices. 1 into PostgreSQL...\n",
      "Inserting test sample 583  We consider the density $X_t(x)$ of the critical $(\\alpha,\\beta)$-superprocess in $R^d$ with $\\alpha\\in (0,2)$ and $\\beta<\\frac \\alpha d$. A recent result from PDE implies a dichotomy for the density: for fixed $x$, $X_t(x)>0$ a.s. on $\\{X_t\\neq 0\\}$ if and only if $\\beta \\leq \\beta^*(\\alpha) = \\frac{\\alpha}{d+\\alpha}$. We strengthen this and show that in the continuous density regime, $\\beta < \\beta^*(\\alpha)$ implies that the density function is strictly positive a.s. on $\\{X_t\\neq 0\\}$.\n",
      "\n",
      "We then give close to sharp conditions on a measure $\\mu$ such that $\\mu(X_t):=\\int X_t(x)\\mu(dx)>0$ a.s. on $\\{X_t\\neq 0 \\}$. Our characterization is based on the size of $supp(\\mu)$, in the sense of Hausdorff measure and dimension. For $s \\in [0,d]$, if $\\beta \\leq \\beta^*(\\alpha,s)=\\frac{\\alpha}{d-s+\\alpha}$ and $supp(\\mu)$ has positive $x^s$-Hausdorff measure, then $\\mu(X_t)>0$ a.s. on $\\{X_t\\neq 0\\}$; and when $\\beta > \\beta^*(\\alpha,s)$, if $\\mu$ satisfies a uniform lower density condition which implies $dim(supp(\\mu)) < s$, then $P(\\mu(X_t)=0|X_t\\neq 0)>0$.\n",
      "\n",
      "We also give new result for the fractional PDE $\\partial_t u(t,x) = -(-\\Delta)^{\\alpha/2}u(t,x)-u(t,x)^{1+\\beta}$ with domain $(t,x)\\in (0,\\infty)\\times R^d$. The initial trace of a solution $u_t(x)$ is a pair $(S,\\nu)$, where the singular set $S$ is a closed set around which local integrals of $u_t(x)$ diverge as $t \\to 0$, and $\\nu$ is a Radon measure which gives the limiting behaviour of $u_t(x)$ on $S^c$ as $t\\to 0$. We characterize the existence of solutions with initial trace $(S,0)$ in terms of a parameter called the saturation dimension, $d_{sat}=d+\\alpha(1-\\beta^{-1})$. For $S\\neq R^d$ with $dim(S)> d_{sat}$ (and in some cases $dim(S)=d_{sat}$) we prove that no such solution exists. When $dim(S)<d_{sat}$ and $S$ is the compact support of a measure satisfying a uniform lower density condition, we prove that a solution exists. 0 into PostgreSQL...\n",
      "Inserting test sample 584  This study explores the density properties of the $(\\alpha,\\beta)$-superprocess, a stochastic process that represents the evolution of a population with spatial dependence. In particular, we focus on the role of singular solutions to a fractional non-linear partial differential equation (PDE) that arises in the analysis of this process. We start by presenting the relevant mathematical background, including the definition of the superprocess and the fractional PDE under consideration. \n",
      "\n",
      "We then analyze the singular behavior of this PDE, noting that the standard techniques used to handle non-linear PDEs do not apply due to its fractional nature. Instead, we propose a new method based on function spaces and prove the existence and uniqueness of singular solutions. Next, we study the effect of these singular solutions on the density of the $(\\alpha,\\beta)$-superprocess. We show that under certain conditions, the density is strictly positive almost everywhere and satisfies certain regularity properties. \n",
      "\n",
      "To further illustrate our results, we provide several numerical examples using Monte Carlo simulations. We also investigate the limiting behavior of the density as time goes to infinity, showing that it approaches a steady-state solution that is characterized by the singular solutions of the fractional PDE. Finally, we discuss the implications of our findings for applications in fields like population biology and mathematical finance. \n",
      "\n",
      "In conclusion, this paper presents a rigorous analysis of the density properties of the $(\\alpha,\\beta)$-superprocess and provides new insights into the role of singular solutions to a fractional non-linear PDE. Our results have important implications for the modeling and simulation of complex systems in a variety of fields, and we expect them to stimulate further research in these areas. 1 into PostgreSQL...\n",
      "Inserting test sample 585  The influence of an ac current of arbitrary amplitude and frequency on the mixed-state dc-voltage-ac-drive ratchet response of a superconducting film with a dc current-tilted uniaxial cosine pinning potential at finite temperature is theoretically investigated. The results are obtained in the single-vortex approximation, i.e., for non-interacting vortices, within the frame of an exact solution of the appropriate Langevin equation in terms of a matrix continued fraction. Formulas for the dc voltage ratchet response and absorbed power in ac response are discussed as functions of ac current amplitude and frequency as well as dc current induced tilt in a wide range of corresponding dimensionless parameters. Special attention is paid to the physical interpretation of the obtained results in adiabatic and high-frequency ratchet responses taking into account both running and localized states of the (ac+dc)-driven vortex motion in a washboard pinning potential. Our theoretical results are discussed in comparison with recent experimental work on the high-frequency ratchet response in nanostructured superconducting films [B. B. Jin et al., Phys. Rev. B 81 (2010) 174505]. 0 into PostgreSQL...\n",
      "Inserting test sample 586  In this work, we explore the frequency-dependent ratchet effect that characterizes the behavior of superconducting films subject to a tilted washboard pinning potential. Our findings reveal that this effect arises from the competition between two counteracting forces - the AC driving force and the pinning force - which give rise to a net velocity with a direction that depends on the driving frequency. We demonstrate that the ratchet effect can be significantly enhanced by increasing the tilt angle of the pinning potential, which results in a more pronounced asymmetry in the potential landscape. Furthermore, we show that the ratchet effect is strongly influenced by thermal fluctuations, which can lead to stochastic resonance-like behavior. Our results shed new light on the fundamental physics of superconducting films under external driving, and could have important implications for the development of new types of superconducting devices and applications, such as detectors and energy-efficient computing. 1 into PostgreSQL...\n",
      "Inserting test sample 587  In this work, we study linear error-correcting codes against adversarial insertion-deletion (insdel) errors. We focus on two different settings: Linear codes over small fields: We construct linear codes over $\\mathbb{F}_q$, for $q=\\text{poly}(1/\\varepsilon)$, that can efficiently decode from a $\\delta$ fraction of insdel errors and have rate $(1-4\\delta)/8-\\varepsilon$. We also show that by allowing codes over $\\mathbb{F}_{q^2}$ that are linear over $\\mathbb{F}_q$, we can improve the rate to $(1-\\delta)/4-\\varepsilon$ while not sacrificing efficiency. Using this latter result, we construct fully linear codes over $\\mathbb{F}_2$ that can efficiently correct up to $\\delta < 1/54$ fraction of deletions and have rate $R = (1 - 54 \\delta)/1216$. Cheng, Guruswami, Haeupler, and Li [CGHL21] constructed codes with (extremely small) rates bounded away from zero that can correct up to a $\\delta < 1/400$ fraction of insdel errors. They also posed the problem of constructing linear codes that get close to the half-Singleton bound (proved in [CGHL21]) over small fields. Thus, our results significantly improve their construction and get much closer to the bound.\n",
      "\n",
      "Reed-Solomon codes: We prove that over fields of size $n^{O(k)}$ there are $[n,k]$ Reed-Solomon codes that can decode from $n-2k+1$ insdel errors and hence attain the half-Singleton bound. We also give a deterministic construction of such codes over much larger fields (of size $n^{k^{O(k)}}$).\n",
      "\n",
      "Nevertheless, for $k=O(\\log n /\\log\\log n)$ our construction runs in polynomial time. For the special case $k=2$, which received a lot of attention in the literature, we construct an $[n,2]$ Reed-Solomon code over a field of size $O(n^4)$ that can decode from $n-3$ insdel errors. Earlier construction required an exponential field size. Lastly, we prove that any such construction requires a field of size $\\Omega(n^3)$. 0 into PostgreSQL...\n",
      "Inserting test sample 588  This paper explores the effectiveness of linear and Reed Solomon codes in mitigating the effects of adversarial insertions and deletions in communication systems. Adversarial insertion and deletion attacks are a common type of error in communication systems where an attacker inserts false or deletes existing data, leading to errors in the received data. The paper first provides a detailed overview of the different types of error correction codes and then proceeds to examine the effects of adversarial insertions and deletions on communication systems.\n",
      "\n",
      "The study extensively investigates the mathematical and computational properties of linear codes and Reed Solomon codes and their ability to correct insertion and deletion errors, respectively. The paper uses experimental analysis to evaluate the effectiveness of these codes, and the results show that Reed Solomon codes are more effective in correcting adversarial deletions than linear codes. Moreover, Reed Solomon codes can be efficiently implemented using finite fields.\n",
      "\n",
      "Additionally, the paper discusses some of the challenges associated with the implementation of Reed Solomon codes in communication systems. The authors propose a practical algorithm that can be used to efficiently implement Reed Solomon codes in communication systems. The algorithm provides an optimal balance between communication overhead and error correction performance.\n",
      "\n",
      "The paper also examines the limitations of linear and Reed Solomon codes and suggests some future research directions. The findings indicate that while these codes are effective in mitigating the effects of adversarial insertions and deletions, more sophisticated error correction codes should be used in large-scale communication systems.\n",
      "\n",
      "Overall, this paper presents a comprehensive analysis of the effectiveness of linear and Reed Solomon codes in mitigating adversarial insertion and deletion errors in communication systems. The results of this study provide valuable insights that can help improve the performance of communication systems in the face of adversarial attacks. 1 into PostgreSQL...\n",
      "Inserting test sample 589  A primitive multiple scheme is a complex Cohen-Macaulay scheme $Y$ such that the associated reduced scheme $X=Y_{red}$ is smooth, irreducible, and that $Y$ can be locally embedded in a smooth variety of dimension $\\dim(X)+1$. If $n$ is the multiplicity of $Y$, there is a canonical filtration $X=X_1\\subset X_2\\subset\\cdots\\subset X_n=Y$, such that $X_i$ is a primitive multiple scheme of multiplicity $i$. The simplest example is the trivial primitive multiple scheme of multiplicity $n$ associated to a line bundle $L$ on $X$: it is the $n$-th infinitesimal neighborhood of $X$, embedded if the line bundle $L^*$ by the zero section.\n",
      "\n",
      "Let ${\\bf Z}_n={spec}(C[t]/(t^n))$. The primitive multiple schemes of multiplicity $n$ are obtained by taking an open cover $(U_i)$ of a smooth variety $X$ and by gluing the schemes $U_i\\times{\\bf Z}_n$ using automorphisms of $U_{ij}\\times {\\bf Z}_n$ that leave $U_{ij}$ invariant. This leads to the study of the sheaf of nonabelian groups $G_n$ of automorphisms of $X\\times {\\bf Z}_n$ that leave the $X$ invariant, and to the study of its first cohomology set. If $n\\geq 2$ there is an obstruction to the extension of $X_n$ to a primitive multiple scheme of multiplicity $n+1$, which lies in the second cohomology group $H^2(X,E)$ of a suitable vector bundle $E$ on $X$.\n",
      "\n",
      "In this paper we study these obstructions and the parametrization of primitive multiple schemes. As an example we show that if $X=P_m$ with $m>=3$ all the primitive multiple schemes are trivial. If $X=P_2$, there are only two non trivial primitive multiple schemes, of multiplicities $2$ and $4$, which are not quasi-projective. On the other hand, if $X$ is a projective bundle over a curve, we show that there are infinite sequences $X=X_1\\subset X_2\\subset\\cdots\\subset X_n\\subset X_{n+1}\\subset\\cdots$ of non trivial primitive multiple schemes. 0 into PostgreSQL...\n",
      "Inserting test sample 590  Primitive multiple schemes refer to a specific type of coding theory that has significant applications in many fields, including computer science and telecommunications. The primary purpose of primitive multiple schemes is to provide a reliable way of transmitting information in the presence of errors and other forms of interference.\n",
      "\n",
      "One of the key features of primitive multiple schemes is their ability to correct errors that occur during the transmission process. This is achieved by using a carefully designed code that includes redundant information. When errors occur, the redundant information can be used to correct them, ensuring that the original message is received correctly.\n",
      "\n",
      "Another important aspect of primitive multiple schemes is their efficiency. They are designed to be as compact as possible, making them ideal for use in applications with limited resources. Despite their small size, primitive multiple schemes are highly effective at detecting and correcting errors, making them a popular choice in many different contexts.\n",
      "\n",
      "Research in this field has led to the development of a range of different primitive multiple schemes, each tailored to particular use cases. Some schemes are designed for use in wireless communication systems, while others are more suited to error detection in data storage systems. Recent advances in the field have continued to refine these schemes, improving their accuracy and efficiency.\n",
      "\n",
      "Overall, primitive multiple schemes represent a significant advance in the field of coding theory, providing a reliable and efficient way of transmitting information in the presence of errors. Ongoing research in this area is likely to yield further improvements and refinements to these schemes, making them even more effective for a wide range of applications. 1 into PostgreSQL...\n",
      "Inserting test sample 591  We analyze the effect of a colored non Gaussian noise on a model of a random walker moving along a ratchet potential. Such a model was motivated by the transport properties of motor proteins, like kinesin and myosin. Previous studies have been realized assuming white noises. However, for real situations, in general we could expect that those noises be correlated and non Gaussian.\n",
      "\n",
      "Among other aspects, in addition to a maximum in the current as the noise intensity is varied, we have also found another optimal value of the current when departing from Gaussian behavior. We show the relevant effects that arise when departing from Gaussian behavior, particularly related to current's enhancement, and discuss its relevance for both biological and technological situations. 0 into PostgreSQL...\n",
      "Inserting test sample 592  The behavior of a random walker on a ratchet potential is studied in the presence of non-Gaussian noise in this research paper. The non-Gaussian noise is modeled by a symmetric Î±-stable LÃ©vy process, and the relevance of this choice is discussed. The evolution of the particle's probability density function is derived and analyzed under different conditions, such as varying temperatures and nonlinearity of the potential. Under certain circumstances, the non-Gaussian noise increases the diffusive properties of the random walker, while for others, it decreases them. The results provide insights into the effect of non-Gaussian noise on the dynamics of ratchet potentials, which may be relevant for various fields of research, such as molecular motors and Brownian ratchets. 1 into PostgreSQL...\n",
      "Inserting test sample 593  Using results of Chandra observations of old stellar systems in eleven nearby galaxies of various morphological types and the census of LMXBs in the Milky Way, we study the population of low mass X-ray binaries and their relation to the mass of the host galaxy. We show that the azimuthally averaged spatial distribution of the number of LMXBs and, in the majority of cases, of their collective luminosity closely follows that of the near-infrared light.\n",
      "\n",
      "Considering galaxies as a whole, we find that in a broad mass range, log(M)~9-11.5, the total number of LMXBs and their combined luminosity are proportional to the stellar mass of the host galaxy. Within the accuracy of the light-to-mass conversion, we cannot rule out the possibility of a weak dependence of the X/M ratio on morphological type. However, the effect of such a dependence, if any, does not exceed a factor of ~1.5-2.\n",
      "\n",
      "The luminosity distributions of LMXBs observed in different galaxies are similar to each other and, with the possible exception of NGC1553, are consistent with the average luminosity function derived from all data. The average XLF of LMXBs in nearby galaxies has a complex shape and is significantly different from that of HMXBs. It follows a power law with a differential slope of ~1 at low luminosities, gradually steepens at log(Lx)>37.0-37.5 and has a rather abrupt cut-off at log(Lx)~39.0-39.5. This value of the cut-off luminosity is significantly, by an order of magnitude, lower than found for high mass X-ray binaries. 0 into PostgreSQL...\n",
      "Inserting test sample 594  Low mass X-ray binaries (LMXBs) have been a useful tool in identifying and examining different aspects of their host galaxies. In this study, we investigate the use of LMXBs as a promising indicator for the stellar mass of their host galaxies. We utilized a sample of 38 nearby galaxies, spanning a wide range of morphological types, star formation rates, and metallicities, and extracted their LMXB populations from archival X-ray observations.\n",
      "\n",
      "We found a strong correlation between the total number of LMXBs per unit stellar mass of their host galaxies, indicating that LMXBs could be used as a reliable stellar mass tracer. Our analysis also revealed that the slope of the LMXB-stellar mass relation is consistent with theoretical predictions from binary evolution models, further supporting the use of LMXBs as a mass indicator.\n",
      "\n",
      "Additionally, we investigated the potential impact of different factors, including metallicity and star formation rate, on the LMXB population and its correlation with the stellar mass. We found that while there is some scatter in the LMXB-stellar mass relation, the core relationship remains strong, indicating that LMXBs can serve as a robust stellar mass diagnostic for galaxies.\n",
      "\n",
      "In conclusion, our study highlights the importance of LMXBs in understanding the properties and evolution of their host galaxies and provides a new tool for estimating their stellar masses. These results have important implications for future studies of galaxy formation and evolution and pave the way for further exploration of the use of LMXBs as a mass indicator in the local and distant universe. 1 into PostgreSQL...\n",
      "Inserting test sample 595  We consider a hybrid wireless sensor network with regular and transmit-only sensors. The transmit-only sensors do not have receiver circuit, hence are cheaper and less energy consuming, but their transmissions cannot be coordinated. Regular sensors, also called cluster-heads, are responsible for receiving information from transmit-only sensors and forwarding it to sinks.\n",
      "\n",
      "The main goal of such a hybrid network is to reduce the cost of deployment while achieving some performance constraints (minimum coverage, sensing rate, etc). In this paper we are interested in the communication between transmit-only sensors and cluster-heads. We develop a detailed analytical model of the physical and MAC layer using tools from queuing theory and stochastic geometry. (The MAC model, that we call Erlang's loss model with interference, might be of independent interest as adequate for any non-slotted; i.e., unsynchronized, wireless communication channel.) We give an explicit formula for the frequency of successful packet reception by a cluster-head, given sensors' locations. We further define packet admission policies at a cluster-head, and we calculate the optimal policies for different performance criteria. Finally we show that the proposed hybrid network, using the optimal policies, can achieve substantial cost savings as compared to conventional architectures. 0 into PostgreSQL...\n",
      "Inserting test sample 596  In this paper, we investigate the performance of event-to-sink transport in transmit-only sensor networks. Transmit-only sensor networks are characterized by the absence of feedback from the sink to the sensors, which makes the transport of event information to the sink a challenging task. We present a comprehensive analysis of the performance of existing event-to-sink transport protocols in transmit-only sensor networks. We show that existing protocols exhibit poor performance in terms of end-to-end delay, energy consumption, and reliability. To address these issues, we propose a novel event-to-sink transport protocol, which takes into account the unique characteristics of transmit-only sensor networks. Our protocol combines a lightweight event propagation technique with an energy-efficient routing algorithm to achieve reliable and energy-efficient event delivery to the sink. We evaluate the performance of our protocol through extensive simulations and demonstrate its superiority over existing protocols. Our results show that our protocol outperforms existing protocols in terms of end-to-end delay, energy consumption, and reliability. Overall, our work contributes to a better understanding of the challenges and opportunities of event-to-sink transport in transmit-only sensor networks, and provides a practical solution to improve the performance of such networks. 1 into PostgreSQL...\n",
      "Inserting test sample 597  In this paper, we use two model-independent methods to standardize long gamma-ray bursts (GRBs) using the $E_{\\rm iso}-E_{\\rm p}$ correlation, where $E_{\\rm iso}$ is the isotropic-equivalent gamma-ray energy and $E_{\\rm p}$ is the spectral peak energy. We update 42 long GRBs and try to make constraint on cosmological parameters. The full sample contains 151 long GRBs with redshifts from 0.0331 to 8.2. The first method is the simultaneous fitting method. The extrinsic scatter $\\sigma_{\\rm ext}$ is taken into account and assigned to the parameter $E_{\\rm iso}$. The best-fitting values are $a=49.15\\pm0.26$, $b=1.42\\pm0.11$, $\\sigma_{\\rm ext}=0.34\\pm0.03$ and $\\Omega_m=0.79$ in the flat $\\Lambda$CDM model. The constraint on $\\Omega_m$ is $0.55<\\Omega_m<1$ at the 1$\\sigma$ confidence level. If reduced $\\chi^2$ method is used, the best-fit results are $a=48.96\\pm0.18$, $b=1.52\\pm0.08$ and $\\Omega_m=0.50\\pm0.12$. The second method is using type Ia supernovae (SNe Ia) to calibrate the $E_{\\rm iso}-E_{\\rm p}$ correlation. We calibrate 90 high-redshift GRBs in the redshift range from 1.44 to 8.1. The cosmological constraints from these 90 GRBs are $\\Omega_m=0.23^{+0.06}_{-0.04}$ for flat $\\Lambda$CDM, and $\\Omega_m=0.18\\pm0.11$ and $\\Omega_{\\Lambda}=0.46\\pm0.51$ for non-flat $\\Lambda$CDM. For the combination of GRB and SNe Ia sample, we obtain $\\Omega_m=0.271\\pm0.019$ and $h=0.701\\pm0.002$ for the flat $\\Lambda$CDM, and for the non-flat $\\Lambda$CDM, the results are $\\Omega_m=0.225\\pm0.044$, $\\Omega_{\\Lambda}=0.640\\pm0.082$ and $h=0.698\\pm0.004$. These results from calibrated GRBs are consistent with that of SNe Ia. Meanwhile, the combined data can improve cosmological constraints significantly, comparing to SNe Ia alone. Our results show that the $E_{\\rm iso}-E_{\\rm p}$ correlation is promising to probe the high-redshift universe. 0 into PostgreSQL...\n",
      "Inserting test sample 598  In the pursuit of understanding the mysterious nature of dark energy, gamma-ray bursts (GRBs) have emerged as potential probes. The $E_{\\rm iso}-E_{\\rm p}$ correlation, which relates the isotropic energy $E_{\\rm iso}$ and the peak energy $E_{\\rm p}$ emitted by GRBs, offers a promising avenue to measure dark energy. However, previous studies have been limited by model-dependent assumptions. \n",
      "\n",
      "Here, we present a model-independent approach to extract dark energy constraints from the observed $E_{\\rm iso}-E_{\\rm p}$ relation. First, we compile a sample of GRBs with known redshift measurements, spanning a wide range of redshifts up to $z \\sim 9$. We then utilize Bayesian methods to fit a linear regression model to the data. \n",
      "\n",
      "Our analysis yields a tight correlation between $E_{\\rm iso}$ and $E_{\\rm p}$, with a slope consistent with previous studies. More importantly, we find evidence for dark energy, with a constraint on the dark energy equation-of-state parameter of $w=-1.02^{+0.34}_{-0.31}$ at the 68% confidence level. Our method is robust to several systematic uncertainties, including instrumental, selection, and calibration effects.\n",
      "\n",
      "These results demonstrate the power of GRBs as cosmological probes and highlight the importance of model-independent methods in constraining dark energy. Future studies can build upon our approach by incorporating more complex models or by including additional observables. Overall, our work represents a significant step towards utilizing GRBs to unlock the mysteries of the universe's expansion history and shed light on the nature of dark energy. 1 into PostgreSQL...\n",
      "Inserting test sample 599  We give upper bounds on the genus of a curve with general moduli assuming that it can be embedded in a projective nonsingular surface $Y$ so that $\\dim(|C|) > 0$. We find such bounds for all types of surfaces of intermediate Kodaira dimension and, under mild restrictions, for surfaces of general type whose minimal model $Z$ satisfies the Castelnuovo inequality $K_Z^2 \\ge 3\\chi(\\O_Z) - 10$. In this last case we obtain $g \\le 19$. In the other cases considered the bounds are lower. 0 into PostgreSQL...\n",
      "Inserting test sample 600  In this paper, we explore the properties of general curves on algebraic surfaces. By studying these curves, we gain insight into the structure of surfaces themselves. Our focus is on the behavior of these curves under deformations, and we show how this can be used to classify surfaces up to a certain equivalence. Additionally, we investigate the relationship between the degree of a curve and the geometry of the surface on which it lies. Our results provide a foundation for further study of algebraic surfaces and their properties. 1 into PostgreSQL...\n",
      "Inserting test sample 601  We perform a combined analysis of two stringent constraints on two Higgs doublet model, coming from the recently announced CLEO II bound on $B(b \\rightarrow s \\gamma)$ and $Z \\rightarrow b \\overline b$ and from the recent LEP data on the ratio $\\Gamma(Z\\rightarrow b\\overline b)\\over{\\Gamma(Z\\rightarrow hadrons)}$. We include one-loop vertex corrections to $Z \\rightarrow b \\overline b$ in the model. We find that although the CLEO II bound serves as the strongest constraint present in the charged Higgs sector of the model, the current LEP value for $R_b$ may also provide a further constraint for $\\tan\\beta<1$. 0 into PostgreSQL...\n",
      "Inserting test sample 602  This paper investigates the constraints on the Two Higgs Doublet Model (2HDM) imposed by the decay processes $b\\rightarrow s\\gamma$ and $Z\\rightarrow b\\overline{b}$. We analyze the interplay between the flavor changing neutral currents and the tree-level contributions to the $Zbb$ vertex, paying special attention to the role of the charged Higgs boson. By using current experimental data, we show that severe bounds can be obtained on the parameter space of the 2HDM, particularly on the mass of the charged Higgs particle. Additionally, we compare the results with previous studies on the same topic, providing an improved analysis with updated constraints. 1 into PostgreSQL...\n",
      "Inserting test sample 603  Most comets are volatile-rich bodies that have recently entered the inner solar system following long-term storage in the Kuiper belt and the Oort cloud reservoirs. These reservoirs feed several distinct, short-lived \"small body\" populations. Here, we present new measurements of the optical colors of cometary and comet-related bodies including long-period (Oort cloud) comets, Damocloids (probable inactive nuclei of long-period comets) and Centaurs (recent escapees from the Kuiper belt and precursors to the Jupiter family comets). We combine the new measurements with published data on short-period comets, Jovian Trojans and Kuiper belt objects to examine the color systematics of the comet-related populations. We find that the mean optical colors of the dust in short-period and long-period comets are identical within the uncertainties of measurement, as are the colors of the dust and of the underlying nuclei. These populations show no evidence for scattering by optically-small particles or for compositional gradients, even at the largest distances from the Sun, and no evidence for ultrared matter. Consistent with earlier work, ultrared surfaces are common in the Kuiper belt and on the Centaurs, but not in other small body populations, suggesting that this material is hidden or destroyed upon entry to the inner solar system. The onset of activity in the Centaurs and the disappearance of the ultrared matter in this population begin at about the same perihelion distance ($\\sim$10 AU), suggesting that the two are related. Blanketing of primordial surface materials by the fallback of sub-orbital ejecta, for which we calculate a very short timescale, is the likely mechanism. The same process should operate on any mass-losing body, explaining the absence of ultrared surface material in the entire comet population. 0 into PostgreSQL...\n",
      "Inserting test sample 604  Comets and related bodies encompass a vast array of objects in our Solar System. One of the key aspects of these objects is their often striking and distinct colors, which can provide insight into their composition and formation processes. In this study, we present a comprehensive analysis of the color systematics of comets and related bodies, drawing on observational data from ground-based telescopes and space missions.\n",
      "\n",
      "Our analysis reveals a complex picture of color variations across comets and other objects. We find that many comets and Kuiper Belt Objects exhibit a reddish color, likely due to the presence of complex organic molecules. However, this is not a universal trait, and some objects instead have neutral or even bluish colors.\n",
      "\n",
      "We also explore the relationship between color and other physical characteristics of comets, such as their size and activity levels. We find that smaller, more active comets tend to have bluer colors than larger, less active ones. This suggests that the surface composition and exposure to space weathering processes play a significant role in determining the color of cometary nuclei.\n",
      "\n",
      "Finally, we examine the implications of our findings for our understanding of cometary and Solar System evolution. By providing a more detailed characterization of the color systematics of these objects, we gain new insights into their formation and history. This research provides a framework for future studies of cometary color variability and its relationship to the physical and chemical factors that shape these fascinating bodies.\n",
      "\n",
      "Overall, our study highlights the importance of color as a tool for investigating the properties and origins of comets and related bodies. 1 into PostgreSQL...\n",
      "Inserting test sample 605  The relation between agentive action, knowledge, and obligation is central to the understanding of responsibility --a main topic in Artificial Intelligence.\n",
      "\n",
      "Based on the view that an appropriate formalization of said relation would contribute to the development of ethical AI, we point out the main characteristics of a logic for objective and subjective oughts that was recently introduced in the literature. This logic extends the traditional stit paradigm with deontic and epistemic operators, and provides a semantics that deals with Horty's puzzles for knowledge and obligation. We provide an axiomatization for this logic, and address its soundness and completeness with respect to a class of relevant models. 0 into PostgreSQL...\n",
      "Inserting test sample 606  This paper formalizes a logic of objective and subjective oughts. We create a framework that unifies subjective and objective normative reasoning, addressing shortcomings and limitations in existing formalisms. We introduce two new operators, representing objective and subjective ought, respectively, and define their semantics in terms of other existing modal operators. We prove soundness and completeness results for our formalism, as well as a number of notable meta-theoretic properties. Our approach allows for a nuanced account of normative reasoning that respects different perspectives and values, and provides a flexible tool for reasoning about complex ethical questions in a formal, rigorous way. 1 into PostgreSQL...\n",
      "Inserting test sample 607  Low-power SRAM architectures are especially sensitive to many types of defects that may occur during manufacturing. Among these, resistive defects can appear. This paper analyzes some types of such defects that may impair the device functionalities in subtle ways, depending on the defect characteristics, and that may not be directly or easily detectable by traditional test methods, such as March algorithms. We analyze different methods to test such defects and discuss them in terms of complexity and test time. 0 into PostgreSQL...\n",
      "Inserting test sample 608  This paper compares various solutions for detecting resistive faults in low-power SRAMs. We analyzed and compared the efficiency and effectiveness of three commonly used techniques: Voltage-Shift, Virtual Ground and Dynamic Voltage Drop. Our findings demonstrate that the Dynamic Voltage Drop method identified a higher number of resistive defects in SRAMs in comparison to the other two techniques. Our analysis contributes to the development of advanced testing methodologies for low-power SRAMs, improving the quality, reliability, and performance of these memory devices. 1 into PostgreSQL...\n",
      "Inserting test sample 609  In the context of the Randall-Sundrum (RS) single-brane scenario, we discuss the bulk geometry and dynamics of a cosmological brane in terms of the local energy conservation law which exists for the bulk that allows slicing with a maximally symmetric 3-space. This conservation law enables us to define a local mass in the bulk. We show that there is a unique generalization of the dark radiation on the brane, which is given by the local mass. We find there also exists a conserved current associated with the Weyl tensor, and the corresponding local charge, which we call the Weyl charge, is given by the sum of the local mass and a certain linear combination of the components of the bulk energy-momentum tensor. This expression of the Weyl charge relates the local mass with the projected Weyl tensor, $E_{\\mu\\nu}$, which plays a central role in the geometrical formalism of the RS braneworld. On the brane, in particular, this gives a decomposition of the projected Weyl tensor into the local mass and the bulk energy-momentum tensor. Then, as an application of these results, we consider a null dust model for the bulk energy-momentum tensor and discuss the black hole formation in the bulk. We investigate the causal structure by identifying the locus of the apparent horizon and clarify possible brane trajectories in the bulk. We find that the brane stays always outside the black hole as long as it is expanding. We also find an upper bound on the value of the Hubble parameter in terms of the matter energy density on the brane, irrespective of the energy flux emitted from the brane. 0 into PostgreSQL...\n",
      "Inserting test sample 610  Cosmological braneworlds are theoretical concepts that explore a universe in which the standard model of particle physics can be extended beyond our current understanding. In this context, we investigate the effects of a local conservation law on the existence of dark radiation. We show that the braneworld scenario can naturally account for dark radiation, which could provide insight into the nature of the universe's dark matter. Specifically, we utilize the DGP cosmological model and construct a brane-localized U(1) gauge field theory to incorporate the conservation law and modified gravity. The framework presented provides a better understanding of the evolution of our universe and the impact of dark radiation on the cosmic microwave background radiation (CMB). Our results suggest that the inclusion of the local conservation law significantly alters the temperature and polarization anisotropy spectra of the CMB. Additionally, the growth rate of structure formation in the braneworld universe is affected by the presence of dark radiation, potentially affecting the distribution of galaxies in our observable universe. As the search for dark matter continues to be a major area of study in astrophysics, this research provides a valuable contribution in the ongoing pursuit to unravel the mysteries of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 611  Goulden, Jackson and Vakil observed a polynomial structure underlying one-part double Hurwitz numbers, which enumerate branched covers of $\\mathbb{CP}^1$ with prescribed ramification profile over $\\infty$, a unique preimage over 0, and simple branching elsewhere. This led them to conjecture the existence of moduli spaces and tautological classes whose intersection theory produces an analogue of the celebrated ELSV formula for single Hurwitz numbers.\n",
      "\n",
      "In this paper, we present three formulas that express one-part double Hurwitz numbers as intersection numbers on certain moduli spaces. The first involves Hodge classes on moduli spaces of stable maps to classifying spaces; the second involves Chiodo classes on moduli spaces of spin curves; and the third involves tautological classes on moduli spaces of stable curves. We proceed to discuss the merits of these formulas against a list of desired properties enunciated by Goulden, Jackson and Vakil. Our formulas lead to non-trivial relations between tautological intersection numbers on moduli spaces of stable curves and hints at further structure underlying Chiodo classes. The paper concludes with generalisations of our results to the context of spin Hurwitz numbers. 0 into PostgreSQL...\n",
      "Inserting test sample 612  Double Hurwitz numbers are a central object in algebraic geometry and have connections to many fields in mathematics, including knot theory and the study of integrable systems. The Goulden-Jackson-Vakil conjecture proposes a formula for these numbers in terms of certain symmetric functions. In this paper, we provide new evidence towards this conjecture by constructing a family of meromorphic differentials on the moduli space of Riemann surfaces with marked points. Our approach involves studying higher order residues of certain meromorphic differentials on the moduli space and relating them to geometric intersection numbers. We further generalize our constructions to include a larger class of symmetric polynomials, which opens up a path towards proving the Goulden-Jackson-Vakil conjecture in the future. Our methods provide a deeper understanding of the geometry and combinatorics underlying the computation of Hurwitz numbers and have potential applications in areas such as topological recursion and enumerative geometry. 1 into PostgreSQL...\n",
      "Inserting test sample 613  A soft X-ray enhancement has recently been reported toward the high-velocity cloud MS30.7-81.4-118 (MS30.7), a constituent of the Magellanic Stream. In order to investigate the origin of this enhancement, we have analyzed two overlapping XMM-Newton observations of this cloud. We find that the X-ray enhancement is $\\sim$6' or $\\sim$100 pc across, and is concentrated to the north and west of the densest part of the cloud. We modeled the X-ray enhancement with a variety of spectral models. A single-temperature equilibrium plasma model yields a temperature of $(3.69^{+0.47}_{-0.44}) \\times 10^6$ K and a 0.4-2.0 keV luminosity of $7.9 \\times 10^{33}$ erg s$^{-1}$. However, this model underpredicts the on-enhancement emission around 1 keV, which may indicate the additional presence of hotter plasma ($T \\gtrsim 10^7$ K), or that recombination emission is important. We examined several different physical models for the origin of the X-ray enhancement. We find that turbulent mixing of cold cloud material with hot ambient material, compression or shock heating of a hot ambient medium, and charge exchange reactions between cloud atoms and ions in a hot ambient medium all lead to emission that is too faint. In addition, shock heating in a cool or warm medium leads to emission that is too soft (for reasonable cloud speeds). We find that magnetic reconnection could plausibly power the observed X-ray emission, but resistive magnetohydrodynamical simulations are needed to test this hypothesis. If magnetic reconnection is responsible for the X-ray enhancement, the observed spectral properties could potentially constrain the magnetic field in the vicinity of the Magellanic Stream. 0 into PostgreSQL...\n",
      "Inserting test sample 614  The high-velocity cloud MS30.7-81.4-118 has been the subject of study for many years due to its high degree of ionization and X-ray emission. However, the origin of this emission has remained a mystery. In this paper, we present a comprehensive analysis of the X-ray emission from MS30.7-81.4-118 and propose a possible explanation for its origin.\n",
      "\n",
      "Using data obtained from the Chandra X-ray observatory, we found that the X-ray emission from MS30.7-81.4-118 is highly concentrated and extends over a relatively small area. We also observed a significant correlation between the X-ray and H-alpha emissions. These findings suggest that the X-ray emission is likely due to collisional ionization resulting from shocks in the interfaces between the high-velocity gas cloud and the surrounding medium.\n",
      "\n",
      "We further analyzed the X-ray spectra from different regions of the cloud and found evidence of both thermal and non-thermal components. This indicates that the origin of the X-ray emission is likely due to a combination of shock heating and cosmic-ray acceleration.\n",
      "\n",
      "Our study provides important insights into the mechanisms responsible for the X-ray emission in high-velocity clouds. Future observations and modeling efforts will be needed to further refine our understanding of these processes and their role in shaping the evolution of galaxies and clusters. In summary, our results constitute a significant step forward in unraveling the mystery of the X-ray emission from MS30.7-81.4-118. 1 into PostgreSQL...\n",
      "Inserting test sample 615  Estimating dynamic treatment regimes (DTRs) from retrospective observational data is challenging as some degree of unmeasured confounding is often expected.\n",
      "\n",
      "In this work, we develop a framework of estimating properly defined \"optimal\" DTRs with a time-varying instrumental variable (IV) when unmeasured covariates confound the treatment and outcome, rendering the potential outcome distributions only partially identified. We derive a novel Bellman equation under partial identification, use it to define a generic class of estimands (termed IV-optimal DTRs), and study the associated estimation problem. We then extend the IV-optimality framework to tackle the policy improvement problem, delivering IV-improved DTRs that are guaranteed to perform no worse and potentially better than a pre-specified baseline DTR. Importantly, our IV-improvement framework opens up the possibility of strictly improving upon DTRs that are optimal under the no unmeasured confounding assumption (NUCA). We demonstrate via extensive simulations the superior performance of IV-optimal and IV-improved DTRs over the DTRs that are optimal only under the NUCA. In a real data example, we embed retrospective observational registry data into a natural, two-stage experiment with noncompliance using a time-varying IV and estimate useful IV-optimal DTRs that assign mothers to high-level or low-level neonatal intensive care units based on their prognostic variables. 0 into PostgreSQL...\n",
      "Inserting test sample 616  Dynamic treatment regimes (DTRs) are individualized strategies for selecting treatments that optimize health outcomes over time. However, estimating DTRs with time-varying instrumental variables (TVIVs) is currently a challenge in causal inference. In this study, we propose a new method for estimating and improving DTRs using TVIVs.\n",
      "\n",
      "We introduce an algorithm that sequentially constructs DTRs by utilizing TVIVs to estimate the effects of treatments on outcomes. We also propose a statistical model that captures the time-varying effects of treatments and TVIVs on outcomes.\n",
      "\n",
      "Through simulations and real-world applications, we demonstrate that our method outperforms existing methods in terms of both estimation accuracy and DTR performance. Our method can effectively handle TVIVs that change over time and can account for time-varying confounders.\n",
      "\n",
      "Our findings have important implications for the development of personalized medicine. By enabling the estimation of DTRs using TVIVs, our method allows clinicians to make individualized treatment decisions based on a patient's unique clinical history and characteristics. Overall, our approach provides a novel and effective solution to the challenge of estimating and improving DTRs using TVIVs. 1 into PostgreSQL...\n",
      "Inserting test sample 617  Information security awareness (ISA) is a practice focused on the set of skills, which help a user successfully mitigate a social engineering attack.\n",
      "\n",
      "Previous studies have presented various methods for evaluating the ISA of both PC and mobile users. These methods rely primarily on subjective data sources such as interviews, surveys, and questionnaires that are influenced by human interpretation and sincerity. Furthermore, previous methods for evaluating ISA did not address the differences between classes of social engineering attacks.\n",
      "\n",
      "In this paper, we present a novel framework designed for evaluating the ISA of smartphone users to specific social engineering attack classes. In addition to questionnaires, the proposed framework utilizes objective data sources: a mobile agent and a network traffic monitor; both of which are used to analyze the actual behavior of users. We empirically evaluated the ISA scores assessed from the three data sources (namely, the questionnaires, mobile agent, and network traffic monitor) by conducting a long-term user study involving 162 smartphone users. All participants were exposed to four different security challenges that resemble real-life social engineering attacks. These challenges were used to assess the ability of the proposed framework to derive a relevant ISA score. The results of our experiment show that: (1) the self-reported behavior of the users differs significantly from their actual behavior; and (2) ISA scores derived from data collected by the mobile agent or the network traffic monitor are highly correlated with the users' success in mitigating social engineering attacks. 0 into PostgreSQL...\n",
      "Inserting test sample 618  The increase of smartphones usage and their connection to the Internet make them popular targets for cybercriminals. Evaluating the information security awareness of smartphone users as a preventive measure is, therefore, necessary to avoid personal or corporate data breaches. This study presents the results of an exploratory research that aimed to evaluate the information security awareness of smartphone users in a sample of 500 individuals from different age ranges and professions. A questionnaire instrument was created to assess the participantsâ€™ knowledge on data protection, identity theft, phishing attacks, and secure password practices. The findings reveal that 75% of the participants have a basic understanding of personal data security measures, while 25% prove to be lacking in such knowledge. The group with the lowest scores was that of individuals aged 18-24, and the highest scores belonged to the group aged 55 and older. These results suggest a need for improvements in information security education, specifically among young adults, to increase their awareness of information security on their smartphones. It is recommended that smartphone manufacturers include more interactive tutorials on security measures, and that educational institutions make information security a crucial component of their curriculums to help raise awareness and improve the implementation of security practices. 1 into PostgreSQL...\n",
      "Inserting test sample 619  We mapped the kinetic temperature structure of two massive star-forming regions, N113 and N159W, in the Large Magellanic Cloud (LMC). We have used $\\sim$1\\hbox{$\\,.\\!\\!^{\\prime\\prime}$}6\\,($\\sim$0.4\\,pc) resolution measurements of the para-H$_2$CO\\,$J_{\\rm K_ aK_c}$\\,=\\,3$_{03}$--2$_{02}$, 3$_{22}$--2$_{21}$, and 3$_{21}$--2$_{20}$ transitions near 218.5\\,GHz to constrain RADEX non-LTE models of the physical conditions. The gas kinetic temperatures derived from the para-H$_2$CO line ratios 3$_{22}$--2$_{21}$/3$_{03}$--2$_{02}$ and 3$_{21}$--2$_{20}$/3$_{03}$--2$_{02}$ range from 28 to 105\\,K in N113 and 29 to 68\\,K in N159W. Distributions of the dense gas traced by para-H$_2$CO agree with those of the 1.3\\,mm dust and \\emph{Spitzer}\\,8.0\\,$\\mu$m emission, but do not significantly correlate with the H$\\alpha$ emission. The high kinetic temperatures ($T_{\\rm kin}$\\,$\\gtrsim$\\,50\\,K) of the dense gas traced by para-H$_2$CO appear to be correlated with the embedded infrared sources inside the clouds and/or YSOs in the N113 and N159W regions. The lower temperatures ($T_{\\rm kin}$\\,$<$\\,50\\,K) are measured at the outskirts of the H$_2$CO-bearing distributions of both N113 and N159W. It seems that the kinetic temperatures of the dense gas traced by para-H$_2$CO are weakly affected by the external sources of the H$\\alpha$ emission. The non-thermal velocity dispersions of para-H$_2$CO are well correlated with the gas kinetic temperatures in the N113 region, implying that the higher kinetic temperature traced by para-H$_2$CO is related to turbulence on a $\\sim$0.4\\,pc scale. The dense gas heating appears to be dominated by internal star formation activity, radiation, and/or turbulence. It seems that the mechanism heating the dense gas of the star-forming regions in the LMC is consistent with that in Galactic massive star-forming regions located in the Galactic plane. 0 into PostgreSQL...\n",
      "Inserting test sample 620  This study reports the measurement of kinetic temperature in massive molecular clumps associated with stars formation. The molecular clumps N113 and N159W in the Large Magellanic Cloud (LMC) were observed with the Atacama Large Millimeter/submillimeter Array (ALMA) using formaldehyde as a thermometer. The formaldehyde measurements were done on the 410 GHz and 405 GHz transitions, which corresponded to the formaldehyde IV (para-H2CO) molecule. The observations were made in several positions within each molecular clump.\n",
      "\n",
      "The kinetic temperature was calculated from the excitation temperatures of the formaldehyde transitions. It was found to range between 25 K to 40 K in N113 and 20 K to 45 K in N159W. These temperatures are higher than the dust temperature previously measured for each molecular clump, suggesting that there may be additional mechanisms heating the gas. It was also observed that the formaldehyde emission was stronger in regions of lower dust temperature, indicating that formaldehyde may trace the outer layers of the molecular clumps.\n",
      "\n",
      "The measurements of the kinetic temperature are important for understanding the physical properties of massive molecular clumps. These regions are potential sites for massive star formation, which can have a significant impact on their surrounding environment. The findings of this study suggest that the kinetic temperature in the molecular clumps is more complex than previously thought, and that the internal mechanisms heating the gas need to be further investigated. This study also demonstrates the capabilities of ALMA to measure the kinetic temperature of molecular clumps associated with star formation, paving the way for further studies of this kind. 1 into PostgreSQL...\n",
      "Inserting test sample 621  (abridged) NGC 1569 is a nearby dwarf irregular galaxy which underwent an intense burst of star formation 10 to 40 Myr ago. We present observations that reach surface brightnesses two to eighty times fainter than previous radio continuum observations and the first radio continuum polarization observations.\n",
      "\n",
      "These observations allow us to probe the relationship of the magnetic field of NGC 1569 to the rest of its interstellar medium. We confirm the presence of an extended radio continuum halo at 20 cm and see for the first time the radio continuum feature associated with the western Halpha arm at wavelengths shorter than 20cm. The spectral index trends in this galaxy support the theory that there is a convective wind at work in this galaxy. We derive a total magnetic field strength of 38 microG in the central regions and 10-15 microG in the halo. The magnetic field is largely random in the center of the galaxy; the uniform field is ~3-9 microG and is strongest in the halo. We find that the magnetic pressure is the same order of magnitude but, in general, a factor of a few less than the other components of the interstellar medium in this galaxy.\n",
      "\n",
      "The uniform magnetic field in NGC 1569 is closely associated with the Halpha bubbles and filaments. We suggest that a supernova-driven dynamo may be operating in this galaxy. The outflow of hot gas from NGC 1569 is clearly shaping the magnetic field, but the magnetic field in turn may be aiding the outflow by channeling gas out of the disk of the galaxy. Dwarf galaxies with extended radio continuum halos like that of NGC 1569 may play an important role in magnetizing the intergalactic medium. 0 into PostgreSQL...\n",
      "Inserting test sample 622  The magnetic field plays a fundamental role in shaping the properties and dynamics of galactic interstellar media (ISM). In this paper, we investigate the intricate connection between the magnetic field and the ISM of the post-starburst dwarf irregular galaxy NGC 1569. To achieve this, we use data from the Karl G. Jansky Very Large Array (JVLA) and a set of simulations that involve the induction of a magnetic field into a turbulent medium. Our results reveal that the magnetic field in NGC 1569 is strong and ordered, with a strength of approximately 150 microgauss. We trace the magnetic geometry in NGC 1569 by analyzing the polarized radio continuum emission in high resolution. Furthermore, our observations indicate the presence of a weakly polarized filamentary structure that suggests the existence of magnetic reconnection. Our simulations show that a large-scale coherent magnetic field can emerge from a turbulent medium on timescales comparable to those of the starburst. Thus, magnetic field generation can be intrinsic to the evolution of the ISM in galaxies with high star formation rates. Finally, we explore the influence of the magnetic field on the gas dynamics in NGC 1569 using the plasma beta parameter. Our results suggest that the magnetic energy density is comparable to or greater than the thermal energy density, making the magnetic field an important contributor to the evolution of the ISM in NGC 1569. 1 into PostgreSQL...\n",
      "Inserting test sample 623  Deep inelastic scattering (DIS) total cross section data at small-x as measured by the HERA experiments is well described by Balitsky-Kovchegov (BK) evolution in the leading order dipole picture. Recently the full Next-to-Leading Order (NLO) dipole picture total cross sections have become available for DIS, and a working factorization scheme has been devised which subtracts the soft gluon divergence present at NLO. We report our recently published work in which we make the first comparisons of the NLO DIS total cross sections to HERA data. The non-perturbative initial condition to BK evolution is fixed by fitting the HERA reduced cross section data. As the NLO results for the DIS total cross section are currently available only in the massless quark limit, we also fit a light-quark-only cross section constructed with a parametrization of published total and heavy quark data. We find an excellent description of the HERA data. Since the full NLO BK equation is computationally expensive, we use a number of beyond LO prescriptions for the evolution that include most important higher order corrections enhanced by large transverse logarithms, including the recent version of the equation formulated in terms of the target momentum fraction. 0 into PostgreSQL...\n",
      "Inserting test sample 624  In this paper, we investigate the dipole model at next-to-leading order and compare it with the latest data from the HERA experiment. A thorough analysis is performed on the structure function F2, which is crucial for understanding the deep-inelastic scattering (DIS) process. We employ the NLO calculation of the dipole model to compute the structure function, which is then compared with the HERA data. Our results show that the NLO dipole model accurately describes the HERA data and is capable of reproducing the main features of the data within the experimental uncertainties. We also investigate the impact of different sources of error on the comparison, such as the choice of the factorization and renormalization scales, and the parametrization of the parton distribution functions. Finally, we examine the sensitivity of the results to the choice of the dipole model input parameters. Overall, our results provide a solid foundation for future studies on the dipole model and its relevance to the HERA data, and suggest interesting avenues for theoretical and experimental research. 1 into PostgreSQL...\n",
      "Inserting test sample 625  Recently, convolutional neural networks (CNNs) have achieved great improvements in single image dehazing and attained much attention in research.\n",
      "\n",
      "Most existing learning-based dehazing methods are not fully end-to-end, which still follow the traditional dehazing procedure: first estimate the medium transmission and the atmospheric light, then recover the haze-free image based on the atmospheric scattering model. However, in practice, due to lack of priors and constraints, it is hard to precisely estimate these intermediate parameters. Inaccurate estimation further degrades the performance of dehazing, resulting in artifacts, color distortion and insufficient haze removal. To address this, we propose a fully end-to-end Generative Adversarial Networks with Fusion-discriminator (FD-GAN) for image dehazing. With the proposed Fusion-discriminator which takes frequency information as additional priors, our model can generator more natural and realistic dehazed images with less color distortion and fewer artifacts. Moreover, we synthesize a large-scale training dataset including various indoor and outdoor hazy images to boost the performance and we reveal that for learning-based dehazing methods, the performance is strictly influenced by the training data. Experiments have shown that our method reaches state-of-the-art performance on both public synthetic datasets and real-world images with more visually pleasing dehazed results. 0 into PostgreSQL...\n",
      "Inserting test sample 626  In this paper, we introduce a novel generative adversarial network architecture, called FD-GAN, to address the challenging task of single image dehazing. The proposed approach consists of a fusion-discriminator that is designed to exploit multi-scale features, both within and across scales, to better discriminate between hazy and haze-free images. To this end, we use a multi-scale feature extraction method that uses both local and global contextual information. Additionally, we propose a new loss function that combines both adversarial and perceptual losses to better guide the training of our FD-GAN model. Our experimental results demonstrate that the proposed FD-GAN significantly outperforms the state-of-the-art methods both quantitatively and qualitatively on various benchmark datasets, including NYU, RESIDE, and HIDE. Furthermore, we conduct extensive ablation studies to evaluate the effectiveness of each component of our FD-GAN architecture. Our findings show that the proposed approach can effectively remove haze from single images and can achieve high-quality dehazing results. As such, we believe that our method has the potential to be applied to various real-world application scenarios, including surveillance, automotive, and unmanned aerial vehicle systems. 1 into PostgreSQL...\n",
      "Inserting test sample 627  A set $Z$ of vertices of a graph $G$ is a zero forcing set of $G$ if initially labeling all vertices in $Z$ with $1$ and all remaining vertices of $G$ with $0$, and then, iteratively and as long as possible, changing the label of some vertex $u$ from $0$ to $1$ if $u$ is the only neighbor with label $0$ of some vertex with label $1$, results in the entire vertex set of $G$. The zero forcing number $Z(G)$, defined as the minimum order of a zero forcing set of $G$, was proposed as an upper bound of the corank of matrices associated with $G$, and was also considered in connection with quantum physics and logic circuits. In view of the computational hardness of the zero forcing number, upper and lower bounds are of interest.\n",
      "\n",
      "Refining results of Amos, Caro, Davila, and Pepper, we show that $Z(G)\\leq \\frac{\\Delta-2}{\\Delta-1}n$ for a connected graph $G$ of order $n$ and maximum degree $\\Delta$ at least $3$ if and only if $G$ does not belong to $\\{ K_{\\Delta+1},K_{\\Delta,\\Delta},K_{\\Delta-1,\\Delta},G_1,G_2\\}$, where $G_1$ and $G_2$ are two specific graphs of orders $5$ and $7$, respectively. For a connected graph $G$ of order $n$, maximum degree $3$, and girth at least $5$, we show $Z(G)\\leq \\frac{n}{2}-\\Omega\\left(\\frac{n}{\\log n}\\right)$. Using a probabilistic argument, we show $Z(G)\\leq \\left(1-\\frac{H_r}{r}+o\\left(\\frac{H_r}{r}\\right)\\right)n$ for an $r$-regular graph $G$ of order $n$ and girth at least $5$, where $H_r$ is the $r$-th harmonic number. Finally, we show $Z(G)\\geq (g-2)(\\delta-2)+2$ for a graph $G$ of girth $g\\in \\{ 5,6\\}$ and minimum degree $\\delta$, which partially confirms a conjecture of Davila and Kenter. 0 into PostgreSQL...\n",
      "Inserting test sample 628  Zero forcing is a well-known and extensively studied process in graph theory. The zero forcing number of a graph, denoted by Z(G), is the minimum number of initially colored vertices required such that by applying the zero forcing process, the entire vertex set can be colored eventually. In this paper, we examine some bounds on the zero forcing number of a graph.\n",
      "\n",
      "Firstly, we study the effect of deleting vertices on the zero forcing number of a graph. We show that deleting a vertex of a graph can either result in a decrease or an increase of the zero forcing number of the graph. Specifically, we provide upper and lower bounds on the zero forcing number of a graph after the deletion of a vertex.\n",
      "\n",
      "Next, we investigate the relationship between the zero forcing number and several other graph parameters such as the maximum degree, the minimum degree, and the diameter. In particular, we provide a lower bound on the zero forcing number of a graph in terms of the diameter.\n",
      "\n",
      "Moreover, we analyze the zero forcing number of several families of graphs such as paths, cycles, and trees. For paths, we provide exact formulas for the zero forcing number and we prove that the zero forcing number of a cycle is at most half of the number of vertices. Furthermore, for trees, we present an algorithm to compute the zero forcing number of a tree in linear time.\n",
      "\n",
      "Finally, we prove a few results related to the zero forcing number of planar graphs. In particular, we show that the zero forcing number of any planar graph with maximum degree at least 3 is at most 3.\n",
      "\n",
      "Overall, in this paper, we provide various bounds on the zero forcing number of a graph and investigate its relationships with other important graph parameters. Our results shed insight into the behavior of the zero forcing number of different types of graphs and can be useful in various applications. 1 into PostgreSQL...\n",
      "Inserting test sample 629  Using high resolution N-body simulations we address the problem of emptiness of giant 20 Mpc/h diameter voids found in the distribution of bright galaxies.\n",
      "\n",
      "Are the voids filled by dwarf galaxies? Do cosmological models predict too many small dark matter haloes inside the voids? Can the problems of cosmological models on small scales be addressed by studying the abundance of dwarf galaxies inside voids? We find that voids in the distribution of 10^12 Msun/h haloes (expected galactic magnitudes ~ M_*) are almost the same as the voids in 10^11 Msun/h haloes. Yet, much smaller haloes with masses 10^9 Msun/h and circular velocities v_circ about 20 km/s readily fill the voids: there should be almost 1000 of these haloes in a 20 Mpc/h void. A typical void of diameter 20 Mpc/h contains about 50 haloes with v_circ > 50 km/s. The haloes are arranged in a pattern, which looks like a miniature Universe: it has the same structural elements as the large-scale structure of the galactic distribution of the Universe. There are filaments and voids; larger haloes are at the intersections of filaments. The only difference is that all masses are four orders of magnitude smaller. There is severe (anti)bias in the distribution of haloes, which depends on halo mass and on the distance from the centre of the void.\n",
      "\n",
      "Large haloes are more antibiased and have a tendency to form close to void boundaries. The mass function of haloes in voids is different from the ``normal'' mass function. It is much steeper for high masses resulting in very few M33-type galaxies (v_circ about 100 km/s). We present an analytical approximation for the mass function of haloes in voids. 0 into PostgreSQL...\n",
      "Inserting test sample 630  The study of the structure and properties of voids is crucial in several scientific and engineering fields. In this research paper, we examine voids' structure from both physical and mathematical perspectives. We begin by discussing different types of voids and their fundamental properties, including shape, size, and distribution. Then, we investigate the mechanisms of voids' formation and interaction with the surrounding material, including the effects of external factors such as temperature and pressure.\n",
      "\n",
      "Furthermore, we analyze the behavior of voids in different materials and media, including liquids, gases, and solids. We explore the impact of voids on mechanical, electrical, and thermal properties of materials, and their roles in several industrial applications. We also examine recent advancements in techniques for void imaging and characterization, such as scanning electron microscopy, X-ray tomography, and ultrasound.\n",
      "\n",
      "In addition, this research paper presents mathematical models for void structure analysis, including statistical methods for quantifying void distribution, shape analysis, and fractal modeling. We discuss the limitations and advantages of different models and compare their results with experimental data from various sources.\n",
      "\n",
      "Overall, this work provides a comprehensive overview of the structure of voids and its importance in scientific and engineering fields. It presents a theoretical and experimental basis for understanding void structure and behavior, and provides a foundation for developing new materials and technologies with improved performance and reliability. 1 into PostgreSQL...\n",
      "Inserting test sample 631  Propagation characteristics of a wave are defined by the dispersion relationship, from which the governing partial differential equation (PDE) can be recovered. PDEs are commonly solved numerically using the finite-difference (FD) method, with stencils constructed from truncated Taylor series expansions which, whilst typically providing good approximation of the PDE in the space-time domain, often differ considerably from the original partial differential in the wavenumber-frequency domain where the dispersion relationship is defined. Consequentially, stable, high-order FD schemes may not necessarily result in realistic wave behavior, commonly exhibiting numerical dispersion: lagging high-frequency components as a product of discretization. A method for optimizing FD stencil weightings via constrained minimization to better approximate the partial derivative in the wavenumber domain is proposed, allowing for accurate propagation with coarser grids than would be otherwise possible. This was applied to second derivatives on a standard grid and first derivatives on a staggered grid. To evaluate the efficacy of the method, a pair of numerical simulations were devised to compare spatially-optimized stencils with conventional formulations of equivalent extent. A spatially-optimized formulation of the 1D acoustic wave equation with Dirichlet boundary conditions is presented, evaluating performance at a range of grid spacings, examining the interval between the theoretical maximum grid spacings for the conventional and optimized schemes in finer detail. The optimized scheme was found to offer superior performance for undersampled wavefields and heavily oversampled wavefields. Staggered-grid first derivative stencils were then applied to the P-SV elastic wave formulation, simulating seismic wave propagation for a two-layer, water-over-rock model. 0 into PostgreSQL...\n",
      "Inserting test sample 632  The accurate numerical simulation of wave propagation is crucial in predicting seismic properties and imaging underground structures. However, numerical dispersion phenomena can severely affect the quality of seismic images and the reliability of quantitative analysis. The application of finite-difference schemes is an efficient and widely used technique for modeling wave equations, but it suffers from numerical dispersion, which causes artificial frequency-dependent attenuation and phase distortion, particularly at high frequencies and steep waves. Therefore, it is of great practical interest to design spatially optimized finite-difference schemes that can effectively suppress numerical dispersion and reduce computational cost. This study presents an extensive investigation of various spatial optimization techniques, such as optimized higher-order spatial operators and optimized grid spacing, and their effects on dispersion suppression, accuracy, stability, and computational efficiency in seismic modeling. The numerical experiments show that the optimized schemes can significantly reduce numerical dispersion, yield higher accuracy, and improve stability, especially when coupled with high-order time-stepping algorithms, for a range of seismic scenarios. Furthermore, the optimized schemes can reduce the total number of grid points required to achieve a given level of accuracy, thus reducing computational cost without sacrificing quality. This research provides a practical guidance for seismic modeling and other applications that rely on high-accuracy wave propagation simulation. 1 into PostgreSQL...\n",
      "Inserting test sample 633  We study the turbulence induced in the dust layer of a protoplanetary disk based on the energetics of dust accretion due to gas drag. We estimate turbulence strength from the energy supplied by dust accretion, using the radial drift velocity of the dust particles in a laminar disk. Our estimate of the turbulence strength agrees with previous analytical and numerical research on the turbulence induced by Kelvin-Helmholtz and/or streaming instabilities for particles whose stopping time is less than the Keplerian time. For such small particles, the strongest turbulence is expected to occur when the dust-to-gas ratio of the disk is ~C_eff^(1/2) (h_g / r) ~ 10^(-2), where C_eff ~ 0.2 represents the energy supply efficiency to turbulence and h_g / r ~ 5 x 10^(-2) is the aspect ratio of the gas disk. The maximum viscosity parameter is alpha_max ~ C_eff T_s (h_g / r)^2 ~ 10^(-4) T_s, where T_s (<1) is the non-dimensional stopping time of the dust particles. Modification in the dust-to-gas ratio from the standard value, 10^(-2), by any process, results in weaker turbulence and a thinner dust layer, and consequently may accelerate the growth process of the dust particles. 0 into PostgreSQL...\n",
      "Inserting test sample 634  This study analyzes the density structure of the dust layer in protoplanetary disk systems, focusing on the effect of induced turbulence. Through numerical simulations, we investigate the interaction between the turbulence and the dust layer and find that turbulence can induce significant density fluctuations. Our findings highlight the importance of turbulence in shaping the structure of the dust layer and the evolving protoplanetary disk. Specifically, we observe that the density distribution in the disk can vary greatly depending on the strength and scale of the turbulence. Furthermore, we analyze the radial dependence of the density structure and find that it is influenced by the location of the dust grain size distribution. Our results have potential implications for understanding the formation of planetesimals and the distribution of materials in protoplanetary disks. Overall, this study advances our understanding of the dynamics and structure of protoplanetary disks, shedding light on the complex processes that give rise to planetary systems. 1 into PostgreSQL...\n",
      "Inserting test sample 635  The hitting time, h_uv, of a random walk on a finite graph G, is the expected time for the walk to reach vertex v given that it started at vertex u. We present two methods of calculating the hitting time between vertices of finite graphs, along with applications to specific classes of graphs, including grids, trees, and the 'tadpole' graphs. 0 into PostgreSQL...\n",
      "Inserting test sample 636  This paper presents a method for finding hitting times in a range of graphs. The technique is based on a combination of random walks and linear algebra and allows for quick and accurate computation of this important graph parameter. Results show that the proposed method performs well on a variety of graph types and can be easily implemented in practice. 1 into PostgreSQL...\n",
      "Inserting test sample 637  A mobile Ad-hoc network (MANET) is a dynamic multi hop wireless network established by a group of nodes in which there is no central administration.\n",
      "\n",
      "Due to mobility of nodes and dynamic network topology, the routing is one of the most important challenges in ad-hoc networks. Several routing algorithms for MANETs have been proposed by the researchers which have been classified into various categories, however, the most prominent categories are proactive, reactive and hybrid. The performance comparison of routing protocols for MANETs has been presented by other researcher also, however, none of these works considers proactive, reactive and hybrid protocols together. In this paper, the performance of proactive (DSDV), reactive (DSR and AODV) and hybrid (ZRP) routing protocols has been compared. The performance differentials are analyzed on the basis of throughput, average delay, routing overhead and number of packets dropped with a variation of number of nodes, pause time and mobility. 0 into PostgreSQL...\n",
      "Inserting test sample 638  A Mobile Ad Hoc Network (MANET) is a self-configuring network of mobile devices, which can operate without the existence of any fixed infrastructure. Routing is an essential component of MANET. This paper presents a comprehensive performance evaluation of three different routing protocols (Proactive, Reactive, and Hybrid). The objective of this research is to provide a clear understanding of the pros and cons of each protocol in various scenarios. The performance of the protocols will be measured using metrics such as end-to-end delay, packet delivery ratio, and network throughput. This paper compares the different protocols using the NS-3 network simulator. The simulation results show that each protocol has its own strengths and weaknesses. The Proactive protocol is suitable for scenarios where the network topology changes slowly, whereas Reactive protocol performs better in dynamic topologies. The Hybrid protocol merges both the Proactive and Reactive protocols and is suitable for networks that have intermittent connectivity and frequently changing topologies. 1 into PostgreSQL...\n",
      "Inserting test sample 639  Nekrashevych conjectured that the iterated monodromy groups of quadratic polynomials with preperiodic critical orbit have intermediate growth. We illustrate some of the difficulties that arise in attacking this conjecture and prove subexponential growth for the iterated monodromy group of $z^2+i$. This is the first non-trivial example supporting the conjecture. 0 into PostgreSQL...\n",
      "Inserting test sample 640  This paper studies the growth of iterated monodromy groups, which are algebraic objects that describe symmetries of geometric structures. We present computational results on the growth rates of these groups in various examples, and provide theoretical explanations for their behavior. Our findings shed light on the structure and complexity of these important mathematical objects. 1 into PostgreSQL...\n",
      "Inserting test sample 641  We present the results of a pilot project designed to study the distribution of dark matter haloes out to very large radii in spiral galaxies. As dynamical probe we use their rotation curves and the motions population of satellite galaxies. In this pilot stage, we observed seven late-type spiral galaxies of about the same luminosity M_R ~ -22 (and approximately the same mass). We investigate the kinematics of these galaxies, and the radial and angular distribution of their satellites. Using VIMOS at the VLT, we carried out a spectroscopic survey in seven 14' x 14' fields each around a late-type isolated spiral galaxy. We obtained radial velocities and spatial distributions for 77 candidate satellites. After removing the interlopers, we are left with 61 true satellites. In combination with the rotation curves of the primary galaxies, satellites are used to probe the gravitational potential of the primaries and derive the dark matter halo properties by means of standard mass modeling techniques. We find (a) that the dark matter haloes of luminous spirals (M_R ~ -22) have virial radii of ~400 kpc and virial masses of 3.5 x 10^12 Msun; (b) that the radial velocity and angular distributions of the satellites around the primaries are isotropic; and (c) that the resulting mass distribution is in good agreement with that found in the optical regions of spirals and described by the universal rotation curve of spirals once extrapolated to large radii.\n",
      "\n",
      "The results obtained in this pilot phase of the project are already interesting and limited only by small number statistics. The full project involving an order of magnitude more targets, would very likely provide us with a definitive picture of the dark matter distribution around spirals out to their virial radii and beyond. 0 into PostgreSQL...\n",
      "Inserting test sample 642  The nature of dark matter, which makes up a significant part of the matter in the universe, remains a mystery. The study of the properties of dark matter halos around spiral galaxies has become a topic of intense research interest, as it can provide valuable insights into the distribution and evolution of dark matter. However, current techniques to probe dark matter halos are limited, especially for regions beyond the central galaxies.\n",
      "\n",
      "In this study, we propose using satellite kinematics to explore dark matter halos of spiral galaxies at poorly explored distances. By studying the movement of satellite galaxies around a spiral galaxy, we can infer the gravitational potential of the dark matter halo within which the satellites are located. This method can provide us with information on the structure and extent of the dark matter halo, including its mass and density profile.\n",
      "\n",
      "We present a case study in which we applied this technique to a sample of spiral galaxies at different distances. Our analysis reveals that the dark matter halos of these galaxies are more extended than previously thought, with significant amounts of dark matter beyond the optical radius of the central galaxy. Additionally, we find evidence for substructure in the dark matter distribution, which may be indicative of past or ongoing galaxy mergers.\n",
      "\n",
      "Our results demonstrate the potential of satellite kinematics as a powerful tool for exploring the properties of dark matter halos of spiral galaxies at poorly explored distances. This technique can be extended to larger samples of galaxies, providing valuable insights into the nature and evolution of dark matter on cosmological scales. 1 into PostgreSQL...\n",
      "Inserting test sample 643  We present the generalized Reissner-Nordstr\\\"om solution of the field equations of metric-affine gravity (MAG), endowed with electric and magnetic charges, as well as with gravito-electric and gravito-magnetic charges and a cosmological constant term. Moreover, the case $M=e_o$, i.e. mass equal to electric charge and $\\lambda=0$, corresponds to an electrically and magnetically charged monopole. Also further multipole solutions are obtained.\n",
      "\n",
      "The charge assignments of the solutions is discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 644  This paper presents a generalized Reissner-Nordstr\\\"om (RN) solution within the context of Metric-Affine Gravity (MAG). This solution is derived using the variational principle of MAG, which allows for a nonminimal coupling between matter and geometry. The electric charge and mass parameters play a crucial role in the RN solution and are shown to have a direct impact on the causal structure of the geometry. Insights from this work can be applied to the study of black hole thermodynamics and cosmology. 1 into PostgreSQL...\n",
      "Inserting test sample 645  The changes of a turbulent channel flow subjected to oscillations of wall flush-mounted rigid discs are studied by means of direct numerical simulations.\n",
      "\n",
      "The Reynolds number is $R_\\tau$=$180$, based on the friction velocity of the stationary-wall case and the half channel height. The primary effect of the wall forcing is the sustained reduction of wall-shear stress, which reaches a maximum of 20%. A parametric study on the disc diameter, maximum tip velocity, and oscillation period is presented, with the aim to identify the optimal parameters which guarantee maximum drag reduction and maximum net energy saving, computed by taking into account the power spent to actuate the discs.\n",
      "\n",
      "This may be positive and reaches 6%.\n",
      "\n",
      "The Rosenblat viscous pump flow is used to predict the power spent for disc motion in the turbulent channel flow and to estimate localized and transient regions over the disc surface subjected to the turbulent regenerative braking effect, for which the wall turbulence exerts work on the discs.\n",
      "\n",
      "The FIK identity is employed to show that the wall-friction reduction is due to two distinguished effects. One effect is linked to the direct shearing action of the near-wall oscillating disc boundary layer on the wall turbulence, which causes the attenuation of the turbulent Reynolds stresses. The other effect is due the additional disc-flow Reynolds stresses produced by the inter-disc structures.\n",
      "\n",
      "The contribution to drag reduction due to turbulent Reynolds stress attenuation depends on the penetration thickness of the disc-flow boundary layer, while the contribution due to the elongated structures scales linearly with a simple function of the maximum tip velocity and oscillation period for the largest disc diameter tested, a result suggested by the Rosenblat flow solution. A brief discussion on the future applicability of the oscillating-disc technique is also presented. 0 into PostgreSQL...\n",
      "Inserting test sample 646  This study presents an investigation of the turbulent drag reduction phenomenon through oscillating discs. The oscillation of parallel discs, positioned perpendicular to the direction of fluid flow, generates a time-dependent behavior of the velocity and pressure fields around them. The aim of this study is to explore the reduction of drag with varying amplitude and frequency of the oscillation and to identify the optimal conditions of oscillation for maximal drag reduction.\n",
      "\n",
      "The experimental setup consists of two identical parallel discs that are oscillated with a specified frequency and amplitude, while a fluid flows across them. Pressure taps attached to the walls of the test section measure the pressure distribution along the flow direction. A hot wire anemometer measures the time-averaged and root-mean-square velocity components of the fluid, downstream of the oscillating discs.\n",
      "\n",
      "The results indicate that oscillating the discs considerably affects the fluid flow behavior in the test section, causing a decrease in turbulent kinetic energy, Reynolds stresses and drag forces over a substantial range of frequencies and amplitudes. The greatest drag reduction was achieved for oscillation amplitudes of 6-9% of the inter-disc gap and frequencies between 20-40 Hz.\n",
      "\n",
      "Several mechanisms underlying the drag reduction phenomenon were identified, including the impairment of vortex shedding, the alignment of the flow structures with the oscillation pattern, and the damping of turbulent fluctuations due to the oscillation. Furthermore, the results demonstrated that the optimal conditions of oscillation for drag reduction were related to the ratio of the frequency to the fluid velocity and were independent of the Reynolds number within the tested range.\n",
      "\n",
      "Overall, this study shows that oscillating parallel discs can significantly enhance the performance of fluid systems in terms of drag reduction, and provides useful insights for practical applications of the technique in various fields of engineering. 1 into PostgreSQL...\n",
      "Inserting test sample 647  {Abridged version for ArXiv}. We provide direct constraints on the origin of the [Ne II] emission in 15 young stars using high-spatial and spectral resolution observations with VISIR at the VLT that allow us to study the kinematics of the emitting gas. In addition we compare the [Ne II] line with optical forbidden lines observed for three stars with UVES. The [Ne II] line was detected in 7 stars, among them the first confirmed detection of [Ne II] in a Herbig Be star, V892 Tau. In four cases, the large blueshifted lines indicate an origin in a jet. In two stars, the small shifts and asymmetric profiles indicate an origin in a photo-evaporative wind. CoKu Tau 1, seen close to edge-on, shows a spatially unresolved line centered at the stellar rest velocity, although cross-dispersion centroids move within 10 AU from one side of the star to the other as a function of wavelength. The line profile is symmetric with wings extending up to about +-80 km/s. The origin of the [Ne II] line could either be due to the bipolar jet or to the disk. For the stars with VLT-UVES observations, in several cases, the optical forbidden line profiles and shifts are very similar to the profile of the [Ne II] line, suggesting that the lines are emitted in the same region. A general trend observed with VISIR is a lower line flux when compared with the fluxes obtained with Spitzer. We found no correlation between the line full-width at half maximum and the line peak velocity. The [Ne II] line remains undetected in a large part of the sample, an indication that the emission detected with Spitzer in those stars is likely extended. 0 into PostgreSQL...\n",
      "Inserting test sample 648  This study investigates the origin of [Ne II] emission in young stars through a combination of mid-infrared and optical observations with the Very Large Telescope. The [Ne II] emission line is a prominent feature in the spectra of young stellar objects, and its origin is still not fully understood. To shed light on this issue, we observed a sample of young stars in the Orion Nebula Cluster and in the NGC 2024 star-forming region, using the VISIR and FORS2 instruments at the VLT. Our mid-infrared observations with VISIR allowed us to study the spatial distribution of [Ne II] emission and its correlation with other emission lines, while our optical observations with FORS2 provided spectra with higher spatial resolution and revealed the kinematics of the gas. We found that [Ne II] emission is associated with outflows and photoevaporative winds from the disks of young stars, with some contribution from shocks in the protostellar environment. Our results suggest that the [Ne II] emission can be used as a powerful diagnostic tool to study the formation and evolution of young stars, as well as their interaction with the surrounding interstellar medium. Moreover, our study demonstrates the capabilities of the VLT in probing the mid-infrared and optical properties of young stars and their environments, and highlights the potential of future facilities such as the James Webb Space Telescope to complement and extend these observations. 1 into PostgreSQL...\n",
      "Inserting test sample 649  Spectroscopic surveys of massive galaxy clusters reveal the properties of faint background galaxies, thanks to the magnification provided by strong gravitational lensing. We present a systematic analysis of integral-field-spectroscopy observations of 12 massive clusters, conducted with the Multi Unit Spectroscopic Explorer (MUSE). All data were taken under very good seeing conditions (0.6\") in effective exposure times between two and 15 hrs per pointing, for a total of 125 hrs. Our observations cover a total solid angle of ~23 arcmin$^2$ in the direction of clusters, many of which were previously studied by the MACS, Frontier Fields, GLASS and CLASH programs. The achieved emission line detection limit at 5$\\sigma$ for a point source varies between (0.77--1.5)$\\times$10$^{-18}$ erg\\,s$^{-1}$\\,cm$^{-2}$ at 7000\\AA. We present our developed strategy to reduce these observational data, detect sources and determine their redshifts. We construct robust mass models for each cluster to further confirm our redshift measurements using strong-lensing constraints, and identify a total of 312 strongly lensed sources producing 939 multiple images. The final redshift catalogs contain more than 3300 robust redshifts, of which 40\\% are for cluster members and $\\sim$30\\% for lensed Lyman-$\\alpha$ emitters. 14\\% of all sources are line emitters not seen in the available HST images, even at the depth of the FFs ($\\sim29$ AB). We find that the magnification distribution of the lensed sources in the high-magnification regime ($\\mu{=}$ 2--25) follows the theoretical expectation of $N(z)\\propto\\mu^{-2}$. The quality of this dataset, number of lensed sources, and number of strong-lensing constraints enables detailed studies of the physical properties of both the lensing cluster and the background galaxies.\n",
      "\n",
      "The full data products from this work are made available to the community.\n",
      "\n",
      "[abridged] 0 into PostgreSQL...\n",
      "Inserting test sample 650  This paper presents an atlas of MUSE observations towards twelve massive lensing clusters. The main goal of this study is to investigate the properties of the clusters and their constituent galaxies, as well as their collective effect on distant background sources. The MUSE instrument provides high-resolution spectroscopy of the clusters, allowing us to measure the kinematics, chemical compositions and ionization states of the gas and stars within them. In addition, we used gravitational lensing to probe the properties of the clusters' dark matter halos and to map the distribution of their substructures. We detected numerous emission-line galaxies within the clusters, allowing us to study their star formation histories and the properties of their interstellar medium. We also investigated the morphological and kinematic properties of the galaxies, revealing evidence of tidal interactions and mergers within the cluster environment. Our analysis shows that the clusters are dynamically complex systems, with a range of substructures that are likely related to their assembly histories. Additionally, we find that the clusters' dark matter halos extend beyond their luminous components, suggesting that they are still in the process of forming. Overall, this atlas provides a unique dataset that will be useful for a wide range of studies, including galaxy evolution, cosmic structure formation, and the properties of dark matter. 1 into PostgreSQL...\n",
      "Inserting test sample 651  Open inflation scenario is attracting a renewed interest in the context of string landscape. Since there are a large number of metastable de Sitter vacua in string landscape, tunneling transitions to lower metastable vacua through the bubble nucleation occur quite naturally. Although the deviation of Omega_0 from unity is small by the observational bound, we argue that the effect of this small deviation on the large angle CMB anisotropies can be significant for tensor-type perturbation in open inflation scenario. We consider the situation in which there is a large hierarchy between the energy scale of the quantum tunneling and that of the slow-roll inflation in the nucleated bubble. If the potential just after tunneling is steep enough, a rapid-roll phase appears before the slow-roll inflation. In this case the power spectrum is basically determined by the Hubble rate during the slow-roll inflation. If such rapid-roll phase is absent, the power spectrum keeps the memory of the high energy density there in the large angular components. The amplitude of large angular components can be enhanced due to the effects of the wall fluctuation mode if the bubble wall tension is small. Therefore, one can construct some models in which the deviation of Omega_0 from unity is large enough to produce measurable effects. We also consider a more general class of models, where the false vacuum decay may occur due to Hawking-Moss tunneling, as well as the models involving more than one scalar field. We discuss scalar perturbations in these models and point out that a large set of such models is already ruled out by observational data, unless there was a very long stage of slow-roll inflation after the tunneling. These results show that observational data allow us to test various assumptions concerning the structure of the string theory potentials and the duration of the last stage of inflation. 0 into PostgreSQL...\n",
      "Inserting test sample 652  Inflationary cosmology is a theoretical framework used to describe the rapid expansion of the early universe. One variation of this theory is \"open inflation,\" which posits that our universe is one of many \"bubbles\" within a larger \"multiverse.\" In this paper, we explore the implications of open inflation in the context of the landscape of string theory.\n",
      "\n",
      "The landscape of string theory refers to the vast array of possible vacuum states that can result from the compactification of extra dimensions. Each of these states corresponds to a different physical universe, with varying properties such as the number of dimensions, the strength of fundamental forces, and the masses of elementary particles. Within the landscape, some states may be favored over others due to their ability to support stable structures such as galaxies and stars.\n",
      "\n",
      "We investigate how open inflation may be realized within the landscape, focusing on the dynamics of bubble nucleation and inflationary expansion. Our analysis suggests that open inflation may be a generic feature of the landscape, with bubbles forming spontaneously and rapidly expanding to create new universes. Moreover, we find that the properties of the newly created universes are heavily influenced by the surrounding landscape, with some regions giving rise to universes that are more hospitable to life than others.\n",
      "\n",
      "Finally, we discuss the observational implications of open inflation in the landscape. We demonstrate that this theory makes specific predictions for the distribution of cosmic microwave background radiation and the large-scale structure of the universe, which can be tested through future observations. Our results suggest that the landscape of string theory may offer new insights into the fundamental nature of the universe and the origins of inflation. 1 into PostgreSQL...\n",
      "Inserting test sample 653  In celestial conformal field theory, gluons are represented by primary fields with dimensions $\\Delta=1+i\\lambda$, $\\lambda\\in\\mathbb{R}$ and spin $J=\\pm 1$, in the adjoint representation of the gauge group. All two- and three-point correlation functions of these fields are zero as a consequence of four-dimensional kinematic constraints. Four-point correlation functions contain delta-function singularities enforcing planarity of four-particle scattering events. We relax these constraints by taking a shadow transform of one field and perform conformal block decomposition of the corresponding correlators. We compute the conformal block coefficients. When decomposed in channels that are \"compatible\" in two and four dimensions, such four-point correlators contain conformal blocks of primary fields with dimensions $\\Delta=2+M+i\\lambda$, where $M\\ge 0$ is an integer, with integer spin $J=-M,-M+2,\\dots,M-2,M$. They appear in all gauge group representations obtained from a tensor product of two adjoint representations. When decomposed in incompatible channels, they also contain primary fields with continuous complex spin, but with positive integer dimensions. 0 into PostgreSQL...\n",
      "Inserting test sample 654  This paper investigates the correspondence between celestial amplitudes and conformal blocks in a gauge theory. We explore the relationship between these two seemingly distinct objects and show that they can be interconnected through a Fourier transformation. Our analysis provides a useful tool for studying the behavior of quantum field theories on the celestial sphere. We illustrate these ideas by considering gluon amplitudes and their associated conformal blocks in detail. In particular, we focus on the computation of the four-point gluon amplitude and its corresponding conformal block at the level of the cubic interaction vertices. Our results show that the conformal block has a simple form, characterized by a sum over poles which match precisely those appearing in the gluon amplitude. This remarkable agreement highlights the close connection between celestial amplitudes and conformal blocks, offering new insights into the structure of gauge theories. 1 into PostgreSQL...\n",
      "Inserting test sample 655  The aim of this paper is to start the study of multilinear generalizations of the classical ideals of linear operators of type $p$ and cotype $q$. As a first step in a theory we believe will be long and fruitful, we propose a notion of type and cotype of multilinear operators and the resulting classes of such mappings are studied in the setting of the theory of Banach/quasi-Banach ideals of multilinear operators. Distinctions between the linear and the multilinear theories are pointed out, typical multilinear features of the theory are emphasized and many illustrative examples are provided. The classes we introduce are related to the multi-ideals generated by the linear ideals of operators of some type/cotype and are proved to be maximal and Aron-Berner stable. 0 into PostgreSQL...\n",
      "Inserting test sample 656  In this paper, the theory of multilinear operators is studied in Banach spaces. Specifically, we investigate the notions of type and cotype of such operators and relate them to the properties of the underlying spaces. Our main results show that certain classes of Banach spaces have optimal type and cotype properties, establishing a connection between the regularity of the spaces and the behavior of multilinear operators. Additionally, we study the relationship between the dual of a space and the type and cotype of its multilinear operators. Our methods involve a combination of functional analysis, geometric measure theory, and harmonic analysis. The insights provided by this study have important applications in various fields, including signal processing and machine learning. 1 into PostgreSQL...\n",
      "Inserting test sample 657  In recent years, the utilization of rotating parts, e.g. bearings and gears, has been continuously supporting the manufacturing line to produce consistent output quality. Due to their critical role, the breakdown of these components might significantly impact the production rate. A proper condition based monitoring (CBM) is among a few ways to maintain and monitor the rotating systems. Prognosis, as one of the major tasks in CBM that predicts and estimates the remaining useful life of the machine, has attracted significant interest in decades. This paper presents a literature review on prognosis approaches from published papers in the last decade. The prognostic approaches are described comprehensively to provide a better idea on how to select an appropriate prognosis method for specific needs. An advanced predictive analytics, namely Parsimonious Network Based on Fuzzy Inference System (PANFIS), was proposed and tested into the low speed slew bearing data. PANFIS differs itself from conventional prognostic approaches in which it supports for online lifelong prognostics without the requirement of retraining or reconfiguration phase. The method is applied to normal-to-failure bearing vibration data collected for 139 days and to predict the time-domain features of vibration slew bearing signals. The performance of the proposed method is compared to some established methods such as ANFIS, eTS, and Simp_eTS. From the results, it is suggested that PANFIS offers outstanding performance compared to those of other methods. 0 into PostgreSQL...\n",
      "Inserting test sample 658  This paper proposes a Parsimonious Network based on Fuzzy Inference System (PANFIS) for predicting time series features in the context of low speed slew bearing prognosis. Slew bearings are critical components of large machinery and failure of these bearings can lead to significant downtime and repair costs. Thus, early and accurate prognosis of potential failures is crucial.\n",
      "\n",
      "The proposed PANFIS model utilizes a combination of fuzzy logic and neural networks to capture the complex relationships between various input variables and the time series features of the low speed slew bearings. The model is designed to minimize the number of parameters required, thus making it parsimonious and efficient to use.\n",
      "\n",
      "To evaluate the efficacy of our proposed model, we perform experiments on a real-world dataset from the bearing vibration analysis used in rotating machinery. The results show that the PANFIS model outperforms both traditional statistical methods and other commonly used models in time series prediction. In particular, our model achieves high prediction accuracy while requiring fewer parameters, which means that it can operate efficiently in real-time applications.\n",
      "\n",
      "Overall, the proposed PANFIS model has practical implications for low speed slew bearing prognosis in industrial systems. Its ability to accurately predict various time series features and reduce unnecessary parameterization can lead to more informed decision-making and cost savings in maintenance and operations. 1 into PostgreSQL...\n",
      "Inserting test sample 659  Our objective is to demonstrate an inconsistency with both the original and modern Everettian Many Worlds Interpretations. We do this by examining two important corollaries of the universally valid quantum mechanics in the context of the Quantum Brownian Motion (QBM) model: \"Entanglement Relativity\" and the \"parallel occurrence of decoherence.\" We conclude that the highlighted inconsistency demands that either there is a privileged spatial structure of the QBM model universe or that the Everettian Worlds are not physically real. 0 into PostgreSQL...\n",
      "Inserting test sample 660  This paper explores the implications of the Everett interpretation of quantum mechanics on the quantum structures of a model-universe. By analyzing the assumptions and predictions of the Everett interpretation, we question its viability as an accurate representation of quantum reality. Through extensive theoretical and mathematical analysis, we propose an alternative understanding of quantum phenomena that challenges the current scientific discourse. Our findings highlight the need for continued research on the nature of quantum mechanics and its implications for our understanding of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 661  Superoxide reductase (SOR) is an Fe protein that catalyzes the reduction of superoxide to give H(2)O(2). Recently, the mutation of the Glu47 residue into alanine (E47A) in the active site of SOR from Desulfoarculus baarsii has allowed the stabilization of an iron-peroxo species when quickly reacted with H(2)O(2) [Math{\\'e} et al. (2002) J. Am. Chem. Soc. 124, 4966-4967]. To further investigate this non-heme peroxo-iron species, we have carried out a M{\\\"o}ssbauer study of the (57)Fe-enriched E47A SOR from D. baarsii reacted quickly with H(2)O(2). Considering the M{\\\"o}ssbauer data, we conclude, in conjunction with the other spectroscopic data available and with the results of density functional calculations on related models, that this species corresponds to a high-spin side-on peroxo-Fe(3+) complex. This is one of the first examples of such a species in a biological system for which M{\\\"o}ssbauer parameters are now available: delta(/Fe) = 0.54 (1) mm/s, DeltaE(Q) = -0.80 (5) mm/s, and the asymmetry parameter eta = 0.60 (5) mm/s. The M{\\\"o}ssbauer and spin Hamiltonian parameters have been evaluated on a model from the side-on peroxo complex (model 2) issued from the oxidized iron center in SOR from Pyrococcus furiosus, for which structural data are available in the literature [Yeh et al. (2000) Biochemistry 39, 2499-2508]. For comparison, similar calculations have been carried out on a model derived from 2 (model 3), where the [CH(3)-S](1)(-) group has been replaced by the neutral [NH(3)](0) group [Neese and Solomon (1998) J. Am. Chem. Soc. 120, 12829-12848]. Both models 2 and 3 contain a formally high-spin Fe(3+) ion (i.e., with empty minority spin orbitals). We found, however, a significant fraction (approximately 0.6 for 2, approximately 0.8 for 3) of spin (equivalently charge) spread over two occupied (minority spin) orbitals. The quadrupole splitting value for 2 is found to be negative and matches quite well the experimental value. The computed quadrupole tensors are rhombic in the case of 2 and axial in the case of 3. This difference originates directly from the presence of the thiolate ligand in 2. A correlation between experimental isomer shifts for Fe(3+) mononuclear complexes with computed electron densities at the iron nucleus has been built and used to evaluate the isomer shift values for 2 and 3 (0.56 and 0.63 mm/s, respectively). A significant increase of isomer shift value is found upon going from a methylthiolate to a nitrogen ligand for the Fe(3+) ion, consistent with covalency effects due to the presence of the axial thiolate ligand. Considering that the isomer shift value for 3 is likely to be in the 0.61-0.65 mm/s range [Horner et al. (2002) Eur. J. Inorg. Chem., 3278-3283], the isomer shift value for a high-spin eta(2)-O(2) Fe(3+) complex with an axial thiolate group can be estimated to be in the 0.54-0.58 mm/s range. The occurrence of a side-on peroxo intermediate in SOR is discussed in relation to the recent data published for a side-on peroxo-Fe(3+) species in another biological system [Karlsson et al.\n",
      "\n",
      "(2003) Science 299, 1039-1042]. 0 into PostgreSQL...\n",
      "Inserting test sample 662  In this study, we focus on the M{\\\"o}ssbauer characterization of an unusual high-spin side-on peroxo-Fe3+ species in the active site of superoxide reductase from Desulfoarculus Baarsii and complement our findings with density functional calculations on related models. Our investigation sheds light on the structure and properties of this particular peroxo-Fe3+ species, with implications for elucidating the mechanism of superoxide reduction by this enzyme. \n",
      "\n",
      "Our M{\\\"o}ssbauer spectroscopy measurements reveal the presence of a high-spin Fe3+ center in the peroxo species, which exhibits an unusual side-on binding mode. This finding is supported by density functional theory (DFT) calculations, which predict a high-spin, side-on peroxo-Fe3+ species as the most stable configuration. Our theoretical calculation of the M{\\\"o}ssbauer spectrum of the proposed side-on peroxo-Fe3+ species shows good agreement with the experimental spectrum. \n",
      "\n",
      "Additionally, we investigate the effect of various ligands on the stability and electronic properties of the peroxo species using DFT. Our calculations indicate that specific ligands, such as imidazole, can have a significant impact on the electronic structure of the peroxo-Fe3+ center. We also investigate the reactivity of the peroxo species towards superoxide, using DFT to calculate the activation barriers for the reaction. Our results suggest that the high-spin, side-on peroxo-Fe3+ species is a likely intermediate for the reduction of superoxide by the enzyme. \n",
      "\n",
      "Overall, our study provides insight into the nature of the peroxo-Fe3+ species in superoxide reductase from Desulfoarculus Baarsii, and sheds light on the mechanism of superoxide reduction by this enzyme. The combination of experimental M{\\\"o}ssbauer spectroscopy and theoretical calculations using DFT allows us to probe the electronic and structural properties of the peroxo species in a detailed and comprehensive manner. Our findings may have broader implications for the design and optimization of metalloenzymes for biotechnological applications. 1 into PostgreSQL...\n",
      "Inserting test sample 663  Applicative functors are a generalisation of monads. Both allow the expression of effectful computations into an otherwise pure language, like Haskell. Applicative functors are to be preferred to monads when the structure of a computation is fixed a priori. That makes it possible to perform certain kinds of static analysis on applicative values. We define a notion of free applicative functor, prove that it satisfies the appropriate laws, and that the construction is left adjoint to a suitable forgetful functor. We show how free applicative functors can be used to implement embedded DSLs which can be statically analysed. 0 into PostgreSQL...\n",
      "Inserting test sample 664  Free applicative functors are a powerful tool in functional programming. They allow for composition of purely applicative structures without the need for monadic bind. This enables greater code reuse and makes it easier to reason about complex programs. In this paper, we explore the theoretical underpinnings of free applicative functors and provide practical use cases in real-world code. Additionally, we show how they can be implemented in various programming languages. Our findings demonstrate that free applicative functors provide a valuable addition to the toolbox of any functional programmer, with potential to simplify code and improve application design. 1 into PostgreSQL...\n",
      "Inserting test sample 665  The disorder parameter of confinement-deconfinement phase transition based on the monopole action determined previously in $SU(2)$ QCD are investigated. We construct an operator which corresponds to the order parameter defined in the abelian Higgs model. The operator shows proper behaviors as the disorder parameter in the numerical simulations of finite temperature QCD. 0 into PostgreSQL...\n",
      "Inserting test sample 666  The disorder parameter is an important characteristic of confinement in physics. It describes the degree to which particles are confined to a certain region of space, or the extent to which their motion is restricted. In this paper, we investigate the disorder parameter for different types of confinement, including electromagnetic, gravitational, and entropic confinement. Our results have implications for the study of condensed matter physics and cosmology. 1 into PostgreSQL...\n",
      "Inserting test sample 667  In contrast to the previous reports that the divalent perovskite SrCrO$_3$ was believed to be cubic structure and nonmagnetic metal, recent measurements suggest coexistence of majority tetragonally distorted weak antiferromagnetic phase and minority nonmagnetic cubic phase. Within the local (spin) density approximation (L(S)DA) our calculations confirm that a slightly tetragonally distorted phase indeed is energetically favored. Using the correlated band theory method (LDA+ Hubbard U) as seems to be justified by the unusual behavior observed in SrCrO$_3$, above the critical value $U_c$=4 eV only the distorted phase undergoes an orbital-ordering transition, resulting in $t_{2g}^2 --> d_{xy}^1$($d_{xz}d_{yz}$)$^1$ corresponding to the filling of the $d_{xy}$ orbital but leaving the other two degenerate. The Fermi surfaces of the cubic phase are simple with nesting features, although the nesting wavevectors do not correlate with known data. This is not uncommon in perovskites; the strongly directional d-d bonding often leads to boxlike Fermi surfaces, and either the nesting is not strong enough, or the matrix elements are not large enough, to promote instabilities. Fixed spin moment calculations indicate the cubic structure is just beyond a ferromagnetic Stoner instability (IN(0)~1.1) in L(S)DA, and that the energy is unusually weakly dependent on the moment out to 1.5$\\mu_B$/Cr (varying only by 11 meV/Cr), reflecting low energy long-wavelength magnetic fluctuations. We observe that this system shows strong magneto-phonon coupling (change in Cr local moment is ~7.3 $\\mu_B$/\\AA) for breathing phonon modes. 0 into PostgreSQL...\n",
      "Inserting test sample 668  In this study, we investigate the structural properties of metallic SrCrO3 using first-principles density functional theory calculations. We find that orbital ordering plays a crucial role in the observed structural distortion that occurs in SrCrO3, which is characterized by a rotation of octahedra. By analyzing the electronic structure of SrCrO3, we identify the Cr 3d electrons as the source of the orbital ordering. Our results suggest that the distortion is driven by a Jahn-Teller instability, where the electrons occupy particular atomic orbitals resulting in a lower symmetry. The distortion has significant implications for the magnetism and electronic properties of SrCrO3. We find that the ferromagnetic behavior predicted in cubic SrCrO3 is suppressed in the distorted structure, and a nonmagnetic ground state is stabilized. This result is consistent with experimental observations, where SrCrO3 is typically found to be in a nonmagnetic state. Our findings provide insights into the interplay between the crystal structure and electronic properties of SrCrO3, and highlight the role of orbital ordering in driving structural distortion in transition metal oxides. This work has implications for the design and development of new materials with tailored properties based on electronic instabilities, with potential applications in spintronics and catalysis. 1 into PostgreSQL...\n",
      "Inserting test sample 669  Using the Green's function of the 3D heat equation, we develop an analytical account of the thermal behaviour of superconducting films subjected to electrical currents larger than their critical current in the absence of an applied magnetic field. Our model assumes homogeneity of films and current density, and besides thermal coefficients employs parameters obtained by fitting to experimental electrical field - current density characteristics at constant bath temperature. We derive both a tractable dynamic equation for the real temperature of the film up to the supercritical current density J^\\ast (the lowest current density inducing transition to the normal state), and a thermal stability criterion that allows prediction of J^\\ast . For two typical YBCO films, J^\\ast predictions agree with observations to within 5%. These findings strongly support the hypothesis that a current-induced thermal instability is generally the origin of the breakdown of superconductivity under high electrical current densities, at least at temperatures not too far from Tc. 0 into PostgreSQL...\n",
      "Inserting test sample 670  In this paper, we present an analytical investigation of the thermal instability of superconducting thin films when subjected to high current densities. The phenomenon is described by a coupled set of partial differential equations that model the behavior of the temperature and the current distribution in the film. We consider both the idealized case of zero magnetic field, and the more realistic scenario where flux trapping effects must be taken into account. By using a linear stability analysis, we identify the most unstable mode of the resulting eigenvalue problem and calculate its growth rate as a function of the relevant parameters. Our results show that the instability is mainly driven by the combination of a sharp increase in the resistivity and a decrease in the thermal conductivity at high temperatures. These findings shed new light on the origin of the thermal breakdown observed in many superconducting devices and provide guidance for their optimization and design. 1 into PostgreSQL...\n",
      "Inserting test sample 671  In this study a phenomenological three-dimensional coupled (3DC) mixed-mode cohesive zone model (CZM) is proposed. This is done by extending an improved version of the well established exponential CZM of Xu and Needleman (XN) to 3D contact problems. Coupled traction-separation relationships are individually presented for normal and transverse directions. The proposed model preserves all the essential features of the XN model and yet correctly describes mixed-mode separation and in particular mixed-mode closure conditions.\n",
      "\n",
      "Moreover, it provides the possibility to explicitly account for all three components of the gap function, i.e. separations in different directions. The 3DC model has some independent parameters, i.e. interface properties, similar to the XN model. All the cohesive zone parameters can be determined using mode-I and mode-II experiments. 0 into PostgreSQL...\n",
      "Inserting test sample 672  In this study, we present the development of a coupled mixed-mode cohesive zone model for three-dimensional contact problems. The proposed model incorporates the effect of normal and tangential stresses on the fracture behavior of the interface. A new set of cohesive parameters is obtained by performing numerical simulations on a range of loading conditions. The proposed model is validated through numerical simulations on a variety of problems and compared with other available models. Results show the effectiveness of the proposed model in capturing the complete fracture behavior of contact interfaces in a computationally efficient manner. Our work provides a better understanding of the physics of contact problems involving deformable bodies and interfaces. The model proposed in our study can be utilized in the simulation of various industrial applications, including adhesives, coatings, and composites processing. 1 into PostgreSQL...\n",
      "Inserting test sample 673  Accurate estimation of spatial gait characteristics is critical to assess motor impairments resulting from neurological or musculoskeletal disease.\n",
      "\n",
      "Currently, however, methodological constraints limit clinical applicability of state-of-the-art double integration approaches to gait patterns with a clear zero-velocity phase. We describe a novel approach to stride length estimation that uses deep convolutional neural networks to map stride-specific inertial sensor data to the resulting stride length. The model is trained on a publicly available and clinically relevant benchmark dataset consisting of 1220 strides from 101 geriatric patients. Evaluation is done in a 10-fold cross validation and for three different stride definitions. Even though best results are achieved with strides defined from mid-stance to mid-stance with average accuracy and precision of 0.01 $\\pm$ 5.37 cm, performance does not strongly depend on stride definition. The achieved precision outperforms state-of-the-art methods evaluated on this benchmark dataset by 3.0 cm (36%).\n",
      "\n",
      "Due to the independence of stride definition, the proposed method is not subject to the methodological constrains that limit applicability of state-of-the-art double integration methods. Furthermore, precision on the benchmark dataset could be improved. With more precise mobile stride length estimation, new insights to the progression of neurological disease or early indications might be gained. Due to the independence of stride definition, previously uncharted diseases in terms of mobile gait analysis can now be investigated by re-training and applying the proposed method. 0 into PostgreSQL...\n",
      "Inserting test sample 674  This paper presents a study of stride length estimation through the use of deep learning techniques. The importance of stride length measurement has been well established in various applications, including gait analysis, rehabilitation, and sports performance evaluation. The current methods for stride length estimation, such as motion capture and wearable sensors, have limitations such as cost, complexity, and intrusiveness. With the recent advances in deep learning, it has become possible to estimate stride length accurately and non-invasively.\n",
      "\n",
      "We propose a deep learning approach to this problem, using a convolutional neural network (CNN) to learn the mapping between the input image and the corresponding stride length. The dataset for the study was collected from a variety of sources, including publicly available datasets and our own experiments using a Kinect sensor. The proposed network was trained on this dataset, achieving a high accuracy rate of over 95%.\n",
      "\n",
      "We evaluate the performance of our method on both synthetic and real-world data, and show that it outperforms existing methods in terms of accuracy and robustness. The proposed method also has the advantage of being computationally efficient, enabling practical real-time applications.\n",
      "\n",
      "In conclusion, this study demonstrates the potential of using deep learning for stride length estimation. The proposed method offers a promising alternative to existing approaches, with the potential to improve the accuracy and accessibility of this important measurement. 1 into PostgreSQL...\n",
      "Inserting test sample 675  We present a novel scheme for data processing which is well-suited for implementation at the nanometer scale. The logic circuits comprise two-state cellular units which are driven by externally applied updates, in contrast to earlier proposals which relied on ground-state relaxation. The present structures can simultaneously process many inputs and are suitable for conventional, dissipative computing in addition to classical reversible computing and quantum computing. 0 into PostgreSQL...\n",
      "Inserting test sample 676  This paper presents an overview of structures that enable efficient data processing in the quantum regime. We analyze the characteristics and limitations of existing quantum computing frameworks and propose new approaches for addressing these challenges. Our study identifies key factors in designing scalable and reliable data processing systems for quantum applications. The presented structures include various hardware and software components that are optimized for speeding up the quantum computation process while reducing error rates. 1 into PostgreSQL...\n",
      "Inserting test sample 677  The magnetic field probability P(B) is calculated from the Ginzburg-Landau theory for various lattices of vortex lines in type-II superconductors: Ideal triangular lattices, lattices with various shear strains and with a super lattice of vacancies, and lattices of short vortices in films whose magnetic field ''mushrooms'' near the surface. 0 into PostgreSQL...\n",
      "Inserting test sample 678  Muon spin rotation has been employed to study the behavior of vortex lattice structures in superconductors. By analyzing the magnetic fields inside these materials, researchers have gained insights into their fundamental properties and potential applications. This paper reviews recent advances in this field of study, highlighting notable findings and discussing future research directions. 1 into PostgreSQL...\n",
      "Inserting test sample 679  The Kepler mission has released over 4496 planetary candidates, among which 3483 planets have been confirmed as of April 2017. The statistical results of the planets show that there are two peaks around 1.5 and 2.0 in the distribution of orbital period ratios. The observations indicate that a plenty of planet pairs could have firstly been captured into mean motion resonances (MMRs) in planetary formation. Subsequently, these planets depart from exact resonant locations to be near MMRs configurations. Through type I migration, two low-mass planets have a tendency to be trapped into first-order MMRs (2:1 or 3:2 MMRs), however two scenarios of mass accretion of planets and potential outward migration play an important role in reshaping their final orbital configurations. Under the scenario of mass accretion, the planet pairs can cross 2:1 MMRs and then enter into 3:2 MMRs, especially for the inner pairs.\n",
      "\n",
      "With such formation scenario, the possibility that two planets are locked into 3:2 MMRs can increase if they are formed in a flat disk. Moreover, the outward migration can make planets have a high likelihood to be trapped into 3:2 MMRs.\n",
      "\n",
      "We perform additional runs to investigate the mass relationship for those planets in three-planet systems, and we show that two peaks near 1.5 and 2.0 for the period ratios of two planets can be easily reproduced through our formation scenario. We further show that the systems in chain resonances (e.g., 4:2:1, 3:2:1, 6:3:2 and 9:6:4 MMRs), have been observed in our simulations.\n",
      "\n",
      "This mechanism can be applicable to understand the formation of systems of Kepler-48, Kepler-53, Kepler-100, Kepler-192, Kepler-297, Kepler-399, and Kepler-450. 0 into PostgreSQL...\n",
      "Inserting test sample 680  The Kepler mission has revolutionized our understanding of exoplanetary systems. In particular, the Kepler data have revealed an abundance of systems which host multiple small planets in near mean-motion resonance (MMR). However, it has been pointed out that many of these systems should be dynamically unstable on short timescales. One possibility is that the observed planet configurations are the result of the planets being trapped in MMR during a phase of rapid orbital migration. Here we explore the possibility that type I migration, where the planets' orbits decay due to the interaction with the protoplanetary disk gas, can lead to the formation of systems which remain trapped in MMR once the disk has dissipated.\n",
      "\n",
      "We focus on the effect of mass accretion onto the individual planets during the type I migration process. We find that the inclusion of mass accretion leads to a significant increase in the likelihood that the final systems are well-aligned and stable on Gyr timescales. We also investigate how the properties of the final systems depend on the mass accretion rates and on the initial orbital distribution of the planets.\n",
      "\n",
      "Our results demonstrate that mass accretion has a crucial impact on the formation and long-term stability of near-MMR systems formed through type I migration. Our work has important implications for interpreting the observed populations of systems with small planets in near-MMR, as well as for understanding the types of planetary systems that are likely to be found by future surveys. We stress that our results are robust to a variety of initial conditions and that we have carried out extensive simulations to ensure that our findings are robust. 1 into PostgreSQL...\n",
      "Inserting test sample 681  By numerically integrating the orbits of the giant planets and of test particles over a period of four billion years, we follow the evolution of the location of the midplane of the Kuiper belt. The Classical Kuiper belt conforms to a warped sheet that precesses with a 1.9 Myr period. The present-day location of the Kuiper belt plane can be computed using linear secular perturbation theory: the local normal to the plane is given by the theory's forced inclination vector, which is specific to every semimajor axis. The Kuiper belt plane does not coincide with the invariable plane, but deviates from it by up to a few degrees in stable zones. For example, at a semimajor axis of 38 AU, the local Kuiper belt plane has an inclination of 1.9 deg and a longitude of ascending node of 149.9 deg when referred to the mean ecliptic and equinox of J2000. At a semimajor axis of 43 AU, the local plane has an inclination of 1.9 deg and a nodal longitude of 78.3 deg. Only at infinite semimajor axis does the Kuiper belt plane merge with the invariable plane, whose inclination is 1.6 deg and nodal longitude is 107.7 deg. A Kuiper belt object keeps its inclination relative to the Kuiper belt plane nearly constant, even while the latter plane departs from the trajectory predicted by linear theory. The constancy of relative inclination reflects the undamped amplitude of free oscillation. Current observations of Classical Kuiper belt objects are consistent with the plane being warped by the giant planets alone, but the sample size will need to increase by a few times before confirmation exceeds 3-sigma in confidence. In principle, differences between the theoretically expected plane and the observed plane could be used to infer as yet unseen masses orbiting the Sun, but carrying out such a program would be challenging. 0 into PostgreSQL...\n",
      "Inserting test sample 682  The classical Kuiper Belt is a circumstellar disc located in the outer Solar System, populated by a vast number of small bodies known as Kuiper Belt Objects (KBOs). These objects are believed to be remnants from the formation of the Solar System, and their study can provide important insights into its early stages. In recent years, observations have suggested that the plane of the classical Kuiper Belt is warped, meaning that it deviates from the expected flat shape. This phenomenon has been observed in both the inclination and the longitude of ascending nodes of KBOs, implying the existence of some external perturbation. \n",
      "\n",
      "Several mechanisms have been proposed to explain the warped structure of the Kuiper Belt. One possibility is the existence of a massive object beyond the Kuiper Belt, such as a Planet Nine, which is gravitationally disturbing the orbits of KBOs and causing their misalignment. Alternatively, the warped structure could be the result of past interactions with the giant planets of the Solar System, namely Jupiter and Saturn. The gravitational fields of these planets may have caused the Kuiper Belt to tilt out of its original plane over time.\n",
      "\n",
      "To investigate this intriguing phenomenon, we have analyzed the orbits of a large sample of KBOs using numerical simulations. Our results suggest that the most probable explanation for the warped Kuiper Belt is the presence of a massive object beyond its confines. Our simulations show that the presence of such an object could account for the observed misalignments, and provide important constraints on its properties. Our findings have significant implications for our understanding of the formation and evolution of the Solar System, and stimulate further investigations into the existence and nature of Planet Nine.\n",
      "\n",
      "In conclusion, the warped structure of the classical Kuiper Belt is a fascinating and puzzling phenomenon that has attracted widespread attention in the scientific community. Our study presents a new perspective on this topic, shedding light on possible mechanisms that could explain the anomalous orbits of KBOs. 1 into PostgreSQL...\n",
      "Inserting test sample 683  I discuss here some of the constraints imposed by quantum and gravitational corrections on two hypothetical excitations, axions and quintessence, which have important cosmological implications. Although these corrections can be kept under control, the resulting constraints are not too natural. In particular, to keep the quintessence field light one must essentially decouple it from ordinary matter. Some possible suggestions of how to avoid these troubles are briefly touched upon. 0 into PostgreSQL...\n",
      "Inserting test sample 684  This paper explores the role of light scalar fields in cosmology and their implications for the universe's evolution. These scalar fields are believed to have a significant influence on early universe inflation and the large-scale structure of the universe. Through analysis of recent cosmological observations and theoretical models, we suggest the existence of diverse families of such light scalar fields with unique and distinct cosmological signatures. Our results have implications for the understanding of the universe's fundamental nature. 1 into PostgreSQL...\n",
      "Inserting test sample 685  We consider option pricing in a regime-switching diffusion market. As the market is incomplete, there is no unique price for a derivative. We apply the good-deal pricing bounds idea to obtain ranges for the price of a derivative.\n",
      "\n",
      "As an illustration, we calculate the good-deal pricing bounds for a European call option and we also examine the stability of these bounds when we change the generator of the Markov chain which drives the regime-switching. We find that the pricing bounds depend strongly on the choice of the generator. 0 into PostgreSQL...\n",
      "Inserting test sample 686  In this study, we investigate good-deal bounds in a market with regime-switching diffusion. Our approach is based on the martingale optimality principle and leads to a novel characterization of no-arbitrage regimes in terms of a set of linear inequalities. We establish the existence and uniqueness of the martingale measures associated with each regime, and prove the convergence of the good-deal bounds as the investment horizon increases. Numerical examples are provided to illustrate the effectiveness of our method in various market scenarios. 1 into PostgreSQL...\n",
      "Inserting test sample 687  A direct implementation of the bilateral filter [1] requires O(\\sigma_s^2) operations per pixel, where \\sigma_s is the (effective) width of the spatial kernel. A fast implementation of the bilateral filter was recently proposed in [2] that required O(1) operations per pixel with respect to \\sigma_s. This was done by using trigonometric functions for the range kernel of the bilateral filter, and by exploiting their so-called shiftability property. In particular, a fast implementation of the Gaussian bilateral filter was realized by approximating the Gaussian range kernel using raised cosines. Later, it was demonstrated in [3] that this idea could be extended to a larger class of filters, including the popular non-local means filter [4]. As already observed in [2], a flip side of this approach was that the run time depended on the width \\sigma_r of the range kernel. For an image with (local) intensity variations in the range [0,T], the run time scaled as O(T^2/\\sigma^2_r) with \\sigma_r. This made it difficult to implement narrow range kernels, particularly for images with large dynamic range. We discuss this problem in this note, and propose some simple steps to accelerate the implementation in general, and for small \\sigma_r in particular.\n",
      "\n",
      "[1] C. Tomasi and R. Manduchi, \"Bilateral filtering for gray and color images\", Proc. IEEE International Conference on Computer Vision, 1998.\n",
      "\n",
      "[2] K.N. Chaudhury, Daniel Sage, and M. Unser, \"Fast O(1) bilateral filtering using trigonometric range kernels\", IEEE Transactions on Image Processing, 2011.\n",
      "\n",
      "[3] K.N. Chaudhury, \"Constant-time filtering using shiftable kernels\", IEEE Signal Processing Letters, 2011.\n",
      "\n",
      "[4] A. Buades, B. Coll, and J.M. Morel, \"A review of image denoising algorithms, with a new one\", Multiscale Modeling and Simulation, 2005. 0 into PostgreSQL...\n",
      "Inserting test sample 688  Bilateral filtering and non-local means are popular image restoration and noise reduction techniques used in the field of digital signal processing. However, their computational complexity makes them unsuitable for real-time applications. In this paper, we propose an acceleration of the shiftable O(1) algorithm for bilateral filtering and non-local means, which reduces the complexity to allow for faster processing times.\n",
      "\n",
      "Our approach is based on the observation that the two-dimensional input signal can be broken down into one-dimensional signals that can be processed more efficiently. We introduce a novel method for dynamically updating the one-dimensional signals using a sliding window approach, which improves both the accuracy and speed of the filtering algorithm. We demonstrate the efficacy of our approach through extensive numerical simulations and experiments on a variety of image datasets.\n",
      "\n",
      "Our proposed acceleration has significant practical implications, as it enables the use of bilateral filtering and non-local means in a wide range of real-time applications, such as medical imaging, video analysis, and computer vision. Furthermore, our method can be extended to other O(1) based algorithms, making it a valuable contribution to the field of digital signal processing.\n",
      "\n",
      "Overall, this paper provides a comprehensive overview of the acceleration of the shiftable O(1) algorithm for bilateral filtering and non-local means, including a theoretical analysis and practical applications. Our proposed approach represents a significant step towards real-time processing of images using these popular filtering techniques. 1 into PostgreSQL...\n",
      "Inserting test sample 689  We formulate a version of the Breuil--Mezard conjecture for quaternion algebras, and show that it follows from the Breuil--Mezard conjecture for GL_2.\n",
      "\n",
      "In the course of the proof we establish a mod p analogue of the Jacquet--Langlands correspondence for representations of GL_2(k), k a finite field of characteristic p. 0 into PostgreSQL...\n",
      "Inserting test sample 690  The Breuil-Mezard conjecture concerning the modularity of two-dimensional Galois representations over p-adic fields has been proven in the case of elliptic curves. We establish the validity of the conjecture for quaternion algebras; specifically, we show that every odd two-dimensional irreducible Galois representation over a p-adic field is modular. 1 into PostgreSQL...\n",
      "Inserting test sample 691  We consider the closed subspace of $\\ell_\\infty$ generated by $c_0$ and the characteristic functions of elements of an uncountable, almost disjoint family $\\mathcal A$ of infinite subsets of $\\mathbb N$. This Banach space has the form $C_0(K_{\\mathcal A})$ for a locally compact Hausdorff space $K_{\\mathcal A}$ that is known under many names, such as $\\Psi$-space and Isbell--Mr\\'owka space.\n",
      "\n",
      "We construct an uncountable, almost disjoint family ${\\mathcal A}$ such that the Banach algebra of all bounded linear operators on $C_0(K_{\\mathcal A})$ is as small as possible in the sense that every bounded linear operator on $C_0(K_{\\mathcal A})$ is the sum of a scalar multiple of the identity and an operator that factors through $c_0$ (which in this case is equivalent to having separable range). This implies that $C_0(K_{\\mathcal A})$ has the fewest possible decompositions: whenever $C_0(K_{\\mathcal A})=X\\oplus Y$ with $dim({X})=\\infty$, $dim({Y})=\\infty$, either ${X}$ is isomorphic to $C_0(K_{\\mathcal A})$ and ${Y}$ to $c_0$, or vice versa. These results improve previous work of the first named author in which an extra set-theoretic hypothesis was required. We also discuss the consequences of these results for the algebra of all bounded linear operators on our Banach space $C_0(K_{\\mathcal A})$ concerning the lattice of closed ideals, characters and automatic continuity of homomorphisms.\n",
      "\n",
      "To exploit the perfect set property for Borel sets as in the classical construction of an almost disjoint family of Mr\\'owka we need to deal with $\\mathbb N \\times \\mathbb N$-matrices rather than with the usual partitioners.\n",
      "\n",
      "This noncommutative setting requires new ideas inspired by the theory of compact and weakly compact operators and the use of an extraction principle due to F. van Engelen, K. Kunen and A. Miller concerning Borel subsets of the square. 0 into PostgreSQL...\n",
      "Inserting test sample 692  This paper presents a novel Banach space constructed through an almost disjoint family, featuring limited decompositions and operators. Specifically, we examine a space built using an almost disjoint family of sets, which is proven to be a Banach space. We explore the structure of this space, including its Schauder decomposition and extensions of its operators.\n",
      "\n",
      "Our analysis shows that the resulting Banach space has certain properties that distinguish it from other Banach spaces induced by more well-known families of sets. Notably, it is shown that only a select few operators can act on this space - a result that is of interest both in theory and in applications to operator theory.\n",
      "\n",
      "Furthermore, we investigate the space's decompositions, showing that any operator on the space can only have a minimal number of non-zero coefficients in any of its non-trivial decompositions. This result adds to our understanding of the structure of the space and further highlights its uniqueness.\n",
      "\n",
      "Overall, this paper represents a significant contribution to the study of Banach spaces induced by almost disjoint families. The space presented here exhibits interesting and non-trivial properties, and the results of this study will serve as a valuable resource for those interested in operator theory, functional analysis, and their applications. 1 into PostgreSQL...\n",
      "Inserting test sample 693  The crystal structure of Fe3O4 below the 122 K Verwey transition has been refined using high-resolution X-ray and neutron powder diffraction data. The refinements give direct evidence for charge ordering (CO) over four independent octahedral Fe sites, two with a charge of +2.4 and the other two of +2.6. CO schemes consistent with our model do not meet the widely-accepted Anderson condition of minimum electrostatic repulsion. Instead we propose that CO is driven primarily by a [001] electronic instability, which opens a gap at the transition through a charge density wave mechanism. 0 into PostgreSQL...\n",
      "Inserting test sample 694  Long range charge ordering in magnetite below the Verwey transition has been studied using X-ray diffraction and MÃ¶ssbauer spectroscopy. The results demonstrate that the charge ordering persists well below the Verwey transition temperature, indicating that it is not solely responsible for the transition. The study also suggests that magnetite may undergo an additional, possibly structural, transition at lower temperatures. These findings provide new insights into the complex behavior of magnetite and may have implications for the understanding of other materials exhibiting similar phenomena. 1 into PostgreSQL...\n",
      "Inserting test sample 695  Outfit recommendation requires the answers of some challenging outfit compatibility questions such as 'Which pair of boots and school bag go well with my jeans and sweater?'. It is more complicated than conventional similarity search, and needs to consider not only visual aesthetics but also the intrinsic fine-grained and multi-category nature of fashion items. Some existing approaches solve the problem through sequential models or learning pair-wise distances between items. However, most of them only consider coarse category information in defining fashion compatibility while neglecting the fine-grained category information often desired in practical applications. To better define the fashion compatibility and more flexibly meet different needs, we propose a novel problem of learning compatibility among multiple tuples (each consisting of an item and category pair), and recommending fashion items following the category choices from customers. Our contributions include: 1) Designing a Mixed Category Attention Net (MCAN) which integrates both fine-grained and coarse category information into recommendation and learns the compatibility among fashion tuples. MCAN can explicitly and effectively generate diverse and controllable recommendations based on need. 2) Contributing a new dataset IQON, which follows eastern culture and can be used to test the generalization of recommendation systems. Our extensive experiments on a reference dataset Polyvore and our dataset IQON demonstrate that our method significantly outperforms state-of-the-art recommendation methods. 0 into PostgreSQL...\n",
      "Inserting test sample 696  Outfit recommendation is a challenging task in fashion e-commerce due to the complex relations among items and the personalized preferences of users. In this work, we propose a novel method to learn tuple compatibility for conditional outfit recommendation. Our approach models the compatibility of an item tuple with a user's preferences as the probability of generating the tuple conditioned on the user's preferences. To achieve this, we design a deep generative model that integrates user preferences and item features to capture the complex correlations among items and users. Our model can generate personalized outfits that match a user's taste and can further suggest complementary items to enhance the outfit. We evaluate our proposed method on a large-scale real-world dataset and compare it with several state-of-the-art methods. The experimental results show that our method outperforms the state-of-the-art methods in terms of recommendation accuracy and diversity. Our proposed method offers a promising direction for automated fashion styling and can be applied to various fashion-related domains. Experimental implementation of our model demonstrates its superior performance in dealing with the high dimensionality and complexity of fashion data and providing effective recommendations. 1 into PostgreSQL...\n",
      "Inserting test sample 697  The observation of a 0.5 conductance plateau in asymmetrically biased quantum point contacts with in-plane side gates has been attributed to the onset of spin-polarized current through these structures. For InAs quantum point contacts with the same width but longer channel length, there is roughly a fourfold increase in the range of common sweep voltage applied to the side gates over which the 0.5 conductance plateau is observed when the QPC aspect ratio (ratio of length over width of the narrow portion of the structure) is increased by a factor 3. Non-equilibrium Green s function simulations indicate that the increase in the size of the 0.5 conductance plateau is due to an increased importance, over a larger range of common sweep voltage, of the effects of electron-electron interactions in QPC devices with larger aspect ratio. The use of asymmetrically biased QPCs with in-plane side gates and large aspect ratio could therefore pave the way to build robust spin injectors and detectors for the successful implementation of spin field effect transistors 0 into PostgreSQL...\n",
      "Inserting test sample 698  In this study, we investigate the relationship between the aspect ratio of InAs quantum point contacts with in-plane side gates and the 0.5(2e2/h) conductance plateau. Our experimental results indicate that the conductance plateau strongly depends on the aspect ratio, with a higher plateau conductance observed for wider constrictions. We attribute this behavior to differences in the number of occupied subbands, which affects the overall conductance and plateau formation. Additionally, we find that varying the gate voltage affects the plateau height and width, indicating that the electron density in the channel plays a key role in determining the conductance plateau behavior. Our findings suggest that controlling the aspect ratio and gate voltages can be used to tailor the conductance plateau properties in InAs quantum point contacts, providing important insights for the development of quantum electronic devices based on this system. 1 into PostgreSQL...\n",
      "Inserting test sample 699  We report on a detailed investigation of the gamma-ray emission from 18 broad line radio galaxies (BLRGs) based on two years of Fermi Large Area Telescope (LAT) data. We confirm the previously reported detections of 3C 120 and 3C 111 in the GeV photon energy range; a detailed look at the temporal characteristics of the observed gamma-ray emission reveals in addition possible flux variability in both sources. No statistically significant gamma-ray detection of the other BLRGs was however found in the considered dataset. Though the sample size studied is small, what appears to differentiate 3C 111 and 3C 120 from the BLRGs not yet detected in gamma-rays is the particularly strong nuclear radio flux. This finding, together with the indications of the gamma-ray flux variability and a number of other arguments presented, indicate that the GeV emission of BLRGs is most likely dominated by the beamed radiation of relativistic jets observed at intermediate viewing angles. In this paper we also analyzed a comparison sample of high accretion-rate Seyfert 1 galaxies, which can be considered radio-quiet counterparts of BLRGs, and found none were detected in gamma-rays. A simple phenomenological hybrid model applied for the broad-band emission of the discussed radio-loud and radio-quiet type 1 active galaxies suggests that the relative contribution of the nuclear jets to the accreting matter is > 1 percent on average for BLRGs, whilst <0.1 percent for Seyfert 1 galaxies. 0 into PostgreSQL...\n",
      "Inserting test sample 700  Broad Line Radio Galaxies (BLRGs) are among the most luminous sources of extragalactic radio emission, and are characterized by the presence of broad optical emission lines associated with an accretion disk around a supermassive black hole. The Fermi Large Area Telescope (LAT) has detected GeV gamma-ray emission in a number of BLRGs, indicating the presence of relativistic particles and magnetic fields in these powerful objects. In this work, we present a detailed study of the gamma-ray properties of a sample of BLRGs observed with the Fermi-LAT. Using a maximum likelihood analysis, we investigate the spectral shape and variability of the gamma-ray emission, and compare the results with those obtained in other wavebands, including radio, optical and X-rays. We find that the gamma-ray emission in BLRGs is consistent with a leptonic scenario, in which relativistic electrons are accelerated by the same mechanisms that produce the radio synchrotron emission. We also discuss the possibility that the gamma-ray emission in BLRGs may partially originate from hadronic processes, which would imply the presence of a population of high-energy protons within the radio jets. Our results provide important insights into the acceleration mechanisms and emission processes in powerful extragalactic jets, and contribute to a better understanding of the role of BLRGs in the cosmic-ray energy budget. 1 into PostgreSQL...\n",
      "Inserting test sample 701  The HERschel Inventory of The Agents of Galaxy Evolution (HERITAGE) of the Magellanic Clouds will use dust emission to investigate the life cycle of matter in both the Large and Small Magellanic Clouds (LMC and SMC). Using the Herschel Space Observatory's PACS and SPIRE photometry cameras, we imaged a 2x8 square degree strip through the LMC, at a position angle of ~22.5 degrees as part of the science demonstration phase of the Herschel mission. We present the data in all 5 Herschel bands: PACS 100 and 160 {\\mu}m and SPIRE 250, 350 and 500 {\\mu}m. We present two dust models that both adequately fit the spectral energy distribution for the entire strip and both reveal that the SPIRE 500 {\\mu}m emission is in excess of the models by 6 to 17%. The SPIRE emission follows the distribution of the dust mass, which is derived from the model. The PAH-to-dust mass (f_PAH) image of the strip reveals a possible enhancement in the LMC bar in agreement with previous work. We compare the gas mass distribution derived from the HI 21 cm and CO J=1-0 line emission maps to the dust mass map from the models and derive gas-to-dust mass ratios (GDRs). The dust model, which uses the standard graphite and silicate optical properties for Galactic dust, has a very low GDR = 65(+15,-18) making it an unrealistic dust model for the LMC. Our second dust model, which uses amorphous carbon instead of graphite, has a flatter emissivity index in the submillimeter and results in a GDR = 287(+25,-42) that is more consistent with a GDR inferred from extinction. 0 into PostgreSQL...\n",
      "Inserting test sample 702  The Herschel Inventory of The Agents of Galaxy Evolution (HERITAGE) is a project aimed at studying the various factors driving galaxy evolution and the Large Magellanic Cloud (LMC) dust has been identified as a significant component of such evolutionary mechanisms. The LMC, a satellite galaxy of the Milky Way, hosts a wealth of information about the physical processes that shape galaxies. To that end, we carried out a comprehensive analysis of the LMC dust using the Herschel Space Observatory's PACS and SPIRE instruments, producing high-resolution maps in the far-infrared and sub-millimetre spectral regions. Our study revealed that the dust temperatures show a gradient with the highest temperatures (in excess of 25 K) being found near the H ii regions and the lowest (15 K) being in the diffuse regions. We also found that the dust-to-gas mass ratio shows a constant trend of decrease towards the outer regions of the LMC, indicating a possible role for metallicity in the modification of dust properties. Moreover, our results suggest that a significant fraction of the LMC's dust mass is present in the form of cold dust, which plays an important role in the evolution of LMC galaxies. The HERITAGE project contributes to our understanding of the mechanisms underlying dust formation and evolution in the LMC. Our findings provide a foundation for future investigations into the physical and chemical properties of the dust in this region, and its role in the formation and evolution of galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 703  Carbon nitride compounds have emerged recently as a prominent member of 2D materials beyond graphene. The experimental realizations of 2D graphitic carbon nitride g-C$_3$N$_4$, nitrogenated holey grahpene C$_2$N, polyaniline C$_3$N have shown their promising potential in energy and environmental applications.\n",
      "\n",
      "In this work, we predict a new type of carbon nitride network with a C$_9$N$_4$ stoichiometry from first principle calculations. Unlike common C-N compounds and covalent organic frameworks (COFs), which are typically insulating, surprisingly C$_9$N$_4$ is found to be a 2D nodal-line semimetal (NLSM). The nodal line in C$_9$N$_4$ forms a closed ring centered at $\\Gamma$ point, which originates from the pz orbitals of both C and N. The linear crossing happens right at Fermi level contributed by two sets of dispersive Kagome and Dirac bands, which is robust due to negligible spin-orbital-coupling (SOC) in C and N. Besides, it is revealed that the formation of nodal ring is of accidental band degeneracy in nature induced by the chemical potential difference of C and N, as validated by a single orbital tight-binding model, rather than protected by crystal in-plane mirror symmetry or band topology. Interestingly, a new structure of nodal line, i.e., nodal-cylinder, is found in momentum space for AA-stacking C$_9$N$_4$. Our results imply possible functionalization for a novel metal-free C-N covalent network with interesting semimetallic properties. 0 into PostgreSQL...\n",
      "Inserting test sample 704  The prediction of a two-dimensional (2D) nodal-line semimetal in a covalent network of carbon nitride (C-N) has been explored in this study. The potential for C-N as a semimetal is attributed to its electronic structure, which indicates a characteristic feature known as a nodal-line. Such a structure manifests due to the overlap of the valence and conduction bands in the crystalline lattice, forming a curve in the Brillouin zone instead of a point. The calculated band structure confirms the presence of a nodal line in the C-N network, which is shown to be robust against external perturbations such as uniaxial strains. Our results suggest that C-N could serve as a promising platform for exploring topological phases.\n",
      "\n",
      "Furthermore, we find that the nodal-line semimetallic phase in C-N has a Dirac-like dispersion, resulting in characteristic spectral features, including flatbands and topologically protected edge states. Specifically, our results show that the 2D C-N nodal-line semimetal exhibits anisotropic transport properties, wherein the electronic conductivity is significantly higher along one direction compared to the other. This finding implies the potential for utilizing the anisotropic transport properties of C-N in electronic devices.\n",
      "\n",
      "Finally, we find that through doping or tuning of the lattice structure, one can modify the electronic properties of C-N, leading to the realization of other topological phases such as Weyl semimetals. Our work presents a significant step forward in the pursuit of 2D topological materials and highlights C-N as an exciting platform for future research. 1 into PostgreSQL...\n",
      "Inserting test sample 705  We present in a full analytic form the partial widths for the lepton flavour violating decays $\\mu^\\pm \\to e^\\pm e^+ e^-$ and $\\tau^\\pm \\to \\ell^\\pm \\ell'^{+} \\ell'^{-}$, with $\\ell,\\ell'=\\mu,e$, mediated by neutrino oscillations in the one-loop diagrams. Compared to the first result by Petcov in [1], obtained in the zero momentum limit $\\mathcal{P}\\ll m_{\\nu} \\ll M_W$, we retain full dependence on $\\mathcal{P}$, the momenta and masses of external particles, and we determine the branching ratios in the physical limit $m_\\nu \\ll \\mathcal{P} \\ll M_W$. We show that the claim presented in [2] that the $\\tau \\to \\ell \\ell' \\ell'$ branching ratios could be as large as $10^{-14}$, as a consequence of keeping the $\\mathcal{P}$ dependence, is flawed. We find rates of order $10^{-55}$, even smaller than those obtained in the zero momentum limit, as the latter prediction contains an unphysical logarithmic enhancement. 0 into PostgreSQL...\n",
      "Inserting test sample 706  The Standard Model of particle physics predicts that the rare decay $\\tau \\to \\mu\\mu\\mu$ occurs at an incredibly small rate, with an estimated branching ratio of approximately one in $10^{14}$ tau decays. This makes it an interesting process to investigate in experimental studies. In this paper, we present a detailed analysis of the $\\tau \\to \\mu\\mu\\mu$ decay using data collected from the Belle II experiment which has recently started operating at the SuperKEKB accelerator facility. Our analysis focuses on the search for this rare process in the high-luminosity data samples from this experiment. We will discuss the selection criteria, background estimation methods, and the final results of our search. This study could help provide a better understanding of the Standard Model and beyond, and may also have implications for future searches of new physics phenomena. 1 into PostgreSQL...\n",
      "Inserting test sample 707  Understanding what factors bring about socio-economic development may often suffer from the streetlight effect, of analyzing the effect of only those variables that have been measured and are therefore available for analysis. How do we check whether all worthwhile variables have been instrumented and considered when building an econometric development model? We attempt to address this question by building unsupervised learning methods to identify and rank news articles about diverse events occurring in different districts of India, that can provide insights about what may have transpired in the districts. This can help determine whether variables related to these events are indeed available or not to model the development of these districts. We also describe several other applications that emerge from this approach, such as to use news articles to understand why pairs of districts that may have had similar socio-economic indicators approximately ten years back ended up at different levels of development currently, and another application that generates a newsfeed of unusual news articles that do not conform to news articles about typical districts with a similar socio-economic profile. These applications outline the need for qualitative data to augment models based on quantitative data, and are meant to open up research on new ways to mine information from unstructured qualitative data to understand development. 0 into PostgreSQL...\n",
      "Inserting test sample 708  This paper aims to explore how news articles can serve as a valuable source for understanding the development patterns of districts in India. The study utilizes a mixed-methods approach, combining qualitative analysis of news articles with quantitative assessment of development indicators. The dataset consists of news articles published in English-language newspapers between 2010 and 2019, covering 100 districts across the country. The research findings highlight that news articles can be a rich source of information on development-related issues, including infrastructure, economic growth, and social welfare programs. Moreover, the study indicates that a district's media presence is positively associated with its development outcomes, suggesting that media coverage of development-related issues can act as a catalyst for local progress. However, the study also identifies limitations to using news articles as a data source, including issues of representativeness and the potential for biases inherent in media coverage. Overall, this research demonstrates the potential value of news articles in understanding district-level development patterns in India, while also highlighting the need for caution in interpreting and using media sources as a data source. 1 into PostgreSQL...\n",
      "Inserting test sample 709  The remarkable discovery of Quantum Error Correction (QEC), which can overcome the errors experienced by a bit of quantum information (qubit), was a critical advance that gives hope for eventually realizing practical quantum computers. In principle, a system that implements QEC can actually pass a \"break-even\" point and preserve quantum information for longer than the lifetime of its constituent parts. Reaching the break-even point, however, has thus far remained an outstanding and challenging goal. Several previous works have demonstrated elements of QEC in NMR, ions, nitrogen vacancy (NV) centers, photons, and superconducting transmons. However, these works primarily illustrate the signatures or scaling properties of QEC codes rather than test the capacity of the system to extend the lifetime of quantum information over time. Here we demonstrate a QEC system that reaches the break-even point by suppressing the natural errors due to energy loss for a qubit logically encoded in superpositions of coherent states, or cat states of a superconducting resonator. Moreover, the experiment implements a full QEC protocol by using real-time feedback to encode, monitor naturally occurring errors, decode, and correct. As measured by full process tomography, the enhanced lifetime of the encoded information is 320 microseconds without any post-selection. This is 20 times greater than that of the system's transmon, over twice as long as an uncorrected logical encoding, and 10% longer than the highest quality element of the system (the resonator's 0, 1 Fock states). Our results illustrate the power of novel, hardware efficient qubit encodings over traditional QEC schemes. Furthermore, they advance the field of experimental error correction from confirming the basic concepts to exploring the metrics that drive system performance and the challenges in implementing a fault-tolerant system. 0 into PostgreSQL...\n",
      "Inserting test sample 710  Quantum error correction is one of the principal challenges in building error-resistant quantum computers. The lifetime of stored quantum information is limited by the decoherence time, which is the time scale over which quantum states evolve uncontrollably due to their interactions with the environment. Here, we demonstrate a novel quantum error correction protocol that extends the lifetime of quantum information by more than an order of magnitude. Our protocol employs a custom-designed five-qubit circuit that encodes a logical qubit, uses standard Clifford gates and non-Clifford gates, and implements syndrome measurements. We experimentally demonstrate that this protocol effectively corrects for the collective dephasing error caused by the two-qubit gates in the circuit, by monitoring the evolution of the logical qubit over time. We further quantify the effect of the correction by measuring the average logical state fidelity and the logical phase flip error for varying lengths of time. Our results confirm the effectiveness of our protocol in correcting for both coherent and incoherent errors, and demonstrate that it can extend the coherent lifetime of the logical qubit by up to two orders of magnitude. This work represents an important step towards the realization of large-scale, fault-tolerant quantum computers with lifetimes extended beyond the decoherence time scales of individual qubits. In addition, our protocol can be adapted to other physical platforms, making it a promising candidate for protecting quantum information in a wide range of quantum technologies. 1 into PostgreSQL...\n",
      "Inserting test sample 711  We investigate the decidability and complexity status of model-checking problems on unlabelled reachability graphs of Petri nets by considering first-order and modal languages without labels on transitions or atomic propositions on markings. We consider several parameters to separate decidable problems from undecidable ones. Not only are we able to provide precise borders and a systematic analysis, but we also demonstrate the robustness of our proof techniques. 0 into PostgreSQL...\n",
      "Inserting test sample 712  Petri net reachability graphs have been a widely used tool to model concurrent systems, but verifying properties of such models can be a challenging task. In this paper, we investigate the decidability status of first-order properties of Petri net reachability graphs. We prove that this problem is decidable, which has important implications for the automated analysis of concurrent systems. Our results provide new insights into the computational complexity of reasoning about Petri nets. 1 into PostgreSQL...\n",
      "Inserting test sample 713  With decreasing system sizes, the mechanical properties and dominant deformation mechanisms of metals change. For larger scales, bulk behavior is observed that is characterized by a preservation and significant increase of dislocation content during deformation whereas at the submicron scale very localized dislocation activity as well as dislocation starvation is observed.\n",
      "\n",
      "In the transition regime it is not clear how the dislocation content is built up. This dislocation storage regime and its underlying physical mechanisms are still an open field of research. In this paper, the microstructure evolution of single crystalline copper micropillars with a $\\langle1\\,1\\,0\\rangle$ crystal orientation and varying sizes between $1$ to $10\\,\\mu\\mathrm{m}$ is analysed under compression loading. Experimental in situ HR-EBSD measurements as well as 3d continuum dislocation dynamics simulations are presented. The experimental results provide insights into the material deformation and evolution of dislocation structures during continuous loading. This is complemented by the simulation of the dislocation density evolution considering dislocation dynamics, interactions, and reactions of the individual slip systems providing direct access to these quantities. Results are presented that show, how the plastic deformation of the material takes place and how the different slip systems are involved. A central finding is, that an increasing amount of GND density is stored in the system during loading that is located dominantly on the slip systems that are not mainly responsible for the production of plastic slip. This might be a characteristic feature of the considered size regime that has direct impact on further dislocation network formation and the corresponding contribution to plastic hardening. 0 into PostgreSQL...\n",
      "Inserting test sample 714  The microstructure evolution of compressed micropillars has been extensively investigated through molecular dynamics simulations and ex situ transmission electron microscopy. However, the in situ evolution of micropillar compression remains a challenging task due to difficulties in achieving high-resolution imaging and straining. Here, we report on the use of high-resolution electron backscatter diffraction (HR-EBSD) combined with dislocation density simulations to investigate the microstructure evolution of compressed micropillars in situ. A systematic study of the effect of grain size and orientation on deformation mechanisms and strain hardening was carried out. It was found that the deformation mechanisms were dependent on the micropillar orientation and grain size, with dislocation glide and pile-up being the dominant mechanisms. Furthermore, the dislocation density was observed to increase with increasing strain, and an exponential correlation between the dislocation density and strain was established. Our results suggest that the combination of HR-EBSD and dislocation density simulations provide a powerful tool to investigate the microstructure evolution of compressed micropillars in situ, and shed light on the fundamental mechanisms of plastic deformation at the micrometer scale. This knowledge has important implications for the development of microscale mechanical devices with enhanced mechanical stability and reliability. 1 into PostgreSQL...\n",
      "Inserting test sample 715  In this paper, we present a framework for real-time autonomous robot navigation based on cloud and on-demand databases to address two major issues of human-like robot interaction and task planning in global dynamic environment, which is not known a priori. Our framework contributes to make human-like brain GPS mapping system for robot using spatial information and performs 3D visual semantic SLAM for independent robot navigation. We accomplish the feat by separating robot's memory system into Long-Term Memory (LTM) and Short-Term Memory (STM). We also form robot's behavior and knowledge system by linking these memories to Autonomous Navigation Module (ANM), Learning Module (LM), and Behavior Planner Module (BPM). The proposed framework is assessed through simulation using ROS-based Gazebo-simulated mobile robot, RGB-D camera (3D sensor) and a laser range finder (2D sensor) in 3D model of realistic indoor environment. Simulation corroborates the substantial practical merit of our proposed framework. 0 into PostgreSQL...\n",
      "Inserting test sample 716  This research presents a novel autonomous robot navigation framework capable of human-like high-level interaction and task planning in dynamic environments. The proposed framework includes several modules for perception, decision-making, and control, which operate together to enable the robot to navigate in unknown and dynamic environments autonomously. The perception module incorporates state-of-the-art sensors and object detection techniques to perceive the robot's surroundings accurately and efficiently. The decision-making module utilizes a combination of rule-based and learning-based approaches to reason about high-level human-like interaction and task planning. Finally, the control module integrates the output from the decision-making module to execute the planned task approach. Our experimental results on real-world dynamic environments demonstrate that the proposed framework is capable of effective autonomous navigation and human-like interaction in highly dynamic environments. The framework can be used in various applications such as autonomous driving, robot-assisted living, and search and rescue operations. 1 into PostgreSQL...\n",
      "Inserting test sample 717  Let $G$ be a finite group. We consider the set of the irreducible complex characters of $G$, namely $Irr(G)$, and the related degree set $cd(G)=\\{\\chi(1) : \\chi\\in Irr(G)\\}$. Let $\\rho(G)$ be the set of all primes which divide some character degree of $G$. In this paper we introduce the bipartite divisor graph for $cd(G)$ as an undirected bipartite graph with vertex set $\\rho(G)\\cup (cd(G)\\setminus\\{1\\})$, such that an element $p$ of $\\rho(G)$ is adjacent to an element $m$ of $cd(G)\\setminus\\{1\\}$ if and only if $p$ divides $m$. We denote this graph simply by $B(G)$. Then by means of combinatorial properties of this graph, we discuss the structure of the group $G$. In particular, we consider the cases where $B(G)$ is a path or a cycle. 0 into PostgreSQL...\n",
      "Inserting test sample 718  The bipartite divisor graph is a significant tool in the study of finite groups. In this paper, we investigate the properties of the bipartite divisor graph for the set of irreducible character degrees. We prove that the graph is connected and has a diameter at most five. Moreover, we provide an algorithm to construct the graph and describe its structure for some small groups. We also study the behavior of the graph as we vary the orders of the groups. Our results show that the bipartite divisor graph for the set of irreducible character degrees is a natural and useful object to study in group representation theory. 1 into PostgreSQL...\n",
      "Inserting test sample 719  A sparsifier of a graph $G$ (Bencz\\'ur and Karger; Spielman and Teng) is a sparse weighted subgraph $\\tilde G$ that approximately retains the cut structure of $G$. For general graphs, non-trivial sparsification is possible only by using weighted graphs in which different edges have different weights.\n",
      "\n",
      "Even for graphs that admit unweighted sparsifiers, there are no known polynomial time algorithms that find such unweighted sparsifiers.\n",
      "\n",
      "We study a weaker notion of sparsification suggested by Oveis Gharan, in which the number of edges in each cut $(S,\\bar S)$ is not approximated within a multiplicative factor $(1+\\epsilon)$, but is, instead, approximated up to an additive term bounded by $\\epsilon$ times $d\\cdot |S| + \\text{vol}(S)$, where $d$ is the average degree, and $\\text{vol}(S)$ is the sum of the degrees of the vertices in $S$. We provide a probabilistic polynomial time construction of such sparsifiers for every graph, and our sparsifiers have a near-optimal number of edges $O(\\epsilon^{-2} n {\\rm polylog}(1/\\epsilon))$. We also provide a deterministic polynomial time construction that constructs sparsifiers with a weaker property having the optimal number of edges $O(\\epsilon^{-2} n)$. Our constructions also satisfy a spectral version of the ``additive sparsification'' property.\n",
      "\n",
      "Our construction of ``additive sparsifiers'' with $O_\\epsilon (n)$ edges also works for hypergraphs, and provides the first non-trivial notion of sparsification for hypergraphs achievable with $O(n)$ hyperedges when $\\epsilon$ and the rank $r$ of the hyperedges are constant. Finally, we provide a new construction of spectral hypergraph sparsifiers, according to the standard definition, with ${\\rm poly}(\\epsilon^{-1},r)\\cdot n\\log n$ hyperedges, improving over the previous spectral construction (Soma and Yoshida) that used $\\tilde O(n^3)$ hyperedges even for constant $r$ and $\\epsilon$. 0 into PostgreSQL...\n",
      "Inserting test sample 720  Sparsification is an important tool in graph and hypergraph theory, as it allows us to represent complex structures in a more efficient way. In this paper, we introduce several new notions and constructions of sparsification that have been recently developed.\n",
      "\n",
      "Our first contribution is a new sparsification technique for graphs that is based on spectral properties. This method uses the eigenvalues of the graph Laplacian to identify clusters of vertices that can be contracted together without changing the underlying structure of the graph. We prove that this method preserves certain important properties of the graph, such as connectivity and minimum cut.\n",
      "\n",
      "Next, we consider sparsification for hypergraphs. We propose a new construction based on the idea of hypergraph separators, which are sets of vertices that disconnect the hypergraph into smaller pieces. Our method uses a combination of graph and hypergraph theory to identify a sparse subhypergraph that retains the essential structure of the original hypergraph.\n",
      "\n",
      "Finally, we introduce a unified framework for sparsification that encompasses many existing methods as special cases. This framework is based on the concept of graph homomorphisms, which allow us to map one graph to another while preserving certain properties. We show how this framework can be used to analyze and compare different sparsification techniques, and we provide several applications to demonstrate its usefulness.\n",
      "\n",
      "Overall, our work provides new insights into the theory of sparsification for graphs and hypergraphs, and it opens up new avenues for future research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 721  We review past work using broad emission lines as virial estimators of black hole masses in quasars. Basically one requires estimates of the emitting region radius and virial velocity dispersion to obtain black hole masses. The three major ways to estimate the broad-line emitting region (BLR) radius involve: (1) direct reverberation mapping, (2) derivation of BLR radius for larger samples using the radius-luminosity correlation derived from reverberation measures, and (3) estimates of BLR radius using the definition of the ionization parameter solved for BLR radius (photoionization method). At low redshift (z < 0.7) FWHM H-beta serves as the most widely used estimator of virial velocity dispersion. FWHM H-beta can provide estimates for tens of thousands of quasars out to z ~ 3.8 (IR spectroscopy beyond z ~ 1). A new photoionization method also shows promise for providing many reasonable estimates of BLR radius via high S/N IR spectroscopy of the UV region 1300 -- 2000 A. FWHM MgII 2800 can serve as a surrogate for FWHM H-beta in the range 0.4 < z < 6.5 while CIV 1549 is affected by broadening due to non-virial motions and best avoided (i.e.\n",
      "\n",
      "there is no clear conversion factor between FWHM H-beta and FWHM CIV 1549).\n",
      "\n",
      "Most quasars yield black hole mass estimates in the range 7 < log M< 9.7. There is no strong evidence for values above 10.0 and there may be evidence for a turnover in the maximum black hole mass near z ~ 5. 0 into PostgreSQL...\n",
      "Inserting test sample 722  In this study, we present a new method for estimating the masses of black holes in quasars using broad optical and ultraviolet (UV) emission lines. Quasars are among the most luminous and energetic objects in the universe, and they are powered by accreting supermassive black holes at their centers. Accurate measurements of black hole masses are crucial for understanding the physics of quasars and testing models of black hole growth and evolution. Our method is based on the empirical relationship between the width of broad emission lines and the mass of the black hole, which has been established from detailed studies of nearby active galaxies. We apply this relationship to a sample of high-redshift quasars with high-quality optical and UV spectra, and we demonstrate that our method yields reliable mass estimates for black holes with masses ranging from a few million to several billion times the mass of the sun. Our results will enable studies of the demographics and evolution of quasar black holes over cosmic time, and they will provide important constraints on models of black hole growth and feedback in the early universe. 1 into PostgreSQL...\n",
      "Inserting test sample 723  We discuss the space-time picture of scattering amplitudes with single and double ladder exchange in perturbative QCD. Particular emphasis is given to the Abramovsky Gribov Kanchelli (AGK) rules which describe the relative contributions of the diffractive dissociation and processes with other multiplicities to the elastic scattering amplitude. Inside the Pomeron Pomeron interaction vertices we find a new matrix which describes the transition from one s-cut to another and which goes beyond the original the AGK rules. 0 into PostgreSQL...\n",
      "Inserting test sample 724  This research paper focuses on unraveling the space-time characteristics of wee partons and their implications in perturbative QCD. We investigate the AGK cutting rules and their applications for these partons, highlighting their role in the fragmentation process of high energy particles. By leveraging the positron-electron collision data, we provide numerical estimates to evaluate the AGK rules. Our findings suggest that the momentum-space picture of wee partons is essential to understanding the QCD phenomena, and the AGK cutting rules enhance our comprehension of particle interactions. 1 into PostgreSQL...\n",
      "Inserting test sample 725  Free radicals play a key role in the ageing process. The strongly debated free radical theory of ageing even states that damage caused by free radicals is the main cause of aging on a cellular level. However, free radicals are small, reactive and short lived and thus challenging to measure. We utilize a new technique called diamond magnetometry for this purpose. We make use of nitrogen vacancy centers in nanodiamonds. Via a quantum effect these defects convert a magnetic resonance signal into an optical signal. While this method is increasingly popular for its unprecedented sensitivity in physics, we use this technique here for the first time to measure free radicals in living cells. Our signals are equivalent to T1 signals in conventional MRI but from nanoscale voxels from single cells with sub-cellular resolution. With this powerful tool we are able to follow free radical generation after chemically inducing stress. In addition, we can observe free radical reduction in presence of an antioxidant. We were able to clearly differentiate between mutant strains with altered metabolism. Finally, the excellent stability of our diamond particles allowed us to follow the ageing process and differentiate between young and old cells. We could confirm the expected increase of free radical load in old wild type and sod1{\\Delta} mutants. We further applied this new technique to investigate tor1{\\Delta} and pex19{\\Delta} cells. For these mutants an increased lifespan has been reported but the exact mechanism is unclear. We find a decreased free radical load in, which might offer an explanation for the increased lifespan in these cells. 0 into PostgreSQL...\n",
      "Inserting test sample 726  Quantum monitoring has emerged as a novel tool in the field of nanobiotechnology, allowing the non-invasive study of biological systems. In this study, we utilize this cutting-edge technology to monitor the metabolic activity of individual yeast mutant strain cells under different conditions. Specifically, we investigate the effects of aging, stress, and antioxidant treatment on the metabolic activity of the cells.\n",
      "\n",
      "Through the use of quantum monitoring, we are able to observe the behavior of individual cells, allowing us to capture important information about the heterogeneity of the population. Our results showed that yeast cells undergo significant changes in metabolic activity when subjected to these different conditions. We observed a decrease in metabolic activity with age, an increase under stress, and a decrease in activity in the presence of antioxidants.\n",
      "\n",
      "Furthermore, we were able to identify specific metabolic pathways that were altered under each condition. Aging resulted in a decrease in those pathways associated with energy metabolism and an increase in those related to protein synthesis. Stress led to an increase in glycolytic activity, while antioxidant treatment led to a decrease in oxidative phosphorylation.\n",
      "\n",
      "Overall, our study provides new insights into the metabolic responses of yeast cells under different conditions, demonstrating the power of quantum monitoring as a tool for studying biological systems. These findings could have important implications for the development of novel treatments for aging and stress-related diseases. 1 into PostgreSQL...\n",
      "Inserting test sample 727  The abundance of galactic systems at high redshifts can impose a strong constraint on the cold+hot dark matter (CDM+HDM) models. The hot component reduces the excessive small-scale power in the COBE-normalized CDM model but also delays the epoch of galaxy formation. We present results from the first numerical simulations that have enough dynamic range to address accurately the issue of high-redshift halo abundances in CDM+HDM models. Equivalent high-resolution particle-particle/particle-mesh $N$-body simulations are performed for spatially flat models with $\\Omega_\\nu =0.3$ and 0.2 (with $H_0=50$ km s$^{-1}$ Mpc$^{-1}$ and $\\Omega_b=0.05$). We study the constraints placed on the models by the high-redshift quasar space density and by the mass fraction in neutral dense gas associated with damped Ly$\\alpha$ systems. We find that even with optimistic assumptions, the much-studied $\\Omega_\\nu=0.3$ model does not produce enough massive halos to account for the observed abundance of quasars at $z>4$. The model passes this test if $\\Omega_\\nu$ is decreased to 0.2. Both models do not produce enough high column-density halos to account for the amount of gas in damped Ly$\\alpha$ systems at $z\\go 3$: the $\\Omega_\\nu=0.3$ model falls short by a factor $\\sim$80; the $\\Omega_\\nu=0.2$ model by a factor $\\sim$3. We conclude that only CDM+HDM models with $\\Omega_\\nu\\lo 0.2$ can match observations at high redshift, implying an upper bound of 4.7 eV on the most massive light neutrino (presumably the $\\tau$). 0 into PostgreSQL...\n",
      "Inserting test sample 728  In the standard cosmological model, the hierarchical structure formation occurs due to the gravitational collapse of dark matter halos. However, discrepancies between the theoretical predictions and observations in the properties of the small-scale structures abound and are known as the small-scale crisis. One proposed solution to this crisis lies in the alternative cold + hot dark matter (CHDM) cosmological model. In CHDM, the hot dark matter component suppresses the growth of small-scale structures, while the cold dark matter allows for larger structures to form. \n",
      "\n",
      "In this paper, we investigate whether the CHDM model can resolve the observed discrepancies in the formation time of galactic systems. To study this, we employ the well-established abundance matching technique between observed galaxy luminosity functions and theoretical halo mass functions. We find that the CHDM model is able to reproduce the galaxy formation history, matching the observed numbers of low-mass galaxies without overproducing massive ones, even in the presence of hierarchical structure formation. \n",
      "\n",
      "We also compare our results with other popular alternative models to the standard cosmological model. We find that the CHDM model outperforms the warm dark matter and self-interacting dark matter models on the small-scale structure formation crisis, while being consistent with observations of the cosmic microwave background radiation and large-scale structure of the universe. Overall, this work suggests that including hot dark matter may be a promising avenue for resolving the small-scale structure formation crisis in cosmology. 1 into PostgreSQL...\n",
      "Inserting test sample 729  Corporations across the world are highly interconnected in a large global network of corporate control. This paper investigates the global board interlock network, covering 400,000 firms linked through 1,700,000 edges representing shared directors between these firms. The main focus is on the concept of centrality, which is used to investigate the embeddedness of firms from a particular country within the global network. The study results in three contributions. First, to the best of our knowledge for the first time we can investigate the topology as well as the concept of centrality in corporate networks at a global scale, allowing for the largest cross-country comparison ever done in interlocking directorates literature. We demonstrate, amongst other things, extremely similar network topologies, yet large differences between countries when it comes to the relation between economic prominence indicators and firm centrality. Second, we introduce two new metrics that are specifically suitable for comparing the centrality ranking of a partition to that of the full network. Using the notion of centrality persistence we propose to measure the persistence of a partition's centrality ranking in the full network. In the board interlock network, it allows us to assess the extent to which the footprint of a national network is still present within the global network. Next, the measure of centrality ranking dominance tells us whether a partition (country) is more dominant at the top or the bottom of the centrality ranking of the full (global) network. Finally, comparing these two new measures of persistence and dominance between different countries allows us to classify these countries based the their embeddedness, measured using the relation between the centrality of a country's firms on the national and the global scale of the board interlock network. 0 into PostgreSQL...\n",
      "Inserting test sample 730  The phenomenon known as \"globalization\" has resulted in an increase of the size and complexity of corporate networks. The structure of these networks is of great interest to economists and political scientists alike; specifically, the degree of centrality of certain key actors within the network and the resulting implications for overall control of the global economy. \n",
      "\n",
      "This study analyzes the global network of corporate control through the lens of network theory. Using a dataset of 43,060 transnational corporations and their shareholders, the authors construct a network graph representing the ownership relationships among these entities. The analysis reveals a highly concentrated network, with a small number of entities exerting significant control over a majority of the world's large corporations. \n",
      "\n",
      "The authors measure various centrality indices and find that a small core of financial institutions, primarily from Western countries, occupy highly central positions within the network. These institutions exert significant control over the global economy by virtue of their ability to direct the flow of capital and influence corporate decision-making. Importantly, the authors show that this concentration of power has increased over time, with the top actors exerting even greater control over the network in recent years. \n",
      "\n",
      "The implications of these findings are significant for both economic theory and public policy. From a theoretical perspective, the study reveals the existence of a highly interconnected and centralized network that challenges the assumptions of free-market economics. From a policy perspective, the study highlights the need for increased transparency and accountability in corporate governance, particularly for those institutions that occupy central positions in the network. 1 into PostgreSQL...\n",
      "Inserting test sample 731  Global Sensitivity Analysis (GSA) methods are useful tools to rank input parameters uncertainties regarding their impact on result variability. In practice, such type of approach is still at an exploratory level for studies relying on 2D Shallow Water Equations (SWE) codes as GSA requires specific tools and deals with important computational capacity. The aim of this paper is to provide both a protocol and a tool to carry out a GSA for 2D hydraulic modelling applications. A coupled tool between Prom{\\'e}th{\\'e}e (a parametric computation environment) and FullSWOF 2D (a code relying on 2D SWE) has been set up: Prom{\\'e}th{\\'e}e-FullSWOF 2D (P-FS). The main steps of our protocol are: i) to identify the 2D hydraulic code input parameters of interest and to assign them a probability density function, ii) to propagate uncertainties within the model, and iii) to rank the effects of each input parameter on the output of interest. For our study case, simulations of a river flood event were run with uncertainties introduced through three parameters using P-FS tool.\n",
      "\n",
      "Tests were performed on regular computational mesh, spatially discretizing an urban area, using up to 17.9 million of computational points. P-FS tool has been installed on a cluster for computation. Method and P-FS tool successfully allow the computation of Sobol indices maps. Keywords Uncertainty, flood hazard modelling, global sensitivity analysis, 2D shallow water equation, Sobol index. 0 into PostgreSQL...\n",
      "Inserting test sample 732  Global sensitivity analysis (GSA) is an important technique for uncertainty quantification in hydraulic modelling. The complexity of such models often leads to a very high number of parameters, with consequent difficulty in assessing their influence on the output. GSA allows for a prioritization of these parameters in terms of sensitivity, providing a more efficient way to allocate resources for calibration and to estimate confidence intervals for predictions. \n",
      "\n",
      "In this paper, we propose a standardized protocol for GSA with 2D hydraulic codes, incorporating the best practices from the literature and our own experience. The protocol uses the Sobol' method for variance-based sensitivity analysis, which is particularly suitable for models with strong interactions between parameters. By applying our method to a synthetic test case, we demonstrate its robustness and efficiency, as well as its ability to reveal underlying physical mechanisms.\n",
      "\n",
      "To facilitate the implementation of the proposed protocol, we have developed a practical tool, named GSA-HYD, which dynamically links Sobol' indices to different stages of the hydraulic model workflow. GSA-HYD reduces the need for manual intervention and specialized programming skills, enabling a wider range of users to benefit from GSA. We illustrate the use of GSA-HYD with a real-world application, a flood simulation of a river reach in Australia. The results show that by including GSA in the model development and calibration process, we can greatly improve the accuracy and reliability of the predictions.\n",
      "\n",
      "Finally, we address some of the limitations and challenges of the proposed protocol and tool, such as the need for expert judgment in identifying the input parameter ranges and the computationally intensive nature of the Sobol' method. We also discuss some of the potential future developments in the field, such as the integration of GSA with optimization algorithms and the exploration of alternative sensitivity measures.\n",
      "\n",
      "In conclusion, this paper presents a comprehensive framework for GSA with 2D hydraulic codes, which includes a standardized protocol and a practical tool. By combining theoretical and practical aspects, we aim to facilitate the adoption of GSA by the hydraulic modelling community and to contribute to the improvement of flood risk assessments and management. 1 into PostgreSQL...\n",
      "Inserting test sample 733  With growing data from ongoing and future supernova surveys it is possible to empirically quantify the shapes of SNIa light curves in more detail, and to quantitatively relate the shape parameters with the intrinsic properties of SNIa. Building such relationship is critical in controlling systematic errors associated with supernova cosmology. Based on a collection of well-observed SNIa samples accumulated in the past years, we construct an empirical SNIa light curve model using a statistical method called the functional principal component analysis (FPCA) for sparse and irregularly sampled functional data.\n",
      "\n",
      "Using this method, the entire light curve of an SNIa is represented by a linear combination of principal component functions, and the SNIa is represented by a few numbers called principal component scores. These scores are used to establish relations between light curve shapes and physical quantities such as intrinsic color, interstellar dust reddening, spectral line strength, and spectral classes. These relations allow for descriptions of some critical physical quantities based purely on light curve shape parameters. Our study shows that some important spectral feature information is being encoded in the broad band light curves, for instance, we find that the light curve shapes are correlated with the velocity and velocity gradient of the Si II $\\lambda$6355 line. This is important for supernova surveys, e.g., LSST and WFIRST. Moreover, the FPCA light curve model is used to construct the entire light curve shape, which in turn is used in a functional linear form to adjust intrinsic luminosity when fitting distance models. 0 into PostgreSQL...\n",
      "Inserting test sample 734  This research paper focuses on the application of principal component analysis (PCA) to the study of Type Ia supernova light curves. PCA is a statistical method for identifying patterns of variability in data sets, and has been successfully applied to a wide range of fields. In order to apply this technique to supernova light curves, we first had to develop a new method for handling the time series data, which is both sparse and irregularly sampled. \n",
      "\n",
      "Our results show that PCA can effectively characterize Type Ia supernova light curves, capturing information such as peak brightness and decline rate. Furthermore, we were able to identify groups of supernovae which behaved similarly with respect to their light curves. This has important implications for our understanding of the physical processes underlying supernova explosions, and can be used to refine cosmological models. \n",
      "\n",
      "We also discovered several interesting features in the data that had not previously been noted, such as the existence of a \"secondary maximum\" in some light curves. Additionally, we found that the previously known correlation between supernova brightness and host galaxy properties becomes weaker when PCA is used to extract the underlying patterns in the data. \n",
      "\n",
      "Overall, our study demonstrates the utility of PCA for the analysis of sparse and irregularly sampled time series data, and provides new insights into the behavior of Type Ia supernovae. 1 into PostgreSQL...\n",
      "Inserting test sample 735  With observations of IRIS, we study chromospheric heating and evaporation during an M1.6 flare SOL2015-03-12T11:50. At the flare ribbons, the Mg II 2791.59 line shows quasi-periodic short-duration red-wing enhancement, which is likely related to repetitive chromospheric condensation as a result of episodic heating. On the contrary, the Si IV 1402.77 line reveals a persistent red-wing asymmetry in both the impulsive and decay phases, suggesting that this line responds to both cooling downflows and chromospheric condensation. The first two episodes of red-wing enhancement occurred around 11:42 UT and 11:45 UT, when two moving brightenings indicative of heating fronts crossed the IRIS slit. The greatly enhanced red wings of the Si IV and Mg II lines at these occasions are accompanied by an obvious increase in the line intensities and the HXR flux, suggesting two episodes of energy injection into the lower atmosphere in the form of nonthermal electrons. The Mg II k/h ratio has a small value of ~1.2 at the ribbons and decreases to ~1.1 at these two occasions.\n",
      "\n",
      "Correspondingly, the Fe XXI 1354 line reveals two episodes of chromospheric evaporation, which is characterized as a smooth decrease of the blue shift from ~300 km/s to nearly zero within ~3 minutes. The Fe XXI 1354 line is entirely blueshifted in the first episode, while appears to contain a nearly stationary component and a blueshifted component in the second episode. More episodes of blueshifted Fe XXI emission is found around the northern ribbon in the decay phase, though no obvious response is detected in the Si IV and Mg II emission.\n",
      "\n",
      "We have also examined the Fe XXI emission at the flare loop top and identified a secondary component with a ~200 km/s red shift, which possibly results from the downward moving reconnection outflow. Our analysis also suggests a reference wavelength of 1354.0878 Angstrom for this Fe XXI line. 0 into PostgreSQL...\n",
      "Inserting test sample 736  Solar flares are transitory brightenings in the Sun's outer atmosphere (the corona) that release large amounts of energy. During a flare, magnetic energy is rapidly converted into kinetic energy, thermal energy, and radiation. One of the most important questions in solar physics is how this energy is transported from the corona to the solar surface. Chromospheric evaporation, which is the upward transport of material from the chromosphere to the corona, is one of the mechanisms thought to be responsible for this energy transport. In this study, we present a detailed analysis of a solar flare that occurred on [insert date], using high-resolution observations from the [insert telescope].\n",
      "\n",
      "We find that this flare is characterized by multi-episode chromospheric evaporation. In the first episode, we observe a strong and fast upflow of plasma along the flare loop, which is accompanied by an enhancement of the hard X-ray emission. This episode lasts for approximately [insert time]. In the second episode, we observe a weaker but longer-lasting upflow of plasma, which lasts for approximately [insert time]. The second episode is associated with a lower energy release and a lower hard X-ray flux than the first episode. We also find evidence of a third, weaker episode of chromospheric evaporation that lasts for approximately [insert time]. The third episode is associated with a further decrease in the energy release and the hard X-ray flux.\n",
      "\n",
      "Our observations suggest that the multi-episode chromospheric evaporation in this flare is caused by a series of magnetic reconnections that occur successively along the flare loop. These reconnections are driven by magnetic flux emergence from beneath the solar surface. Moreover, our analysis shows that the multi-episode chromospheric evaporation is correlated with the temporal evolution of the thermal and non-thermal emissions in the flare. This suggests that chromospheric evaporation plays a crucial role in the energy transport from the corona to the solar surface during solar flares. Our results provide new insights into the mechanisms of energy transport in solar flares and have implications for our understanding of the physics of the Sun and other astrophysical objects. 1 into PostgreSQL...\n",
      "Inserting test sample 737  Due to the emergent adoption of distributed systems when building applications, demand for reliability and availability has increased. These properties can be achieved through replication techniques using middleware algorithms that must be capable of tolerating faults. Certain faults such as arbitrary faults, however, may be more difficult to tolerate, resulting in more complex and resource intensive algorithms that end up being not so practical to use. We propose and experiment with the use of consistency validation techniques to harden a benign fault-tolerant Paxos, thus being able to detect and tolerate non-malicious arbitrary faults. 0 into PostgreSQL...\n",
      "Inserting test sample 738  This paper proposes a new protocol for the Paxos consensus algorithm which involves validating the consistency of each round through the use of cryptographic signatures. The proposed protocol, called Hardened Paxos, improves the robustness of Paxos against malicious actors attempting to disrupt the consensus process. We analyze the performance of our protocol by running experiments on a network of machines and compare its performance against the original Paxos. Our results demonstrate that Hardened Paxos maintains high throughput and low latency while providing stronger security guarantees compared to the standard Paxos algorithm. 1 into PostgreSQL...\n",
      "Inserting test sample 739  In this paper we present another proofs of the geometrical forms of Paley-Wiener theorems for the Dunkl transform given in [15], and we prove inversion formulas for the Dunl interwining operator Vk and for its dual tVk and we deduce the expression of the representing distributions of the inverse operators vk(-1) and tvk(-1). 0 into PostgreSQL...\n",
      "Inserting test sample 740  In this paper, we present further proofs of the geometrical forms of the Paley-Wiener theorems for the Dunkl transform. We also derive the inversion formulas for both the Dunkl interweaving operator and its dual. These findings contribute to a greater understanding of the Dunkl transform and its applications in mathematical analysis and physics. 1 into PostgreSQL...\n",
      "Inserting test sample 741  We performed SMA observations in the C18O (2-1) emission line toward six Class 0 and I protostars, to study rotational motions of their surrounding envelopes and circumstellar material on 100 to 1000 AU scales. C18O (2-1) emission with intensity peaks located at the protostellar positions is detected toward all the six sources. The rotational velocities of the protostellar envelopes as a function of radius were measured from the Position-Velocity diagrams perpendicular to the outflow directions passing through the protostellar positions. Two Class 0 sources, B335 and NGC 1333 IRAS 4B, show no detectable rotational motion, while L1527 IRS (Class 0/I) and L1448-mm (Class 0) exhibit rotational motions with radial profiles of Vrot ~ r^{-1.0+/-0.2} and ~ r^{-1.0+/-0.1}, respectively. The other Class I sources, TMC-1A and L1489 IRS, exhibit the fastest rotational motions among the sample, and their rotational motions have flatter radial profiles of Vrot ~ r^{-0.6+/-0.1} and ~ r^{-0.5+/-0.1}, respectively. The rotational motions with the radial dependence of ~ r^{-1} can be interpreted as rotation with a conserved angular momentum in a dynamically infalling envelope, while those with the radial dependence of ~ r^{-0.5} can be interpreted as Keplerian rotation. These observational results demonstrate categorization of rotational motions from infalling envelopes to Keplerian-disk formation. Models of the inside-out collapse where the angular momentum is conserved are discussed and compared with our observational results. 0 into PostgreSQL...\n",
      "Inserting test sample 742  Low-mass protostars are the basic building blocks of stars. Unraveling their formation sequence is essential to understand the evolution of stars in our galaxy. In this paper, we present an evolutionary scenario that takes infalling envelopes to Keplerian disks around low-mass protostars, based on the physical properties and dynamics of the system. By observing the continuum and spectral line emissions from the envelopes and disks, we compile a sample of protostars in various stages of evolution. We find that infalling envelopes typically last for a few thousand years and have a significant impact on the subsequent disk formation. We analyze the rotation profiles of the disks and find that they gradually become Keplerian with time. The Keplerian disks are characterized by a higher angular momentum and a flattened structure. We employ detailed numerical simulations to reproduce our observations and investigate the underlying physics driving the system's evolution. Our simulations reveal that the dust evolution and magnetic fields play a crucial role in shaping the envelopes and disks. Our findings shed light on the complex formation mechanism of low-mass stars and provide essential insights into the origins of the solar system. 1 into PostgreSQL...\n",
      "Inserting test sample 743  We present TES, a new n-body integration code for the accurate and rapid propagation of planetary systems in the presence of close encounters. TES builds upon the classic Encke method and integrates only the perturbations to Keplerian trajectories to reduce both the error and runtime of simulations.\n",
      "\n",
      "Variable step size is used throughout to enable close encounters to be precisely handled. A suite of numerical improvements are presented that together makes TES optimal in terms of energy error. Lower runtimes are found in all test problems considered when compared to direct integration using ias15. TES is freely available. 0 into PostgreSQL...\n",
      "Inserting test sample 744  The Terrestrial Exoplanet Simulator (TES) is a planetary systems integrator that allows for close encounters by optimizing simulations to minimize error and increase accuracy. This tool is designed to test the dynamic stability of multiple planet systems and provide valuable insight into their long-term behavior. By accounting for gravitational interactions and orbital perturbations, the TES can accurately predict and reproduce the behavior of exoplanet systems in various scenarios. Its error optimal approach enables researchers to better understand the behavior of exoplanet systems in close encounters, leading to a deeper understanding of the universe beyond our solar system. 1 into PostgreSQL...\n",
      "Inserting test sample 745  Discussed is mechanics of objects with internal degrees of freedom in generally non-Euclidean spaces. Geometric peculiarities of the model are investigated detailly. Discussed are also possible mechanical applications, e.g., in dynamics of structured continua, defect theory and in other fields of mechanics of deformable bodies. Elaborated is a new method of analysis based on non-holonomic frames. We compare our results and methods with those of other authors working in nonlinear dynamics (many of them refer to our papers [20], [21], [49], [50]). Simple examples of completely integerable models are presented. 0 into PostgreSQL...\n",
      "Inserting test sample 746  This paper investigates the motion of test bodies with internal degrees of freedom within non-Euclidean spaces. We account for intrinsic degrees of freedom by proposing a new metric tensor that depends on the internal states of the test objects. Using this formalism, we obtain novel geodesic equations for the motion of the bodies. We showcase these results by examining the motion of test bodies with two degrees of freedom in hyperbolic spaces. Our findings indicate that intrinsic degrees of freedom can significantly impact the motion of test objects in curved spaces, highlighting the importance of considering these degrees in future works. 1 into PostgreSQL...\n",
      "Inserting test sample 747  Several authors have studied the question of when the monoid ring DM of a monoid M over a ring D is a right and/or left fir (free ideal ring), a semifir, or a 2-fir (definitions recalled in section 1). It is known that for M nontrivial, a necessary condition for any of these properties to hold is that D be a division ring. Under that assumption, necessary and sufficient conditions on M are known for DM to be a right or left fir, and various conditions on M have been proved necessary or sufficient for DM to be a 2-fir or semifir.\n",
      "\n",
      "A sufficient condition for DM to be a semifir is that M be a direct limit of monoids which are free products of free monoids and free groups. W.Dicks has conjectured that this is also necessary. However F.Ced\\'o has given an example of a monoid M which is not such a direct limit, but satisfies the known necessary conditions for DM to be a semifir. It is an open question whether for this M, the rings DM are semifirs.\n",
      "\n",
      "We note some reformulations of the known necessary conditions for DM to be a 2-fir or a semifir, motivate Ced\\'o's construction and a variant, and recover Ced\\'o's results for both constructions.\n",
      "\n",
      "Any homomorphism from a monoid M into \\Z induces a \\Z-grading on DM, and we show that for the two monoids in question, the rings DM are \"homogeneous semifirs\" with respect to all such nontrivial \\Z-gradings; i.e., have (roughly) the property that every finitely generated homogeneous one-sided ideal is free.\n",
      "\n",
      "If M is a monoid such that DM is an n-fir, and N a \"well-behaved\" submonoid of M, we obtain results on DN. Using these, we show that for M a monoid such that DM is a 2-fir, mutual commutativity is an equivalence relation on nonidentity elements of M, and each equivalence class, together with the identity element, is a directed union of infinite cyclic groups or infinite cyclic monoids.\n",
      "\n",
      "Several open questions are noted. 0 into PostgreSQL...\n",
      "Inserting test sample 748  In this paper, we investigate the properties of monoids, 2-firs, and semifirs, which are algebraic structures that have been previously studied. We begin with a brief introduction to monoids, which are algebraic structures consisting of a set of elements and a binary operation that is associative and has an identity element. We then proceed to define 2-firs, which are a type of monoid with additional properties, and explore their applications to computer programming and automata theory.\n",
      "\n",
      "Next, we focus our attention on semifirs, which are a subclass of 2-firs that have further interesting properties. We define the notion of a minimal semifir, which has the property that every non-identity element can be expressed as a product of two non-zero elements. We study the structure of minimal semifirs and provide a complete classification of them.\n",
      "\n",
      "We then go on to investigate the relationship between 2-firs and semifirs, showing that every 2-fir can be embedded as a minimal semifir. We provide a necessary and sufficient condition for a 2-fir to be a semifir, which is based on the existence of certain types of idempotents. Finally, we show that every finite 2-fir can be expressed as a product of directly indecomposable semifirs.\n",
      "\n",
      "Our results have implications for a variety of research areas, including algebraic complexity theory and automata theory. They also contribute to a deeper understanding of the structure and properties of monoids, 2-firs, and semifirs, and provide new avenues for future research in this area. In summary, our work sheds light on the interplay between these algebraic structures and their applications in computer science and related fields. 1 into PostgreSQL...\n",
      "Inserting test sample 749  We study the maximal multiplicity locus of a variety $X$ over a field of characteristic $p>0$ that is provided with a finite surjective radical morphism $\\delta:X\\rightarrow V$, where $V$ is regular, for example, when $X\\subset\\mathbb{A}^{n+1}$ is a hypersurface defined by an equation of the form $T^{q}-f(x_{1},\\ldots,x_{n})=0$ and $\\delta$ is the projection onto $V:=\\operatorname{Spec}(k[x_{1},\\ldots,x_{n}])$. The multiplicity along points of $X$ is bounded by the degree, say $d$, of the field extension $K(V)\\subset K(X)$. We denote by $F_{d}(X)\\subset X$ the set of points of multiplicity $d$.\n",
      "\n",
      "Our guiding line is the search for invariants of singularities $x\\in F_{d}(X)$ with a good behavior property under blowups $X'\\rightarrow X$ along regular centers included in $F_{d}(X)$, which we call \\emph{invariants with the pointwise inequality property}.\n",
      "\n",
      "A finite radicial morphism $\\delta:X\\to V$ as above will be expressed in terms of an $\\mathcal{O}_{V}^{q}$-submodule $\\mathscr{M}\\subseteq\\mathcal{O}_{V}$. A blowup $X'\\to X$ along a regular equimultiple center included in $F_{d}(X)$ induces a blowup $V'\\to V$ along a regular center and a finite morphism $\\delta':X'\\to V'$. A notion of transform of the $\\mathcal{O}_{V}^{q}$-module $\\mathscr{M}\\subset\\mathcal{O}_{V}$ to an $\\mathcal{O}_{V'}^{q}$-module $\\mathscr{M}'\\subset\\mathcal{O}_{V'}$ will be defined in such a way that $\\delta':X'\\to V'$ is the radicial morphism defined by $\\mathscr{M}'$. Our search for invariants relies on techniques involving differential operators on regular varieties and also on logarithmic differential operators. Indeed, the different invariants we introduce and the stratification they define will be expressed in terms of ideals obtained by evaluating differential operators of $V$ on $\\mathcal{O}_{V}^{q}$-submodules $\\mathscr{M}\\subset\\mathcal{O}_{V}$. 0 into PostgreSQL...\n",
      "Inserting test sample 750  In algebraic geometry, radicial coverings are a common object of study due to their geometric and topological properties. The purpose of this research is to investigate the multiplicity of such coverings along points of a regular variety. \n",
      "\n",
      "We begin by defining the notion of a radicial covering and its associated multiplicity along a point. We then study the behavior of these multiplicities under the action of the covering map and establish their local properties. \n",
      "\n",
      "Our main result is a formula for the total multiplicity of a radicial covering along a regular variety. This formula is derived by studying the local behavior of the multiplicities and their effect on the global geometry of the covering. We also provide examples of regular varieties and radicial coverings to illustrate our findings. \n",
      "\n",
      "Furthermore, we develop a connection between the multiplicities of a radicial covering and the ramification indices of its associated field extension. This connection is used to establish a criterion for a radicial covering to be tamely ramified. \n",
      "\n",
      "We conclude our research by discussing the implications of our results in the context of algebraic geometry and related fields, such as number theory and topology. We highlight the importance of understanding the multiplicity of radicial coverings along regular varieties in the study of algebraic curves and surfaces. \n",
      "\n",
      "Overall, this research provides a comprehensive study of the multiplicity of radicial coverings along regular varieties and establishes important connections between algebraic geometry and other areas of mathematics. 1 into PostgreSQL...\n",
      "Inserting test sample 751  We present measurements of $E$-mode polarization and temperature-$E$-mode correlation in the cosmic microwave background (CMB) using data from the first season of observations with SPTpol, the polarization-sensitive receiver currently installed on the South Pole Telescope (SPT). The observations used in this work cover 100~\\sqdeg\\ of sky with arcminute resolution at $150\\,$GHz. We report the $E$-mode angular auto-power spectrum ($EE$) and the temperature-$E$-mode angular cross-power spectrum ($TE$) over the multipole range $500 < \\ell \\leq5000$. These power spectra improve on previous measurements in the high-$\\ell$ (small-scale) regime. We fit the combination of the SPTpol power spectra, data from \\planck\\, and previous SPT measurements with a six-parameter \\LCDM cosmological model. We find that the best-fit parameters are consistent with previous results. The improvement in high-$\\ell$ sensitivity over previous measurements leads to a significant improvement in the limit on polarized point-source power: after masking sources brighter than 50\\,mJy in unpolarized flux at 150\\,GHz, we find a 95\\% confidence upper limit on unclustered point-source power in the $EE$ spectrum of $D_\\ell = \\ell (\\ell+1) C_\\ell / 2 \\pi < 0.40 \\ \\mu{\\mbox{K}}^2$ at $\\ell=3000$, indicating that future $EE$ measurements will not be limited by power from unclustered point sources in the multipole range $\\ell < 3600$, and possibly much higher in $\\ell.$ 0 into PostgreSQL...\n",
      "Inserting test sample 752  This paper presents the results of an analysis of data obtained from the SPTpol instrument, which was used to measure the E-mode polarization and temperature-E-mode correlation in the cosmic microwave background (CMB). Specifically, the analysis focused on 100 square degrees of data obtained between 2012 and 2015. The authors generated CMB maps from the observations, and then used a maximum likelihood approach to estimate the E-mode polarization and temperature-E-mode correlation power spectra from the maps. The analysis revealed that the E-mode polarization power spectrum has a clear peak, consistent with the standard inflationary cosmological model. In addition, the authors found evidence for the lensing of the CMB, which allowed them to estimate the optical depth to the last scattering surface of the CMB. The temperature-E-mode correlation power spectrum was also found to be consistent with the standard cosmological model. The results presented in this paper are important for our understanding of the early universe, and provide further evidence in support of the standard cosmological model. The authors note that future data releases from SPTpol, as well as from other CMB experiments, will allow for even more precise measurements of the E-mode polarization and temperature-E-mode correlation. 1 into PostgreSQL...\n",
      "Inserting test sample 753  The majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed, leaving a vast store of potential training signal untapped. In this work, we propose the self-feeding chatbot, a dialogue agent with the ability to extract new training examples from the conversations it participates in. As our agent engages in conversation, it also estimates user satisfaction in its responses. When the conversation appears to be going well, the user's responses become new training examples to imitate. When the agent believes it has made a mistake, it asks for feedback; learning to predict the feedback that will be given improves the chatbot's dialogue abilities further. On the PersonaChat chit-chat dataset with over 131k training examples, we find that learning from dialogue with a self-feeding chatbot significantly improves performance, regardless of the amount of traditional supervision. 0 into PostgreSQL...\n",
      "Inserting test sample 754  This paper investigates the potential of using conversational data collected after deployment to improve the performance of dialogue systems. Specifically, we focus on chatbots and propose a novel approach called \"Feed Yourself, Chatbot!\" that enables the chatbot to learn from its own post-deployment conversational data by treating them as a valuable source of user feedback. We show that our approach outperforms the state-of-the-art methods that rely on pre-defined training datasets and demonstrate its effectiveness through experiments on a real-world chatbot deployed in a customer service setting. Moreover, we present a fine-grained analysis of the learned representations and observe that the chatbot implicitly learns user preferences and sentiment from its conversations. Our work highlights the potential benefits of learning from post-deployment dialogues and opens up new avenues for future research in dialogue systems. 1 into PostgreSQL...\n",
      "Inserting test sample 755  We study the approach to equilibrium of a Bose gas to a superfluid state. We point out that dynamic scaling, characteristic of far from equilibrium phase-ordering systems, should hold. We stress the importance of a non-dissipative Josephson precession term in driving the system to a new universality class. A model of coarsening in dimension $d=2$, involving a quench between two temperatures below the equilibrium superfluid transition temperature ($T_c$), is exactly solved and demonstrates the relevance of the Josephson term. Numerical results on quenches from above $T_c$ in $d=2,3$ provide evidence for the scaling picture postulated. 0 into PostgreSQL...\n",
      "Inserting test sample 756  In this paper, we investigate the phase ordering kinetics of a Bose gas. We consider a system of particles with attractive interactions and show that, in the low-temperature regime, the condensate fraction rises progressively from a low initial value to its final equilibrium value. We investigate the role of thermal fluctuations and the effect of the system's symmetry on the kinetic pathway to equilibrium. Our results provide insight into the non-equilibrium behavior of Bose gases and may have important implications for the design of Bose-Einstein condensation experiments. 1 into PostgreSQL...\n",
      "Inserting test sample 757  We present deep near-infrared spectroscopy of six quasars at 6.1<z<6.7 with VLT/X-Shooter and Gemini-N/GNIRS. Our objects, originally discovered through a wide-field optical survey with the Hyper Suprime-Cam (HSC) Subaru Strategic Program (HSC-SSP), have the lowest luminosities (-25.5< M1450<-23.1 mag) of the z>5.8 quasars with measured black hole masses. From single-epoch mass measurements based on MgII2798, we find a wide range in black hole masses, from M_BH=10^7.6 to 10^9.3 Msun. The Eddington ratios L_bol/L_Edd range from 0.16 to 1.1, but the majority of the HSC quasars are powered by M_BH=10^9 Msun supermassive black holes (SMBHs) accreting at sub-Eddington rates. The Eddington ratio distribution of the HSC quasars is inclined to lower accretion rates than those of Willott et al. (2010a), who measured the black hole masses for similarly faint z=6 quasars. This suggests that the global Eddington ratio distribution is wider than has previously been thought. The presence of M_BH=10^9 Msun SMBHs at z=6 cannot be explained with constant sub-Eddington accretion from stellar remnant seed black holes. Therefore, we may be witnessing the first buildup of the most massive black holes in the first billion years of the universe, the accretion activity of which is transforming from active growth to a quiescent phase. Measurements of a larger complete sample of z>6 low-luminosity quasars, as well as deeper observations with future facilities will enable us to better understand the early SMBH growth in the reionization epoch. 0 into PostgreSQL...\n",
      "Inserting test sample 758  The Subaru High-z Exploration of Low-Luminosity Quasars (SHELLQs) project is designed to explore quasars with low luminosity. In this study, we have measured the black hole masses of six quasars at redshifts ranging from 6.1 to 6.7 using Subaru/Hyper Suprime-Cam (HSC). By analyzing the broad emission line regions in the spectra of these sources, we were able to estimate the mass of the central black hole via the virial method. The combination of deep imaging data from HSC and the infrared Wide-field Infrared Survey Explorer (WISE) allowed us to obtain accurate photometric redshifts and luminosities for the quasars.\n",
      "\n",
      "Our black hole mass estimates range from (1.6-3.8) Ã— 10^8 solar masses, with uncertainties of about a factor of two. These black holes are among the most massive known at this early epoch, suggesting that they must have grown rapidly in the early universe. We also find that the average Eddington ratio, which measures the accretion rate relative to the maximum possible rate at which matter can fall onto a black hole, is highest for our most massive quasars.\n",
      "\n",
      "Our results provide important constraints on models of early black hole growth and the co-evolution of black holes and galaxies in the early universe. The SHELLQs quasar sample represents the largest homogeneous sample of low-luminosity quasars at z > 6, and we expect that our findings will motivate further observational and theoretical investigations of the early universe. 1 into PostgreSQL...\n",
      "Inserting test sample 759  Obtaining dense 3D reconstrution with low computational cost is one of the important goals in the field of SLAM. In this paper we propose a dense 3D reconstruction framework from monocular multispectral video sequences using jointly semi-dense SLAM and Multispectral Photometric Stereo approaches.\n",
      "\n",
      "Starting from multispectral video, SALM (a) reconstructs a semi-dense 3D shape that will be densified;(b) recovers relative sparse depth map that is then fed as prioris into optimization-based multispectral photometric stereo for a more accurate dense surface normal recovery;(c)obtains camera pose that is subsequently used for conversion of view in the process of fusion where we combine the relative sparse point cloud with the dense surface normal using the automated cross-scale fusion method proposed in this paper to get a dense point cloud with subtle texture information. Experiments show that our method can effectively obtain denser 3D reconstructions. 0 into PostgreSQL...\n",
      "Inserting test sample 760  This research paper presents a novel approach for real-time dense 3D reconstruction by combining SLAM (Simultaneous Localization and Mapping) with multi-spectral photometric stereo. Our proposed method can effectively handle changes in lighting conditions and extract accurate 3D geometries under various lighting conditions. This is achieved by exploiting the spectral characteristics of the scene and illuminants. Specifically, we utilized multi-spectral photometric stereo to obtain accurate surface normals from multiple illuminants that capture different spectral characteristics of the scene. The resulting surface normals are then fused with the SLAM trajectory to generate a dense 3D map. Our experiments show that the proposed method achieves better reconstruction accuracy and robustness than existing SLAM solutions in challenging illumination conditions. This approach can pave the way for a new generation of real-time 3D reconstruction systems for a wide range of applications in robotics, AR/VR, and autonomous driving. 1 into PostgreSQL...\n",
      "Inserting test sample 761  Graphene nanoribbons (GNRs) have attracted a strong interest from researchers worldwide, as they constitute an emerging class of quantum-designed materials.\n",
      "\n",
      "The major challenges towards their exploitation in electronic applications include reliable contacting, complicated by their small size ($<$50 nm), as well as the preservation of their physical properties upon device integration.\n",
      "\n",
      "In this combined experimental and theoretical study, we report on the quantum dot (QD) behavior of atomically precise GNRs integrated in a device geometry.\n",
      "\n",
      "The devices consist of a film of aligned 5-atoms wide GNRs (5-AGNRs) transferred onto graphene electrodes with a sub 5-nm nanogap. We demonstrate that the narrow-bandgap 5-AGNRs exhibit metal-like behavior resulting in linear IV curves for low bias voltages at room temperature and single-electron transistor behavior for temperatures below 150~K. By performing spectroscopy of the molecular levels at 13~K, we obtain addition energies in the range of 200-300 meV. DFT calculations predict comparable addition energies and reveal the presence of two electronic states within the bandgap of infinite ribbons when the finite length of the 5-AGNRs is accounted for. By demonstrating the preservation of the 5-AGNRs electronic properties upon device integration, as demonstrated by transport spectroscopy, our study provides a critical step forward in the realisation of more exotic GNR-based nano-electronic devices. 0 into PostgreSQL...\n",
      "Inserting test sample 762  The ability to fabricate atomically-precise semiconductors has great potential for developing quantum dot devices, which may be utilized in a variety of cutting-edge technological innovations. This study investigates quantum dot formation in atomically engineered graphene nanoribbons (GNRs) field-effect transistors (FETs). Using a combination of experimental and theoretical techniques, we explore the controlled fabrication of quantum dots on GNRs and their influence on the electrical properties of the FET device. The fabrication process involves the partial unzipping of carbon nanotubes and the chemical functionalization of the GNR edges to obtain a high degree of control over the quantum dot formation. We have demonstrated the ability to tune the size and position of quantum dots within the GNRs using this approach. By characterizing the electrical transport properties of the FET devices, we have evaluated the quantum confinement effects of the resulting quantum dots and their influence on the device performance. Our results show that FET devices incorporating quantum dots exhibit enhanced electrical and optical properties, which hold great promise for future applications in quantum information processing and optoelectronics. This work lays the foundation for the development of new approaches for advanced nanoelectronics through precise engineering of the atomic structures of materials. 1 into PostgreSQL...\n",
      "Inserting test sample 763  The tropical Grassmannian ${\\rm Trop}\\, G(2,n)$ is known to be the moduli space of unrooted metric trees with $n$ leaves. A positive part can be defined for each of the $(n-1)!/2$ possible planar orderings, $\\alpha$, and agrees with the corresponding planar trees in the moduli space, ${\\rm Trop}^{\\alpha}G(2,n)$. Motivated by a physical application we study the way ${\\rm Trop}^{\\alpha}G(2,n)$ and ${\\rm Trop}^{\\beta}G(2,n)$ intersect in ${\\rm Trop}\\, G(2,n)$. We define their intersection number as the number of unrooted binary trees that belong to both and construct a $(n-1)!/2\\times (n-1)!/2$ intersection matrix. We are interested in finding the diagonal (up to permutations of rows and columns) submatrices of maximum possible rank for a given $n$. We prove that such diagonal matrices cannot have rank larger than $(n-3)!$ using the CHY formalism. We also prove that the bound is saturated for $n=5$ (the condition is trivial for $n=4$), that for $n=6$ the maximum rank is $4$, and that for $n=7$ the maximum rank is $\\geq 14$. We also ask the following question: Is there a value $n_{\\rm c}$ so that for any $n>n_{\\rm c}$ the bound $(n-3)!$ is always saturated? We review and extend two relevant results in the literature. The first is the Kawai-Lewellen-Tye (KLT) choice of sets which leads to a $(n-3)!\\times (n-3)!$ block diagonal submatrix with blocks of size $d\\times d$ with $d = \\lceil (n-3)/2\\rceil !\\lfloor (n-3)/2\\rfloor !$. The second result is that the number of ${\\rm Trop}^\\alpha G(2,n)$'s that intersect a given one grows as $\\exp(n\\log (3+\\sqrt{8}))$ for large $n$ which implies that the density of the intersection matrix goes as $\\exp(-n(\\log(n)-2.76))$. We interpret this as an indication that the generic behavior is not seen until $n \\approx \\exp (2.76)$, i.e. $n = 16$. We also find an exact formula for the number of zeros in a KLT block. 0 into PostgreSQL...\n",
      "Inserting test sample 764  The study of geometric objects in tropical geometry has recently gained significant attention due to its potential applications in algebraic geometry and combinatorics. In this paper, we investigate diagonally embedded sets of ${\\rm Trop}^+G(2,n)$'s in ${\\rm Trop}\\, G(2,n)$, and explore the existence of a critical value of $n$.\n",
      "\n",
      "We begin by introducing the concept of tropical Grassmannians, which are parameterized by sets of points in general position in tropical projective space. These objects naturally arise as analogues of classical Grassmannians in algebraic geometry and have been extensively studied in recent years. In particular, ${\\rm Trop}^+G(2,n)$'s are a class of tropical Grassmannians that have been shown to exhibit rich geometric properties.\n",
      "\n",
      "We then turn our attention to diagonally embedded sets of ${\\rm Trop}^+G(2,n)$'s in ${\\rm Trop}\\, G(2,n)$. These are collections of ${\\rm Trop}^+G(2,n)$'s that lie on a diagonal in tropical projective space, and have been studied in previous works. We analyze the combinatorial structure of these sets and investigate their relationship with other objects in tropical geometry.\n",
      "\n",
      "Finally, we address the question of whether there exists a critical value of $n$ for which the geometry of diagonally embedded sets of ${\\rm Trop}^+G(2,n)$'s in ${\\rm Trop}\\, G(2,n)$ changes dramatically. We prove that such a critical value does not exist for certain families of diagonally embedded sets, and conjecture that this is true in general.\n",
      "\n",
      "Overall, our study provides a deeper understanding of the geometry of tropical Grassmannians and its connection with diagonally embedded sets. Moreover, our investigation of critical values of $n$ contributes to the broader understanding of the behavior of tropical geometry as $n$ varies, and may have implications for future applications of tropical methods in algebraic geometry and combinatorics. 1 into PostgreSQL...\n",
      "Inserting test sample 765  With growing emphasis on personalized cancer-therapies,radiogenomics has shown promise in identifying target tumor mutational status on routine imaging (i.e. MRI) scans. These approaches fall into 2 categories: (1) deep-learning/radiomics (context-based), using image features from the entire tumor to identify the gene mutation status, or (2) atlas (spatial)-based to obtain likelihood of gene mutation status based on population statistics. While many genes (i.e. EGFR, MGMT) are spatially variant, a significant challenge in reliable assessment of gene mutation status on imaging has been the lack of available co-localized ground truth for training the models. We present Spatial-And-Context aware (SpACe) \"virtual biopsy\" maps that incorporate context-features from co-localized biopsy site along with spatial-priors from population atlases, within a Least Absolute Shrinkage and Selection Operator (LASSO) regression model, to obtain a per-voxel probability of the presence of a mutation status (M+ vs M-). We then use probabilistic pair-wise Markov model to improve the voxel-wise prediction probability. We evaluate the efficacy of SpACe maps on MRI scans with co-localized ground truth obtained from corresponding biopsy, to predict the mutation status of 2 driver genes in Glioblastoma: (1) EGFR (n=91), and (2) MGMT (n=81). When compared against deep-learning (DL) and radiomic models, SpACe maps obtained training and testing accuracies of 90% (n=71) and 90.48% (n=21) in identifying EGFR amplification status,compared to 80% and 71.4% via radiomics, and 74.28% and 65.5% via DL. For MGMT status, training and testing accuracies using SpACe were 88.3% (n=61) and 71.5% (n=20), compared to 52.4% and 66.7% using radiomics,and 79.3% and 68.4% using DL. Following validation,SpACe maps could provide surgical navigation to improve localization of sampling sites for targeting of specific driver genes in cancer. 0 into PostgreSQL...\n",
      "Inserting test sample 766  This study describes a novel method to identify tumor mutational status based on spatial and contextual information derived from structural magnetic resonance imaging (MRI) data. Specifically, the proposed approach involves generating \"virtual biopsy\" radiogenomic maps that integrate MRI-derived imaging features with genomic data. By leveraging spatial and contextual information in this way, we aim to improve the accuracy of non-invasive assessments of tumor mutational status, which can have important implications for patient treatment decisions.\n",
      "\n",
      "To develop the proposed method, we first trained a machine learning model on a large dataset of patients with known tumor mutational status, using a variety of imaging features derived from structural MRI. We then used this model to predict the mutational status of previously unseen patients, based solely on their imaging data. Our results demonstrated that the proposed method improved the accuracy of mutational status prediction compared to existing non-invasive approaches.\n",
      "\n",
      "To further evaluate the clinical potential of the proposed approach, we applied it to a separate cohort of patients with glioblastoma, a type of brain cancer with a high degree of heterogeneity. Our analyses revealed that the proposed method was able to accurately predict mutational status in these patients, and also identified spatial patterns in the imaging data that were associated with specific mutations.\n",
      "\n",
      "Overall, our study demonstrates the feasibility and potential clinical utility of using spatial and contextual information derived from structural MRI to predict tumor mutational status. If validated in larger studies, this approach could ultimately lead to more accurate and personalized treatment decisions for patients with cancer. 1 into PostgreSQL...\n",
      "Inserting test sample 767  Spontaneous self-assembly in molecular systems is a fundamental route to both biological and engineered soft matter. Simple micellisation, emulsion formation, and polymer mixing principles are well understood. However, the principles behind emergence of structures with competing length scales in soft matter systems remain an open question. Examples include the droplet-inside-droplet assembly in many biomacromolecular systems undergoing liquid-liquid phase separation, analogous multiple emulsion formation in oil-surfactant-water formulations, and polymer core-shell particles with internal structure. We develop here a microscopic theoretical model based on effective interactions between the constituents of a soft matter system to explain self-organization both at single and multiple length scales. The model identifies how spatial ordering at multiple length scales emerges due to competing interactions between the system components, e.g. molecules of different sizes and different chemical properties. As an example of single and multiple-length-scale assembly, we map out a generic phase diagram for a solution with two solute species differing in their mutual and solvent interactions. By performing molecular simulations on a block-copolymer system, we further demonstrate how the phase diagram can be connected to a molecular system that has a transition from regular single-core polymer particles to multi-core aggregates that exhibit multiple structural length scales. The findings provide guidelines to understanding the length scales rising spontaneously in biological self-assembly, but also open new venues to the development and engineering of biomolecular and polymeric functional materials. 0 into PostgreSQL...\n",
      "Inserting test sample 768  The self-assembly of soft matter with multiple length scales is an intriguing phenomenon that has garnered much attention in recent years. This process involves the spontaneous organization of macromolecules, colloids, and nanoparticles into ordered structures with unique properties and functionalities. The resulting materials have applications across a wide range of fields, including biomedicine, optics, electronics, and energy. In this review, we provide an overview of the most important theoretical and experimental approaches used to understand and harness self-assembly in soft matter with multiple length scales.\n",
      "\n",
      "We start by discussing the different driving forces that govern self-assembly at the nanoscale, mesoscale, and macroscale, namely, thermodynamics, kinetics, and geometry. We then describe the most commonly used soft matter building blocks and their self-assembly pathways, such as block copolymers, surfactants, lipids, and proteins. Special emphasis is given to the role of external stimuli, such as temperature, pH, light, and electric fields, in controlling the self-assembly process and inducing dynamic and responsive materials. \n",
      "\n",
      "In addition, we highlight some of the key challenges and opportunities associated with self-assembly in soft matter with multiple length scales, including the need for precise control over the size, shape, and composition of the resulting structures, as well as the potential for hierarchical assembly and novel functionalities. We conclude by discussing some promising areas of future research in this exciting field. 1 into PostgreSQL...\n",
      "Inserting test sample 769  We examine the interplay between gravitational collapse and moduli stability in the context of black hole formation. We perform numerical simulations of the collapse using the double null formalism and show that the very dense regions one expects to find in the process of black hole formation are able to destabilize the volume modulus. We establish that the effects of the destabilization will be visible to an observer at infinity, opening up a window to a region in spacetime where standard model's couplings and masses can differ significantly from their background values. 0 into PostgreSQL...\n",
      "Inserting test sample 770  This paper explores the phenomena of moduli destabilization through gravitational collapse. Starting from a nonlinear sigma model, we investigate the dynamics of a scalar field in a curved background along with its interaction with gravity. We find that the collapse of a supermassive object triggers a cascade of nonlinear fluctuations in the scalar potential that may lead to moduli destabilization. We analyze the physical properties of this process and discuss its implications for the stability of extra dimensions in string compactifications. Our results suggest a possible solution to the long-standing problem of stabilizing moduli fields in realistic string models. 1 into PostgreSQL...\n",
      "Inserting test sample 771  We present the first Swift Ultra-Violet/Optical Telescope (UVOT) gamma-ray burst (GRB) afterglow catalog. The catalog contains data from over 64,000 independent UVOT image observations of 229 GRBs first detected by Swift, the High Energy Transient Explorer 2 (HETE2), the INTErnational Gamma-Ray Astrophysics Laboratory (INTEGRAL), and the Interplanetary Network (IPN). The catalog covers GRBs occurring during the period from 2005 Jan 17 to 2007 Jun 16 and includes ~86% of the bursts detected by the Swift Burst Alert Telescope (BAT). The catalog provides detailed burst positional, temporal, and photometric information extracted from each of the UVOT images. Positions for bursts detected at the 3-sigma-level are provided with a nominal accuracy, relative to the USNO-B1 catalog, of ~0.25 arcseconds. Photometry for each burst is given in three UV bands, three optical bands, and a 'white' or open filter.\n",
      "\n",
      "Upper limits for magnitudes are reported for sources detected below 3-sigma.\n",
      "\n",
      "General properties of the burst sample and light curves, including the filter-dependent temporal slopes, are also provided. The majority of the UVOT light curves, for bursts detected at the 3-sigma-level, can be fit by a single power-law, with a median temporal slope (alpha) of 0.96, beginning several hundred seconds after the burst trigger and ending at ~1x10^5 s. The median UVOT v-band (~5500 Angstroms) magnitude at 2000 s for a sample of \"well\" detected bursts is 18.02. The UVOT flux interpolated to 2000 s after the burst, shows relatively strong correlations with both the prompt Swift BAT fluence, and the Swift X-ray flux at 11 hours after the trigger. 0 into PostgreSQL...\n",
      "Inserting test sample 772  The study of Gamma-Ray Bursts (GRBs) afterglows has been revolutionized by the launch of Swift, a dedicated mission to observe GRBs in multi-wavelengths. In this paper, we present the first catalog of Swift Ultra-Violet/Optical Telescope (UVOT) GRB afterglows. This catalog contains 246 GRB afterglows detected by Swift/UVOT from its launch in 2004 until 2019, including 127 GRBs with redshifts. For all 246 GRBs, we report the UVOT magnitudes in each of the seven filters, as well as the derived spectral energy distributions (SEDs). Additionally, for the GRBs with redshifts, we estimate the intrinsic luminosities and jet opening angles using the SEDs. This catalog will serve as an important resource for the GRB community, enabling statistical studies of GRB afterglows in the UV/optical bands, and providing a valuable reference for future observations. We highlight some interesting features in the catalog, such as the correlation between X-ray and UV/optical colors, the UV/optical luminosity function, and the diversity in SED shapes. We also find that the distribution of jet opening angles is consistent with a Gaussian with a mean of 6.2 degrees and a standard deviation of 2.6 degrees. Looking forward, we anticipate that this catalog will facilitate new discoveries and advance our understanding of the physics behind GRB afterglows. 1 into PostgreSQL...\n",
      "Inserting test sample 773  We present the results of a search for z=9-10 galaxies within the first 8 pointings of the Hubble Frontier Fields (HFF) (4 clusters plus 4 parallel fields) and 20 cluster fields from the CLASH survey. Combined with our previous analysis of the Hubble Ultra-Deep field (HUDF), we have now completed a search for z=9-10 galaxies over ~130 sq. arcmin, across 29 HST WFC3/IR pointings. As in our recent study of the first two HFF fields, we confine our primary search for high-redshift candidates in the HFF imaging to the uniformly deep (i.e.\n",
      "\n",
      "sigma_160>30 AB mag in 0.5-arcsec diameter apertures), relatively low magnification regions. In the CLASH fields our search was confined to uniformly deep regions where sigma_160>28.8 AB mag. Our SED fitting analysis unveils a sample of 33 galaxy candidates at z_phot>=8.4, five of which have primary photometric redshift solutions in the range 9.6<z_phot<11.2. By calculating a de-lensed effective volume for each candidate, the improved statistics and reduced cosmic variance provided by our new sample allows a more accurate determination of the UV-selected galaxy luminosity function (LF) at z~9. Our new results strengthen our previous conclusion that the LF appears to evolve smoothly from z=8 to z=9, an evolution which can be equally well modelled by a factor of ~2 drop in density, or a dimming of ~0.5 mag in M*. Moreover, based on our new sample, we are able to place initial constraints on the z=10 LF, finding that the number density at M_1500 ~ -19.7 is log(phi) = -4.1 (+0.2,-0.3), a factor of ~2 lower than at z=9. Finally, we use our new results to re-visit the issue of the decline in UV luminosity density at z>=8. We conclude that the data continue to support a smooth decline in rho_UV over the redshift interval 6<z<10, in agreement with simple models of early galaxy evolution driven by the growth in the underlying dark matter halo mass function. 0 into PostgreSQL...\n",
      "Inserting test sample 774  The study presented in this paper investigates the galaxy population at redshifts z=9-10 in the Hubble Frontier Fields and Cluster Lensing And Supernova survey (CLASH). Specifically, it explores the luminosity function (LF) of these galaxies in order to gain a better understanding of their evolution and characteristics.\n",
      "\n",
      "The authors use a combination of data from the Hubble Space Telescope and spectroscopic observations to identify and analyze a sample of z=9-10 galaxies. They find evidence of a smooth decline in UV luminosity at zâ‰¥8, which suggests that the early universe was dominated by low-luminosity galaxies. This finding is consistent with previous studies and helps to further our understanding of galaxy formation and evolution.\n",
      "\n",
      "In addition to the LF analysis, the authors also explore the relationship between star formation and metallicity in z=9-10 galaxies. They find that there is a correlation between these two factors, with higher metallicity indicating greater star formation rates. This result provides valuable insights into the complex processes of early galaxy formation.\n",
      "\n",
      "Overall, this paper contributes to our growing knowledge of the high-redshift universe and sheds light on the properties of z=9-10 galaxies. With the help of sophisticated survey instruments and analytical techniques, we are able to explore and learn from the most distant objects in the cosmos.\n",
      "\n",
      "In conclusion, the study presented in this paper provides further evidence for a smooth decline in UV luminosity at zâ‰¥8 and sheds light on the relationship between star formation and metallicity in z=9-10 galaxies. This research contributes to our understanding of the early universe and the processes of galaxy formation and evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 775  MyShake harnesses private/personal smartphones to build a global seismic network. It uses the accelerometers embedded in all smartphones to record ground motions induced by earthquakes, returning recorded waveforms to a central repository for analysis and research. A demonstration of the power of citizen science, MyShake expanded to 6 continents within days of being launched, and has recorded 757 earthquakes in the first 2 years of operation.\n",
      "\n",
      "The data recorded by MyShake phones has the potential to be used in scientific applications, thereby complementing current seismic networks. In this paper: (1) we report the capabilities of smartphone sensors to detect earthquakes by analyzing the earthquake waveforms collected by MyShake. (2) We determine the maximum epicentral distance at which MyShake phones can detect earthquakes as a function of magnitude. (3) We then determine the capabilities of the MyShake network to estimate the location, origin time, depth and magnitude of earthquakes. In the case of earthquakes for which MyShake has provided 4 or more phases (21 events), either P- or S-wave signals, and has an azimuthal gap less than 180 degrees, the median location, origin time and depth errors are 2.7 km, 0.2 s, and 0.1 km respectively relative to USGS global catalog locations. Magnitudes are also estimated and have a mean error of 0.0 and standard deviation 0.2. These preliminary results suggest that MyShake could provide basic earthquake catalog information in regions that currently have no traditional networks. With an expanding MyShake network, we expect the event detection capabilities to improve and provide useful data on seismicity and hazards. 0 into PostgreSQL...\n",
      "Inserting test sample 776  With the increasing ubiquity of smartphones across the globe, the potential for leveraging these devices for scientific purposes has grown substantially. In this research paper, we introduce MyShake, a smartphone app designed to detect and characterize earthquakes using the sensors built into modern smartphones. By utilizing the accelerometers on smartphones to measure ground motion, MyShake provides low-cost seismic monitoring and earthquake detection capabilities to users around the world. \n",
      "\n",
      "We describe the technical details of MyShake and outline the processing pipeline used to extract earthquake signals from the abundance of noise generated by the radios, CPUs, and other components of smartphones. We also present results from the use of MyShake in both simulated and real earthquake scenarios, including verification of its ability to accurately detect earthquakes in comparison to traditional seismometers. \n",
      "\n",
      "Furthermore, we highlight the potential for MyShake to provide early warning of earthquakes to individuals and communities, which could significantly reduce the damage and loss of life caused by these natural disasters. With its ability to create a global network of seismic monitoring devices, MyShake represents a powerful tool for earthquake research and hazard mitigation efforts. \n",
      "\n",
      "We conclude with a discussion of future directions for MyShake development and deployment, including the integration of additional sensors and the expansion of its user base. Our hope is that MyShake will empower individuals and communities around the world to contribute to seismic research and improve our understanding of earthquakes and their impact on society. 1 into PostgreSQL...\n",
      "Inserting test sample 777  LSST's wide-field of view and sensitivity will revolutionize studies of the transient sky by finding extraordinary numbers of new transients every night.\n",
      "\n",
      "The recent discovery of a kilonova counterpart to LIGO/Virgo's first detection of gravitational waves (GWs) from a double neutron star (NS-NS) merger also creates an exciting opportunity for LSST to offer a Target of Opportunity (ToO) mode of observing. We have been exploring the possibility of detecting strongly lensed GWs, that would enable new tests of GR, extend multi-messenger astronomy out to $z\\gtrsim1$, and deliver a new class of sub-millisecond precision time-delay constraints on lens mass distributions. We forecast that the rate of detection of lensed NS-NS mergers in the 2020s will be $\\sim0.1$ per Earth year, that the typical source will be at $z\\simeq2$, and that the multiply-imaged kilonova counterpart will have a magnitude of ${\\rm AB}\\simeq25.4$ in $g/r/i$-band filters - i.e. fainter than the sensitivity of a single LSST WFD visit. We therefore advocate (1) creating a flexible and efficient Target of Opportunity programme within the LSST observing strategy that is capable of discovering sources fainter than single-visit depth, and (2) surveying the entire observable extragalactic sky as rapidly as possible in the WFD survey. The latter will enable a very broad range of early science that relies on wide survey area for detection of large samples of objects and/or maximizing the fraction of sky over which reference imaging is available. For example, it will enable prompt discovery of a uniform and all-sky sample of galaxy/group/cluster-scale lenses that will underpin LSST strong-lensing science. This white paper complements submissions from DESC, SLSC, and TVSSC, that discuss kilonova, GW, and strong lensing. 0 into PostgreSQL...\n",
      "Inserting test sample 778  This study reports the discovery of strongly-lensed gravitational waves, which can have a significant impact on the LSST observing strategy. The results are based on data collected by the Laser Interferometer Gravitational-Wave Observatory (LIGO) and the European Gravitational Observatory (EGO). The observed gravitational waves originated from the merger of two black holes, which were located at a distance of 3 billion light-years from Earth.\n",
      "\n",
      "The gravitational waves were strongly lensed by a massive galaxy cluster that is located between the source and the Earth. The strong gravitational lensing effect caused the observed signal to be amplified and split into multiple images. By analyzing the time delays and the signal strengths of the different images, we were able to reconstruct the emission history of the black hole binary and obtain new insights into the properties of the lensing cluster.\n",
      "\n",
      "These results have important implications for the LSST observing strategy, which aims to obtain deep, wide-field images of the sky in multiple bands. The strong gravitational lensing effect can amplify the brightness of distant sources and reveal faint structures that are otherwise hard to detect. This effect can be used to study the properties of galaxies and clusters at high redshifts and to probe the cosmological parameters that govern the expansion rate and the geometry of the Universe.\n",
      "\n",
      "Our findings demonstrate the potential of strongly-lensed gravitational waves as a powerful tool for astrophysical and cosmological studies. They highlight the importance of multi-wavelength observations and the use of synergistic techniques to fully exploit the scientific potential of future facilities such as the LSST. 1 into PostgreSQL...\n",
      "Inserting test sample 779  The effective use of parallel computing resources to speed up algorithms in current multi-core parallel architectures remains a difficult challenge, with ease of programming playing a key role in the eventual success of various parallel architectures. In this paper we consider an alternative view of parallelism in the form of an ultra-wide word processor. We introduce the Ultra-Wide Word architecture and model, an extension of the word-RAM model that allows for constant time operations on thousands of bits in parallel. Word parallelism as exploited by the word-RAM model does not suffer from the more difficult aspects of parallel programming, namely synchronization and concurrency. For the standard word-RAM algorithms, the speedups obtained are moderate, as they are limited by the word size. We argue that a large class of word-RAM algorithms can be implemented in the Ultra-Wide Word model, obtaining speedups comparable to multi-threaded computations while keeping the simplicity of programming of the sequential RAM model. We show that this is the case by describing implementations of Ultra-Wide Word algorithms for dynamic programming and string searching. In addition, we show that the Ultra-Wide Word model can be used to implement a nonstandard memory architecture, which enables the sidestepping of lower bounds of important data structure problems such as priority queues and dynamic prefix sums. While similar ideas about operating on large words have been mentioned before in the context of multimedia processors [Thorup 2003], it is only recently that an architecture like the one we propose has become feasible and that details can be worked out. 0 into PostgreSQL...\n",
      "Inserting test sample 780  The ultra-wide word model is a newly established theoretical framework for memory hierarchies that has generated significant interest among computer scientists. The model assumes that a computer has access to infinite storage, allowing the processing of strings of considerable length relative to those that can be handled on conventional computing devices. Algorithms for this model of computation have been shown to be computationally efficient, making them a viable option for practical applications.\n",
      "\n",
      "Although initial research on the ultra-wide word model focused on relatively simple problems such as pattern matching, recent work has shown that the model can tackle more complex problems with ease. The ability of the model to handle vast amounts of information in a single operation opens up new directions for algorithm design. This has also led to the development of novel algorithms for tasks such as string sorting, sequence alignment, and DNA analysis, among others.\n",
      "\n",
      "This paper surveys the state of the art algorithms in the ultra-wide word model. We present an overview of techniques and results achieved in this area, with a focus on the theoretical and practical aspects of these algorithms. We examine some of the current challenges and opportunities for further research, including the possibility of extending the model to other contexts and exploring the limits of the model's computational power.\n",
      "\n",
      "Overall, this paper provides a comprehensive review of the use of algorithms in the ultra-wide word model, highlighting the potential of this approach to solve big-data challenges and optimize computational performance. The exploration of this model and its applications will continue to be an exciting topic of research in the years to come. 1 into PostgreSQL...\n",
      "Inserting test sample 781  In strong line-of-sight millimeter-wave (mmWave) wireless systems, the rank-deficient channel severely hampers spatial multiplexing. To address this inherent deficiency, distributed reconfigurable intelligent surfaces (RISs) are introduced in this study to customize the wireless channel. Capitalizing on the ability of the RIS to reshape electromagnetic waves, we theoretically show that a favorable channel with an arbitrary tunable rank and a minimized truncated condition number can be established by elaborately designing the placement and reflection matrix of RISs. Different from existing works on distributed RISs, the number of elements needed for each RIS to combat the path loss and the limited phase control is also considered in this research. On the basis of the proposed channel customization, a joint transmitter-RISs-receiver (Tx-RISs-Rx) design under a hybrid mmWave system is investigated to maximize the downlink spectral efficiency. Using the proposed scheme, the optimal singular value decomposition-based hybrid beamforming at the Tx and Rx can be easily obtained without matrix decomposition for the digital and analog beamforming. The bottoms of the sub-channel mode in the water-filling power allocation algorithm, which are conventionally uncontrollable when the noise power is fixed, are proven to be independently adjustable by RISs. Moreover, the transmit power required for realizing multi-stream transmission is derived.\n",
      "\n",
      "Numerical results are presented to verify our theoretical analysis and exhibit substantial gains over systems without RISs. 0 into PostgreSQL...\n",
      "Inserting test sample 782  The use of millimeter wave (mmWave) frequencies in wireless communication systems can provide high data rates and more efficient spectrum utilization. However, the propagation characteristics of mmWave signals make the design of such systems challenging. In this study, we propose a joint transmitter (Tx), reconfigurable intelligent surface (RIS), and receiver (Rx) design for hybrid mmWave systems. Specifically, we investigate the channel customization for this joint design and evaluate its performance under different scenarios.\n",
      "\n",
      "Our proposed approach enables the simultaneous optimization of the Tx, RIS, and Rx parameters to achieve optimal system performance. We develop a mathematical model to describe the system behavior and employ a heuristic algorithm to determine the optimal channel and beamforming configurations in real time. Simulation results illustrate that our proposed joint design outperforms traditional mmWave systems in terms of data rate, signal-to-noise ratio, and outage probability.\n",
      "\n",
      "Furthermore, we investigate the impact of various system parameters, such as the number of RIS elements and the distance between the RIS and Tx/Rx, on system performance. We also explore trade-offs between system performance and hardware complexity. Our findings demonstrate the potential of joint Tx-RISs-Rx design in hybrid mmWave systems and highlight key research directions for future investigation. 1 into PostgreSQL...\n",
      "Inserting test sample 783  To provide the census of the sources contributing to the X-ray background peak above 10 keV, NuSTAR is performing extragalactic surveys using a three-tier \"wedding cake\" approach. We present the NuSTAR survey of the COSMOS field, the medium sensitivity and medium area tier, covering 1.7 deg2 and overlapping with both Chandra and XMM-Newton data. This survey consists of 121 observations for a total exposure of ~3 Ms. To fully exploit these data, we developed a new detection strategy, carefully tested through extensive simulations. The survey sensitivity at 20% completeness is 5.9, 2.9 and 6.4 x 10^-14 erg/cm2/s in the 3-24 keV, 3-8 keV and 8-24 keV bands, respectively. By combining detections in 3 bands, we have a sample of 91 NuSTAR sources with 10^42 -10^45.5 erg/s luminosities and redshift z=0.04-2.5. Thirty two sources are detected in the 8-24 keV band with fluxes ~100 times fainter than sources detected by Swift-BAT. Of the 91 detections, all but four are associated with a Chandra and/or XMM-Newton point-like counterpart. One source is associated with an extended lower energy X-ray source. We present the X-ray (hardness ratio and luminosity) and optical-to-X-ray properties. The observed fraction of candidate Compton-thick AGN measured from the hardness ratio is between 13-20%. We discuss the spectral properties of NuSTAR J100259+0220.6 (ID 330) at z=0.044, with the highest hardness ratio in the entire sample. The measured column density exceeds 10^24 /cm2, implying the source is Compton-thick. This source was not previously recognized as such without the >10 keV data. 0 into PostgreSQL...\n",
      "Inserting test sample 784  The NuSTAR Extragalactic Surveys (NuSES) are an observation program that combines hard X-ray imaging and spectroscopy to uncover the most obscured active galactic nuclei in the universe. In this paper, we present an overview of the NuSES program and a catalog of sources in the COSMOS field. The catalog is composed of 497 NuSES sources detected in a survey of ~2.3 square degrees.\n",
      "\n",
      "We describe the NuSTAR data processing and analysis techniques, including the use of a Bayesian methodology to identify and characterize sources. We discuss the selection criteria used to define the catalog and compare its properties to previous X-ray catalogs in the same field. \n",
      "\n",
      "The NuSES program has several unique advantages over previous X-ray surveys, including its sensitivity to heavily obscured sources, its ability to provide spectral information in the hard X-ray band, and its high spatial resolution. Moreover, the NuSTAR hard X-ray data can be combined with observations in other wavelength bands to obtain a more complete picture of the source morphology and environment.\n",
      "\n",
      "The catalog we present includes a wide range of source types, including active galactic nuclei, star-forming galaxies, and X-ray binaries, among others. We analyze the basic properties of the sources in the catalog, including their X-ray fluxes and spectral parameters. We also present a comparison between the NuSTAR and Chandra X-ray properties of sources in the same field, showing good agreement between the two surveys.\n",
      "\n",
      "Overall, the NuSES program and the catalog presented in this paper provide a valuable resource for studying the most obscured and energetic phenomena in the universe. The catalog is publicly available and can be used for a wide range of scientific investigations, including the study of accretion processes in active galactic nuclei and the properties of heavily obscured star-forming galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 785  Japanese laser interferometric gravitational wave detectors, TAMA300 and LISM, performed a coincident observation during 2001. We perform a coincidence analysis to search for inspiraling compact binaries. The length of data used for the coincidence analysis is 275 hours when both TAMA300 and LISM detectors are operated simultaneously. TAMA300 and LISM data are analyzed by matched filtering, and candidates for gravitational wave events are obtained. If there is a true gravitational wave signal, it should appear in both data of detectors with consistent waveforms characterized by masses of stars, amplitude of the signal, the coalescence time and so on. We introduce a set of coincidence conditions of the parameters, and search for coincident events. This procedure reduces the number of fake events considerably, by a factor $\\sim 10^{-4}$ compared with the number of fake events in single detector analysis. We find that the number of events after imposing the coincidence conditions is consistent with the number of accidental coincidences produced purely by noise.\n",
      "\n",
      "We thus find no evidence of gravitational wave signals. We obtain an upper limit of 0.046 /hours (CL $= 90 %$) to the Galactic event rate within 1kpc from the Earth. The method used in this paper can be applied straightforwardly to the case of coincidence observations with more than two detectors with arbitrary arm directions. 0 into PostgreSQL...\n",
      "Inserting test sample 786  This research paper presents a coincidence analysis conducted to detect inspiraling compact binaries utilizing TAMA300 and LISM data. The aim of this study is to identify astrophysical sources of gravitational waves that would lead to a better understanding of the universe. The data analyzed consisted of GPS timestamps and strain data collected by the Laser Interferometer Gravitational Wave Observatory (LIGO) from the LHO and LLO detectors. The coincidence time window was set to Â±10 ms, and the analysis was conducted using the coherent WaveBurst algorithm. The results of the analysis revealed no candidate events, indicating that the search was consistent with the expected background noise. Upper limits were set on the rate of compact binary coalescence events in the Milky Way, significantly improving on previous constraints. This work demonstrates the effectiveness of using coincidence analysis and highlights the importance of combining data from multiple detectors to improve the prospects of detecting gravitational waves. Further studies are necessary to continue the search for compact binary coalescence events and other astrophysical sources of gravitational waves, which would contribute to ongoing efforts to better understand the nature of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 787  We have performed new wide-field photometry of the young open cluster NGC 6231 to study the shape of the initial mass function (IMF) and mass segregation. We also investigated the reddening law toward NGC 6231 from optical to mid-infrared color excess ratios, and found that the total-to-selective extinction ratio is Rv = 3.2, which is very close to the normal value. But many early-type stars in the cluster center show large color excess ratios. We derived the surface density profiles of four member groups, and found that they reach the surface density of field stars at about 10', regardless of stellar mass.\n",
      "\n",
      "The IMF of NGC 6231 is derived for the mass range 0.8 -- 45 Msun. The slope of the IMF of NGC 6231 (Gamma = -1.1 +/- 0.1) is slightly shallower than the canonical value, but the difference is marginal. In addition, the mass function varies systematically, and is a strong function of radius - it is is very shallow at the center, and very steep at the outer ring suggesting the cluster is mass segregated. We confirm the mass segregation for the massive stars (m >~ 8 Msun) by a minimum spanning tree analysis. Using a Monte Carlo method, we estimate the total mass of NGC 6231 to be about 2.6 (+/- 0.6) x 10^3 Msun. We constrain the age of NGC 6231 by comparison with evolutionary isochrones. The age of the low-mass stars ranges from 1 to 7 Myr with a slight peak at 3 Myr.\n",
      "\n",
      "However the age of the high mass stars depends on the adopted models and is 3.5 +/- 0.5 Myr from the non- or moderately-rotating models of Brott et al. as well as the non-rotating models of Ekstr\\\"om et al. But the age is 4.0 -- 7.0 Myr if the rotating models of Ekstr\\\"om et al. are adopted. This latter age is in excellent agreement with the time scale of ejection of the high mass runaway star HD 153919 from NGC 6231, albeit the younger age cannot be entirely excluded. 0 into PostgreSQL...\n",
      "Inserting test sample 788  The initial mass function (IMF) and surface density profile of galactic clusters have been extensively studied in the astronomical community in order to understand the mechanisms of star formation and evolution in these systems. In this paper, we explore NGC 6231, a young and massive galactic cluster located in the southern Milky Way, which provides an excellent opportunity to test the validity of existing models of star formation.\n",
      "\n",
      "Using deep imaging data obtained with the VLT and HST, we present a comprehensive analysis of the IMF and surface density profile of NGC 6231. Our results reveal a distinct deviation from the traditional Salpeter IMF, indicating that the massive stars in NGC 6231 are formed at a higher rate relative to lower mass stars than previously believed. This finding highlights the importance of considering the potential biases and uncertainties in any approximation of the IMF and the need for further exploration of modified IMF models.\n",
      "\n",
      "Furthermore, we find that the surface density profile of NGC 6231 follows a power law with a slope of approximately -2.1, consistent with other galactic clusters. However, the inner region of NGC 6231 exhibits a shallow slope, which could indicate early dynamical evolution.\n",
      "\n",
      "Based on our analysis, we propose that NGC 6231 may have formed through the coalescence of smaller sub-clusters or through triggered star formation due to the interaction with neighboring clusters. Future observations with higher spatial resolution and wider spectral coverage will help to confirm or refute these hypotheses.\n",
      "\n",
      "In summary, our study provides important insights into the IMF and surface density profile of NGC 6231, shedding light on the formation and evolution processes of massive galactic clusters. These results contribute to our understanding of star formation and the physical properties of galaxies, with implications for the broader field of astrophysics. 1 into PostgreSQL...\n",
      "Inserting test sample 789  Supervised cross-modal hashing aims to embed the semantic correlations of heterogeneous modality data into the binary hash codes with discriminative semantic labels. Because of its advantages on retrieval and storage efficiency, it is widely used for solving efficient cross-modal retrieval. However, existing researches equally handle the different tasks of cross-modal retrieval, and simply learn the same couple of hash functions in a symmetric way for them. Under such circumstance, the uniqueness of different cross-modal retrieval tasks are ignored and sub-optimal performance may be brought.\n",
      "\n",
      "Motivated by this, we present a Task-adaptive Asymmetric Deep Cross-modal Hashing (TA-ADCMH) method in this paper. It can learn task-adaptive hash functions for two sub-retrieval tasks via simultaneous modality representation and asymmetric hash learning. Unlike previous cross-modal hashing approaches, our learning framework jointly optimizes semantic preserving that transforms deep features of multimedia data into binary hash codes, and the semantic regression which directly regresses query modality representation to explicit label. With our model, the binary codes can effectively preserve semantic correlations across different modalities, meanwhile, adaptively capture the query semantics. The superiority of TA-ADCMH is proved on two standard datasets from many aspects. 0 into PostgreSQL...\n",
      "Inserting test sample 790  This study proposes a novel deep cross-modal hashing algorithm that is task-adaptive and asymmetric. The proposed method effectively addresses the challenges that arise when cross-modality retrieval is performed, as modalities are often heterogeneous and the features differ in their semantic or discriminative information. We adopt a task-based approach in the proposed method, where the hash codes are learned by maximizing the retrieval performance of a particular query task. Specifically, we introduce a synthetic triplet constraint that exploits the underlying relevance between modalities, making the learned hash codes more task-sensitive. Moreover, to alleviate the computational complexity in the retrieval stage, an asymmetric hashing network is designed, which allows for compact hash codes without sacrificing the retrieval accuracy. Experiments on several benchmark datasets demonstrate the superiority of our proposed method over state-of-the-art hashing approaches. In conclusion, the task-adaptive asymmetric deep cross-modal hashing is a promising approach for cross-modal retrieval, which can be widely applied in multimedia analysis, recommendation systems, and other related fields. 1 into PostgreSQL...\n",
      "Inserting test sample 791  Dielectric barrier discharges (DBDs) are commonly used to generate cold plasmas at atmospheric pressure. Whatever their configuration (tubular or planar), the presence of a dielectric barrier is mandatory to prevent too much charge build up in the plasma and the formation of a thermal arc. In this article, the role of the barrier thickness (2.0, 2.4 and 2.8 mm) and of the kind of dielectric material (alumina, mullite, pyrex, quartz) is investigated on the filamentary behavior in the plasma and on the CO2 conversion in a tubular flowing DBD, by means of mass spectrometry measurements correlated with electrical characterization and IR imaging. Increasing the barrier thickness decreases the capacitance, while preserving the electrical charge. As a result, the voltage over the dielectric increases and a larger number of microdischarges is generated, which enhances the CO2 conversion. Furthermore, changing the dielectric material of the barrier, while keeping the same geometry and dimensions, also affects the CO2 conversion. The highest CO2 conversion and energy efficiency are obtained for quartz and alumina, thus not following the trend of the relative permittivity. From the electrical characterization, we clearly demonstrate that the most important parameters are the somewhat higher effective plasma voltage (yielding a somewhat higher electric field and electron energy in the plasma) for quartz, as well as the higher plasma current (and thus larger electron density) and the larger number of microdischarge filaments (mainly for alumina, but also for quartz). The latter could be correlated to the higher surface roughness for alumina and to the higher voltage over the dielectric for quartz. 0 into PostgreSQL...\n",
      "Inserting test sample 792  This study examines the effect of barrier thickness and dielectric material on the filamentary mode and CO2 conversion in a flowing dielectric barrier discharge (DBD) reactor. By varying the thickness of the dielectric barrier from 0.5 to 1.2 mm, we observed that the discharge voltage increased with barrier thickness, while the discharge current decreased owing to a decrease in the electric field. Furthermore, time-resolved optical emission spectroscopy (TROES) measurements were employed to investigate the generation and evolution of the filamentary modes. The results showed that the thickness of the dielectric barrier significantly influenced the morphology and distribution of the filaments. In particular, a thicker barrier promoted the transition from a discrete to a diffuse filamentary mode. Finally, we evaluated the CO2 conversion performance of the reactor by introducing CO2 as the target gas and monitoring its conversion rate. The highest CO2 conversion rate was observed for a reactor with a 0.9 mm dielectric barrier. The result was attributed to the balance between the electric field strength, the energy density, and the rate of electron dissociation of CO2 molecules. In conclusion, this study provides valuable insights into the effect of barrier thickness and dielectric material on the filamentary mode and CO2 conversion performance in a flowing DBD reactor, which can inform the design of more efficient plasma-based reactors for CO2 conversion. 1 into PostgreSQL...\n",
      "Inserting test sample 793  We use photometric redshifts and statistical background subtraction to measure stellar mass functions in galaxy group-mass ($4.5-8\\times10^{13}~\\mathrm{M}_\\odot$) haloes at $1<z<1.5$. Groups are selected from COSMOS and SXDF, based on X-ray imaging and sparse spectroscopy.\n",
      "\n",
      "Stellar mass ($M_{\\mathrm{stellar}}$) functions are computed for quiescent and star-forming galaxies separately, based on their rest-frame $UVJ$ colours. From these we compute the quiescent fraction and quiescent fraction excess (QFE) relative to the field as a function of $M_{\\mathrm{stellar}}$. QFE increases with $M_{\\mathrm{stellar}}$, similar to more massive clusters at $1<z<1.5$.\n",
      "\n",
      "This contrasts with the apparent separability of $M_{\\mathrm{stellar}}$ and environmental factors on galaxy quiescent fractions at $z\\sim 0$. We then compare our results with higher mass clusters at $1<z<1.5$ and lower redshifts.\n",
      "\n",
      "We find a strong QFE dependence on halo mass at fixed $M_{\\mathrm{stellar}}$; well fit by a logarithmic slope of $\\mathrm{d}(\\mathrm{QFE})/\\mathrm{d}\\log (M_{\\mathrm{halo}}) \\sim 0.24 \\pm 0.04$ for all $M_{\\mathrm{stellar}}$ and redshift bins. This dependence is in remarkably good qualitative agreement with the hydrodynamic simulation BAHAMAS, but contradicts the observed dependence of QFE on $M_{\\mathrm{stellar}}$. We interpret the results using two toy models: one where a time delay until rapid (instantaneous) quenching begins upon accretion to the main progenitor (\"no pre-processing\") and one where it starts upon first becoming a satellite (\"pre-processing\"). Delay times appear to be halo mass dependent, with a significantly stronger dependence required without pre-processing. We conclude that our results support models in which environmental quenching begins in low-mass ($<10^{14}M_\\odot$) haloes at $z>1$. 0 into PostgreSQL...\n",
      "Inserting test sample 794  The GOGREEN survey is a large spectroscopic survey targeting massive galaxies at z > 1 in 21 galaxy clusters and their surrounding fields. In this paper, we investigate the dependence of galaxy properties, such as stellar mass, star formation rate, and morphology, on halo mass at these high redshifts. We use a sample of over 300 spectroscopically-confirmed cluster members and > 1000 galaxies in the field, and classify them according to their halo mass using weak lensing measurements. Our main findings are that galaxies with higher halo masses tend to have higher stellar masses, lower specific star formation rates, and are more likely to be quenched. Additionally, we find that the environmental quenching efficiency, defined as the fraction of quenched galaxies in denser environments compared to the field, increases with halo mass. This suggests that the process of environmental quenching is more effective in more massive halos. However, we also find some evidence for a mass threshold below which the environment has no significant effect on quenching, indicating that internal mechanisms such as feedback from active galactic nuclei could also play a role. Our results have important implications for our understanding of galaxy evolution and the role of the environment in driving this evolution. The GOGREEN survey provides a valuable dataset for further exploring these topics and understanding the complex interplay between galaxies and their larger-scale environments at high redshifts. 1 into PostgreSQL...\n",
      "Inserting test sample 795  We report the discovery by the WASP transit survey of three new hot Jupiters, WASP-68 b, WASP-73 b and WASP-88 b. WASP-68 b has a mass of 0.95+-0.03 M_Jup, a radius of 1.24-0.06+0.10 R_Jup, and orbits a V=10.7 G0-type star (1.24+-0.03 M_sun, 1.69-0.06+0.11 R_sun, T_eff=5911+-60 K) with a period of 5.084298+-0.000015 days. Its size is typical of hot Jupiters with similar masses. WASP-73 b is significantly more massive (1.88-0.06+0.07 M_Jup) and slightly larger (1.16-0.08+0.12 R_Jup) than Jupiter. It orbits a V=10.5 F9-type star (1.34-0.04+0.05 M_sun, 2.07-0.08+0.19 R_sun, T_eff=6036+-120 K) every 4.08722+-0.00022 days. Despite its high irradiation (2.3 10^9 erg s^-1 cm^-2), WASP-73 b has a high mean density (1.20-0.30+0.26 \\rho_Jup) that suggests an enrichment of the planet in heavy elements. WASP-88 b is a 0.56+-0.08 M_Jup planet orbiting a V=11.4 F6-type star (1.45+-0.05 M_sun, 2.08-0.06+0.12 R_sun, T_eff=6431+-130 K) with a period of 4.954000+-0.000019 days. With a radius of 1.70-0.07+0.13 R_Jup, it joins the handful of planets with super-inflated radii. The ranges of ages we determine through stellar evolution modeling are 4.2-8.3 Gyr for WASP-68, 2.7-6.4 Gyr for WASP-73 and 1.8-5.3 Gyr for WASP-88.\n",
      "\n",
      "WASP-73 appears to be a significantly evolved star, close to or already in the subgiant phase. WASP-68 and WASP-88 are less evolved, although in an advanced stage of core H-burning. 0 into PostgreSQL...\n",
      "Inserting test sample 796  This study presents the discovery and characterization of three hot Jupiters, namely WASP-68 b, WASP-73 b, and WASP-88 b, transiting evolved solar-type stars through observations from the WASP-South, Euler, and TRAPPIST telescopes. The three planets have similar masses, radii, and orbital periods but are orbiting stars with different evolutionary stages. The transit method was used to detect them, and their physical properties were derived through a joint analysis of the transit light curves and radial velocity data. \n",
      "\n",
      "WASP-68 b is a Jupiter-mass planet with a radius of 1.49 times that of Jupiter and an orbital period of 5.084 days. It orbits a star that has evolved beyond the main sequence, being about twice as old as the Sun. WASP-73 b also has a mass similar to Jupiter and a radius of 1.15 times that of Jupiter. Its orbital period is shorter, at 4.087 days, and it transits a star that has just left the main sequence. WASP-88 b is the largest of the three planets, with a radius of 1.57 times that of Jupiter, but has a slightly smaller mass. It orbits a star that is still on the main sequence and has an orbital period of 6.267 days. \n",
      "\n",
      "The data obtained from the telescopes allowed us to infer the atmospheric properties of the planets. We found that WASP-73 b and WASP-88 b have atmospheres that are likely dominated by hydrogen, while the atmosphere of WASP-68 b is metal-rich and has a surprisingly large scale height. Our observations also revealed variations in the transit light curves, indicative of atmospheric heterogeneity and unocculted starspot signals.\n",
      "\n",
      "The transiting exoplanets discovered in this study add to our growing knowledge of planetary systems in the Universe and will provide valuable information for future studies on the formation and evolution of planets. Their characterization offers insights into the diversity of exoplanet atmospheres and the influence of host star evolution on planetary properties. 1 into PostgreSQL...\n",
      "Inserting test sample 797  Given a graph G with a distinguished vertex s, the critical group of (G,s) is the cokernel of their reduced Laplacian matrix L(G,s). In this article we generalize the concept of the critical group to the cokernel of any matrix with entries in a commutative ring with identity. In this article we find diagonal matrices that are equivalent to some matrices that generalize the reduced Laplacian matrix of the path, the cycle, and the complete graph over an arbitrary commutative ring with identity. We are mainly interested in those cases when the base ring is the ring of integers and some subrings of matrices.\n",
      "\n",
      "Using these equivalent diagonal matrices we calculate the critical group of the m-cones of the l-duplications of the path, the cycle, and the complete graph.\n",
      "\n",
      "Also, as byproduct, we calculate the critical group of another matrices, as the m-cones of the l-duplication of the bipartite complete graph with m vertices in each partition, the bipartite complete graph with 2m vertices minus a matching. 0 into PostgreSQL...\n",
      "Inserting test sample 798  The critical group of matrices is a fundamental object in algebraic topology that provides information about the homotopy type of various spaces. In this paper, we study the critical group of the general linear group of matrices over a commutative ring. We show that the critical group of this group is closely related to the determinant of the matrices and the algebraic K-theory of the ring. Specifically, we prove that the critical group is the quotient of the free abelian group generated by the matrices whose determinant is invertible by the subgroup generated by the matrices whose determinant is not invertible. Furthermore, we compute the critical group for several examples and establish connections with previous work on the critical group of other groups of matrices. Our results have important consequences for the study of algebraic topology, representation theory, and algebraic geometry. 1 into PostgreSQL...\n",
      "Inserting test sample 799  The survey phase of the Kepler Mission includes a number of hot subdwarf B (sdB) stars to search for nonradial pulsations. We present our analysis of two sdB stars that are found to be g-mode pulsators of the V1093 Her class. These two stars also display the distinct irradiation effect typical of sdB stars with a close M-dwarf companion with orbital periods of less than half a day.\n",
      "\n",
      "Because the orbital period is so short, the stars should be in synchronous rotation, and if so, the rotation period should imprint itself on the multiplet structure of the pulsations. However, we do not find clear evidence for such rotational splitting. Though the stars do show some frequency spacings that are consistent with synchronous rotation, they also display multiplets with splittings that are much smaller. Longer-duration time series photometry will be needed to determine if those small splittings are in fact rotational splitting, or caused by slow amplitude or phase modulation. Further data should also improve the signal-to-noise, perhaps revealing lower amplitude periodicities that could confirm the expectation of synchronous rotation. The pulsation periods seen in these stars show period spacings that are suggestive of high-overtone g-mode pulsations. 0 into PostgreSQL...\n",
      "Inserting test sample 800  This paper presents the fifth set of results from the Kepler mission on compact pulsating stars, focusing on slowly pulsating subdwarf B (sdB) stars in short-period binaries. Using high-precision Kepler photometry, we have identified 29 new pulsating sdB stars in binary systems, which exhibit a wide range of pulsation properties. Combining these observations with ground-based follow-up spectroscopy and additional radial velocity measurements, we are able to derive fundamental parameters of both the pulsating sdB stars and their binary companions, including mass, radius, and orbital parameters. These measurements allow us to better understand the formation and evolution of these compact binary systems, as well as the pulsational properties of sdB stars. Additionally, we carefully analyze the photometric and spectroscopic properties of the sdB stars in our sample, identifying interesting objects such as a \"hybrid\" pulsator with both p- and g-mode pulsations and a pulsating sdB+WD eclipsing binary. These new observations and analyses significantly expand our knowledge of the pulsational properties of sdB stars in short-period binaries and their binary companions. 1 into PostgreSQL...\n",
      "Inserting test sample 801  We compute the Standard Model semileptonic vector and axial-vector form factors for $B_s\\to D_s^*$ decay across the full $q^2$ range using lattice QCD.\n",
      "\n",
      "We use the Highly Improved Staggered Quark (HISQ) action for all valence quarks, enabling us to normalise weak currents nonperturbatively. We use gluon field configurations including $u$, $d$, $s$ and $c$ HISQ sea quarks and multiple HISQ heavy quarks with masses from the $c$ mass up to that of the $b$ on our finest lattices. We determine the physical form factors, with which we construct the differential and total rates for $\\Gamma(B_s^0\\to D_s^{*-}\\ell^+{\\nu}_\\ell)$. We find $\\Gamma_{\\ell=e}/|\\eta_\\mathrm{EW}V_{cb}|^2=2.07(21)_\\mathrm{latt}(2)_\\mathrm{EM}\\times 10^{13} ~\\mathrm{s}^{-1}$, $\\Gamma_{\\ell=\\mu}/|\\eta_\\mathrm{EW}V_{cb}|^2=2.06(21)_\\mathrm{latt}(2)_\\mathrm{EM}\\times 10^{13} ~\\mathrm{s}^{-1}$ and $\\Gamma_{\\ell=\\tau}/|\\eta_\\mathrm{EW}V_{cb}|^2=5.03(47)_\\mathrm{latt}(5)_\\mathrm{EM}\\times 10^{12} ~\\mathrm{s}^{-1}$, where $\\eta_\\mathrm{EW}$ contains the electroweak correction to $G_F$, the first uncertainty is from our lattice calculation, and the second allows for long-distance QED effects. We compute the ratio $R(D_s^{*-})\\equiv \\Gamma_{\\ell=\\tau}/\\Gamma_{\\ell=\\mu}=0.2442(79)_\\mathrm{latt}(35)_\\mathrm{EM}$ and obtain a value for the ratio of decay rates $\\Gamma_{\\ell=\\mu}(B_s\\to D_s)/\\Gamma_{\\ell=\\mu}(B_s\\to D_s^*)=0.429(43)_\\mathrm{latt}(4)_\\mathrm{EM}$, which agrees well with recent LHCb results. We determine $|V_{cb}|=43.0(2.1)_\\mathrm{latt}(1.7)_\\mathrm{exp}(0.4)_\\mathrm{EM} \\times 10^{-3}$ by combining our lattice results across the full q^2 range with experimental results from LHCb. A comparison of our results to the normalised differential decay rate from LHCb shows good agreement. We also test the impact of new physics couplings on observables sensitive to lepton flavor universality violation. 0 into PostgreSQL...\n",
      "Inserting test sample 802  This paper presents an investigation of the $B_s \\rightarrow D_s^*$ form factors using lattice QCD methods. The $B_s \\rightarrow D_s^*$ transition is a decay process of the $B_s$ meson to a $D_s^*$ meson that has been studied extensively in experimental and theoretical physics. The form factors are essential to calculate various branching ratios and physical observables, and the ultimate goal is to provide predictions that agree with experimental measurements.\n",
      "\n",
      "Our study covers the full $q^2$ range, which is defined as the squared four-momentum transfer from the $B_s$ meson to the $D_s^*$ meson. We perform a non-perturbative calculation using the gauge field configurations generated by the MILC collaboration. Our lattice calculation uses four ensembles with different lattice spacings and volumes and three light quark masses, allowing us to examine finite-size and discretization effects.\n",
      "\n",
      "We compute the form factors using the vector and axial-vector currents and apply the z-expansion to extrapolate to the physical point. We follow the procedure outlined in our previous work for the $B \\rightarrow D^*$ form factors, which involves a Bayesian statistical analysis of the lattice data, taking into account correlations and uncertainties.\n",
      "\n",
      "Our results for the $B_s \\rightarrow D_s^*$ form factors exhibit small statistical uncertainties and good agreement with experimental measurements where available. We also compare our results with other recent lattice QCD calculations and find general agreement within uncertainties. We provide predictions for various observables, including branching ratios and differential decay rates, which are relevant for experimental measurements and comparisons with future experimental data.\n",
      "\n",
      "In conclusion, we present a comprehensive study of the $B_s \\rightarrow D_s^*$ form factors using lattice QCD methods, covering the full $q^2$ range. Our study contributes to the understanding of heavy-to-heavy meson decays and provides valuable predictions for experimental measurements. 1 into PostgreSQL...\n",
      "Inserting test sample 803  We study the twistor equation on pseudo-Riemannian $Spin^c-$manifolds whose solutions we call charged conformal Killing spinors (CCKS). We derive several integrability conditions for the existence of CCKS and study their relations to spinor bilinears. A construction principle for Lorentzian manifolds admitting CCKS with nontrivial charge starting from CR-geometry is presented. We obtain a partial classification result in the Lorentzian case under the additional assumption that the associated Dirac current is normal conformal and complete the Classification of manifolds admitting CCKS in all dimensions and signatures $\\leq 5$ which has recently been initiated in the study of supersymmetric field theories on curved space. 0 into PostgreSQL...\n",
      "Inserting test sample 804  In this paper, we investigate the properties of charged conformal killing spinors on a Riemannian manifold. We establish conditions under which these spinors exist, and show that they are related to certain symmetries of the manifold. Furthermore, we study their role in the context of supersymmetric field theories and derive various equations of motion. We also provide examples of manifolds which admit charged conformal killing spinors and discuss their physical implications. Our results shed light on the interplay between geometry, symmetry and supersymmetry, and may have important applications in theoretical physics and cosmology. 1 into PostgreSQL...\n",
      "Inserting test sample 805  Given a set of ideas collected from crowds with regard to an open-ended question, how can we organize and prioritize them in order to determine the preferred ones based on preference comparisons by crowd evaluators? As there are diverse latent criteria for the value of an idea, multiple ideas can be considered as \"the best\". In addition, evaluators can have different preference criteria, and their comparison results often disagree.\n",
      "\n",
      "In this paper, we propose an analysis method for obtaining a subset of ideas, which we call frontier ideas, that are the best in terms of at least one latent evaluation criterion. We propose an approach, called CrowDEA, which estimates the embeddings of the ideas in the multiple-criteria preference space, the best viewpoint for each idea, and preference criterion for each evaluator, to obtain a set of frontier ideas. Experimental results using real datasets containing numerous ideas or designs demonstrate that the proposed approach can effectively prioritize ideas from multiple viewpoints, thereby detecting frontier ideas. The embeddings of ideas learned by the proposed approach provide a visualization that facilitates observation of the frontier ideas. In addition, the proposed approach prioritizes ideas from a wider variety of viewpoints, whereas the baselines tend to use to the same viewpoints; it can also handle various viewpoints and prioritize ideas in situations where only a limited number of evaluators or labels are available. 0 into PostgreSQL...\n",
      "Inserting test sample 806  CrowDEA is a novel approach to idea prioritization that leverages crowd wisdom from multiple perspectives. The concept of crowd wisdom recognizes that aggregated opinions from a group of diverse individuals are often more accurate and reliable than those of an individual expert. CrowDEA builds on this premise by incorporating multiple views of ideas and feedback from crowds to prioritize ideas. \n",
      "\n",
      "The system allows users to generate, share, and rate ideas according to their own perspective. The system harmonizes feedback from multiple groups, including experts, managers, and customers, to generate a comprehensive view of the most promising ideas. The integration of multi-view feedback is achieved through a unique algorithm that analyzes the reliability of each feedback source, weights it according to its reliability and aggregates it into a single composite rating for each idea.\n",
      "\n",
      "The effectiveness of CrowDEA is demonstrated through several experiments and case studies. Results show that the system outperforms other state-of-the-art techniques and consistently produces high-quality results. The benefits of CrowDEA are twofold: it produces more accurate and reliable idea prioritization, and it engages and incorporates diverse perspectives from crowds in the innovation process. \n",
      "\n",
      "In summary, CrowDEA introduces a novel approach to idea prioritization that leverages crowd wisdom from multiple perspectives. The approach has been shown to produce reliable and high-quality results. CrowDEA holds great promise as a tool for organizations seeking to innovate and engage diverse stakeholders. 1 into PostgreSQL...\n",
      "Inserting test sample 807  We introduce a method to relate a possible truncation of the star cluster mass function at the high mass end to the shape of the cluster luminosity function (LF). We compare the observed LFs of five galaxies containing young star clusters with synthetic cluster population models with varying initial conditions. The LF of the SMC, the LMC and NGC 5236 are characterized by a power-law behavior NdL~L^-a dL, with a mean exponent of <a> = 2.0 +/- 0.2. This can be explained by a cluster population formed with a constant cluster formation rate, in which the maximum cluster mass per logarithmic age bin is determined by the size-of-sample effect and therefore increases with log(age/yr). The LFs of NGC 6946 and M51 are better described by a double power-law distribution or a Schechter function. When a cluster population has a mass function that is truncated below the limit given by the size-of-sample effect, the total LF shows a bend at the magnitude of the maximum mass, with the age of the oldest cluster in the population, typically a few Gyr due to disruption. For NGC 6946 and M51 this implies a maximum mass of M_max = 5*10^5 M_sun. Faint-ward of the bend the LF has the same slope as the underlying initial cluster mass function and bright-ward of the bend it is steeper. This behavior can be well explained by our population model. We compare our results with the only other galaxy for which a bend in the LF has been observed, the ``Antennae'' galaxies (NGC 4038/4039). There the bend occurs brighter than in NGC 6946 and M51, corresponding to a maximum cluster mass of M_max = 2*10^6 M_sun (abridged). 0 into PostgreSQL...\n",
      "Inserting test sample 808  In this study, we investigate the luminosity function of young star clusters and its relevance to the maximum mass and luminosity of those clusters. We analyzed a high-quality sample of young clusters in the nearby galaxy M83, utilizing deep optical and near-infrared photometry from the Hubble Space Telescope. Our results show that the luminosity function of young star clusters can be described by a power law with a slope of -2.2Â±0.1, consistent with previous studies.\n",
      "\n",
      "We find that the luminosity function depends on the age of the cluster, with older clusters having a flatter luminosity function than younger clusters. This indicates that the most massive and luminous clusters dissipate energy more efficiently over time. By comparing our results to theoretical models, we estimate the maximum mass and luminosity of young clusters to be around 10^6 Mâ˜‰ and 10^39 erg/s, respectively.\n",
      "\n",
      "Our findings have important implications for our understanding of the formation and evolution of young star clusters. The observed power-law behavior of the luminosity function suggests that the most massive clusters may form via a hierarchical merging process, while lower mass clusters may form through a more dispersed mode of star formation. The estimated maximum mass and luminosity values provide constraints on theoretical models of cluster formation and evolution.\n",
      "\n",
      "Overall, our study provides new insights into the properties and formation of young star clusters, shedding light on the mechanisms that govern their evolution and the processes that lead to the formation of massive, luminous clusters. 1 into PostgreSQL...\n",
      "Inserting test sample 809  In this work we present the production of charged particles associated with high-$p_{\\rm T}$ trigger particles ($8<p_{\\rm T}^{\\rm trig.}<15$ GeV /$c$) at mid-pseudorapidity in proton-proton collisions at $\\sqrt{s}=5.02$\\,TeV simulated with the \\textsc{PYTHIA 8} Monte Carlo model. The study is performed as a function of the relative transverse activity classifier, $R_{\\rm T}$, which is the relative charged-particle multiplicity in the transverse region ($\\pi/3 <|\\Delta\\phi| <2\\pi/3$) of the di-hadron correlations, and it is sensitive to the Multi-Parton Interactions. The evolution of the yield of associated particles on both the toward and the away regions with $3\\leq p_{\\rm T}^{\\rm assoc.}<8$ GeV/$c$ as a function of $R_{\\rm T}$ is investigated. We propose a strategy which allows for the modelling and subtraction of the Underlying Event (UE) contribution from the toward and the away regions in challenging environments like those characterised by large $R_{\\rm T}$. We found that the signal in the away region becomes broader with increasing $R_{\\rm T}$. Contrarily, the yield increases with $R_{\\rm T}$ in the toward region. This effect is reminiscent of that seen in heavy-ion collisions, where an enhancement of the yield in the toward region for $0-5\\%$ central Pb--Pb collisions at $\\sqrt{s}_{\\rm NN}=2.76$\\,TeV was reported. To further understand the role of the UE and additional jet activity, the transverse region is divided into two one-sided sectors, \"trans-max\" and \"trans-min\" selected in each event according to which region has larger or smaller charged particle multiplicity. Based on this selection criterion, the observables are studied as a function of $R_{\\rm T}^{\\rm max}$ and $R_{\\rm T}^{\\rm min}$, respectively.\n",
      "\n",
      "The presented results have been published in J. Phys. G 48, no.1, 015007 (2020) and Phys. Rev. D 104, no.1, 016017 (2021). 0 into PostgreSQL...\n",
      "Inserting test sample 810  This research paper investigates the variation of particle production as a function of the underlying event in proton-proton (pp) collisions. Using the PYTHIA 8 event generator, a Monte Carlo simulation was performed to model pp collisions at varying underlying event conditions.\n",
      "\n",
      "The study begins by defining several variables related to the underlying event, such as the transverse momentum of the hardest parton and the number of particles in the event. The simulation then generates events with different underlying event conditions and records the resulting particle production.\n",
      "\n",
      "The results of the simulation show a clear correlation between the underlying event and particle production. In particular, events with a higher transverse momentum of the hardest parton tend to have a higher particle multiplicity. Additionally, events with a higher number of particles tend to have a higher average transverse momentum per particle.\n",
      "\n",
      "The analysis also includes a comparison of the PYTHIA 8 results to experimental data from the CMS collaboration at the Large Hadron Collider. The agreement between the simulation and experimental observations suggests that PYTHIA 8 is a reliable tool for modeling pp collisions and their underlying event conditions.\n",
      "\n",
      "Finally, the paper discusses potential applications of the study's findings, such as improving the accuracy of predictions for future experimental analyses. Overall, the research presented in this paper contributes to a deeper understanding of the relationship between the underlying event and particle production in pp collisions. 1 into PostgreSQL...\n",
      "Inserting test sample 811  We perform forecasts for how baryon acoustic oscillation (BAO) scale and redshift-space distortion (RSD) measurements from future spectroscopic emission line galaxy (ELG) surveys such as Euclid are degraded in the presence of spectral line misidentification. Using analytic calculations verified with mock galaxy catalogs from log-normal simulations we find that constraints are degraded in two ways, even when the interloper power spectrum is modeled correctly in the likelihood. Firstly, there is a loss of signal-to-noise ratio for the power spectrum of the target galaxies, which propagates to all cosmological constraints and increases with contamination fraction, $f_c$.\n",
      "\n",
      "Secondly, degeneracies can open up between $f_c$ and cosmological parameters.\n",
      "\n",
      "In our calculations this typically increases BAO scale uncertainties at the 10-20% level when marginalizing over parameters determining the broadband power spectrum shape. External constraints on $f_c$, or parameters determining the shape of the power spectrum, for example from cosmic microwave background (CMB) measurements, can remove this effect. There is a near-perfect degeneracy between $f_c$ and the power spectrum amplitude for low $f_c$ values, where $f_c$ is not well determined from the contaminated sample alone. This has the potential to strongly degrade RSD constraints. The degeneracy can be broken with an external constraint on $f_c$, for example from cross-correlation with a separate galaxy sample containing the misidentified line, or deeper sub-surveys. 0 into PostgreSQL...\n",
      "Inserting test sample 812  Galaxy surveys have become an essential tool in modern cosmology for estimating the fundamental properties of the universe. However, these surveys are not immune to errors in identifying different types of galaxy lines. In this paper, we investigate the impact of line misidentification on the accuracy of cosmological constraints derived from Euclid and other spectroscopic galaxy surveys. We simulate several scenarios of misidentification of the [OII] and [OIII] lines in the spectra of galaxies and study their effects on the power spectrum and the cosmological parameters. Our results show that uncertainties in the line identification can lead to biased estimations of the amplitude of the matter power spectrum, the growth rate of structures, and the dark energy equation of state parameters. We also find that the impact of misidentification is greater for surveys with narrow wavelength coverage and high noise levels. Our study highlights the importance of careful line identification and its impact on the accuracy of cosmological measurements. Future surveys, such as the Euclid mission, should account for line misidentification uncertainties in their data analysis to better constrain the properties of our universe. 1 into PostgreSQL...\n",
      "Inserting test sample 813  CONTEXT.Large field-of-view imaging/polarimetry instruments operating at millimeter and submm wavelengths are fundamental tools to understand the role of magnetic fields (MF) in channeling filament material into prestellar cores providing a unique insight in the physics of galactic star-forming regions.\n",
      "\n",
      "Among other topics, at extra-galactic scales, polarization observations of AGNs will allow us to constrain the possible physical conditions of the emitting plasma from the jets and/or exploring the physics of dust inside supernova remnants. The kilo-pixel NIKA2 camera, installed at the IRAM 30-m telescope, represents today one of the best tools available to the astronomers to produce simultaneous intensity/polarimetry maps over large fields at 260 GHz (1.15 mm).\n",
      "\n",
      "AIMS.The polarization measurement, in NIKA and NIKA2, is achieved by rapidly modulating the total incoming polarization. This allows in the end to safely isolate the small science signal from the large, un-polarized and strongly variable, atmospheric background. METHODS.The polarization modulation is achieved by inserting a fast rotating Half-Wave Plate (HWP) in the optical beam. In order to allow wide field-of-view observations, the plate has to be large, with a diameter exceeding 250 mm. The modulation of the polarized signal, at 12 Hz, requires also the waveplate to be sufficiently light. In addition, this key optical element has to exhibit optimal electromagnetic characteristics in terms of transmission and differential phase-shift. For this purpose, three metamaterial HWPs have been developed using the mesh-filter technology. The knowledge acquired in developing the first two single-band HWPs was used to achieve the more challenging performance requirements of the last dual-band HWP. The first and the third waveplates met the requirements for both the NIKA and NIKA2 instruments. RESULTS.(abridged) 0 into PostgreSQL...\n",
      "Inserting test sample 814  This paper focuses on the development and application of Half-Wave Plates (HWPs) based on metamaterials for the NIKA (New Instrumentation for the Kahler Acoustics) and NIKA2 polarimeters. The challenge of creating such plates was the need to produce a component that could provide the desired polarization state without causing any loss or distortion in the signal. In the context of millimeter and sub-millimeter astronomy, this is particularly important due to the low signal-to-noise ratio of the detected signal.\n",
      "\n",
      "We present our approach to designing and fabricating HWPs based on the anisotropic properties of metamaterials, which allow for the tuning of their effective refractive indices. We used metamaterials based on metallic resonators suspended on a thin dielectric layer to achieve this. Our study involved characterization of the performance of these devices in lab-controlled environments as well as in-field trials.\n",
      "\n",
      "Our results show that the metamaterial-based HWPs achieved polarization rotation in the desired range with low insertion loss and negligible phase distortion across the signal bandwidth. We also highlight the benefits of using these types of components, primarily their small size, weight, and ease of integration into the existing instrumentation. These benefits make them particularly useful as components for millimeter and sub-millimeter astronomy experiments.\n",
      "\n",
      "With the increasing sensitivity and precision required of polarization-sensitive detectors in cosmology research, the importance of developing reliable and efficient polarization modulators such as our metamaterial HWPs cannot be overstated. We believe that the successful application of these devices in the NIKA and NIKA2 polarimeters serves as a proof-of-concept for future designs and applications. 1 into PostgreSQL...\n",
      "Inserting test sample 815  We study a weighted online bipartite matching problem: $G(V_1, V_2, E)$ is a weighted bipartite graph where $V_1$ is known beforehand and the vertices of $V_2$ arrive online. The goal is to match vertices of $V_2$ as they arrive to vertices in $V_1$, so as to maximize the sum of weights of edges in the matching. If assignments to $V_1$ cannot be changed, no bounded competitive ratio is achievable. We study the weighted online matching problem with {\\em free disposal}, where vertices in $V_1$ can be assigned multiple times, but only get credit for the maximum weight edge assigned to them over the course of the algorithm. For this problem, the greedy algorithm is $0.5$-competitive and determining whether a better competitive ratio is achievable is a well known open problem.\n",
      "\n",
      "We identify an interesting special case where the edge weights are decomposable as the product of two factors, one corresponding to each end point of the edge. This is analogous to the well studied related machines model in the scheduling literature, although the objective functions are different. For this case of decomposable edge weights, we design a 0.5664 competitive randomized algorithm in complete bipartite graphs. We show that such instances with decomposable weights are non-trivial by establishing upper bounds of 0.618 for deterministic and $0.8$ for randomized algorithms.\n",
      "\n",
      "A tight competitive ratio of $1-1/e \\approx 0.632$ was known previously for both the 0-1 case as well as the case where edge weights depend on the offline vertices only, but for these cases, reassignments cannot change the quality of the solution. Beating 0.5 for weighted matching where reassignments are necessary has been a significant challenge. We thus give the first online algorithm with competitive ratio strictly better than 0.5 for a non-trivial case of weighted matching with free disposal. 0 into PostgreSQL...\n",
      "Inserting test sample 816  The problem of online bipartite matching with decomposable weights has attracted significant attention in recent years. Given a bipartite graph and a set of edges arriving in an online fashion, the goal is to match as many vertices as possible while optimizing a given objective function that can be decomposed additively over the edges. To address this problem, we present a novel algorithm that achieves near-optimal performance in terms of both the competitive ratio and the regret with respect to the optimal offline solution. Our algorithm builds upon the recent breakthroughs in the theory of bipartite matching and the related field of online algorithms, and adapts them to the setting in which the weights can be decomposed. Specifically, our algorithm combines techniques from primal-dual algorithms, randomized rounding, and online convex optimization to achieve a regret bound that is within a logarithmic factor of the optimal offline solution. Furthermore, we present a lower bound on the competitive ratio of any algorithm for this problem, which shows that our algorithm is close to optimal in terms of the competitive ratio. We also demonstrate the practical effectiveness of our algorithm through extensive experiments on real-world datasets. Our results show that our algorithm is significantly more robust and efficient than the state-of-the-art methods for online bipartite matching with decomposable weights. Overall, our work provides a significant step forward in the development of efficient and scalable algorithms for online bipartite matching with decomposable weights, with promising applications in diverse fields such as network routing and recommendation systems. 1 into PostgreSQL...\n",
      "Inserting test sample 817  The nucleus of white blood cells (WBCs) plays a significant role in their detection and classification. Appropriate feature extraction of the nucleus is necessary to fit a suitable artificial intelligence model to classify WBCs.\n",
      "\n",
      "Therefore, designing a method is needed to segment the nucleus accurately.\n",
      "\n",
      "There should be a comparison between the ground truths distinguished by a hematologist and the detected nuclei to evaluate the performance of the nucleus segmentation method accurately. It is a time-consuming and tedious task for experts to establish the ground truth manually. This paper presents an intelligent open-source software called Easy-GT to create the ground truth of WBCs' nucleus faster and easier. This software first detects the nucleus by employing a new Otsu's thresholding-based method with a dice similarity coefficient (DSC) of 95.42 %; the hematologist can then create a more accurate ground truth, using the designed buttons to modify the threshold value. This software can speed up ground truth's forming process more than six times. 0 into PostgreSQL...\n",
      "Inserting test sample 818  Easy-GT is a new open-source software tool developed to aid the process of creating ground truth for the recognition of white blood cell nuclei. Due to the complexity and variability of blood samples, obtaining precise and accurate annotations can be challenging and time-consuming. Easy-GT addresses this issue by providing a user-friendly interface for annotators to mark the cells and their corresponding nuclei, while also automatically correcting common annotation mistakes. The software implements a machine learning algorithm which can leverage annotations from multiple annotators with varying levels of expertise to improve accuracy. Additionally, the tool includes features such as image visualization and quality control metrics to streamline the annotation process. The software was evaluated on a large dataset of blood samples, and the results showed that Easy-GT significantly reduced annotation time while improving accuracy, even when compared to manual annotation. Overall, Easy-GT represents a valuable contribution to the biomedical imaging community by providing a reliable and robust solution for ground truth annotation of white blood cell nuclei. 1 into PostgreSQL...\n",
      "Inserting test sample 819  We present high-quality ground-based spectroscopic observations of 54 supergiant H II regions in 50 low-metallicity blue compact galaxies with oxygen abundances 12 + log O/H between 7.1 and 8.3. We use the data to determine abundances for the elements N, O, Ne, S, Ar and Fe. We also analyze Hubble Space Telescope (HST) Faint Object Spectrograph archival spectra of 10 supergiant H II regions to derive C and Si abundances in a subsample of 7 BCGs.\n",
      "\n",
      "The main result of the present study is that none of the heavy element-to-oxygen abundance ratios studied here (C/O, N/O, Ne/O, Si/O, S/O, Ar/O, Fe/O) depend on oxygen abundance for BCGs with 12 + log O/H < 7.6 (Z < Zsun/20). This constancy implies that all these heavy elements have a primary origin and are produced by the same massive (M > 10Msun) stars responsible for O production. The dispersion of the C/O and N/O ratios in these galaxies is found to be remarkably small, being only +/-0.03 dex and +/-0.02 dex respectively. This very small dispersion is strong evidence against any time-delayed production of C and primary N in the lowest-metallicity BCGs (secondary N production is negligible at these low metallicities). The absence of a time-delayed production of C and N is consistent with the scenario that galaxies with 12 + log O/H < 7.6 are undergoing now their first burst of star formation, and that they are therefore young, with ages not exceeding 40 Myr.\n",
      "\n",
      "If very low metallicities BCGs are indeed young, this would argue against the commonly held belief that C and N are produced by intermediate-mass (3Msun < M < 9Msun) stars at very low metallicities, as these stars would not have yet completed their evolution in these lowest metallicity galaxies. 0 into PostgreSQL...\n",
      "Inserting test sample 820  Blue compact galaxies (BCGs) are low-mass, gas-rich systems that are known to harbor active star formation, despite their small size. Recent studies have revealed that BCGs exhibit peculiar properties, including high ionization parameters and the presence of strong emission lines from heavy elements. In this study, we investigate the heavy element abundances in a sample of 50 BCGs using high-resolution spectroscopy. Our analysis reveals that BCGs have systematically higher heavy element abundances than what is expected from their stellar masses, indicating a significant contribution from external sources, such as mergers or gas accretion events. We find that the heavy element enrichment is higher in BCGs with lower metallicities and higher star formation rates, consistent with the notion that these systems are still actively forming stars from freshly accreted gas. We also compare our findings with theoretical models, and find that our results are generally consistent with predictions from cosmological simulations, albeit with a few notable discrepancies. Finally, we discuss the implications of our results for our understanding of the origin and evolution of low-mass galaxies and the chemical enrichment of the intergalactic medium. Our study sheds light on the complex interplay between star formation, gas accretion, and mergers in shaping the chemical makeup of BCGs, providing important constraints for future theoretical and observational studies of galaxy evolution and cosmology. 1 into PostgreSQL...\n",
      "Inserting test sample 821  The LIGO-Virgo and Fermi collaborations recently reported a possible joint detection of a sub-threshold gravitational wave (GW) event and a sub-threshold gamma-ray burst (GRB), GBM-190816, that occurred 1.57 s after the merger. We perform an independent analysis of the publicly available data and investigate the physical implications of this potential association. By carefully studying the following properties of GBM-190816 using Fermi/GBM data, including signal-to-noise ratio, duration, f-parameter, spectral properties, energetic properties, and its compliance with some GRB statistical correlations, we confirm that this event is likely a typical short GRB. Assuming its association with the sub-threshold GW event, the inferred luminosity is $1.47_{-1.04}^{+3.40} \\times 10^{49}$ erg s$^{-1}$. Based on the available information of the sub-threshold GW event, we infer the mass ratio q of the compact binary as $q=2.26_{-1.43}^{+2.75}$ according to the reported range of luminosity distance. If the heavier compact object has a mass > 3 solar masses, q can be further constrained to $q=2.26_{-0.12}^{+2.75}$. The leading physical scenario invokes an NS-BH merger system with the NS tidally disrupted. Within this scenario, we constrain the physical properties of such a system to produce a GRB. The GW data may also allow an NS-BH system with no tidal disruption of the NS or a BH-BH merger. We apply the charged compact binary coalescence (cCBC) theory (for both a constant charge and an increasing charge for the merging members) to derive the model parameters to account for GBM-190816 and found that the required parameters are extreme. Finally, we argue that the fact that the observed GW-GRB delay time scale is comparable to that of GW170817/GRB 170817A suggests that the GW-GRB time delay of these two cases is mainly defined by the time scale for the jet to propagate to the GRB emission site. 0 into PostgreSQL...\n",
      "Inserting test sample 822  This research paper investigates the implications of the sub-threshold gamma-ray burst (GRB) GBM-190816 and its associated sub-threshold gravitational wave event. We analyze the observed data from the Fermi Gamma-ray Burst Monitor, the Laser Interferometer Gravitational-Wave Observatory (LIGO), and the Virgo gravitational-wave detector, utilizing a joint likelihood analysis. Our results suggest that the GRB and the gravitational wave event originate from the same astrophysical source, likely a binary neutron star merger.\n",
      "\n",
      "We perform a detailed analysis of the prompt and afterglow emission properties of the GRB, finding that the burst exhibits an unusual spectral hardness evolution and temporal profile. Additionally, the GRB's isotropic equivalent energy release is estimated to be unusually low for a typical short-duration GRB.\n",
      "\n",
      "In order to study the possible origin and nature of the sub-threshold gravitational-wave event, we analyze the LIGO-Virgo data surrounding the time of the GRB observation. Our analysis reveals a localized excess of gravitational-wave power, consistent with the binary neutron star merger hypothesis inferred from the GRB observations. The estimated distance and rough localization of the event suggest this could be a promising candidate for follow-up observations in multiple electromagnetic bands.\n",
      "\n",
      "The sub-threshold nature of both the GRB and the gravitational-wave event highlights the importance of multi-messenger astronomy in uncovering new astrophysical phenomena. This study underscores the need for future efforts to improve the sensitivity and sophistication of GRB and gravitational-wave observations. Overall, our findings have far-reaching implications for our understanding of the properties and origins of gamma-ray bursts and gravitational waves, and may shed light on the nature of dark matter and other fundamental mysteries of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 823  Complex networks are a powerful modeling tool, allowing the study of countless real-world systems. They have been used in very different domains such as computer science, biology, sociology, management, etc. Authors have been trying to characterize them using various measures such as degree distribution, transitivity or average distance. Their goal is to detect certain properties such as the small-world or scale-free properties. Previous works have shown some of these properties are present in many different systems, while others are characteristic of certain types of systems only. However, each one of these studies generally focuses on a very small number of topological measures and networks. In this work, we aim at using a more systematic approach. We first constitute a dataset of 152 publicly available networks, spanning over 7 different domains. We then process 14 different topological measures to characterize them in the most possible complete way. Finally, we apply standard data mining tools to analyze these data. A cluster analysis reveals it is possible to obtain two significantly distinct clusters of networks, corresponding roughly to a bisection of the domains modeled by the networks. On these data, the most discriminant measures are density, modularity, average degree and transitivity, and at a lesser extent, closeness and edgebetweenness centralities.Abstract--Complex networks are a powerful modeling tool, allowing the study of countless real-world systems. They have been used in very different domains such as computer science, biology, sociology, management, etc. Authors have been trying to characterize them using various measures such as degree distribution, transitivity or average distance.\n",
      "\n",
      "Their goal is to detect certain properties such as the small-world or scale-free properties. Previous works have shown some of these properties are present in many different systems, while others are characteristic of certain types of systems only. However, each one of these studies generally focuses on a very small number of topological measures and networks. In this work, we aim at using a more systematic approach. We first constitute a dataset of 152 publicly available networks, spanning over 7 different domains. We then process 14 different topological measures to characterize them in the most possible complete way. Finally, we apply standard data mining tools to analyze these data. A cluster analysis reveals it is possible to obtain two significantly distinct clusters of networks, corresponding roughly to a bisection of the domains modeled by the networks. On these data, the most discriminant measures are density, modularity, average degree and transitivity, and at a lesser extent, closeness and edgebetweenness centralities. 0 into PostgreSQL...\n",
      "Inserting test sample 824  The study of complex networks has been a major field of research in various scientific disciplines. In this paper, we focus on the classification of complex networks based on topological properties. Topology refers to the study of the properties of space that are preserved under continuous transformations. Topological properties of complex networks include measures such as degree distribution, clustering coefficient, and the presence of hubs.\n",
      "\n",
      "Our research aims to provide an overview of the current state of the field, to identify the most significant topological properties that can be used for classification, and to investigate how these properties are related to the structure and function of complex networks. We begin by defining the term \"complex network\" and presenting a brief history of its development. We then discuss the main types of complex networks that have been studied, such as social networks, biological networks, and technological networks, and provide examples of each.\n",
      "\n",
      "We proceed by presenting the most common topological properties that have been used for classification, and explain their significance. Degree distribution refers to the distribution of the number of links per node in a network and has been found to be a key feature in several classification studies. The clustering coefficient measures the density of connections among a node's neighbors and is important in determining the resilience of a network to random failures. Hubs are nodes with the highest degree of connectivity and play a crucial role in determining the flow of information in a network.\n",
      "\n",
      "We then discuss various classification methods that have been proposed, such as hierarchical clustering, community detection, and machine learning algorithms. We compare the effectiveness of different methods in classifying complex networks based on topological properties and evaluate their strengths and weaknesses.\n",
      "\n",
      "Our final contribution is to investigate the relationship between the topological properties of complex networks and their functional characteristics, such as their ability to perform tasks such as information diffusion or transportation. We provide examples of how topological properties can be linked to functional properties in specific complex network types.\n",
      "\n",
      "In conclusion, this paper provides an overview of the current state of the field of classifying complex networks based on topological properties. By identifying the most significant topological properties and investigating their relationship to the structure and function of networks, this paper contributes to a better understanding of the fundamental principles underlying the organization of complex systems. This research has potential applications in fields such as network engineering, social media analysis, and biological network analysis. 1 into PostgreSQL...\n",
      "Inserting test sample 825  American universities use a procedure based on a rolling six-year graduation rate to calculate statistics regarding their students' final educational outcomes (graduating or not graduating). As~an alternative to the six-year graduation rate method, many studies have applied absorbing Markov chains for estimating graduation rates. In both cases, a frequentist approach is used.\n",
      "\n",
      "For~the standard six-year graduation rate method, the frequentist approach corresponds to counting the number of students who finished their program within six years and dividing by the number of students who entered that year.\n",
      "\n",
      "In the case of absorbing Markov chains, the frequentist approach is used to compute the underlying transition matrix, which is then used to estimate the graduation rate. In this paper, we apply a sensitivity analysis to compare the performance of the standard six-year graduation rate method with that of absorbing Markov chains. Through the analysis, we highlight significant limitations with regards to the estimation accuracy of both approaches when applied to small sample sizes or cohorts at a university. Additionally, we note that the Absorbing Markov chain method introduces a significant bias, which leads to an underestimation of the true graduation rate. To~overcome both these challenges, we propose and evaluate the use of a regularly updating multi-level absorbing Markov chain (RUML-AMC) in which the transition matrix is updated year to year. We empirically demonstrate that the proposed RUML-AMC approach nearly eliminates estimation bias while reducing the estimation variation by more than 40%, especially for populations with small sample sizes. 0 into PostgreSQL...\n",
      "Inserting test sample 826  Accurate graduation rate estimation is vital for universities and policymakers alike. However, traditional methods of calculating graduation rates have been criticized for their inability to account for individual-level factors that influence student success. In this study, we propose a new approach for estimating graduation rates that utilizes regularly updating multi-level absorbing Markov chains. Our method is able to capture the impact of both individual-level and contextual factors on student success, such as socioeconomic background and institutional characteristics. Furthermore, by utilizing regularly updating data, our approach can account for changes in the student body and the university environment over time. To test the effectiveness of our method, we applied it to data from a large public university in the United States and compared our estimates to those obtained using traditional methods. We found that our method produced more accurate estimates of graduation rates, particularly for subgroups of students who are historically at risk of not graduating, such as low-income students and students of color. Our results suggest that regularly updating multi-level absorbing Markov chains can be a powerful tool for improving graduation rate estimates and ensuring that universities are able to make informed decisions about their policies and practices. 1 into PostgreSQL...\n",
      "Inserting test sample 827  We present a new method for identifying the latent categorization of items based on their rankings. Complimenting a recent work that uses a Dirichlet prior on preference vectors and variational inference, we show that this problem can be effectively dealt with using existing community detection algorithms, with the communities corresponding to item categories. In particular we convert the bipartite ranking data to a unipartite graph of item affinities, and apply community detection algorithms. In this context we modify an existing algorithm - namely the label propagation algorithm to a variant that uses the distance between the nodes for weighting the label propagation - to identify the categories. We propose and analyze a synthetic ordinal ranking model and show its relation to the recently much studied stochastic block model. We test our algorithms on synthetic data and compare performance with several popular community detection algorithms. We also test the method on real data sets of movie categorization from the Movie Lens database. In all of the cases our algorithm is able to identify the categories for a suitable choice of tuning parameter. 0 into PostgreSQL...\n",
      "Inserting test sample 828  In many real-world scenarios, the ranking of items according to an ordinal scale is a common way to collect data. However, standard methods for categorization fail to take into account the distinct nature of this type of data. Hence, this study focuses on the development of algorithms for item categorization based on ordinal ranking data. We propose a framework which combines a Bayesian approach with rank-based optimization methods. Additionally, we introduce a distance metric tailored to ordinal data and a novel feature extraction technique based on rank correlations. Our algorithm outperforms state-of-the-art methods on both simulated and real-world datasets in terms of classification accuracy and area under the receiver operating characteristic curve. Furthermore, the proposed technique can be easily adapted to various applications, including music recommendation, product and service rating, and sentiment analysis. Consequently, our work provides a promising direction for ordinal-based data analysis in various fields. 1 into PostgreSQL...\n",
      "Inserting test sample 829  Nuclear fusion cross-sections considerably higher than corresponding theoretical predictions are observed in low-energy experiments with metal matrix targets and accelerated deuteron beams. The cross-section increment is significantly higher for liquid than for solid targets. We propose that the same two-body correlation entropy used in evaluating the metal melting entropy explains the large liquid-solid difference of the effective screening potential that parameterizes the cross-section increment. This approach is applied to the specific case of the $^6$Li(d,$\\alpha$)$^4$He reaction, whose measured screening potential liquid-solid difference is $(235 \\pm 63)$ eV. Cross sections in the two metals with the highest two-body correlation entropy (In and Hg) have not yet been measured: increments of the cross sections in liquid relative to the ones in solid metals are estimated with the same procedure. 0 into PostgreSQL...\n",
      "Inserting test sample 830  The phenomenon of nuclear fusion is one of great interest to researchers, as it has the potential to create a clean and virtually limitless source of energy. In this study, we investigate the role of correlation entropy in nuclear fusion in liquid lithium, indium, and mercury. Specifically, we analyze the effect of particle interactions on the fusion reaction by considering their entropic contributions to the system. By incorporating correlation entropy into our calculations, we are able to provide a more complete picture of the fusion process, as it occurs in these different liquid metals. Our results suggest that correlation entropy has a significant impact on the fusion reaction rates in these systems and must be accounted for in future models and simulations. These findings have important implications for efforts to achieve sustainable energy production through nuclear fusion. 1 into PostgreSQL...\n",
      "Inserting test sample 831  The connection between the proper time equation and the Zamolodchikov metric is discussed. The connection is two-fold: First, as already known, the proper time equation is the product of the Zamolodchikov metric and the renormalization group beta function. Second, the condition that the two-point function is the Zamolodchikov metric, implies the proper time equation. We study the massless vector of the open string in detail. In the exactly calculable case of a uniform electromgnetic field strength we recover the Born-Infeld equation. We describe the systematics of the perturbative evaluation of the gauge invariant proper time equation for the massless vector field. The method is valid for non-uniform fields and gives results that are exact to all orders in derivatives. As a non trivial check, we show that in the limit of uniform fields it reproduces the lowest order Born-Infeld equation. 0 into PostgreSQL...\n",
      "Inserting test sample 832  The Proper Time Equation and the Zamolodchikov Metric are two important concepts in theoretical physics that have recently been interconnected through research. The Proper Time Equation is a fundamental equation that is used to describe the relationship between time and distance in special relativity. The Zamolodchikov Metric, on the other hand, is a tool used to measure distances in the space of quantum field theories. Recent research has shown that the Proper Time Equation can be written in terms of the Zamolodchikov Metric, revealing a deep connection between the two concepts. This discovery has far-reaching implications for our understanding of quantum field theories, special relativity, and the interplay between them. Further research is needed to fully explore the implications of this connection, but it represents a significant advance in our understanding of the fundamental laws of physics. 1 into PostgreSQL...\n",
      "Inserting test sample 833  We report the relative abundances of the three stable isotopes of silicon, $^{28}$Si, $^{29}$Si and $^{30}$Si, across the Galaxy using the $v = 0, J = 1 \\to 0$ transition of silicon monoxide. The chosen sources represent a range in Galactocentric radii ($R_{\\rm GC}$) from 0 to 9.8 kpc. The high spectral resolution and sensitivity afforded by the GBT permit isotope ratios to be corrected for optical depths. The optical-depth-corrected data indicate that the secondary-to-primary silicon isotope ratios $^{29}{\\rm Si}/^{28}{\\rm Si}$ and $^{30}{\\rm Si}/^{28}{\\rm Si}$ vary much less than predicted on the basis of other stable isotope ratio gradients across the Galaxy. Indeed, there is no detectable variation in Si isotope ratios with $R_{\\rm GC}$. This lack of an isotope ratio gradient stands in stark contrast to the monotonically decreasing trend with $R_{\\rm GC}$ exhibited by published secondary-to-primary oxygen isotope ratios. These results, when considered in the context of the expectations for chemical evolution, suggest that the reported oxygen isotope ratio trends, and perhaps that for carbon as well, require further investigation. The methods developed in this study for SiO isotopologue ratio measurements are equally applicable to Galactic oxygen, carbon and nitrogen isotope ratio measurements, and should prove useful for future observations of these isotope systems. 0 into PostgreSQL...\n",
      "Inserting test sample 834  The uniformity of silicon isotope ratios across the Milky Way galaxy has been a topic of significant interest in astrophysics. In this research paper, we present detailed measurements of silicon isotopes in stars from different regions of the Milky Way. Our analysis shows that there is indeed a remarkable consistency in silicon isotopic ratios across the entire galaxy. This finding is significant because it implies that the processes involved in silicon nucleosynthesis have been consistent throughout the evolution of the Milky Way. Our data also enables us to refine previous models of galactic chemical evolution and provides insight into the contribution of Type Ia supernovae to the Milky Way's chemical enrichment. We show that the observed uniformity is consistent with theoretical predictions, barring a few outliers that may require further investigation. Furthermore, our results have implications for the search for Earth-like exoplanets, as silicon is a principal component of rocky planets and a key element in the emergence of life. Overall, our study sheds new light on the nature of galactic evolution and the role of silicon in the cosmos. 1 into PostgreSQL...\n",
      "Inserting test sample 835  In this work we study the Schwarzschild metric in the context of canonical quantum gravity inside the horizon, close of horizon and near the black hole singularity. Using this standard quantization procedure, we show that the horizon is quantized and the black hole singularity disappears. For the first case, quantization of the Schwarzschild radius was obtained in terms of the Planck length $l_{Pl}$, a positive integer $n$ and the ordering factor of the operator $p$. From the quantization of the Schwarzschild radius it was possible to determine the area of the black hole event horizon, its mass and the quantum energy of the Hawking radiation as well as its frequency. For the solution close to the interior black hole singularity, the wave function was determined and applied the DeBroglie-Bohm interpretation. The Bohm's trajectory was found near to the singularity. It which describes how the spacetime evolves over time and depends on the ordering factor of the operator $p$. Thus, for the case where $|1-p|\\neq0,3$, the Bohm's trajectory is finite and regular, that is, the singularity is removed. For the case where $|1-p|=3$, the Bohm's trajectory assumes an exponential behavior, never going to zero, avoiding the singularity.That result allows that spacetime be extended beyond the classical singularity. 0 into PostgreSQL...\n",
      "Inserting test sample 836  This paper discusses the quantization of the interior of a black hole, a long-standing problem in the field of theoretical physics. Much of the research on this topic has focused on the behavior of singularities within a black hole, which are currently not well-understood but are believed to play a key role in the quantization process. Some researchers have proposed that the singularity is actually a region of space-time where classical physics breaks down and quantum mechanics takes over, allowing for the quantization of the interior. Others have suggested that the singularity may be a gateway to other universes or dimensions, which could have implications for our understanding of the nature of reality. Recent advancements in theoretical physics, including developments in string theory and quantum gravity, may help shed light on this complex phenomena. By better understanding the quantization of the interior of the black hole, we can gain insights into the fundamental nature of the universe and help further our understanding of the mysteries of the cosmos. 1 into PostgreSQL...\n",
      "Inserting test sample 837  We demonstrate a new method for determining the 81Kr/Kr ratio in environmental samples based upon two measurements: the 85Kr/81Kr ratio measured by Atom Trap Trace Analysis (ATTA) and the 85Kr/Kr ratio measured by Low-Level Counting (LLC). This method can be used to determine the mean residence time of groundwater in the range of 10^5 - 10^6 a. It requires a sample of 100 micro-l STP of Kr extracted from approximately two tons of water. With modern atmospheric Kr samples, we demonstrate that the ratios measured by ATTA and LLC are directly proportional to each other within the measurement error of +/- 10%; we calibrate the 81Kr/Kr ratio of modern air measured using this method; and we show that the 81Kr/Kr ratios of samples extracted from air before and after the development of the nuclear industry are identical within the measurement error. 0 into PostgreSQL...\n",
      "Inserting test sample 838  This study presents a novel method for measuring the abundances of the radioisotopes 81Kr and 85Kr in environmental samples. The development of such a method has been a challenging endeavor due to the low concentrations of these isotopes in the atmosphere and lack of suitable measurement techniques. Our approach employs a combination of gas chromatography and isotope dilution mass spectrometry, which allows for the reliable and accurate measurement of both isotopes. Initial tests of the method on atmospheric samples have yielded promising results, demonstrating its suitability for use in a wide range of environmental applications. The ability to precisely measure these isotopes is important for various fields, including nuclear forensics, hydrology, and global climate research. This new method provides a valuable tool for researchers to better understand the behavior and movement of these isotopes in environmental systems. 1 into PostgreSQL...\n",
      "Inserting test sample 839  We present experimental and theoretical studies of the dynamics of molecular motors in microtubule arrays and asters. By solving a convection-diffusion equation we find that the density profile of motors in a two-dimensional aster is characterized by continuously varying exponents. Simulations are used to verify the assumptions of the continuum model. We observe the concentration profiles of kinesin moving in quasi two-dimensional artificial asters by fluorescent microscopy and compare with our theoretical results. 0 into PostgreSQL...\n",
      "Inserting test sample 840  The organization of microtubules in cells plays a pivotal role in intracellular transport, cellular division and differentiation. Our study presents a theoretical model exploring the dynamic concentration of motors in microtubule arrays. Our approach suggests motor proteins can be selectively enriched in distinct regions of the microtubule lattice, leading to the formation of motor-rich domains. Our findings deepen our understanding of the coordinated function of microtubule-associated proteins and motor proteins during intracellular transport. 1 into PostgreSQL...\n",
      "Inserting test sample 841  We study the striking case of a blue narrow stream with a possible globular cluster-like progenitor around the Milky Way-size galaxy NGC 7241 and its foreground dwarf companion. We present a follow-up spectroscopic study of this stream based on data taken with the MEGARA instrument at the 10.4-m Gran Telescopio Canarias using the integral field spectroscopy mode. Although our data suggest that this compact object in the stream is actually a foreground Milky Way halo star, we detect emission lines overlapping a less compact, bluer and fainter blob of the stream that is clearly visible in both ultra-violet and optical deep images. From its heliocentric systemic radial velocity derived from the [OIII] 5007A lines (V_syst= 1548.58+/-1.80 km\\s^-1) and new UV and optical broad-band photometry, we conclude that this over-density could be the actual core of the stream, with an absolute magnitude of Mg~ -10 and a g-r = 0.08+/- 0.11, consistent with a remnant of a low-mass dwarf satellite undergoing a current episode of star formation. From the width of the stream, we calculate that the progenitor mass is between 6.4 x 10^6 Mo -2.7 x 10^7 Mo, which is typical of a dwarf galaxy. These estimates suggest that this is one of the lowest mass streams detected so far beyond the Local Group. We find that blue stellar streams containing star formation regions are commonly predicted by high-resolution cosmological simulations of galaxies lighter than the Milky Way. This scenario is consistent with the processes explaining the bursty star formation history of some dwarf satellites, which are followed by a gas depletion and a fast quenching once they enter within the virial radius of their host galaxies. Thus, it is likely that the stream's progenitor is suffering a star-formation burst comparable to those that have shaped the star-formation history of several Local Group dwarfs in the last few Gigayears. 0 into PostgreSQL...\n",
      "Inserting test sample 842  Recent investigations have led to the discovery of young stellar populations in the stellar streams of the Milky Way galaxy. In particular, the NGC 7241 stellar stream has proven to be a rich source of information regarding recent star formation, and recent observations with the MEGARA spectrograph have provided new insights into this phenomenon.\n",
      "\n",
      "Our study focuses on the detection and characterization of young stars in the NGC 7241 stream, as well as their implications for our understanding of galaxy evolution. We utilized MEGARA's unique sensitivity to faint emission lines to identify a population of blue stars along the stream, indicative of recent star formation.\n",
      "\n",
      "Our analysis suggests that these stars formed within the last 100 million years, making them some of the youngest stars ever discovered in a Galactic stellar stream. In addition to their youth, these stars display a range of metallicities, indicating that they formed from a variety of interstellar gas sources. We also detected ionized gas emission coincident with the young stars, providing further evidence of ongoing star formation activity in the NGC 7241 stream.\n",
      "\n",
      "Our results have important implications for our understanding of the Milky Way's formation and evolution. The discovery of young stars in stellar streams suggests that these streams may be important sites for ongoing galaxy rejuvenation, as fresh gas supplies are accreted and form new stars. The range of metallicities observed among the young stars in NGC 7241 also supports the idea that galaxies build up their chemical abundance over time through a process of continuous accretion.\n",
      "\n",
      "Overall, our study represents a significant step forward in our understanding of recent star formation in Galactic stellar streams. Through our observations with MEGARA, we have identified a population of extremely young stars in the NGC 7241 stream and provided new insights into the ongoing evolution of the Milky Way. 1 into PostgreSQL...\n",
      "Inserting test sample 843  Static bug finders have been widely-adopted by developers to find bugs in real world software projects. They leverage predefined heuristic static analysis rules to scan source code or binary code of a software project, and report violations to these rules as warnings to be verified. However, the advantages of static bug finders are overshadowed by such issues as uncovered obvious bugs, false positives, etc. To improve these tools, many techniques have been proposed to filter out false positives reported or design new static analysis rules. Nevertheless, the under-performance of bug finders can also be caused by the incorrectness of current rules contained in the static bug finders, which is not explored yet. In this work, we propose a differential testing approach to detect bugs in the rules of four widely-used static bug finders, i.e., SonarQube, PMD, SpotBugs, and ErrorProne, and conduct a qualitative study about the bugs found. To retrieve paired rules across static bug finders for differential testing, we design a heuristic-based rule mapping method which combines the similarity in rules description and the overlap in warning information reported by the tools. The experiment on 2,728 open source projects reveals 46 bugs in the static bug finders, among which 24 are fixed or confirmed and the left are awaiting confirmation. We also summarize 13 bug patterns in the static analysis rules based on their context and root causes, which can serve as the checklist for designing and implementing other rules and or in other tools. This study indicates that the commonly-used static bug finders are not as reliable as they might have been envisaged. It not only demonstrates the effectiveness of our approach, but also highlights the need to continue improving the reliability of the static bug finders. 0 into PostgreSQL...\n",
      "Inserting test sample 844  This paper presents a novel approach for detecting bugs in pattern-based bug detectors. These detectors are widely used to identify and locate software bugs. However, despite their efficacy, they are prone to errors, as they rely on patterns to catch bugs. Our approach involves augmenting the pattern-based detectors with a set of rules which allows them to better detect errors that fall beyond the scope of the patterns. We accomplish this by leveraging techniques such as data-flow analysis and constraint solving.\n",
      "\n",
      "To validate our approach, we performed a series of experiments on real-world software, comparing the performance of our augmented detectors against traditional pattern-based detectors. Our results show that our approach significantly outperforms pattern-based detectors in detecting previously undetected errors, while retaining a low false positive rate.\n",
      "\n",
      "A key contribution of our approach is the ability to detect errors in complex software systems that are difficult to detect using traditional pattern-based detectors. In particular, our approach is effective in detecting errors such as null pointer exceptions, resource leaks, and concurrency issues, which can have serious consequences on the overall performance and reliability of the software system.\n",
      "\n",
      "Overall, our approach represents a major step towards improving the efficiency and accuracy of software bug detection. With the increasing complexity and scale of software systems, detecting and resolving software bugs is becoming more challenging, and our approach offers a promising solution for improving software quality, reducing costs, and enhancing end-user experience. 1 into PostgreSQL...\n",
      "Inserting test sample 845  Weakly Interacting Massive Particles (WIMPs) are a theoretical class of particles that are excellent dark matter candidates. WIMP annihilation or decay may produce essentially monochromatic gamma rays detectable by the Fermi Large Area Telescope (LAT) against the astrophysical gamma-ray emission of the Galaxy. We have searched for spectral lines in the energy range 5--300 GeV using 3.7 years of data, reprocessed with updated instrument calibrations and an improved energy dispersion model compared to the previous Fermi-LAT Collaboration line searches. We searched in five regions selected to optimize sensitivity to different theoretically-motivated dark matter density distributions. We did not find any globally significant lines in our a priori search regions and present 95% confidence limits for annihilation cross sections of self-conjugate WIMPs and decay lifetimes. Our most significant fit occurred at 133 GeV in our smallest search region and had a local significance of 3.3 standard deviations, which translates to a global significance of 1.5 standard deviations. We discuss potential systematic effects in this search, and examine the feature at 133 GeV in detail. We find that both the use of reprocessed data and of additional information in the energy dispersion model contribute to the reduction in significance of the line-like feature near 130 GeV relative to significances reported in other works. We also find that the feature is narrower than the LAT energy resolution at the level of 2 to 3 standard deviations, which somewhat disfavors the interpretation of the 133 GeV feature as a real WIMP signal. 0 into PostgreSQL...\n",
      "Inserting test sample 846  The Fermi Large Area Telescope (LAT) has been at the forefront of gamma-ray astrophysics since its launch in 2008. In particular, it has enabled the search for gamma-ray spectral lines, which could be a signature of dark matter particles, through the study of gamma-ray emission from dwarf galaxies. In this paper, we present an analysis of the LAT data from a sample of dwarf galaxies, focusing on the search for spectral lines in the gamma-ray energy range of 1-100 GeV.\n",
      "\n",
      "The data analysis consists of a combination of a line search algorithm and a likelihood ratio test, which allows us to distinguish spectral lines from the continuum background. We find no statistically significant evidence for spectral lines in our analysis, placing strong constraints on the properties of potential dark matter particles. Our results rule out some of the most common dark matter models and provide important information for future studies of dark matter in the gamma-ray regime.\n",
      "\n",
      "Finally, we discuss the implications of our analysis for the properties of dark matter particles. Our results are consistent with the possibility that dark matter is composed of weakly interacting massive particles (WIMPs), which would produce gamma rays through their annihilation or decay. However, alternative dark matter scenarios, such as axions or sterile neutrinos, cannot be ruled out with our current data. Overall, our study demonstrates the power of gamma-ray spectral line searches with the LAT and highlights the importance of continuing this effort in order to fully explore the nature of dark matter. 1 into PostgreSQL...\n",
      "Inserting test sample 847  Asteroids of size larger than 0.15 km generally do not have periods P smaller than about 2.2 hours, a limit known as cohesionless spin-barrier. This barrier can be explained by means of the cohesionless rubble-pile structure model. In this paper we will explore the possibility for the observed spin-barrier value to be different for C and S-type Main Asteroids Belt (MBAs). On the basis of the actual bulk density values, the expected ratio between the maximum rotation periods is $P_C/P_S \\approx 1.4 \\pm 0.3$. Using the data available in the asteroid LightCurve Data Base (LCDB) we have found that, as regards the mean spin-barrier values and for asteroids in the 4-20 km range, there is a little difference between the two asteroids population with a ratio $P_C/P_S \\approx 1.20 \\pm 0.04$. Uncertainties are still high, mainly because of the small number of MBAs with known taxonomic class in the considered range. In the 4-10 km range, instead, the ratio between the spin-barriers seems closer to 1 because $P_C/P_S \\approx 1.11 \\pm 0.05$. This behavior could be a direct consequence of a different cohesion strength for C and S-type asteroids of which the ratio can be estimated. 0 into PostgreSQL...\n",
      "Inserting test sample 848  Asteroid populations in our solar system are known to exhibit differences in spin properties based on their spectral types. Here, we present a comprehensive study of the spin-barrier ratio for S and C-type main belt asteroids based on data gathered from the Sloan Digital Sky Survey. Our analysis reveals that S-type asteroids exhibit a higher fraction of slow rotators compared to C-type asteroids, which have a higher proportion of fast rotators. Furthermore, we find that the size distribution of slow rotators is biased towards larger objects in both populations. The spin-barrier ratio is also found to be correlated with asteroid size, with larger asteroids having a higher probability of exhibiting fast rotation. Our results suggest that the mechanism responsible for spin barrier evolution is likely to be more efficient for S-type asteroids, which are believed to originate from the inner regions of the solar system. This study sheds new light on the underlying physical processes that govern the spin state of asteroids, and has broader implications for our understanding of the formation and evolution of our solar system. 1 into PostgreSQL...\n",
      "Inserting test sample 849  Deep Neural Networks have achieved huge success at a wide spectrum of applications from language modeling, computer vision to speech recognition.\n",
      "\n",
      "However, nowadays, good performance alone is not sufficient to satisfy the needs of practical deployment where interpretability is demanded for cases involving ethics and mission critical applications. The complex models of Deep Neural Networks make it hard to understand and reason the predictions, which hinders its further progress. To tackle this problem, we apply the Knowledge Distillation technique to distill Deep Neural Networks into decision trees in order to attain good performance and interpretability simultaneously. We formulate the problem at hand as a multi-output regression problem and the experiments demonstrate that the student model achieves significantly better accuracy performance (about 1\\% to 5\\%) than vanilla decision trees at the same level of tree depth. The experiments are implemented on the TensorFlow platform to make it scalable to big datasets. To the best of our knowledge, we are the first to distill Deep Neural Networks into vanilla decision trees on multi-class datasets. 0 into PostgreSQL...\n",
      "Inserting test sample 850  Deep neural networks are powerful models used in various applications. However, their inherent complexity often leads to a lack of interpretability, which hinders trust and understanding of the results. Knowledge distillation is an effective technique that simplifies the models and improves their interpretability without sacrificing much performance. In this paper, we propose an approach that combines knowledge distillation with interpretability constraints to generate a more interpretable network with minimal loss in accuracy. Our approach aims to improve the transparency and trustworthiness of prediction models while preserving their accuracy. We evaluate our model on several benchmarks and compared it to other state-of-the-art algorithms. The results show that our approach can significantly improve interpretability while maintaining accuracy. Furthermore, we demonstrate the use case of our model in a real-world scenario, where interpretability is crucial for decision making. Our proposed method can help enhance the interpretability of deep neural networks and make them more useful in various applications. 1 into PostgreSQL...\n",
      "Inserting test sample 851  Massive black hole binaries (BHBs) are expected to form as the result of galaxy mergers; they shrink via dynamical friction and stellar scatterings, until gravitational waves (GWs) bring them to the final coalescence. It has been argued that BHBs may stall at a parsec scale and never enter the GW stage if stars are not continuously supplied to the BHB loss cone. Here we perform several N-body experiments to study the effect of an 80,000 solar masses stellar cluster (SC) infalling on a parsec-scale BHB. We explore different orbital elements for the SC and we perform runs both with and without accounting for the influence of a rigid stellar cusp (modelled as a rigid Dehnen potential). We find that the semi-major axis of the BHB shrinks by more than 10 per cent if the SC is on a nearly radial orbit; the shrinking is more efficient when a Dehnen potential is included and the orbital plane of the SC coincides with that of the BHB. In contrast, if the SC orbit has non-zero angular momentum, only a few stars enter the BHB loss cone and the resulting BHB shrinking is negligible. Our results indicate that SC disruption might significantly contribute to the shrinking of a parsec-scale BHB only if the SC approaches the BHB on a nearly radial orbit. 0 into PostgreSQL...\n",
      "Inserting test sample 852  Star clusters, which are groups of stars that are gravitationally bound, are known to be disrupted by the gravitational pull of massive black holes. However, little is known about the effects of a binary system of massive black holes on these star clusters. In this study, we investigate the disruption of star clusters caused by a massive black hole binary using a combination of numerical simulations and analytical methods.\n",
      "\n",
      "Our simulations show that the interaction between a star cluster and a massive black hole binary can lead to significant disruptions, including the ejection of stars from the cluster and the formation of a new, smaller cluster. The disruption process is influenced by the mass ratio and separation of the black hole binary, as well as the initial properties of the star cluster. We also find that the disruption can occur on timescales ranging from tens of millions to billions of years depending on the specific parameters of the system.\n",
      "\n",
      "Our analytical model, which takes into account the effects of gravitational perturbations and energy transfer, provides insights into the underlying mechanisms of the disruption process. Despite the complexity of the system, our results suggest that the disruption of star clusters by massive black hole binaries is a fundamental process in galactic evolution. Understanding the disruptions caused by these binaries will lead to a better understanding of their role in shaping the properties of galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 853  For studying the group theoretical classification of the solutions of the density functional theory in relativistic framework, we propose quantum electrodynamical density-matrix functional theory (QED-DMFT). QED-DMFT gives the energy as a functional of a local one-body $4\\times4$ matrix $Q(x)\\equiv -<\\psi(x)\\bar{\\psi}(x)>$, where $\\psi$ and $\\bar{\\psi}$ are 4-component Dirac field and its Dirac conjugate, respectively. We examine some characters of QED-DMFT. After these preparations, by using Q(x), we classify the solutions of QED-DMFT under O(3) rotation, time reversal and spatial inversion. The behavior of Q(x) under nonrelativistic and ultrarelativistic limits are also presented.\n",
      "\n",
      "Finally, we give plans for several extensions and applications of QED-DMFT. 0 into PostgreSQL...\n",
      "Inserting test sample 854  In this paper, we present a new quantum electrodynamical density-matrix functional theory (QED-DMFT) for the calculation of excited states of many-electron systems. The QED-DMFT is based on the principles of density functional theory and provides an accurate and efficient way to calculate the Coulomb correlation energy of excited states. We also apply group-theoretical considerations to the solution of QED-DMFT and discuss the use of symmetry properties to reduce the complexity of the problem. We demonstrate the power of our approach on a series of benchmark calculations and find excellent agreement with experimental data. Our results open new avenues for the calculation of excited states of complex systems in condensed matter physics, chemistry, and materials science. 1 into PostgreSQL...\n",
      "Inserting test sample 855  Isolated Local Group (LG) dwarf galaxies have evolved most or all of their life unaffected by interactions with the large LG spirals and therefore offer the opportunity to learn about the intrinsic characteristics of this class of objects. Here we explore the internal kinematic and metallicity properties of one of the three isolated LG dwarf spheroidal galaxies, i.e. the Tucana dSph.\n",
      "\n",
      "This is an intriguing system, as it has been found in the literature to have an internal rotation of up to 16 km/s, a much higher velocity dispersion than other dSphs of similar luminosity, and a possible exception to the too-big-too-fail problem. We present results for a new VLT/FORS2 spectroscopic dataset in the CaII triplet region for 50 candidate red giant branch stars in the direction of Tucana, which yielded line-of-sight velocity and metallicity ([Fe/H]) measurements of 39 effective members. This doubles the number of Tucana's stars with such measurements. In addition, we re-reduce and include in our analysis the other two spectroscopic datasets presented in the literature, the VLT/FORS2 sample by Fraternali et al. (2009) and the VLT/FLAMES one by Gregory et al. (2019). We measure a systemic velocity of $180.0\\pm1.3$ km/s, consistently across the various datasets analyzed, and find that a dispersion-only model is moderately favored over models accounting also for internal rotation. Our best estimate of the internal velocity dispersion is $6.2_{-1.3}^{+1.6}$ km/s, much smaller than the values reported in the literature and in line with similarly luminous dSphs; this is consistent with Tucana not being an exception to the too-big-to-fail problem, nor living in a dark matter halo much more massive than those of its siblings. As for the metallicity properties, we do not find anything unusual; there are hints of the presence of a [Fe/H] gradient but more data are needed to pin its presence down. 0 into PostgreSQL...\n",
      "Inserting test sample 856  The Tucana dwarf spheroidal galaxy, initially thought to be a classic example of a \"massive failure\" due to its low mass and lack of dark matter, has turned out to be a fascinating object of study with unexpected properties. Recent observations with the Hubble Space Telescope and other instruments have revealed a complex stellar population with distinct metallicity patterns and a history of multiple star formation events.\n",
      "\n",
      "Using data from the Dark Energy Survey, we have conducted a detailed study of the Tucana galaxy's optical properties and kinematics, including its surface brightness profile, velocity dispersion, and rotation curve. We find that the galaxy's outer regions are significantly elongated along its major axis, possibly indicating the presence of a bar or tidal disturbance.\n",
      "\n",
      "Additionally, we have detected a kinematic substructure in the form of a \"counter-rotating\" core, where stars within a radius of 10 arcminutes rotate in the opposite direction to those at larger radii. This is a rare phenomenon in dwarf galaxies, and may provide valuable insights into the formation and evolution of these objects.\n",
      "\n",
      "Our analysis also supports the idea that the Tucana galaxy is embedded within an extended dark matter halo, contrary to previous claims that it lacks this ubiquitous component. Furthermore, we have used a Bayesian method to infer the galaxy's mass, finding a value consistent with previous estimates but with tighter constraints.\n",
      "\n",
      "These results demonstrate that the Tucana dwarf spheroidal galaxy is a worthy target for further study and highlight the importance of multi-wavelength, multi-technique investigations of low-mass galaxies. We hope that our work will motivate future observations and simulations aimed at understanding the nature and evolution of these enigmatic objects. 1 into PostgreSQL...\n",
      "Inserting test sample 857  The diffraction peaks of Zircaloy-2 and Zr-2.5%Nb alloys at various deformations are found to be asymmetric in nature. In order to characterize the microstructure from these asymmetric peaks of these deformed alloys, X-Ray Diffraction Line Profile Analysis like Williamson-Hall technique, Variance method based on second and fourth order restricted moments and Stephens model based on anisotropic strain distribution have been adopted. The domain size and dislocation density have been evaluated as a function of deformation for both these alloys. These techniques are useful where the dislocation structure is highly inhomogeneous inside the matrix causing asymmetry in the line profile, particularly for deformed polycrystalline materials. 0 into PostgreSQL...\n",
      "Inserting test sample 858  This paper presents X-Ray diffraction studies on the heavily deformed Zirconium-based alloys with asymmetrically broadened peaks. The asymmetric peak broadening observed in these alloys is attributed to the presence of anisotropic microstrains that arise from the deformation. The broadening effects are investigated using a line profile analysis procedure to obtain the width, asymmetry and strain parameters of the diffraction peaks. The results of this study reveal that the degree of asymmetry is an indicator of the nature and level of microstrain distribution, and hence can be used to determine the extent of deformation in the material. Overall, this work provides insights into the microstructural characteristics of heavily deformed Zirconium-based alloys. 1 into PostgreSQL...\n",
      "Inserting test sample 859  We determine detailed elemental abundances in stars belonging to the so-called Group 1 of the Geneva-Copenhagen survey (GCS) and compare the chemical composition with the Galactic thin- and thick-disc stars, with the GCS Group 2 and Group 3 stars, as well as with several kinematic streams of similar metallicities. The aim is to search for chemical signatures that might give information about the formation history of this kinematic group of stars.\n",
      "\n",
      "High-resolution spectra were obtained with the Fibre-fed Echelle Spectrograph (FIES) spectrograph at the Nordic Optical Telescope, La Palma, and were analysed with a differential model atmosphere method. Comparison stars were observed and analysed with the same method. The average value of [Fe/H] for the 37 stars of Group 1 is -0.20 +- 0.14 dex. Investigated Group 1 stars can be separated into three age subgroups. Along with the main 8- and 12-Gyr-old populations, a subgroup of stars younger than 5 Gyr can be separated as well.\n",
      "\n",
      "Abundances of oxygen, alpha-elements, and r-process dominated elements are higher than in Galactic thin-disc dwarfs. This elemental abundance pattern has similar characteristics to that of the Galactic thick disc and differs slightly from those in Hercules, Arcturus, and AF06 stellar streams. The similar chemical composition of stars in Group 1, as well as in Group 2 and 3, with that in stars of the thick disc might suggest that their formation histories are linked. The chemical composition pattern together with the kinematic properties and ages of stars in the investigated GCS groups provide evidence of their common origin and possible relation to an ancient merging event. A gas-rich satellite merger scenario is proposed as the most likely origin. 0 into PostgreSQL...\n",
      "Inserting test sample 860  This paper presents a study of kinematic Group 1 in the Geneva-Copenhagen survey of the solar neighborhood. Using a sample of approximately 2200 dwarf stars, we identify a kinematically distinct group in the solar neighborhood. The group exhibits enhanced dispersion in velocity, indicating that it is not a chance alignment of randomly moving stars. We found that the member stars of this group share similar spatial and chemical properties, including metallicity and abundance ratios, suggesting they may have a common origin. The distribution of member stars shows a loose concentration towards a region close to the Sun, with a mean distance of approximately 90 parsecs. Our estimate of the group's velocity dispersion indicates that it is dynamically young, possibly with an age of 2-3 Gyr. Additionally, we explored the group's kinematic properties using a set of dynamical models, providing an insight into its nature and origin. Our results show that the kinematic Group 1 is likely a disrupted open cluster or a moving group, with a possible origin in the thin disk's inner regions. We also explore several scenarios explaining the group's origin, including its formation from an expanding stellar association or the disruption of a molecular cloud. Overall, our study provides valuable insight into the kinematic and chemical properties of kinematic Group 1 and its place in the larger context of the Milky Way's stellar substructures. 1 into PostgreSQL...\n",
      "Inserting test sample 861  In 1982, Drezner proposed the (1|1)-centroid problem on the plane, in which two players, called the leader and the follower, open facilities to provide service to customers in a competitive manner. The leader opens the first facility, and then the follower opens the second. Each customer will patronize the facility closest to him (ties broken in favor of the leader's one), thereby decides the market share of the two players. The goal is to find the best position for the leader's facility so that his market share is maximized. The best algorithm for this problem is an $O(n^2 \\log n)$-time parametric search approach, which searches over the space of possible market share values.\n",
      "\n",
      "In the same paper, Drezner also proposed a general version of (1|1)-centroid problem by introducing a minimal distance constraint $R$, such that the follower's facility is not allowed to be located within a distance $R$ from the leader's. He proposed an $O(n^5 \\log n)$-time algorithm for this general version by identifying $O(n^4)$ points as the candidates of the optimal solution and checking the market share for each of them. In this paper, we develop a new parametric search approach searching over the $O(n^4)$ candidate points, and present an $O(n^2 \\log n)$-time algorithm for the general version, thereby close the $O(n^3)$ gap between the two bounds. 0 into PostgreSQL...\n",
      "Inserting test sample 862  The $(1|1)_R$-centroid problem on the plane is a well-known optimization problem within the field of computational geometry. The problem consists of finding a point on the plane that minimizes the sum of Euclidean distances to a given set of points. In this paper, we consider the generalized version of this problem, where the weights of the points have a ratio of $1:1$. We refer to this as the $(1|1)_R$-centroid problem. We present a new algorithm to solve this problem which is based on a divide-and-conquer approach. The essence of our algorithm is to recursively partition the input points into subsets of size at most three, for which we can compute the optimal solution using a direct formula. We prove that our algorithm runs in $O(n \\log n)$ time, where $n$ is the number of input points, and that it is correct in all cases. We also provide a comparison of our algorithm with other existing algorithms in the literature, highlighting the advantages and disadvantages of each. Finally, we discuss some possible applications of our algorithm in computational physics and biology. 1 into PostgreSQL...\n",
      "Inserting test sample 863  A novel strategy to handle divergences typical of perturbative calculations is implemented for the Nambu--Jona-Lasinio model and its phenomenological consequences investigated. The central idea of the method is to avoid the critical step involved in the regularization process, namely the explicit evaluation of divergent integrals. This goal is achieved by assuming a regularization distribution in an implicit way and making use, in intermediary steps, only of very general properties of such regularization. The finite parts are separated of the divergent ones and integrated free from effects of the regularization. The divergent parts are organized in terms of standard objects which are independent of the (arbitrary) momenta running in internal lines of loop graphs. Through the analysis of symmetry relations, a set of properties for the divergent objects are identified, which we denominate consistency relations, reducing the number of divergent objects to only a few ones. The calculational strategy eliminates unphysical dependencies of the arbitrary choices for the routing of internal momenta, leading to ambiguity-free, and symmetry-preserving physical amplitudes. We show that the imposition of scale properties for the basic divergent objects leads to a critical condition for the constituent quark mass such that the remaining arbitrariness is removed.\n",
      "\n",
      "The model become predictive in the sense that its phenomenological consequences do not depend on possible choices made in intermediary steps. Numerical results are obtained for physical quantities at the one-loop level for the pion and sigma masses and pion-quark and sigma-quark coupling constants. 0 into PostgreSQL...\n",
      "Inserting test sample 864  In this paper, we present a predictive formulation of the Nambu--Jona-Lasinio (NJL) model. This model is a valuable tool for studying the dynamics of spontaneous chiral symmetry breaking and effective quark masses in quantum chromodynamics (QCD). We consider an extension of the standard NJL model which includes a scalar auxiliary field coupled to the chiral condensate and fermion bilinears. This allows us to study the quantum corrections to the effective potential and the phase structure of the theory. We calculate the one-loop effective potential using a proper-time regularization scheme and find that the inclusion of the auxiliary field leads to a nontrivial dependence on the phase of the chiral condensate. We analyze the phase diagram of the model in the chiral limit and show that it exhibits a second-order phase transition at vanishing temperature and chemical potential. At nonzero chemical potential, we find that the phase transition becomes a crossover. Finally, we compare our results with previous studies of the NJL model and discuss the implications of our work for QCD at low energy. Our predictive formulation of the NJL model provides a valuable tool for future studies of the dynamics of chiral symmetry breaking and effective quark masses in QCD. 1 into PostgreSQL...\n",
      "Inserting test sample 865  The paper deals with the automatic analysis of real-life telephone conversations between an agent and a customer of a customer care service (ccs).\n",
      "\n",
      "The application domain is the public transportation system in Paris and the purpose is to collect statistics about customer problems in order to monitor the service and decide priorities on the intervention for improving user satisfaction. Of primary importance for the analysis is the detection of themes that are the object of customer problems. Themes are defined in the application requirements and are part of the application ontology that is implicit in the ccs documentation. Due to variety of customer population, the structure of conversations with an agent is unpredictable. A conversation may be about one or more themes. Theme mentions can be interleaved with mentions of facts that are irrelevant for the application purpose. Furthermore, in certain conversations theme mentions are localized in specific conversation segments while in other conversations mentions cannot be localized. As a consequence, approaches to feature extraction with and without mention localization are considered. Application domain relevant themes identified by an automatic procedure are expressed by specific sentences whose words are hypothesized by an automatic speech recognition (asr) system. The asr system is error prone.\n",
      "\n",
      "The word error rates can be very high for many reasons. Among them it is worth mentioning unpredictable background noise, speaker accent, and various types of speech disfluencies. As the application task requires the composition of proportions of theme mentions, a sequential decision strategy is introduced in this paper for performing a survey of the large amount of conversations made available in a given time period. The strategy has to sample the conversations to form a survey containing enough data analyzed with high accuracy so that proportions can be estimated with sufficient accuracy. Due to the unpredictable type of theme mentions, it is appropriate to consider methods for theme hypothesization based on global as well as local feature extraction. Two systems based on each type of feature extraction will be considered by the strategy. One of the four methods is novel. It is based on a new definition of density of theme mentions and on the localization of high density zones whose boundaries do not need to be precisely detected. The sequential decision strategy starts by grouping theme hypotheses into sets of different expected accuracy and coverage levels. For those sets for which accuracy can be improved with a consequent increase of coverage a new system with new features is introduced. Its execution is triggered only when specific preconditions are met on the hypotheses generated by the basic four systems. Experimental results are provided on a corpus collected in the call center of the Paris transportation system known as ratp. The results show that surveys with high accuracy and coverage can be composed with the proposed strategy and systems. This makes it possible to apply a previously published proportion estimation approach that takes into account hypothesization errors . 0 into PostgreSQL...\n",
      "Inserting test sample 866  This paper discusses the identification of multiple topics in human-to-human conversations, a task which is crucial for effective communication and natural language processing. The ability to accurately identify topics in conversations has many applications such as information retrieval, summarization, and sentiment analysis. We begin by reviewing the relevant literature on topic identification and summarize the state-of-the-art techniques for topic modeling. We then introduce a new method based on statistical natural language processing that is designed to improve the accuracy of topic identification in both structured and unstructured conversations. Our approach uses a combination of supervised and unsupervised machine learning techniques such as support vector machines, clustering, and latent semantic analysis to effectively identify multiple topics in conversation. \n",
      "\n",
      "To evaluate the effectiveness of our approach, we conducted experiments on several different datasets. Our results show that our model significantly outperforms other state-of-the-art methods on all of the datasets we tested. We also investigate the effects of different conversation characteristics such as topic distribution, conversation length, and topic correlation on topic identification accuracy. \n",
      "\n",
      "Finally, we discuss several potential applications of our model in real-world conversational settings. For example, our method could be used to identify key topics in social media discussions or email threads in order to facilitate information retrieval. Our model could also be used to summarize conversations or identify sentiment and emotional tone in conversations. \n",
      "\n",
      "In conclusion, we present an effective approach for multiple topic identification in human-to-human conversations using machine learning techniques. Our method outperforms existing techniques and has several potential applications in conversational settings. 1 into PostgreSQL...\n",
      "Inserting test sample 867  The unified model of active galactic nuclei (AGN) predicts silicate emission features at 10 and 18 microns in type 1 AGN, and such features have now been observed in objects ranging from distant QSOs to nearby LINERs. More surprising, however, is the detection of silicate emission in a few type 2 AGN.\n",
      "\n",
      "By combining Gemini and Spitzer mid-infrared imaging and spectroscopy of NGC 2110, the closest known Seyfert 2 galaxy with silicate emission features, we can constrain the location of the silicate emitting region to within 32 pc of the nucleus. This is the strongest constraint yet on the size of the silicate emitting region in a Seyfert galaxy of any type. While this result is consistent with a narrow line region origin for the emission, comparison with clumpy torus models demonstrates that emission from an edge-on torus can also explain the silicate emission features and 2-20 micron spectral energy distribution of this object. In many of the best-fitting models the torus has only a small number of clouds along the line of sight, and does not extend far above the equatorial plane. Extended silicate-emitting regions may well be present in AGN, but this work establishes that emission from the torus itself is also a viable option for the origin of silicate emission features in active galaxies of both type 1 and type 2. 0 into PostgreSQL...\n",
      "Inserting test sample 868  The Seyfert 2 galaxy, NGC 2110, is known for its distinctive silicate emission features, which are indicative of the presence of circumnuclear dust. In this paper, we present a detailed analysis of the origin of these silicate emission features. Using high-resolution infrared spectroscopy, we have identified several emission features that can be attributed to different components of the galaxy, such as the active galactic nucleus (AGN), the starburst region, and the dusty torus. We have found that the silicate emission features are primarily emitted from the AGN and originate from warm dust in its vicinity. In addition, we have identified several other emission and absorption features that are likely associated with molecular or atomic species in the galaxy. By comparing our observations with theoretical models and previous studies of similar galaxies, we have developed a detailed model for the physical processes that are responsible for the observed silicate emission features. Our findings suggest that the dusty torus plays a crucial role in shaping the properties of the galaxy, and may be responsible for regulating the activity of the AGN. Overall, our study sheds new light on the complex interplay between the various components of the galaxy, and provides important insights into the complex physics of galactic evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 869  This paper studies an unmanned aerial vehicle (UAV)-enabled multiple access channel (MAC), in which multiple ground users transmit individual messages to a mobile UAV in the sky. We consider a linear topology scenario, where these users locate in a straight line and the UAV flies at a fixed altitude above the line connecting them. Under this setup, we jointly optimize the one-dimensional (1D) UAV trajectory and wireless resource allocation to reveal the fundamental rate limits of the UAV-enabled MAC, under the users' individual maximum power constraints and the UAV's maximum flight speed constraints. First, we consider the capacity-achieving non-orthogonal multiple access (NOMA) transmission with successive interference cancellation (SIC) at the UAV receiver. In this case, we characterize the capacity region by maximizing the average sum-rate of users subject to rate profile constraints. To optimally solve this highly non-convex problem, we transform the original speed-constrained trajectory optimization problem into a speed-free problem that is optimally solvable via the Lagrange dual decomposition. It is rigorously proved that the optimal 1D trajectory solution follows the successive hover-and-fly (SHF) structure. Next, we consider two orthogonal multiple access (OMA) transmission schemes, i.e., frequency-division multiple access (FDMA) and time-division multiple access (TDMA). We maximize the achievable rate regions in the two cases by jointly optimizing the 1D trajectory design and wireless resource (frequency/time) allocation. It is shown that the optimal trajectory solutions still follow the SHF structure but with different hovering locations. Finally, numerical results show that the proposed optimal trajectory designs achieve considerable rate gains over other benchmark schemes, and the capacity region achieved by NOMA significantly outperforms the rate regions by FDMA and TDMA. 0 into PostgreSQL...\n",
      "Inserting test sample 870  The increasing popularity of unmanned aerial vehicles (UAVs) has led to the development of various applications for wireless communications. One such application is the use of UAV-enabled multiple access channels, which allow for multiple users to connect to a single UAV and communicate simultaneously. In this paper, we investigate the fundamental rate limits of such channels, taking into account the optimization of UAV trajectories. \n",
      "\n",
      "Our analysis considers a scenario where a single UAV is used as a base station for multiple ground users. We investigate the impact of UAV trajectory optimization on the achievable rates of the users, and derive theoretical upper bounds on the rates. Specifically, we consider the benefits of optimizing the UAVâ€™s trajectory in terms of distance from the users and the UAVâ€™s altitude. Our findings indicate that optimizing the UAVâ€™s trajectory can significantly improve the achievable rates, especially in scenarios with high user densities. \n",
      "\n",
      "Moreover, we investigate the impact of different antenna configurations on the achievable rates. Our results show that an antenna with a narrower beamwidth can provide higher rates for users in close proximity to the UAV, while a wider beamwidth is preferred for users further away from the UAV.\n",
      "\n",
      "Overall, our study provides insights into the fundamental limits of UAV-enabled multiple access channels with trajectory optimization, providing guidelines on how to optimize UAV trajectories and antenna configurations to maximize the achievable rates of the users. Our findings are relevant to the design and implementation of UAV-based communication systems in various applications, such as disaster response and remote sensing. 1 into PostgreSQL...\n",
      "Inserting test sample 871  Dirac neutrino masses require two distinct neutral Weyl spinors per generation, with a special arrangement of masses and interactions with charged leptons. Once this arrangement is perturbed, lepton number is no longer conserved and neutrinos become Majorana particles. If these lepton number violating perturbations are small compared to the Dirac mass terms, neutrinos are quasi-Dirac particles. Alternatively, this scenario can be characterized by the existence of pairs of neutrinos with almost degenerate masses, and a lepton mixing matrix which has 12 angles and 12 phases. In this work we discuss the phenomenology of quasi-Dirac neutrino oscillations and derive limits on the relevant parameter space from various experiments. In one parameter perturbations of the Dirac limit, very stringent bounds can be derived on the mass splittings between the almost degenerate pairs of neutrinos. However, we also demonstrate that with suitable changes to the lepton mixing matrix, limits on such mass splittings are much weaker, or even completely absent. Finally, we consider the possibility that the mass splittings are too small to be measured and discuss bounds on the new, non-standard lepton mixing angles from current experiments for this case. 0 into PostgreSQL...\n",
      "Inserting test sample 872  Quasi-Dirac neutrinos refer to the class of massive neutrinos that have small mass-squared differences and large mixing angles. Recent studies have shown that these quasi-Dirac neutrinos could potentially exhibit novel oscillation patterns that are distinct from those predicted by the standard three-neutrino paradigm. In this work, we investigate the evolution of neutrino flavor states in matter and their eventual detection at a neutrino observatory. Our analysis reveals the existence of quasi-Dirac neutrino oscillations and their potential to yield new insights into the fundamental nature of neutrinos. We show that the probability of detecting a given neutrino flavor state is modulated by a complex interplay of mass and mixing parameters, as well as neutrino production and propagation effects. Our findings have important implications for future neutrino experiments and may ultimately shed light on the origin of neutrino mass and the broader landscape of particle physics beyond the Standard Model. 1 into PostgreSQL...\n",
      "Inserting test sample 873  We have studied a sample of 296 faint (> 0.5 mJy) radio sources selected from an area of the Tenth Cambridge (10C) survey at 15.7 GHz in the Lockman Hole. By matching this catalogue to several lower frequency surveys (e.g. including a deep GMRT survey at 610 MHz, a WSRT survey at 1.4 GHz, NVSS, FIRST and WENSS) we have investigated the radio spectral properties of the sources in this sample; all but 30 of the 10C sources are matched to one or more of these surveys. We have found a significant increase in the proportion of flat spectrum sources at flux densities below approximately 1 mJy - the median spectral index between 15.7 GHz and 610 MHz changes from 0.75 for flux densities greater than 1.5 mJy to 0.08 for flux densities less than 0.8 mJy.\n",
      "\n",
      "This suggests that a population of faint, flat spectrum sources is emerging at flux densities below 1 mJy.\n",
      "\n",
      "The spectral index distribution of this sample of sources selected at 15.7 GHz is compared to those of two samples selected at 1.4 GHz from FIRST and NVSS. We find that there is a significant flat spectrum population present in the 10C sample which is missing from the samples selected at 1.4 GHz. The 10C sample is compared to a sample of sources selected from the SKADS Simulated Sky by Wilman et al. and we find that this simulation fails to reproduce the observed spectral index distribution and significantly underpredicts the number of sources in the faintest flux density bin. It is likely that the observed faint, flat spectrum sources are a result of the cores of FRI sources becoming dominant at high frequencies. These results highlight the importance of studying this faint, high frequency population. 0 into PostgreSQL...\n",
      "Inserting test sample 874  This paper presents a study on the faint source population at 15.7 GHz, analyzing its radio properties. We used the Australia Telescope Compact Array to observe a selected sample of faint sources, with a flux density limit of 50 mJy. Our results show that the majority of the sources in our sample have steep radio spectra, ranging from -0.8 to -1.6, which is consistent with previous studies at similar frequencies. We also found a correlation between the source size and radio spectral index, indicating that the larger sources have flatter spectra. Furthermore, we analyzed the polarization properties of the sources and found that 45% of the sample exhibited linearly polarized emission. We also detected weak circular polarization signals in some sources. Our analysis suggests that the polarized emission is likely produced by synchrotron radiation, which is commonly observed in astrophysical sources. Additionally, we identified several high-redshift sources in our sample, which are potential candidates for follow-up studies to investigate their properties in more detail. Overall, our findings provide important insights into the radio properties of the faint source population at 15.7 GHz, which is crucial for understanding the physical mechanisms that produce radio emission in the universe. Our future work will expand the sample size and investigate the correlation between radio properties and other astrophysical parameters. 1 into PostgreSQL...\n",
      "Inserting test sample 875  In this paper we present a model containing modifications to the Signal-passing Tile Assembly Model (STAM), a tile-based self-assembly model whose tiles are capable of activating and deactivating glues based on the binding of other glues. These modifications consist of an extension to 3D, the ability of tiles to form \"flexible\" bonds that allow bound tiles to rotate relative to each other, and allowing tiles of multiple shapes within the same system. We call this new model the STAM*, and we present a series of constructions within it that are capable of self-replicating behavior. Namely, the input seed assemblies to our STAM* systems can encode either \"genomes\" specifying the instructions for building a target shape, or can be copies of the target shape with instructions built in. A universal tile set exists for any target shape (at scale factor 2), and from a genome assembly creates infinite copies of the genome as well as the target shape. An input target structure, on the other hand, can be \"deconstructed\" by the universal tile set to form a genome encoding it, which will then replicate and also initiate the growth of copies of assemblies of the target shape. Since the lengths of the genomes for these constructions are proportional to the number of points in the target shape, we also present a replicator which utilizes hierarchical self-assembly to greatly reduce the size of the genomes required. The main goals of this work are to examine minimal requirements of self-assembling systems capable of self-replicating behavior, with the aim of better understanding self-replication in nature as well as understanding the complexity of mimicking it. 0 into PostgreSQL...\n",
      "Inserting test sample 876  In the field of self-assembly, one challenge is to design systems that can replicate themselves without external intervention. Recently, self-replication via tile self-assembly has emerged as an approach that shows great promise for realizing this goal. In this extended abstract, we review recent advancements in this field.\n",
      "\n",
      "Tile self-assembly is a process where tiles, typically made of DNA or other molecular components, spontaneously assemble into a larger structure due to complementary binding interactions. The beauty of this approach lies in the fact that the tiles are specifically designed to interact with each other in a way that constrains the structure to a specific shape. This makes it possible to design systems that can self-replicate with a high degree of accuracy.\n",
      "\n",
      "Several different strategies have been proposed for achieving self-replication via tile self-assembly. Some approaches rely on the use of \"seed\" structures that can act as a template for the assembly of new copies. Other strategies involve the use of \"growing\" structures that can recruit new tiles to the assembly as they grow.\n",
      "\n",
      "One key challenge in this field is to ensure that the assembled structures are stable and reproducible. For this reason, much research has focused on developing new design rules and computational models that can predict the behavior of tile self-assembly systems.\n",
      "\n",
      "Overall, self-replication via tile self-assembly is a promising approach for developing self-assembling systems with the ability to replicate themselves. While many challenges remain, recent developments have brought this goal within reach. 1 into PostgreSQL...\n",
      "Inserting test sample 877  The formation of supermassive black holes at high redshift still remains a puzzle to astronomers. Their growth becomes reasonable only when starting from a massive seed black hole with mass of the order of 10^2 - 10^5 M_SUN.\n",
      "\n",
      "Intermediate-mass black holes (IMBHs) are therefore an important field of research. Especially the possibility of finding them in the centers of globular clusters has recently drawn attention. The search for IMBHs in the centers of globular clusters could therefore shed light on the process of black-hole formation and cluster evolution. We are investigating six galactic globular clusters for the presence of an IMBH at their centers. Based on their kinematic and photometric properties, we selected the globular clusters NGC 1851, NGC 1904 (M79), NGC 5694, NGC 5824, NGC 6093 (M80) and NGC 6266 (M62). We use integral field spectroscopy in order to obtain the central velocity-dispersion profile of each cluster. We compute the cluster photometric center and the surface brightness profile using HST data. After combining these datasets we compare them to analytic Jeans models. We use varying M/L_V profiles for clusters with enough data points in order to reproduce their kinematic profiles in an optimal way. Finally, we vary the mass of the central black hole and test whether the cluster is better fitted with or without an IMBH. We present the statistical significance, including upper limits, of the black-hole mass for each cluster. NGC 1904 and NGC 6266 provide the highest significance for a black hole. Jeans models in combination with a M/L_V profile obtained from N-body simulations (in the case of NGC 6266) predict a central black hole of M_BH = (3 +- 1) x 10^3 M_SUN for NGC 1904 and M_BH = (2 +- 1) x 10^3 M_SUN for NGC 6266. Furthermore, we discuss the possible influence of dark remnants and mass segregation at the center of the cluster on the detection of an IMBH. 0 into PostgreSQL...\n",
      "Inserting test sample 878  This research paper investigates the presence of intermediate-mass black holes (IMBHs) in six Galactic globular clusters using integral-field spectroscopy. IMBHs, with masses ranging from hundreds to thousands of times that of the sun, are believed to be the missing link between stellar-mass black holes and supermassive black holes. Globular clusters serve as ideal environments to study the existence of IMBHs as the high stellar density promotes frequent interactions between stars, which can lead to the formation of such black holes.\n",
      "\n",
      "The research team conducted observations using the Multi-Unit Spectroscopic Explorer (MUSE) at the European Southern Observatory's Very Large Telescope (VLT) in Chile. By analyzing the spectral signatures of stars within each cluster, the team was able to detect any potential presence of an IMBH. \n",
      "\n",
      "The results of the study revealed that none of the six globular clusters contained any evidence of an IMBH. The upper limits on the masses of any possible IMBHs were found to range from 540 to 1,200 times the mass of the sun, depending on the cluster. These limits are consistent with the absence of an IMBH and provide valuable constraints on the formation and evolution of these enigmatic objects.\n",
      "\n",
      "The study also sheds light on the role of globular clusters in the formation of black holes. While they are known to be important environments for the production of stellar-mass black holes, the absence of IMBHs in these clusters suggests a different formation pathway for these intermediate-mass objects. It is possible that they form through the direct collapse of massive gas clouds, or through the merging of smaller black holes over time.\n",
      "\n",
      "In conclusion, this research provides important new insights into the presence (or lack thereof) of intermediate-mass black holes in globular clusters. By placing upper limits on their masses, the study contributes to our understanding of the formation and evolution of these mysterious objects, which are key players in the dynamics of galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 879  We present a three-dimensional map of interstellar dust reddening, covering three-quarters of the sky out to a distance of several kiloparsecs, based on Pan-STARRS 1 and 2MASS photometry. The map reveals a wealth of detailed structure, from filaments to large cloud complexes. The map has a hybrid angular resolution, with most of the map at an angular resolution of 3.4' to 13.7', and a maximum distance resolution of ~25%. The three-dimensional distribution of dust is determined in a fully probabilistic framework, yielding the uncertainty in the reddening distribution along each line of sight, as well as stellar distances, reddenings and classifications for 800 million stars detected by Pan-STARRS 1. We demonstrate the consistency of our reddening estimates with those of two-dimensional emission-based maps of dust reddening.\n",
      "\n",
      "In particular, we find agreement with the Planck 353 GHz optical depth-based reddening map to within 0.05 mag in E(B-V) to a depth of 0.5 mag, and explore systematics at reddenings less than E(B-V) ~ 0.08 mag. We validate our per-star reddening estimates by comparison with reddening estimates for stars with both SDSS photometry and SEGUE spectral classifications, finding per-star agreement to within 0.1 mag out to a stellar E(B-V) of 1 mag. We compare our map to two existing three-dimensional dust maps, by Marshall et al. (2006) and Lallement et al. (2013), demonstrating our finer angular resolution, and better distance resolution compared to the former within ~3 kpc. The map can be queried or downloaded at http://argonaut.skymaps.info. We expect the three-dimensional reddening map presented here to find a wide range of uses, among them correcting for reddening and extinction for objects embedded in the plane of the Galaxy, studies of Galactic structure, calibration of future emission-based dust maps and determining distances to objects of known reddening. 0 into PostgreSQL...\n",
      "Inserting test sample 880  The Milky Way galaxy is a vast and complex system that has intrigued astronomers for centuries. One of the most perplexing challenges in studying our galaxy is the presence of interstellar dust, which obscures our view of the stars beyond. In order to address this puzzle, a team of astrophysicists set out to create a three-dimensional map of Milky Way dust. \n",
      "\n",
      "To achieve this map, the team utilized data from the Gaia satellite and the Sloan Digital Sky Survey to study the light that passes through interstellar dust. By analyzing the patterns of light that interact with the dust, they were able to reconstruct the distribution and properties of the dust throughout the galaxy. This three-dimensional map provides a detailed view of the Milky Way's dust, revealing its composition, density, and location in relation to the stars. \n",
      "\n",
      "In addition to shedding light on the nature of interstellar dust itself, this study has important implications for understanding other astronomical phenomena. For example, dust plays a crucial role in the formation and evolution of stars, and this map will provide valuable insights into these processes. The map will also facilitate the study of other galaxies, which can be obscured by their own interstellar dust.\n",
      "\n",
      "This research project was a collaborative effort between international teams of astrophysicists and computer scientists, who used advanced analytical techniques, including artificial intelligence and machine learning, to process and interpret the data. While the creation of this three-dimensional map required the integration of diverse datasets and sophisticated modeling, it represents a major breakthrough in our understanding of the composition and distribution of dust in the Milky Way. \n",
      "\n",
      "In conclusion, the creation of a three-dimensional map of Milky-Way dust represents a significant milestone in the study of our galaxy. It provides a detailed view of the interstellar dust obscuring our vision, and holds promise for illuminating the processes of star formation and evolution, as well as aiding the study of other galaxies. This project demonstrates how interdisciplinary collaboration and advanced analytical techniques can reveal new insights into complex systems, and sets the stage for further discoveries in the future. 1 into PostgreSQL...\n",
      "Inserting test sample 881  Currently, there is an increase in the number of Peruvian families living in apartments instead of houses for the lots of advantage; However, in some cases there are troubles such as robberies of goods that are usually left at the parking lots or the entrance of strangers that use the tenants parking lots (this last trouble sometimes is related to kidnappings or robberies in building apartments). Due to these problems, the use of a self-driving mini-car is proposed to implement a monitoring system of license plates in an underground garage inside a building using a deep learning model with the aim of recording the vehicles and identifying their owners if they were tenants or not. In addition, the small robot has its own location system using beacons that allow us to identify the position of the parking lot corresponding to each tenant of the building while the mini-car is on its way. Finally, one of the objectives of this work is to build a low-cost mini-robot that would replace expensive cameras or work together in order to keep safe the goods of tenants. 0 into PostgreSQL...\n",
      "Inserting test sample 882  This research paper proposes a car monitoring system in apartment garages using a small autonomous car equipped with deep learning technology. The proposed system aims to address the problems of parking space management and vehicle security in crowded areas, particularly in apartment garages. The small autonomous car is capable of collecting real-time data such as vehicle locations and movements, which can be processed and analyzed using deep learning algorithms. The system can alert the garage management in case of any malfunctions or issues pertaining to vehicle locations or security. The proposed system is highly efficient and cost-effective in terms of infrastructure and maintenance. The deep learning algorithms employed in the system can learn from the data inputs and improve the accuracy and reliability of the system. The system has been tested and validated through simulations and experiments, proving its effectiveness and reliability. The proposed technology can be extended to other areas, such as public parking lots and commercial garages, providing an innovative solution to parking space management and vehicle security. 1 into PostgreSQL...\n",
      "Inserting test sample 883  The well-known theorem of Dybvig, Ingersoll and Ross shows that the long zero-coupon rate can never fall. This result, which, although undoubtedly correct, has been regarded by many as surprising, stems from the implicit assumption that the long-term discount function has an exponential tail. We revisit the problem in the setting of modern interest rate theory, and show that if the long \"simple\" interest rate (or Libor rate) is finite, then this rate (unlike the zero-coupon rate) acts viably as a state variable, the value of which can fluctuate randomly in line with other economic indicators. New interest rate models are constructed, under this hypothesis and certain generalizations thereof, that illustrate explicitly the good asymptotic behaviour of the resulting discount bond systems. The conditions necessary for the existence of such \"hyperbolic\" and \"generalized hyperbolic\" long rates are those of so-called social discounting, which allow for long-term cash flows to be treated as broadly \"just as important\" as those of the short or medium term.\n",
      "\n",
      "As a consequence, we are able to provide a consistent arbitrage-free valuation framework for the cost-benefit analysis and risk management of long-term social projects, such as those associated with sustainable energy, resource conservation, and climate change. 0 into PostgreSQL...\n",
      "Inserting test sample 884  Social discounting and the long rate of interest are two concepts in finance that are of great interest to scholars and policymakers. Social discounting is the practice of valuing future events or outcomes less than present ones, and is often used in decision-making regarding public policies. The long rate of interest, on the other hand, refers to the interest rate on long-term bonds and is an important measure of a country's economic growth prospects.\n",
      "\n",
      "This paper explores the relationship between social discounting and the long rate of interest by using a theoretical model and empirical analysis. We show that social discounting can have important implications for the long rate of interest, as it affects how people perceive and value future returns on investment. Our empirical analysis further supports the theoretical findings, suggesting that countries with higher social discount rates tend to have lower long-term interest rates.\n",
      "\n",
      "These findings have important implications for policymakers, as they highlight the need to consider social preferences and values when making decisions about long-term investments. Moreover, they suggest that policies aimed at promoting social welfare can have important positive effects on long-term economic growth. Overall, this paper contributes to our understanding of the interplay between social preferences and financial markets, and provides insights into how policymakers can better manage long-term investments. 1 into PostgreSQL...\n",
      "Inserting test sample 885  Traffic assignment methods are some of the key approaches used to model flow patterns that arise in transportation networks. Since static traffic assignment does not have a notion of time, it is not designed to represent temporal dynamics that arise as vehicles flow through the network and demand varies through the day. Dynamic traffic assignment methods attempt to resolve these issues, but require significant computational resources if modeling urban-scale regions (on the order of millions of links and vehicles) and often take days of compute time to complete. The focus of this work is two-fold: 1) to introduce a new traffic assignment approach - a quasi-dynamic traffic assignment (QDTA) model and 2) to describe how we parallelized the QDTA algorithms to leverage High-Performance Computing (HPC) and scale to large metropolitan areas while dramatically reducing compute time. We examine and compare different scenarios, including a baseline static traffic assignment (STA) and a quasi-dynamic scenario inspired by the user-equilibrium (UET). Results are presented for the San Francisco Bay Area which accounts for 19M trips/day and an urban road network of 1M links. We utilize an iterative gradient descent method, where the step size is selected using a Quasi-Newton method with parallelized cost function evaluations and compare it to using pre-defined step sizes (MSA).\n",
      "\n",
      "Using the parallelized line search provides a 16 percent reduction in total execution time due to a reduction in the number of gradient descent iterations required for convergence. The full day QDTA comprising 96 optimization steps over 15 minute intervals runs in about 4 minutes on 1,024 cores of the NERSC Cori computer, which represents a speedup of over 36x versus serial execution.\n",
      "\n",
      "To our knowledge, this compute time is significantly lower than other traffic assignment solutions for a problem of this scale. 0 into PostgreSQL...\n",
      "Inserting test sample 886  With the ever-increasing number of vehicles on the road, traffic management has become a critical concern. The development and implementation of intelligent transportation systems (ITS) have been instrumental in traffic management efforts. One such approach is the Quasi-Dynamic Traffic Assignment (QDTA), a method that simulates traffic operations in near real-time. However, the effectiveness of QDTA depends on the computational power of the system used, and implementing it effectively can be challenging.\n",
      "\n",
      "The present study explores the potential of High-Performance Computing (HPC) in implementing QDTA to support traffic management. We used a parallel computing architecture to optimize QDTA performance by splitting the traffic network into smaller subnetworks that can be simulated concurrently. The approach allowed us to overcome the challenges of processing big traffic data and enabled us to produce results in near real-time.\n",
      "\n",
      "Through simulation experiments that included typical traffic volumes and scenarios, we compared our approach's performance to that of traditional QDTA methods. The results demonstrated that our method runs over one hundred times faster than the traditional QDTA, generating equally accurate results. Further, the HPC approach allows us to scale up the simulations to cover larger geographic areas without affecting the model's accuracy.\n",
      "\n",
      "Our proposed method has significant implications for the transportation sector, where it can facilitate a near-real-time traffic management system. The approach's speed and accuracy make it possible to provide real-time traffic updates to road users, enabling them to make informed travel decisions. Additionally, traffic engineers can use our method to understand the impacts of changes in traffic management policies, such as road pricing and congestion charges, on traffic flow. In conclusion, our study shows that HPC is a promising method for implementing QDTA with potential significant applications in the transportation sector. 1 into PostgreSQL...\n",
      "Inserting test sample 887  Sound scattering by a finite width beam on a single rigid body rotation vortex flow is detected by a linear array of transducers (both smaller than a flow cell), and analyzed using a revised scattering theory. Both the phase and amplitude of the scattered signal are obtained on 64 elements of the detector array and used for the analysis of velocity and vorticity fields. Due to averaging on many pulses the signal-to-noise ratio of the phases difference in the scattered sound signal can be amplified drastically, and the resolution of the method in the detection of circulation, vortex radius, vorticity, and vortex location becomes comparable with that obtained earlier by time-reversal mirror (TRM) method (P. Roux, J. de Rosny, M. Tanter, and M. Fink, {\\sl Phys.\n",
      "\n",
      "Rev. Lett.} {\\bf 79}, 3170 (1997)). The revised scattering theory includes two crucial steps, which allow overcoming limitations of the existing theories.\n",
      "\n",
      "First, the Huygens construction of a far field scattering signal is carried out from a signal obtained at any intermediate plane. Second, a beam function that describes a finite width beam is introduced, which allows using a theory developed for an infinite width beam for the relation between a scattering amplitude and the vorticity structure function. Structure functions of the velocity and vorticity fields deduced from the sound scattering signal are compared with those obtained from simultaneous particle image velocimetry (PIV) measurements. Good quantitative agreement is found. 0 into PostgreSQL...\n",
      "Inserting test sample 888  This research paper discusses experimental investigations on flow-induced ultrasound scattering that occurs in fluid media. A comprehensive description of the experimental setup is presented where flow is generated by a pump in a specially designed container and ultrasound is generated by a piezoelectric transducer which is positioned at a certain distance from the container. The study is conducted by varying the flow rate, angle of incidence, and frequency of ultrasound waves. The scattered ultrasound signals are recorded by a hydrophone and analyzed to reveal the scattering patterns and the ultrasound attenuation coefficient. The results reveal that the scattering patterns depend significantly on the flow rate, angle of incidence, and frequency of the transmitted ultrasound. Analysis of the attenuation coefficient demonstrates that the amount of ultrasound energy lost due to scattering is higher at higher flow rates and larger angles of incidence. This study shows that flow-induced scattering is an essential factor that can affect the performance and accuracy of ultrasound imaging systems that operate in fluid media. These findings can aid in the development of more accurate methods for non-destructive testing and imaging applications where fluids are commonly used. 1 into PostgreSQL...\n",
      "Inserting test sample 889  The accuracy of least squares calibration using option premiums and particle filtering of price data to find model parameters is determined. Derivative models using exponential L\\'evy processes are calibrated using regularized weighted least squares with respect to the minimal entropy martingale measure.\n",
      "\n",
      "Sequential importance resampling is used for the Bayesian inference problem of time series parameter estimation with proposal distribution determined using extended Kalman filter. The algorithms converge to their respective global optima using a highly parallelizable statistical optimization approach using a grid of initial positions. Each of these methods should produce the same parameters. We investigate this assertion. 0 into PostgreSQL...\n",
      "Inserting test sample 890  This paper examines the calibration and filtering strategies of exponential L\\'evy option pricing models. The study compares the performance of different filtering techniques, including the Kalman filter, particle filter, and unscented Kalman filter, in estimating the model parameters. We show that these models provide an accurate description of the market, and that the filtering techniques can improve their predictive power. In particular, the particle filter produces more accurate estimates of the model parameters than other filters, making it a promising tool for option pricing in financial markets. These findings have implications for risk management and financial decision-making. 1 into PostgreSQL...\n",
      "Inserting test sample 891  We introduce a two-parameter deformation of the classical Bosonic, Fermionic, and Boltzmann Fock spaces that is a refinement of the $q$-Fock space of [BS91].\n",
      "\n",
      "Starting with a real, separable Hilbert space $H$, we construct the $(q,t)$-Fock space and the corresponding creation and annihilation operators, $\\{a_{q,t}(h)^\\ast\\}_{h\\in H}$ and $\\{a_{q,t}(h)\\}_{h\\in H}$, satifying the $(q,t)$-commutation relation $a_{q,t}(f)a_{q,t}(g)^\\ast-q \\,a_{q,t}(g)^\\ast a_{q,t}(f)= <f,g>_{_H}\\, t^{N},$ for $h,g\\in H$, with $N$ denoting the number operator. Interpreting the bounded linear operators on the $(q,t)$-Fock space as non-commutative random variables, the analogue of the Gaussian random variable is given by the deformed field operator $s_{q,t}(h):=a_{q,t}(h)+a_{q,t}(h)^\\ast$, for $h\\in H$. The resulting refinement is particularly natural, as the moments of $s_{q,t}(h)$ are encoded by the joint statistics of crossings \\emph{and nestings} in pair partitions.\n",
      "\n",
      "Furthermore, the orthogonal polynomial sequence associated with the normalized $(q,t)$-Gaussian $s_{q,t}$ is that of the $(q,t)$-Hermite orthogonal polynomials, a deformation of the $q$-Hermite sequence that is given by the recurrence $zH_n(z;q,t)=H_{n+1}(z;q,t)+[n]_{q,t}H_{n-1}(z;q,t),$ with $H_0(z;q,t)=1$, $H_1(z;q,t)=z$, and $[n]_{q,t}=\\sum_{i=1}^n q^{i-1}t^{n-i}$.\n",
      "\n",
      "The $q=0<t$ specialization yields a new single-parameter deformation of the full Boltzmann Fock space of free probability. The probability measure associated with the corresponding deformed semicircular operator turns out to be encoded, in various forms, via the Rogers-Ramanujan continued fraction, the Rogers-Ramanujan identities, the $t$-Airy function, the $t$-Catalan numbers of Carlitz-Riordan, and the first-order statistics of the reduced Wigner process. 0 into PostgreSQL...\n",
      "Inserting test sample 892  The $(q,t)$-Gaussian process is a powerful mathematical tool that has been gaining popularity among researchers in various fields. This process is an extension of the standard Gaussian process, which is widely used in machine learning, statistics, and optimization. The $(q,t)$-Gaussian process is a probability distribution over functions that enjoys a wide range of desirable properties. \n",
      "\n",
      "One of the key benefits of the $(q,t)$-Gaussian process is its ability to model complex and non-stationary data. In contrast to the standard Gaussian process, which assumes a stationary covariance function, the $(q,t)$-Gaussian process can capture non-stationary behavior by allowing the covariance function to vary across input space. This makes it particularly useful for applications such as time series analysis, where the covariance structure may change over time.\n",
      "\n",
      "The $(q,t)$-Gaussian process is also closely related to quantum physics, where it arises in the context of the quantum harmonic oscillator. The process is intimately connected to a broad class of special functions known as $q$-special functions, which have been extensively studied in the mathematical physics literature. This has led to a rich connection between the $(q,t)$-Gaussian process and a wide range of different areas of mathematics and physics.\n",
      "\n",
      "Despite the vast potential of the $(q,t)$-Gaussian process, its usage has been hindered by the lack of efficient inference algorithms. However, recent advances in computational methods and machine learning have begun to make this process more tractable, and we anticipate that it will become an increasingly important tool in the future.\n",
      "\n",
      "In conclusion, the $(q,t)$-Gaussian process is an exciting and powerful tool with a diverse range of applications across mathematics, physics, and machine learning. Its ability to model complex and non-stationary data, as well as its connection to quantum physics and $q$-special functions, make it an area of active research with enormous potential for future scientific discovery. 1 into PostgreSQL...\n",
      "Inserting test sample 893  One simplified black hole model constructed from a semiclassical analysis of loop quantum gravity (LQG) is called self-dual black hole. This black hole solution depends on a free dimensionless parameter P known as the polymeric parameter and also on the $a_{0}$ area related to the minimum area gap of LQG.\n",
      "\n",
      "In the limit of P and $a_{0}$ going to zero, the usual Schwarzschild-solution is recovered. Here we investigate the quasinormal modes (QNMs) of massless scalar perturbations in the self-dual black hole background. We compute the QN frequencies using the sixth order WKB approximation method and compare them with numerical solutions of the Regge-Wheeler equation. Our results show that as the parameter P grows, the real part of the QN frequencies suffers an initial increase and then starts to decrease while the magnitude of the imaginary one decreases for fixed area gap $a_{0}$. This particular feature means that the damping of scalar perturbations in the self-dual black hole spacetimes are slower, and their oscillations are faster or slower according to the value of P. 0 into PostgreSQL...\n",
      "Inserting test sample 894  In this paper, we investigate the quasinormal frequencies of self-dual black holes. Quasinormal frequencies, which are the characteristic oscillations of black holes, are important for understanding their underlying properties. Self-dual black holes, also known as extreme Kerr black holes, have particularly interesting properties such as the absence of classical hair and maximal entropy. We calculate the quasinormal frequencies of these black holes using the third-order WKB approximation, and compare them with the quasinormal frequencies of non-self-dual black holes. Our results show that the quasinormal frequencies of self-dual black holes are more densely spaced than those of non-self-dual ones, and the damping is slower in the former case. These findings shed light on the underlying physics of self-dual black holes and may have implications for understanding the fundamental nature of black holes. Overall, our study contributes to the ongoing efforts to unravel the mysteries of black holes and the universe at large. 1 into PostgreSQL...\n",
      "Inserting test sample 895  We report the evolution of magnetic field and its energy in NOAA active region 11158 over 5 days based on a vector magnetogram series from the Helioseismic and Magnetic Imager (HMI) on board the Solar Dynamic Observatory (SDO). Fast flux emergence and strong shearing motion led to a quadrupolar sunspot complex that produced several major eruptions, including the first X-class flare of Solar Cycle 24. Extrapolated non-linear force-free coronal fields show substantial electric current and free energy increase during early flux emergence near a low-lying sigmoidal filament with sheared kilogauss field in the filament channel. The computed magnetic free energy reaches a maximum of ~2.6e32 erg, about 50% of which is stored below 6 Mm. It decreases by ~0.3e32 erg within 1 hour of the X-class flare, which is likely an underestimation of the actual energy loss. During the flare, the photospheric field changed rapidly: horizontal field was enhanced by 28% in the core region, becoming more inclined and more parallel to the polarity inversion line. Such change is consistent with the conjectured coronal field \"implosion\", and is supported by the coronal loop retraction observed by the Atmospheric Imaging Assembly (AIA).\n",
      "\n",
      "The extrapolated field becomes more \"compact\" after the flare, with shorter loops in the core region, probably because of reconnection. The coronal field becomes slightly more sheared in the lowest layer, relaxes faster with height, and is overall less energetic. 0 into PostgreSQL...\n",
      "Inserting test sample 896  The evolution of the magnetic field and energy within an active region known for its eruptive activity has been studied based on observational data from the Helioseismic and Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO). By analyzing the data obtained over several days, we have identified changes in the configuration of the magnetic fields that correspond to the emergence, cancellation, and reconnection of magnetic flux. The magnetic topology of this active region is complex, characterized by a strong, non-potential field with a high degree of free magnetic energy. We observe a gradual increase in magnetic energy over time, leading to the formation of a magnetic flux rope that eventually erupts.\n",
      "\n",
      "Our analysis shows that the emergence of twisted flux ropes and the development of a strong confinement potential can lead to magnetic flux rope eruptions, flares, and coronal mass ejections (CMEs). The eruption of the flux rope is associated with a strong increase in magnetic energy, while the subsequent reconnection of the field releases a significant amount of energy, driving the development of a flare and a CME. Our findings provide insight into the physical properties and evolution of active regions and their potential for eruptive activity. By analyzing the magnetic field and energy, we can begin to understand the underlying mechanisms that govern the dynamics of the Sun's atmosphere and the initiation of space weather events. 1 into PostgreSQL...\n",
      "Inserting test sample 897  Robert Griffiths has recently addressed, within the framework of a 'consistent quantum theory' that he has developed, the issue of whether, as is often claimed, quantum mechanics entails a need for faster-than-light transfers of information over long distances. He argues that the putative proofs of this property that involve hidden variables include in their premises some essentially classical-physics-type assumptions that are fundamentally incompatible with the precepts of quantum physics. One cannot logically prove properties of a system by establishing, instead, properties of a system modified by adding properties alien to the original system. Hence Griffiths' rejection of hidden-variable-based proofs is logically warranted. Griffiths mentions the existence of a certain alternative proof that does not involve hidden variables, and that uses only macroscopically described observable properties. He notes that he had examined in his book proofs of this general kind, and concluded that they provide no evidence for nonlocal influences. But he did not examine the particular proof that he cites. An examination of that particular proof by the method specified by his 'consistent quantum theory' shows that the cited proof is valid within that restrictive version of quantum theory. An added section responds to Griffiths' reply, which cites general possibilities of ambiguities that make what is to be proved ill-defined, and hence render the pertinent 'consistent framework' ill defined. But the vagaries that he cites do not upset the proof in question, which, both by its physical formulation and by explicit identification, specify the framework to be used.\n",
      "\n",
      "Griffiths confirms the validity of the proof insofar as that framework is used.\n",
      "\n",
      "The section also shows, in response to Griffiths' challenge, why a putative proof of locality that he has described is flawed. 0 into PostgreSQL...\n",
      "Inserting test sample 898  Quantum Locality is a phenomenon that has intrigued scientists for decades. At the heart of the matter is the question of whether quantum mechanics violates the principle of locality. Quantum mechanics is a fundamental theory that describes the behavior of matter and energy at the microscopic scale. Locality, on the other hand, is the idea that the properties of an object are determined by its local environment. This principle applies to both classical and quantum systems, and it is a cornerstone of our understanding of the physical world.\n",
      "\n",
      "The question of whether quantum mechanics violates locality has been the subject of intense theoretical and experimental investigation. On the theoretical side, there have been many proposals for reconciling quantum mechanics with the principle of locality. One such proposal is the idea of hidden variables, which suggests that there are underlying variables that determine the properties of quantum systems, but which are not directly observable.\n",
      "\n",
      "On the experimental side, there have been many tests of the principle of locality using a variety of physical systems. One particularly interesting approach is the use of entangled particles, which are particles that are correlated in such a way that measuring one particle can instantly determine the state of the other particle, regardless of the distance between them. These experiments have provided strong evidence that quantum mechanics violates locality, and have sparked much debate about the nature of the physical world.\n",
      "\n",
      "In conclusion, the question of whether quantum mechanics violates locality remains a subject of intense scrutiny and debate. While there are many proposals for reconciling quantum mechanics with the principle of locality, experimental evidence suggests that quantum mechanics does indeed violate locality. This has far-reaching implications for our understanding of the physical world, and opens up new avenues for exploring the mysteries of quantum mechanics. 1 into PostgreSQL...\n",
      "Inserting test sample 899  Predictability of flow is examined in a barotropic vorticity model that admits low frequency regime transitions between zonal and dipolar states. Such transitions in the model were first studied by Bouchet and Simonnet (2009) and are reminiscent of regime change phenomena in the weather and climate systems wherein extreme and abrupt qualitative changes occur, seemingly randomly, after long periods of apparent stability. Mechanisms underlying regime transitions in the model are not well understood yet. From the point of view of atmospheric and oceanic dynamics, a novel aspect of the model is the lack of any source of background gradient of potential-vorticity such as topography or planetary gradient of rotation rate (e.g., as in Charney & DeVore '79).\n",
      "\n",
      "We consider perturbations that are embedded onto the system's chaotic attractor under the full nonlinear dynamics as bred vectors---nonlinear generalizations of the leading (backward) Lyapunov vector. We find that ensemble predictions that use bred vector perturbations are more robust in terms of error-spread relationship than those that use Lyapunov vector perturbations. In particular, when bred vector perturbations are used in conjunction with a simple data assimilation scheme (nudging to truth), we find that at least some of the evolved perturbations align to identify low-dimensional subspaces associated with regions of large forecast error in the control (unperturbed, data-assimilating) run; this happens less often in ensemble predictions that use Lyapunov vector perturbations. Nevertheless, in the inertial regime we consider, we find that (a) the system is more predictable when it is in the zonal regime, and that (b) the horizon of predictability is far too short compared to characteristic time scales associated with processes that lead to regime transitions, thus precluding the possibility of predicting such transitions. 0 into PostgreSQL...\n",
      "Inserting test sample 900  This paper investigates the low-frequency regime transitions and predictability of regimes in a barotropic model. The study focuses on understanding the nature of the regime transitions observed in the model, as well as identifying the key factors that influence the predictability of these transitions. The results of this research will provide insights into the behavior of barotropic models, as well as contribute to the development of more effective methods for predicting regime transitions in these models.\n",
      "\n",
      "To achieve this goal, the study employs a combination of analytical and numerical methods. First, the authors analyze the mathematical structure of the barotropic model to gain insight into the properties of the different regimes that it exhibits. They then use numerical simulations to explore the behavior of the model under different initial conditions and forcing scenarios. In particular, the authors focus on the importance of small-scale variability in the predictability of the regime transitions.\n",
      "\n",
      "The findings suggest that low-frequency regime transitions in the barotropic model are characterized by a complex interplay between large-scale and small-scale dynamics. Moreover, the predictability of these transitions is highly dependent on the initial conditions and the characteristics of the forcing. In particular, the authors find that the presence of small-scale variability can significantly enhance the predictability of regime transitions.\n",
      "\n",
      "Overall, the study provides a detailed analysis of the low-frequency regime transitions and predictability of regimes in a barotropic model. The results highlight the importance of understanding the underlying dynamics of these models, as well as the need for more sophisticated methods for predicting regime transitions in complex systems. 1 into PostgreSQL...\n",
      "Inserting test sample 901  We present deep near-infrared JHK imaging of four 10'x10' fields. The observations were carried out as part of the Multiwavelength Survey by Yale-Chile (MUSYC) with ISPI on the CTIO 4m telescope. The typical point source limiting depths are J~22.5, H~21.5, and K~21 (5sigma; Vega). The effective seeing in the final images is ~1.0\". We combine these data with MUSYC UBVRIz imaging to create K-selected catalogs that are unique for their uniform size, depth, filter coverage, and image quality. We investigate the rest-frame optical colors and photometric redshifts of galaxies that are selected using common color selection techniques, including distant red galaxies (DRGs), star-forming and passive BzKs, and the rest-frame UV-selected BM, BX, and Lyman break galaxies (LBGs). These techniques are effective at isolating large samples of high redshift galaxies, but none provide complete or uniform samples across the targeted redshift ranges. The DRG and BM/BX/LBG criteria identify populations of red and blue galaxies, respectively, as they were designed to do. The star-forming BzKs have a very wide redshift distribution, a wide range of colors, and may include galaxies with very low specific star formation rates. In comparison, the passive BzKs are fewer in number, have a different distribution of K magnitudes, and have a somewhat different redshift distribution. By combining these color selection criteria, it appears possible to define a reasonably complete sample of galaxies to our flux limit over specific redshift ranges. However, the redshift dependence of both the completeness and sampled range of rest-frame colors poses an ultimate limit to the usefulness of these techniques. 0 into PostgreSQL...\n",
      "Inserting test sample 902  The Multiwavelength Survey by Yale-Chile (MUSYC) is a ground-breaking effort aiming to characterize the properties of galaxies in the distant universe, based on deep near-infrared imaging observations. The MUSYC project utilized the Wide-Field Infrared Camera on the Gemini South telescope in Chile, covering a large survey area of five separate fields. The resulting dataset includes multi-wavelength images, catalogs of photometric sources, and spectroscopic redshifts. \n",
      "\n",
      "The MUSYC observations allowed for a comprehensive study of galaxy properties, particularly at high redshifts. The project focused on the selection of distant galaxies using a range of criteria, including the photometric redshift technique. This technique was instrumental in selecting a sample of over 2000 galaxies with redshifts ranging from z~1.5 to z~3.5. The spectroscopic follow-up of a subset of these sources confirmed the efficiency of the selection criteria, as well as provided crucial information on galaxy physical properties, including their star formation rates and metallicities. \n",
      "\n",
      "The MUSYC dataset represents a unique resource for the scientific community investigating galaxy formation and evolution. The high-quality photometric data and spectroscopic information are suitable for a variety of studies, including the characterization of star-forming and quiescent galaxies, the measurements of stellar mass functions and halo occupation distributions, and the investigation of the interplay between active galactic nuclei and their host galaxies. The MUSYC dataset also offers a great potential for synergy with other multi-wavelength surveys at different wavelengths, from radio to X-rays, enabling a comprehensive understanding of the physical processes that shaped the evolution of galaxies in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 903  Using HST imaging of CL1358+62 (z=0.33) structural parameters are derived for 53 cluster members. We fit integrated r^{1/4}-laws to the integrated S.B.\n",
      "\n",
      "profiles, and fit two-dimensional r^{1/4}-law model galaxies to the images directly. The results from the two methods agree very well, with an rms scatter of 13% in r_e. The half-light radii range from 1 to 20 kpc, with a median of 3 kpc. We compared r_e from the r^{1/4}-law fits to r_{1/2} derived using other profiles. In particular, we fit Sersic r^{1/n}-laws and superpositions of r^{1/4}-law bulges with exponential disks. The r_{1/2} derived from the best-fit Sersic profiles varied with respect to r_e from the r^{1/4}-law fits by 1% in the median, but with a standard deviation of more than 40%. For the bulge-plus-disk fits, the derived r_{1/2} were offset from r_e of the r^{1/4}-law fits by 10% in the mean, also with a standard deviation of more than 40%. By comparing the Sersic r_{1/2} with those derived from the bulge-plus-disk fitting, that scatter is also large, at 30%. We conclude that r_{1/2} is generally measured with a typical accuracy of 30-40%. The large uncertainties in r_{1/2} do not impact the fundamental plane analysis because the combination r<I>^{0.76}, which enters the fundamental plane, is extremely stable. The rms scatter in r<I>^{0.76} is less than 3%, regardless of the form of the profile fit to the galaxies. We find that the median bulge fraction of the sample is 84% and that the few E+A galaxies in this sample have disks which make up about 10-35% of their total light. These results are consistent with residuals from fitting 2D r^{1/4}-law models directly to the galaxies, in which disk-like structures are revealed in about half of the sample. Two of the three E+A galaxies show spiral arm structure. 0 into PostgreSQL...\n",
      "Inserting test sample 904  This research investigates the evolution of early-type galaxies in distant clusters, specifically examining 53 galaxies in the z=0.33 cluster CL1358+62 through surface photometry and structural parameter analysis. The study employs advanced imaging analysis and machine learning algorithms to investigate the morphological features and detailed structural properties of these galaxies. Using this data, the physical properties of these galaxies are analyzed, with particular focus on the distribution of surface brightness, ellipticity, and position angle profiles. \n",
      "\n",
      "The results of this study reveal significant insights into the evolution of early-type galaxies in distant clusters. Analysis of the structural parameters and morphological features of these galaxies reveals important differences between high and low-mass galaxies. Specifically, it is found that high-mass galaxies within these clusters are more prone to mergers and interactions, resulting in distinct changes to their morphological features. Furthermore, it is observed that the luminosity profiles of these galaxies exhibit a strong dependence on their position within the cluster.\n",
      "\n",
      "In addition, this study also highlights the potential of machine learning algorithms in effectively analyzing large datasets of distant galaxies. The use of these algorithms was crucial in identifying galaxies with similar morphological features and structural properties, enabling deeper analysis of the physical properties of these galaxies.\n",
      "\n",
      "Overall, these findings have important implications for our understanding of galaxy formation and evolution. By shedding light on the differences between early-type galaxies in distant clusters, this research advances our knowledge of how these galaxies form and evolve over time. Ultimately, this study paves the way for future research into the evolution of galaxies in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 905  In parameter estimation problems one computes a posterior distribution over uncertain parameters defined jointly by a prior distribution, a model, and noisy data. Markov Chain Monte Carlo (MCMC) is often used for the numerical solution of such problems. An alternative to MCMC is importance sampling, which can exhibit near perfect scaling with the number of cores on high performance computing systems because samples are drawn independently. However, finding a suitable proposal distribution is a challenging task. Several sampling algorithms have been proposed over the past years that take an iterative approach to constructing a proposal distribution. We investigate the applicability of such algorithms by applying them to two realistic and challenging test problems, one in subsurface flow, and one in combustion modeling. More specifically, we implement importance sampling algorithms that iterate over the mean and covariance matrix of Gaussian or multivariate t-proposal distributions. Our implementation leverages massively parallel computers, and we present strategies to initialize the iterations using \"coarse\" MCMC runs or Gaussian mixture models. 0 into PostgreSQL...\n",
      "Inserting test sample 906  This paper proposes a novel iterative importance sampling algorithm for parameter estimation in complex systems. The algorithm is designed to efficiently sample from the posterior distribution of model parameters by successively improving upon previously obtained samples. The proposed approach outperforms traditional Monte Carlo methods in terms of accuracy and computation time, particularly in scenarios where the likelihood function is computationally expensive to evaluate or when the posterior distribution is multimodal. We provide theoretical analysis and numerical experiments demonstrating the effectiveness of our proposed algorithm, using a variety of benchmark datasets and real-world applications. Furthermore, we show how our approach can be easily extended to include prior distributions and Bayesian model comparison. Overall, this research contributes to the development of more efficient and accurate methods for parameter estimation in complex systems, with potential applications in diverse fields such as physics, biology, and engineering. 1 into PostgreSQL...\n",
      "Inserting test sample 907  A theory for the dispersion of collective magnetic excitations in superconducting cuprates is presented with the aim to cover both high and low doping regimes. Besides of spin fluctuations describable in the random phase approximation (RPA) we allow for local spin rotations within a mode-coupling theory. At low temperatures and moderately large correlation lengths we obtain two branches of excitations which disperse up- and downwards exhibiting the hourglass behavior observed experimentally at intermediate dopings. At large and small dopings our theory essentially reduces to the RPA and spin wave theory, respectively. 0 into PostgreSQL...\n",
      "Inserting test sample 908  We present a theoretical framework for the hourglass dispersion of magnetic excitations in high-temperature cuprate superconductors (HTSs). The hourglass shape arises due to the interplay between antiferromagnetic correlations in the CuOâ‚‚ plane and spin excitations in the nearby chains or bilayers. Our approach considers the dynamic fluctuation of the charge density wave and assumes that it plays a crucial role in determining the magnetic excitation spectrum. We explain the emergence of hourglass-like features in the neutron scattering experiments on various HTSs and suggest new directions of research for a better understanding of the cuprate physics. 1 into PostgreSQL...\n",
      "Inserting test sample 909  When doing impact evaluation and making causal inferences, it is important to acknowledge the heterogeneity of the treatment effects for different domains (geographic, socio-demographic, or socio-economic). If the domain of interest is small with regards to its sample size (or even zero in some cases), then the evaluator has entered the small area estimation (SAE) dilemma.\n",
      "\n",
      "Based on the modification of the Inverse Propensity Weighting estimator and the traditional small area predictors, the paper proposes a new methodology to estimate area specific average treatment effects for unplanned domains. By means of these methods we can also provide a map of policy impacts, that can help to better target the treatment group(s). We develop analytical Mean Squared Error (MSE) estimators of the proposed predictors. An extensive simulation analysis, also based on real data, shows that the proposed techniques in most cases lead to more efficient estimators. 0 into PostgreSQL...\n",
      "Inserting test sample 910  Small area estimation (SAE) allows us to provide accurate and reliable estimates of parameters for small domains where direct estimation is not feasible due to sample size limitations. However, SAE techniques require the assumption of a causal relationship between auxiliary variables and the variable of interest. Causal inference in SAE has recently gained attention as it offers the opportunity to go beyond simple correlation analysis and to better inform policy decisions. In this paper, we review the literature on causal inference in SAE and discuss the various methods that have been proposed to address the challenges inherent in SAE, such as selection bias and treatment effect heterogeneity. We also provide real-world examples to illustrate the importance and practical implications of causal inference in SAE. Our findings are relevant for policymakers and researchers interested in obtaining accurate and reliable estimates for small areas. 1 into PostgreSQL...\n",
      "Inserting test sample 911  Gas behavior in systems at microscale has been receiving significant attention from researchers in the last two decades [1-4]. Today, there is an enhanced emphasis on developing new experimental techniques to capture the local temperature profiles in gases at rarefied conditions. The main underlying reason behind this focus is the interesting physics exhibited by gases at these rarefied conditions, especially in the transition regime. There is the onset of local thermodynamic disequilibrium, which manifests as velocity slip and temperature jump [1-4] at the wall. However, there is limited experimental evidence on understanding these aforementioned phenomena. With the advances in experimental facilities, it is today possible, at least in principle, to map the local temperature profiles in gases at rarefied conditions. Molecular tagging approach is one such technique which has shown the potential to map the temperature profile in low pressure conditions [5]. In molecular tagging approach, a very small percentage of tracer molecules are introduced into the gas of interest, referred as carrier gas. In gas flow studies, the typical tracers employed are acetone and biacetyl. These tracer molecules, assumed to be in equilibrium with the carrier gas, are excited with a source of energy at a specific wavelength, typically a laser. The excited molecules are unstable and tend to de-excite in a radiative and non-radiative manner, which is manifested as fluorescence and phosphorescence. Following the deformation with time of a tagged line permits to obtain the flow velocity. In addition, the dependence of the phosphorescence and fluorescence intensity to the gas temperature could also allow to use this technique for local temperature measurements. The objective of this study is to develop an experimental setup capable of simultaneously mapping the wall and fluid near-wall temperatures with the final goal to measure temperature jump at the wall when rarefied conditions are reached. The originality of this setup shown in Figure 1 is to couple surface temperature measurements using an infrared camera with Molecular Tagging Thermometry (MTT) for gas temperature measurements. The bottom wall of the channel will be made of Sapphire substrate of 650 $\\mu$m thickness coated with a thin film of Indium Tin Oxide (ITO). The average roughness of this ITO layer is about 3 nm. The top wall of the channel will be made of SU8 and bonded with the bottom wall with a layer of PDMS. The channel will be filled in with acetone vapor, 0 into PostgreSQL...\n",
      "Inserting test sample 912  This paper presents a proposed design for simultaneously measuring wall and near-wall temperatures in gas microflows. The ability to measure both temperatures is critical in understanding the fundamental behaviors of gas flows at microscales and in developing more efficient and accurate microscale devices.\n",
      "\n",
      "The proposed design involves the use of microscale thermocouples integrated onto thin-film membranes. The thin-film membranes are made of silicon nitride, which has excellent thermal and mechanical properties, and are bonded onto the walls of microchannels. The thermocouples are then deposited onto the membranes using microfabrication techniques.\n",
      "\n",
      "To validate the proposed design, simulations were carried out using COMSOL Multiphysics software. The results show that the proposed design is capable of measuring both wall and near-wall temperatures with high accuracy and resolution. The simulations also demonstrate the importance of having both temperature measurements, as the near-wall temperature can deviate significantly from the bulk temperature in gas microflows.\n",
      "\n",
      "The proposed design has several advantages compared to existing methods. First, it provides simultaneous measurements of both wall and near-wall temperatures, which is not possible with most existing techniques. Second, the thin-film membranes are highly sensitive and can detect small temperature changes, making the design suitable for a wide range of applications. Finally, the microfabrication techniques used to manufacture the design are well-established, making it easy to replicate and adapt to different microscale devices.\n",
      "\n",
      "In conclusion, this paper presents a proposed design for simultaneous measurement of wall and near-wall temperatures in gas microflows using microscale thermocouples integrated onto thin-film membranes. The proposed design has several advantages over existing methods and simulations show that it is capable of measuring both temperatures with high accuracy and resolution. The proposed design has potential applications in a wide range of fields, including microfluidics, heat transfer, and microscale sensing. 1 into PostgreSQL...\n",
      "Inserting test sample 913  The new spectroscopic classes, L and T, are defined by the role of dust clouds in their atmospheres, the former by their presence and the latter by their removal and near absence. Moreover, the M to L and L to T transitions are intimately tied to the condensation and character of silicate and iron grains, and the associated clouds play pivotal roles in the colors and spectra of such brown dwarfs. Spanning the effective temperature range from $\\sim$2200 K to $\\sim$600 K, these objects are being found in abundance and are a new arena in which condensation chemistry and the optical properties of grains is assuming astronomical importance. In this short paper, I summarize the role played by such refractories in determining the properties of these \"stars\" and the complexities of their theoretical treatment. 0 into PostgreSQL...\n",
      "Inserting test sample 914  Dust clouds have long been thought to play a role in the dynamics of brown dwarf atmospheres. These cloudy objects sit between the mass range of giant planets and small stars, with atmospheres that bridge the gap between these two classes of objects. In this paper, we aim to clarify the specific ways in which dust clouds affect the composition, spectral appearance, and thermal structure of brown dwarf atmospheres. We discuss the role of grain sizes, abundance, and vertical distribution on the observed spectral features and variability for the L and T spectral types. Finally, we demonstrate how models of dust cloud formation and evolution provide an avenue to understand the atmospheric properties of these enigmatic objects. 1 into PostgreSQL...\n",
      "Inserting test sample 915  Monolayer graphene epitaxially grown on SiC(0001) was etched by H-plasma and studied by scanning tunneling microscopy and spectroscopy. The etching created partly hexagonal nanopits of monatomic depth as well as elevated regions with a height of about 0.12 nm which are stable at $T$ = 78 K. The symmetric tunnel spectrum about the Femi energy and the absence of a $6\\times6$ corrugation on the elevated regions suggest that in these regions the carbon buffer layer is decoupled from the SiC substrate and quasi-free-standing bilayer graphene appears at originally monolayer graphene on the buffer layer. This is a result of passivation of the SiC substrate by intercalated hydrogen as in previous reports for graphene on SiC(0001) heat treated in atomic hydrogen. 0 into PostgreSQL...\n",
      "Inserting test sample 916  Scanning tunneling microscopy/spectroscopy (STM/S) observations are essential in studying graphene's properties on SiC(0001) etched by hydrogen-plasma. Hydrogen-plasma etching creates a surface free of any contaminants, and STM/S images have shown that the process of graphene etching is highly dependent on the substrate's quality. By analyzing the spectroscopic measurements, the researchers have observed the graphene layer's electronic structure, providing insight into graphene's behavior and interactions with the substrate. The STM/S technique has also been used to manipulate graphene's structure selectively, demonstrating that hydrogen-plasma-treated SiC(0001) provides a versatile platform for graphene-based devices. The results suggest that the use of hydrogen-plasma etching can lead to a better understanding of graphene's properties and its potential applications in the field of electronics. 1 into PostgreSQL...\n",
      "Inserting test sample 917  The correlation between geomagnetic activity and the sunspot number in the 11-year solar cycle exhibits long-term variations due to the varying time lag between the sunspot-related and non-sunspot related geomagnetic activity, and the varying relative amplitude of the respective geomagnetic activity peaks. As the sunspot-related and non-sunspot related geomagnetic activity are caused by different solar agents, related to the solar toroidal and poloidal fields, respectively, we use their variations to derive the parameters of the solar dynamo transforming the poloidal field into toroidal field and back. We find that in the last 12 cycles the solar surface meridional circulation varied between 5 and 20 m/s (averaged over latitude and over the sunspot cycle), the deep circulation varied between 2.5 and 5.5 m/s, and the diffusivity in the whole of the convection zone was ~10**8 m2/s. In the last 12 cycles solar dynamo has been operating in moderately diffusion dominated regime in the bulk of the convection zone. This means that a part of the poloidal field generated at the surface is advected by the meridional circulation all the way to the poles, down to the tachocline and equatorward to sunspot latitudes, while another part is diffused directly to the tachocline at midlatitudes, \"short-circuiting\" the meridional circulation. The sunspot maximum is the superposition of the two surges of toroidal field generated by these two parts of the poloidal field, which is the explanation of the double peaks and the Gnevyshev gap in sunspot maximum. Near the tachocline, dynamo has been operating in diffusion dominated regime in which diffusion is more important than advection, so with increasing speed of the deep circulation the time for diffusive decay of the poloidal field decreases, and more toroidal field is generated leading to a higher sunspot maximum. During the Maunder minimum the dynamo was operating in advection dominated regime near the tachocline, with the transition from diffusion dominated to advection dominated regime caused by a sharp drop in the surface meridional circulation which is in general the most important factor modulating the amplitude of the sunspot cycle. 0 into PostgreSQL...\n",
      "Inserting test sample 918  This paper explores the relationship between the solar dynamo and geomagnetic activity. The sun's magnetic field undergoes a regular cycle, with peaks and valleys corresponding to the number of sunspots, which is called the solar cycle. Solar storms occur when these magnetic fields become unstable and release energy in the form of solar flares and coronal mass ejections. These events can cause geomagnetic disturbances on Earth that can disrupt power grids, communication systems, and satellites. Understanding the connection between the solar dynamo and geomagnetic activity is important for predicting and mitigating the effects of these disruptions.\n",
      "\n",
      "There are several theories that attempt to explain the solar dynamo's behavior. One of the leading theories proposes that the dynamo is driven by convective motions in the sun's outer layers, while another suggests that it is the result of nonlinear interactions between magnetic fields. Researchers have been able to observe these interactions through the use of solar telescopes, and have made significant progress in modeling the solar cycle and predicting its behavior.\n",
      "\n",
      "The relationship between the solar dynamo and geomagnetic activity is complex and not fully understood. However, researchers have identified several key factors that contribute to the strength and frequency of solar storms. These include the tilt of the sun's magnetic field, the strength of the interplanetary magnetic field, and the length of time between solar cycles. By studying these factors and their role in geomagnetic disturbances, scientists are working to improve space weather forecasting and protect society from the harmful effects of solar storms.\n",
      "\n",
      "In summary, the solar dynamo and geomagnetic activity are intricately linked and understanding this relationship has important implications for space weather forecasting and mitigating the effects of solar storms. Through the use of solar telescopes and advanced modeling techniques, researchers have made significant progress in predicting the behavior of the solar cycle and identifying key factors that contribute to geomagnetic disturbances. However, further research is needed to fully understand the dynamics of the solar dynamo and its impact on Earth's magnetic field. 1 into PostgreSQL...\n",
      "Inserting test sample 919  We use high-resolution zoom-in cosmological simulations of galaxies of Romano-Diaz et al., post-processing them with a panchromatic three-dimensional radiation transfer code to obtain the galaxy UV luminosity function (LF) at z ~ 6-12. The galaxies are followed in a rare, heavily overdense region within a ~ 5-sigma density peak, which can host high-z quasars, and in an average density region, down to the stellar mass of M_star ~ 4* 10^7 Msun. We find that the overdense regions evolve at a substantially accelerated pace --- the most massive galaxy has grown to M_star ~ 8.4*10^10 Msun by z = 6.3, contains dust of M_dust~ 4.1*10^8 Msun, and is associated with a very high star formation rate, SFR ~ 745 Msun/yr.The attained SFR-M_star correlation results in the specific SFR slowly increasing with M_star. Most of the UV radiation in massive galaxies is absorbed by the dust, its escape fraction f_esc is low, increasing slowly with time. Galaxies in the average region have less dust, and agree with the observed UV LF. The LF of the overdense region is substantially higher, and contains much brighter galaxies. The massive galaxies are bright in the infrared (IR) due to the dust thermal emission, with L_IR~ 3.7*10^12 Lsun at z = 6.3, while L_IR < 10^11 Lsun for the low-mass galaxies. Therefore, ALMA can probe massive galaxies in the overdense region up to z ~ 10 with a reasonable integration time. The UV spectral properties of disky galaxies depend significantly upon the viewing angle.The stellar and dust masses of the most massive galaxy in the overdense region are comparable to those of the sub-millimetre galaxy (SMG) found by Riechers et al. at z = 6.3, while the modelled SFR and the sub-millimetre flux fall slightly below the observed one.\n",
      "\n",
      "Statistical significance of these similarities and differences will only become clear with the upcoming ALMA observations. 0 into PostgreSQL...\n",
      "Inserting test sample 920  This research investigates the observational properties of galaxies by studying simulated samples located in both average and overdense regions at high redshifts of z=6-12. The analysis of simulated data is done by implementing a state-of-the-art numerical simulation tool, which proved to be effective for this research.\n",
      "\n",
      "Through systematic comparison between galaxies in overdense and average regions, we discovered that the former tend to have a higher number of massive galaxies at any given redshift. The mass of the individual galaxies is calculated based on their star formation rates, sizes, and spatial distribution.\n",
      "\n",
      "Another interesting observation is that overdense regions exhibit more clustering as compared to average regions. This leads to the conclusion that overdense environments at high redshifts yield a superior means of identifying and studying galaxies that were formed there.\n",
      "\n",
      "Simulated galaxies in both overdense and average regions show signs of mergers and interactions with other galaxies. This finding provides a valuable opportunity to investigate and understand the mechanisms behind the formation of massive galaxies in these environments.\n",
      "\n",
      "Furthermore, the study of simulated star-forming galaxies revealed a trend of increasing metallicity with increasing stellar mass in average regions; however, this trend is not observed in overdense regions. The underlying reasons for this observation require further investigation.\n",
      "\n",
      "Overall, our results demonstrate that the study of galaxy populations on high-redshifted overdense regions yields valuable insights into the formation of massive galaxies. The application of numerical simulation tools in this type of research has proved successful, and the findings have exciting implications for our understanding of galaxy formation and the evolution of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 921  We present the analysis of a large sample of gamma-ray burst (GRB) X-ray light curves in the rest frame to characterise their intrinsic properties in the context of different theoretical scenarios. We determine the morphology, time scales, and energetics of 64 long GRBs observed by \\emph{Swift}/XRT \\emph{without} flaring activity. We furthermore provide a one-to-one comparison to the properties of GRBs \\emph{with} X-ray flares. We find that the steep decay morphology and its connection with X-ray flares favour a scenario in which a central engine origin. We show that this scenario can also account for the shallow decay phase, provided that the GRB progenitor star has a self-similar structure with a constant envelope-to-core mass ratio $\\sim 0.02-0.03$. However, difficulties arise for very long duration ($t_p\\gtrsim10^4$ s) shallow phases. Alternatively, a spinning-down magnetar whose emitted power refreshes the forward shock can quantitatively account for the shallow decay properties. In particular we demonstrate that this model can account for the plateau luminosity vs. end time anticorrelation. 0 into PostgreSQL...\n",
      "Inserting test sample 922  Gamma-ray bursts (GRBs) are among the most energetic events in the Universe, releasing immense amounts of energy over a short duration. The X-ray light curve of GRBs has proven to be a valuable tool in understanding the central engine responsible for these events. In this paper, we analyze X-ray light curves from a sample of GRBs detected by the Swift satellite. Our analysis reveals a diversity of light curve shapes and durations, suggesting a variety of central engines and ejecta properties. We find evidence for both long-lasting activity and rapid cessation of X-ray emission, possibly indicative of different physical mechanisms at work. We argue that these results support the idea of a common origin for GRBs, but with a range of progenitor systems and central engine configurations. Overall, our study provides new insights into the nature of GRBs and the role of the central engine in driving these explosive events. 1 into PostgreSQL...\n",
      "Inserting test sample 923  Stationary circularly symmetric solutions of General Relativity with negative cosmological constant coupled to the Maxwell field are analyzed in three spacetime dimensions. Taking into account that the fall-off of the fields is slower than the standard one for a localized distribution of matter, it is shown that, by virtue of a suitable choice of the electromagnetic Lagrange multiplier, the action attains a bona fide extremum provided the asymptotic form of the electromagnetic field fulfills a nontrivial integrability condition. As a consequence, the mass and the angular momentum become automatically finite, without the need of any regularization procedure, and they generically acquire contributions from the electromagnetic field.\n",
      "\n",
      "Therefore, unlike the higher-dimensional case, it is found that the precise value of the mass and the angular momentum explicitly depends on the choice of boundary conditions. It can also be seen that requiring compatibility of the boundary conditions with the Lorentz and scaling symmetries of the class of stationary solutions, singles out a very special set of \"holographic boundary conditions\" that is described by a single parameter. Remarkably, in stark contrast with the somewhat pathological behaviour found in the standard case, for the holographic boundary conditions (i) the energy spectrum of an electrically charged (rotating) black hole is nonnegative, and (ii) for a fixed value of the mass, the electric charge is bounded from above. 0 into PostgreSQL...\n",
      "Inserting test sample 924  In this research paper, we explore the conserved charges associated with black holes in Einstein-Maxwell theory on AdS$_{3}$. We begin by reviewing the relevant literature and discussing the limitations of previous studies. We then present new calculations of the conserved charges and compare them to previous results. Our analysis reveals that there are new contributions to the charges that were not previously considered. This leads us to propose a new way of understanding the conserved charges associated with black holes in AdS$_{3}$. \n",
      "\n",
      "We also investigate the thermodynamics of these black holes, including their entropy and temperature. We find that the entropy is proportional to the area of the event horizon and the temperature is inversely proportional to the black hole mass. This is consistent with previous work on the subject. However, our new calculations of the conserved charges provide a better understanding of the thermodynamic properties of these black holes.\n",
      "\n",
      "Finally, we discuss the implications of our results for the AdS/CFT correspondence, which relates black holes in AdS$_{3}$ to conformal field theories in 2 dimensions. We suggest that our new understanding of the conserved charges may provide a better framework for this correspondence and lead to new insights into the relationship between black holes and quantum field theories. 1 into PostgreSQL...\n",
      "Inserting test sample 925  We examine the reconstruction of galaxy cluster radial density profiles obtained from Chandra and XMM X-ray observations, using high quality data for a sample of twelve objects covering a range of morphologies and redshifts. By comparing the results obtained from the two observatories and by varying key aspects of the analysis procedure, we examine the impact of instrumental effects and of differences in the methodology used in the recovery of the density profiles. We find that the final density profile shape is particularly robust. We adapt the photon weighting vignetting correction method developed for XMM for use with Chandra data, and confirm that the resulting Chandra profiles are consistent with those corrected a posteriori for vignetting effects. Profiles obtained from direct deprojection and those derived using parametric models are consistent at the 1% level. At radii larger than $\\sim$6\", the agreement between Chandra and XMM is better than 1%, confirming an excellent understanding of the XMM PSF. We find no significant energy dependence. The impact of the well-known offset between Chandra and XMM gas temperature determinations on the density profiles is found to be negligible.\n",
      "\n",
      "However, we find an overall normalisation offset in density profiles of the order of $\\sim$2.5%, which is linked to absolute flux cross-calibration issues.\n",
      "\n",
      "As a final result, the weighted ratios of Chandra to XMM gas masses computed at R2500 and R500 are r=1.03$\\pm$0.01 and r=1.03$\\pm$0.03, respectively. Our study confirms that the radial density profiles are robustly recovered, and that any differences between Chandra and XMM can be constrained to the $\\sim$ 2.5% level, regardless of the exact data analysis details. These encouraging results open the way for the true combination of X-ray observations of galaxy clusters, fully leveraging the high resolution of Chandra and the high throughput of XMM. 0 into PostgreSQL...\n",
      "Inserting test sample 926  In this paper, we present a method for recovering gas density profiles of galaxy clusters using XMM-Newton and Chandra observations. The study of gas density profiles is crucial for understanding the physical properties and evolution of galaxy clusters, as they can provide information on the cluster's mass, temperature and composition.\n",
      "\n",
      "We compare the performance of different analysis techniques to extract density profiles from X-ray observations of galaxy clusters. Our method consists of fitting radial profiles of the X-ray surface brightness with models that take into account the cluster's gas density and temperature. We use two sets of XMM-Newton observations and one set of Chandra observations of galaxy clusters to test our technique.\n",
      "\n",
      "We find that our method provides reliable and consistent results across different data sets and instruments. The recovered gas density profiles show good agreement with theoretical predictions and with previous studies. We also investigate the impact of various systematic uncertainties on our results, such as the modeling assumptions, background contamination, and instrumental effects.\n",
      "\n",
      "Our results demonstrate the potential of XMM-Newton and Chandra observations for studying galaxy cluster gas density profiles and their physical properties. The recovered density profiles can be used to constrain the cluster's mass distribution and formation history, as well as to study the properties of the intra-cluster medium. Our method can be applied to future X-ray observations of galaxy clusters from upcoming missions such as Athena, eROSITA or SPHEREX. 1 into PostgreSQL...\n",
      "Inserting test sample 927  New ab-initio surface hopping simulations of the excited state dynamics of CS$_2$ including spin-orbit coupling are compared to new experimental measurements using a multiphoton ionisation probe in a photoelectron spectroscopy experiment. The calculations highlight the importance of the triplet states even in the very early time dynamics of the dissociation process and allow us to unravel the signatures in the experimental spectrum, linking the observed changes to both electronic and nuclear degrees of freedom within the molecule. 0 into PostgreSQL...\n",
      "Inserting test sample 928  In this study, the photodissociation dynamics of CS$_2$ were investigated using ab-initio surface hopping and multiphoton ionisation techniques. The results showed that electronic transitions to the dissociative states are responsible for the photodissociation, with the yield of fragmented molecules varying with the wavelength of the incident light. These findings contribute to a better understanding of the photodissociation process and provide insights for potential future applications in electronic processes and materials science. 1 into PostgreSQL...\n",
      "Inserting test sample 929  The physics of the coolest phases in the hot Intra-Cluster Medium (ICM) of clusters of galaxies is yet to be fully unveiled. X-ray cavities blown by the central Active Galactic Nucleus (AGN) contain enough energy to heat the surrounding gas and stop cooling, but locally blobs or filaments of gas appear to be able to cool to low temperatures of 10^4 K. In X-rays, however, gas with temperatures lower than 0.5 keV is not observed. Using a deep XMM-Newton observation of the cluster of galaxies Abell 2052, we derive 2D maps of the temperature, entropy, and iron abundance in the core region. About 130 kpc South-West of the central galaxy, we discover a discontinuity in the surface brightness of the hot gas which is consistent with a cold front. Interestingly, the iron abundance jumps from ~0.75 to ~0.5 across the front. In a smaller region to the North-West of the central galaxy we find a relatively high contribution of cool 0.5 keV gas, but no X-ray emitting gas is detected below that temperature. However, the region appears to be associated with much cooler H-alpha filaments in the optical waveband. The elliptical shape of the cold front in the SW of the cluster suggests that the front is caused by sloshing of the hot gas in the clusters gravitational potential. This effect is probably an important mechanism to transport metals from the core region to the outer parts of the cluster. The smooth temperature profile across the sharp jump in the metalicity indicates the presence of heat conduction and the lack of mixing across the discontinuity. The cool blob of gas NW of the central galaxy was probably pushed away from the core and squeezed by the adjacent bubble, where it can cool efficiently and relatively undisturbed by the AGN. Shock induced mixing between the two phases may cause the 0.5 keV gas to cool non-radiatively and explain our non-detection of gas below 0.5 keV. 0 into PostgreSQL...\n",
      "Inserting test sample 930  The galaxy cluster, Abell 2052, is known for its intricate structure and complex dynamics. In this research paper, we investigate the presence of cold fronts and multi-temperature structures in the core of the cluster.\n",
      "\n",
      "Utilizing data from the Chandra X-ray Observatory, we identify several regions of temperature discontinuities, or cold fronts, in the intracluster medium. These cold fronts are likely formed due to the interaction between the cluster gas and the subcluster that is currently merging with Abell 2052. Our analysis reveals that the cold fronts are associated with regions of enhanced turbulence and shock heating.\n",
      "\n",
      "In addition to the cold fronts, we also detect the presence of multi-temperature structures in the core of Abell 2052. These features are characterized by regions of gas with different temperatures occupying the same space. We find that these multi-temperature structures are most prominent in the subcluster, indicating that the ongoing merger is responsible for their formation.\n",
      "\n",
      "Our investigation of the cold fronts and multi-temperature structures in Abell 2052 provides new insights into the complex dynamics of galaxy clusters and the role of mergers in shaping their properties. The cold fronts and multi-temperature structures we detect are indicative of the complex interplay between the cluster gas, dark matter, and galaxies.\n",
      "\n",
      "Our findings have important implications for understanding the nature of intracluster gas and the physics of galaxy cluster mergers. They also demonstrate the crucial role that high-resolution X-ray observations can play in unraveling the complex dynamics of galaxy clusters and shedding light on the formation and evolution of large-scale structures in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 931  Chemical analyses of raw materials are often repeated in duplicate or triplicate. The assay values obtained are then combined using a predetermined formula to obtain an estimate of the true value of the material of interest.\n",
      "\n",
      "When duplicate observations are obtained, their average typically serves as an estimate of the true value. On the other hand, the \"best of three\" method involves taking three measurements and using the average of the two closest ones as estimate of the true value.\n",
      "\n",
      "In this paper, we consider another method which potentially involves three measurements. Initially two measurements are obtained and if their difference is sufficiently small, their average is taken as estimate of the true value.\n",
      "\n",
      "However, if the difference is too large then a third independent measurement is obtained. The estimator is then defined as the average between the third observation and the one among the first two which is closest to it.\n",
      "\n",
      "Our focus in the paper is the conditional distribution of the estimate in cases where the initial difference is too large. We find that the conditional distributions are markedly different under the assumption of a normal distribution and a Laplace distribution. 0 into PostgreSQL...\n",
      "Inserting test sample 932  In statistics, the conditional distribution of the mean of the two closest among a set of three observations is a topic of interest. In this paper, we investigate this distribution and provide theoretical results related to its characterization. Specifically, we derive closed-form expressions for the probability density function and cumulative distribution function of the conditional mean. We also explore its moments and derive explicit expressions for them. Furthermore, we investigate the asymptotic behavior of the distribution as the sample size grows and show that it converges to a normal distribution. Finally, we apply our results to a real-world dataset and show how our findings can be used in practice. Our work is significant as it sheds new light on an important statistical problem and provides theoretical and practical insights into it. Moreover, our results have implications for various areas of research where this distribution is used, such as finance, economics, and engineering. 1 into PostgreSQL...\n",
      "Inserting test sample 933  We introduce a new method for generating text, and in particular song lyrics, based on the speech-like acoustic qualities of a given audio file. We repurpose a vocal source separation algorithm and an acoustic model trained to recognize isolated speech, instead inputting instrumental music or environmental sounds.\n",
      "\n",
      "Feeding the \"mistakes\" of the vocal separator into the recognizer, we obtain a transcription of words \\emph{imagined} to be spoken in the input audio. We describe the key components of our approach, present initial analysis, and discuss the potential of the method for machine-in-the-loop collaboration in creative applications. 0 into PostgreSQL...\n",
      "Inserting test sample 934  In this paper, we propose a novel method to generate lyrics using speech recognition systems as a starting point. By crafting adversarial examples to \"break\" the recognizers, we extract phonetic representations which, when reassembled creatively, can form lyrics. We demonstrate the effectiveness of our method with state-of-the-art speech recognizers, and conduct a human evaluation to judge the quality of the lyrics generated. We also discuss the implications of our approach for creative writing and music generation. 1 into PostgreSQL...\n",
      "Inserting test sample 935  We present here identification and characterization of the young stellar population associated with an active star-forming site Sh2-242. We used our own new optical imaging and spectroscopic observational data, as well as several archival catalogs, e.g., Pan-STARRS 1, $Gaia$ DR2, IPHAS, WIRCam, 2MASS, and $Spitzer$. Slit spectroscopic results confirm the classification of the main ionizing source BD+26 980 as an early-type star of spectral type B0.5 V. The spectrophotometric distance of the star is estimated as 2.08 $\\pm$ 0.24 kpc, which confirms the source as a member of the cluster. An extinction map covering a large area (diameter $\\sim$ 50') is generated with $H$ and $K$ photometry toward the region. From the map, three distinct locations of peak extinction complexes ($A_{V}$ $\\simeq$ 7$-$17 mag) are identified for the very first time. Using the infrared color excess, a total of 33 Class I and 137 Class II young objects are classified within the region. The IPHAS photometry reveals classification of 36 H$\\alpha$ emitting sources, which might be class II objects. Among 36 H$\\alpha$ emitting sources, 5 are already identified using infrared excess emission. In total, 201 young objects are classified toward S242 from this study. The membership status of the young sources is further windowed with the inclusion of parallax from the $Gaia$ DR2 catalog. Using the optical and infrared color-magnitude diagrams, the young stellar objects are characterized with an average age of $\\sim$ 1 Myr and the masses in the range 0.1$-$3.0 $M_\\odot$. The census of the stellar content within the region is discussed using combined photometric and spectroscopic data. 0 into PostgreSQL...\n",
      "Inserting test sample 936  The Galactic H II region Sh2-242 is a young and massive star-forming region located in the Sagittarius arm of the Milky Way. In this study, we present a comprehensive census of the young stellar population in Sh2-242, based on our deep near-infrared observations obtained with the Hubble Space Telescope. We identified a total of 337 candidate young stellar objects (YSOs) with a median age of 0.5 Myr and a median mass of 1.2 Msun, distributed throughout the region. We classified these YSOs into three groups: Class 0/I, Class II, and Class III, based on their spectral energy distributions and spatial distributions. The Class II and III sources are concentrated in the central part of Sh2-242, while the Class 0/I sources are mostly found in the outer parts.\n",
      "\n",
      "We also investigated the cluster properties of the YSOs, such as the surface density and the mass function. We found that the surface density of YSOs in Sh2-242 is higher than the average value of other massive star-forming regions, and it shows an increasing trend toward the center. The mass function of Sh2-242 is flatter than the initial mass function, which suggests that the cluster has undergone dynamical evolution.\n",
      "\n",
      "Our results provide new insights into the formation and evolution of young stellar clusters in massive star-forming regions. The census of the young stellar population in Sh2-242 is not only important for understanding the physics of star formation, but also for constraining the initial conditions of massive star clusters and their subsequent evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 937  Quasielectrons and quasiholes in the fractional quantum Hall liquids obey fractional (including nontrivial mutual) exclusion statistics. Their statistics matrix can be determined from several possible state-counting scheme, involving different assumptions on statistical correlations. Thermal activation of quasiparticle pairs and thermodynamic properties of the fractional quantum Hall liquids near fillings $1/m$ ($m$ odd) at low temperature are studied in the approximation of generalized ideal gas. The existence of hierarchical states in the fractional quantum Hall effect is shown to be a manifestation of the exclusonic nature of the relevant quasiparticles. For magnetic properties, a paramagnetism-diamagnetism transition appears to be possible at finite temperature. 0 into PostgreSQL...\n",
      "Inserting test sample 938  This paper investigates the thermodynamic behavior of fractional quantum Hall liquids in the presence of exclusonic quasiparticles. By analyzing the thermodynamic functions, we show that the excitations of non-Abelian quasiparticles have a significant contribution to the thermodynamic properties of fractional quantum Hall systems. By considering the effects of both direct and exchange interactions on the thermodynamic quantities, we provide a clear understanding of the role of exclusonic excitations in these systems. Our results point towards a new avenue for controlling the properties of fractional quantum Hall liquids using the concept of exclusonic quasiparticles, which may have implications for the development of future quantum technologies. 1 into PostgreSQL...\n",
      "Inserting test sample 939  The globular cluster HP~1 is projected at only 3.33 degrees from the Galactic center. Together with its distance, this makes it one of the most central globular clusters in the Milky Way. It has a blue horizontal branch (BHB) and a metallicity of [Fe/H]~-1.0. This means that it probably is one of the oldest objects in the Galaxy. Abundance ratios can reveal the nucleosynthesis pattern of the first stars as well as the early chemical enrichment and early formation of stellar populations. High-resolution spectra obtained for six stars were analyzed to derive the abundances of the light elements C, N, O, Na, and Al, the alpha-elements Mg, Si, Ca, and Ti, and the heavy elements Sr, Y , Zr, Ba, La, and Eu.} High-resolution spectra of six red giants that are confirmed members of the bulge globular cluster HP~1 were obtained with the 8m VLT UT2-Kueyen telescope with the UVES spectrograph in FLAMES-UVES configuration.\n",
      "\n",
      "The spectroscopic parameter derivation was based on the excitation and ionization equilibrium of FeI and FeII. We confirm a mean metallicity of [Fe/H] = -1.06~0.10, by adding the two stars that were previously analyzed in HP~1.\n",
      "\n",
      "The alpha-elements O and Mg are enhanced by about +0.3<[O,Mg/Fe]<+0.5 dex, Si is moderately enhanced with +0.15<[Si/Fe]<+0.35dex, while Ca and Ti show lower values of -0.04<[Ca,Ti/Fe]<+0.28dex. The r-element Eu is also enhanced with [Eu/Fe]~+0.4, which together with O and Mg is indicative of early enrichment by type II supernovae. Na and Al are low, but it is unclear if Na-O are anticorrelated. The heavy elements are moderately enhanced, with -0.20<[La/Fe]<+0.43dex and 0.0<[Ba/Fe]<+0.75~dex, which is compatible with r-process formation. The spread in Y, Zr, Ba, and La abundances, on the other hand, appears to be compatible with the spinstar scenario or other additional mechanisms such as the weak r-process. 0 into PostgreSQL...\n",
      "Inserting test sample 940  We present a high-resolution abundance analysis of red giant stars in the metal-poor bulge globular cluster HP~1 using spectral synthesis techniques. Our sample consists of 58 stars which have been observed with the MIKE spectrograph on the Magellan Clay telescope. We derive atmospheric parameters (effective temperature, surface gravity, metallicity, and microturbulent velocity) using the excitation and ionization equilibria of Fe I and Fe II lines. \n",
      "\n",
      "Abundances of O, Na, Mg, Al, Si, Ca, Sc, Ti, V, Cr, Mn, Co, Ni, Cu, Zn, Y, Ba, La, Ce, Nd, and Eu have been determined by matching synthetic spectra to the observed spectra. We find that the cluster has a mean metallicity of [Fe/H]=-1.79Â±0.01 dex, which is in agreement with previous studies. We explore the abundance patterns of Î±, iron-peak, and neutron-capture elements and compare them to those of other globular clusters in the Milky Way. \n",
      "\n",
      "Our analysis reveals that HP~1 is a metal-poor globular cluster with a large range of [X/Fe] ratios, where X can be any of O, Mg, Ca, or Ti. The Î±-elements (O, Mg, Ca, and Ti) show a positive correlation with [Fe/H], which is expected for nucleosynthesis processes in Type II supernovae. The iron-peak elements (Sc, V, Mn, Co, Ni, and Zn) exhibit a flatter trend with [Fe/H], indicating that Type Ia supernovae also played a role in their production. The neutron-capture elements (Y, Ba, La, Ce, Nd, and Eu) display a spread in abundances, which implies the occurrence of multiple nucleosynthesis channels.\n",
      "\n",
      "Our results emphasize the importance of studying metal-poor globular clusters such as HP~1 for understanding the chemical evolution of the Milky Way and the nucleosynthesis processes in different types of stars. 1 into PostgreSQL...\n",
      "Inserting test sample 941  We experimentally study Bose-Einstein condensation of photons (phBEC) in a dye-filled microcavity. Through multiple absorption and emission cycles the photons inside the microcavity thermalize to the rovibronic temperature of the dye solution. Raising the photon density of the thermalized photon gas beyond the critical photon density yields a macroscopic occupation of the ground state, i.e. phBEC. For increasing density, we observe an increase of the condensate radius which we attribute to effective repulsive interactions. For several dye concentrations we accurately determine the radius of the condensate as a function of the number of condensate photons, and derive an effective interaction strength parameter $\\tilde{g}$. For all concentrations we find $\\tilde{g} \\sim 10^{-2}$, one order larger than previously reported. 0 into PostgreSQL...\n",
      "Inserting test sample 942  We investigate the effective interaction strength between photons in a dye-filled microcavity Bose-Einstein condensate (BEC). We derive a general Hamiltonian for such a system which takes into account the dipole-dipole interactions, radiation pressure, and photon number fluctuations. Our analysis shows that the effective interaction strength is significantly enhanced due to the nature of the microcavity BEC, leading to a novel regime of physics. We explore this regime by studying the stability of the equilibrium ground state in the presence of both attractive and repulsive interactions. Our results demonstrate the potential for engineering photon-photon interactions in a BEC and provide insights into the behavior of strongly interacting photon systems. 1 into PostgreSQL...\n",
      "Inserting test sample 943  For 0 < x < 1, take the binary expansion with infinitely many 0's, replace each 0 with -1, this gives the polarized binary expansion of x. Let R_i(x) be the ith \"polarized bit\" and let S_n(x) be the sum of the first n R_i(x). {S_n} is the Z-valued random walk on (0,1). Normalize, by dividing each S_n by the square root of n: the resulting sequence converges weakly to the standard normal distribution on (0,1). The quantiles of S_n are random variables on (0,1), denoted S*_n, which are equal in distribution to the S_n, Skorokhod showed that the sequence of normalized quantiles converges almost surely to the standard normal distribution on (0,1).\n",
      "\n",
      "For n > 2, S*_n cannot be represented as the sum of the first n terms of a fixed sequence, R*_i, of random variables with the properties of the R_i. We introduce a method of constructing, for each n, an i.i.d family, R*_(n,1), ...\n",
      "\n",
      "R*_(n,n) which sums to S*_n, pointwise, as a function, not just in distribution. Each R*_(n,i) is a mean 0, variance 1 Rademacher random variable depending only on the first n bits. For each n, we get a bijection between the set of all such i.i.d. families, R*_(n,1), ... R*_(n,n), and the set of all admissible permutations of {0, ..., (2^n)-1}.\n",
      "\n",
      "Varying n, any doubly indexed such family, gives a triangular array representation of the sequence {S*_n} which is strong (because for each n, S*_n is the pointwise sum of the R*_(n,i)). Such representations are classified by sequences of admissible permutations. We show that the complexity of any sequence of admissible permutations is bounded below by that of 2^n. We explicitly construct three such polynomial time computable sequences whose complexity is bounded above by that of the function SBC (sum of binomial coefficients). We also initiate the study of some additional fine properties of admissible permutations. 0 into PostgreSQL...\n",
      "Inserting test sample 944  This paper presents a new technique for computing triangular arrays, an essential tool in statistics and probability theory, with a focus on their almost sure convergence. Triangular arrays are widely used in these fields due to their ability to model sequences of random variables that are dependent on each other, making them an integral part of statistical analysis.\n",
      "\n",
      "We present a novel approach to constructing triangular arrays which can be computed in polynomial time. Previously existing computation methods were highly inefficient, requiring enormous computational resources. Our proposed method provides an efficient and effective way to compute triangular arrays which would greatly benefit the fields of probability theory and statistics.\n",
      "\n",
      "Our work also focuses on the concept of almost sure convergence, a theoretical concept which has been widely studied in probability theory and statistics. Almost sure convergence refers to the convergence of a certain sequence of random variables with probability one. It is a crucial concept in the development of statistical models, as it allows for the approximation of complex systems by simpler models.\n",
      "\n",
      "We prove that our polynomial-time computable triangular arrays converge almost surely, making them an essential tool for statisticians and probabilists. Our proof is based on well-known concepts in mathematics and probability theory, and we provide a rigorous explanation for the almost sure convergence of our proposed method.\n",
      "\n",
      "Furthermore, we also provide several examples of applications of our technique to real-world problems, such as option pricing and risk management in finance. By applying our methodology, we were able to create statistically sound and efficient models for these problems, which are essential for the decision-making processes in finance.\n",
      "\n",
      "In summary, our paper presents a groundbreaking technique for computing triangular arrays that converge almost surely, providing an efficient and effective tool for statisticians and probabilists. Our work extends the scope of traditional probability theory by providing a new way to model complex systems, with applications in real-world problems. 1 into PostgreSQL...\n",
      "Inserting test sample 945  Using the Green Bank Telescope, we have conducted a survey for 1.3 cm water maser emission toward the nuclei of nearby active galaxies, the most sensitive large survey for H2O masers to date. Among 145 galaxies observed, maser emission was newly detected in eleven sources and confirmed in one other. Our survey targeted nearby (v < 12,000 km/s, mainly type 2 AGNs north of declination -20 degrees, and includes a few additional sources as well. We find that more than a third of Seyfert 2 galaxies have strong maser emission, though the detection rate declines beyond v about 5000 km/s due to sensitivity limits.\n",
      "\n",
      "Two of the masers discovered during this survey are found in unexpected hosts: NGC 4151 (Seyfert 1.5) and NGC 2782 (starburst). We discuss the possible relations between the large X-ray column to NGC 4151 and a possible hidden AGN in NGC 2782 to the detected masers. Four of the masers discovered here, NGC 591, NGC 4388, NGC 5728 and NGC 6323, have high-velocity lines symmetrically spaced about the systemic velocity, a likely signature of molecular gas in a nuclear accretion disk. The maser source in NGC 6323, in particular, reveals the classic spectrum of a \"disk maser\" represented by three distinct groups of Doppler components. Future single-dish and VLBI observations of these four galaxies could provide a measurement of the distance to each galaxy, and of the Hubble constant, independent of standard candle calibrations. 0 into PostgreSQL...\n",
      "Inserting test sample 946  We report the results of a search for water masers in 41 nearby active galactic nuclei (AGNs) using the Green Bank Telescope (GBT). Water masers, observed at a frequency of 22 GHz, can trace the kinematics of circumnuclear gas on sub-parsec scales and provide important clues to the structure and dynamics of AGNs. Our GBT observations resulted in the detection of 14 new water maser systems and confirmation of 2 previously reported ones. We also detected 7 tentative signals that require further observations for confirmation. Our sample includes several interesting objects, including the nearby Seyfert 2 galaxy NGC 2273, which hosts two maser systems at different velocities, and several galaxies with double-peaked water maser spectra that may indicate the presence of Keplerian accretion disks. The detection rate of 34% is consistent with previous surveys of AGN masers and indicates that luminous AGNs are not necessarily strong maser emitters. Our work demonstrates the potential of the GBT in searching for weak maser emission from nearby AGNs and sets the stage for more sensitive surveys in the future with planned upgrades. The full data and analysis in this report can serve as a reference for future studies of the kinematics of circumnuclear gas in AGNs. 1 into PostgreSQL...\n",
      "Inserting test sample 947  We study the physics of Kaluza-Klein (KK) top quarks in the framework of a non-minimal Universal Extra Dimension (nmUED) with an orbifolded (S1/Z2) flat extra spatial dimension in the presence of brane-localized terms (BLTs). In general, BLTs affect the masses and the couplings of the KK excitations in a non-trivial way including those for the KK top quarks. On top of that, BLTs also influence the mixing of the top quark chiral states at each KK level and trigger mixings among excitations from different levels with identical KK parity (even or odd). The latter phenomenon of mixing of KK levels is not present in the popular UED scenario known as the minimal UED (mUED) at the tree level. Of particular interest are the mixings among the KK top quarks from level `0' and level `2' (driven by the mass of the Standard Model (SM) top quark). These open up new production modes in the form of single production of a KK top quark and the possibility of its direct decays to Standard Model (SM) particles leading to rather characteristic signals at the colliders.\n",
      "\n",
      "Experimental constraints and the restrictions they impose on the nmUED parameter space are discussed. The scenario is implemented in MadGraph 5 by including the quark, lepton, the gauge-boson and the Higgs sectors up to the second KK level. A few benchmark scenarios are chosen for preliminary studies of the decay patterns of the KK top quarks and their production rates at the LHC in various different modes. Recast of existing experimental analyzes in scenarios having similar states is found to be not so straightforward for the KK top quarks of the nmUED scenario under consideration. 0 into PostgreSQL...\n",
      "Inserting test sample 948  This research paper aims to explore the effects of non-minimal Universal Extra Dimensions (UED) on the top quark sector by incorporating brane local terms. UED offers a solution to the hierarchy problem by allowing for the existence of extra dimensions, leading to the prediction of Kaluza-Klein (KK) excitations of known particles. However, minimal UED does not entirely solve the hierarchy problem and requires the introduction of non-minimal UED.\n",
      "\n",
      "This study presents a thorough analysis of the top quark sector in the non-minimal UED scenario. We investigate the impact of brane local terms, which arise due to the curvature of extra dimensions at the location of branes, on the KK modes of the top quark. We find that the brane local terms can significantly impact the mass spectrum of KK excitations in the top quark sector, leading to a shift in the mass hierarchy and possible deviations from minimal UED predictions.\n",
      "\n",
      "Our calculations reveal that the top quark Yukawa coupling plays a crucial role in determining the impact of brane local terms on the KK top quark modes. Moreover, we observe that the KK gluon contribution to the top quark self-energy undergoes significant modifications due to the introduction of these terms. Our results show that the inclusion of brane local terms can alter the collider phenomenology of the KK modes of the top quark, leading to potential implications for current and future experimental searches.\n",
      "\n",
      "In conclusion, this study provides a comprehensive analysis of non-minimal UED with brane local terms, specifically focused on the top quark sector. Our findings highlight the importance of considering these effects in future experimental searches and shed light on new avenues of research in the field of extra dimensions and particle physics. 1 into PostgreSQL...\n",
      "Inserting test sample 949  We study PTF11mnb, a He-poor supernova (SN) whose pre-peak light curves (LCs) resemble those of SN 2005bf, a peculiar double-peaked stripped-envelope (SE) SN. LCs, colors and spectral properties are compared to those of SN 2005bf and normal SE SNe. A bolometric LC is built and modeled with the SNEC hydrodynamical code explosion of a MESA progenitor star, as well as with semi-analytic models. The LC of PTF11mnb turns out to be similar to that of SN 2005bf until $\\sim$50 d, when the main (secondary) peaks occur at $-18.5$ mag.\n",
      "\n",
      "The early peak occurs at $\\sim$20 d, and is about 1.0 mag fainter. After the main peak, the decline rate of PTF11mnb is remarkably slower than that of SN 2005bf, and it traces the $^{56}$Co decay rate. The spectra of PTF11mnb reveal no traces of He unlike in the case of SN Ib 2005bf. The bolometric LC is well reproduced by the explosion of a massive ($M_{ej} =$ 7.8 $M_{\\odot}$), He-poor star with a double-peaked $^{56}$Ni distribution, a total $^{56}$Ni mass of 0.59 $M_{\\odot}$ and an explosion energy of 2.2$\\times$10$^{51}$ erg.\n",
      "\n",
      "Alternatively, a normal SN Ib/c explosion [M($^{56}$Ni)$=$0.11 $M_{\\odot}$, $E_{K}$ = 0.2$\\times$10$^{51}$ erg, $M_{ej} =$ 1 $M_{\\odot}$] can power the first peak while a magnetar ($B$=5.0$\\times$10$^{14}$ G, $P=18.1$ ms) provides energy for the main peak. The early $g$-band LC implies a radius of at least 30 $R_{\\odot}$. If PTF11mnb arose from a massive He-poor star characterized by a double-peaked $^{56}$Ni distribution, the ejecta mass and the absence of He imply a large ZAMS mass ($\\sim85 M_{\\odot}$) for the progenitor, which most likely was a Wolf-Rayet star, surrounded by an extended envelope formed either by a pre-SN eruption or due to a binary configuration. Alternatively, PTF11mnb could be powered by a normal SE SN during the first peak and by a magnetar afterwards. 0 into PostgreSQL...\n",
      "Inserting test sample 950  The discovery of PTF11mnb heralds a significant advance in our understanding of supernovae. It is the first analog of the well-known supernova 2005bf and studying it will undoubtedly shed light on new aspects of these mysterious phenomena. The spectroscopic properties of PTF11mnb are very similar to those of other Type Ic supernovae, which suggests that the explosion mechanism is likely similar to other events that have been carefully studied.\n",
      "\n",
      "The discovery of PTF11mnb was aided by advances in automated detection systems, such as Palomar Transient Factory, and the development of new algorithms to sift through large datasets to identify promising events. The PTF11mnb candidate was initially detected based on the rapid increase in brightness within a short time frame, as well as the lack of any host galaxy in the surrounding area. Subsequent observations ruled out alternative explanations, such as a tidal disruption event or a flare from a Galactic star.\n",
      "\n",
      "With its properties now established, PTF11mnb offers a wealth of potential study possibilities. For example, the progenitor system of PTF11mnb could be very different from the progenitor of other Type Ic supernovae, which would imply that there is a greater diversity of supernova progenitor systems than previously thought. Additionally, the explosion mechanism, which is still poorly understood, could be significantly different from the model that has been widely accepted for decades.\n",
      "\n",
      "Finally, PTF11mnb provides an opportunity to test theoretical models in detail, particularly those relating to mass loss in the progenitor star prior to the explosion. The detailed spectroscopic observations of PTF11mnb, taken at different phases and across the electromagnetic spectrum, will allow researchers to study this crucial aspect in more detail than ever before.\n",
      "\n",
      "In summary, the discovery of PTF11mnb is a crucial milestone in the investigation of supernovae, and its properties are already yielding new insights into these spectacular, yet enigmatic, cosmic events. Further observations and analyses of PTF11mnb will undoubtedly deepen our understanding of this fascinating and important topic. 1 into PostgreSQL...\n",
      "Inserting test sample 951  Traditional learning approaches for classification implicitly assume that each mistake has the same cost. In many real-world problems though, the utility of a decision depends on the underlying context $x$ and decision $y$. However, directly incorporating these utilities into the learning objective is often infeasible since these can be quite complex and difficult for humans to specify.\n",
      "\n",
      "We formally study this as agnostic learning with unknown utilities: given a dataset $S = \\{x_1, \\ldots, x_n\\}$ where each data point $x_i \\sim \\mathcal{D}$, the objective of the learner is to output a function $f$ in some class of decision functions $\\mathcal{F}$ with small excess risk. This risk measures the performance of the output predictor $f$ with respect to the best predictor in the class $\\mathcal{F}$ on the unknown underlying utility $u^*$.\n",
      "\n",
      "This utility $u^*$ is not assumed to have any specific structure. This raises an interesting question whether learning is even possible in our setup, given that obtaining a generalizable estimate of utility $u^*$ might not be possible from finitely many samples. Surprisingly, we show that estimating the utilities of only the sampled points~$S$ suffices to learn a decision function which generalizes well.\n",
      "\n",
      "We study mechanisms for eliciting information which allow a learner to estimate the utilities $u^*$ on the set $S$. We introduce a family of elicitation mechanisms by generalizing comparisons, called the $k$-comparison oracle, which enables the learner to ask for comparisons across $k$ different inputs $x$ at once. We show that the excess risk in our agnostic learning framework decreases at a rate of $O\\left(\\frac{1}{k} \\right)$. This result brings out an interesting accuracy-elicitation trade-off -- as the order $k$ of the oracle increases, the comparative queries become harder to elicit from humans but allow for more accurate learning. 0 into PostgreSQL...\n",
      "Inserting test sample 952  The problem of agnostic learning with unknown utilities arises when a learner needs to make predictions despite a lack of knowledge about the best action to be taken. Such a scenario usually occurs in situations where the optimal decision is determined by a reward function that is unknown to the learner. In this theoretical study, we provide a thorough analysis of the challenges and opportunities posed by agnostic learning with unknown utilities.\n",
      "\n",
      "We first introduce the formal framework for agnostic learning and then examine how it applies to new and existing learning models. We then provide an in-depth exploration of decision-making under uncertainty, analyzing the trade-offs between exploration and exploitation and examining how various factors, including the level of uncertainty and the complexity of the environment, can impact performance.\n",
      "\n",
      "Our analysis also extends to the burgeoning field of machine learning, where we show how traditional algorithms can be modified to accommodate unknown utility scenarios. We also propose new methods that take advantage of unstructured data, as well as deep reinforcement learning algorithms that operate effectively in high-dimensional data.\n",
      "\n",
      "Finally, we provide an empirical evaluation of the performance of our proposed algorithms on various benchmark datasets, demonstrating their effectiveness in dealing with the unknown utility problem. Our results suggest that agnostic learning with unknown utilities can be a powerful paradigm for learning in complex, dynamic environments. Thus, our study provides a new theoretical framework as well as practical tools for effectively solving decision-making problems with unknown utilities. 1 into PostgreSQL...\n",
      "Inserting test sample 953  We point out that vector boson fusion (VBF) at the Large Hadron Collider (LHC) can lead to useful signals for charginos and neutralinos in supersymmetric scenarios where these particles are almost invisible. The proposed signals are just two forward jets with missing transverse energy. It is shown that in this way one can put by far the strongest constraint on the parameter space of a theory with anomaly mediated supersymmetry breaking (AMSB) at the LHC. In addition, scenarios where the lightest neutralinos and charginos are Higgsino-like can give signals of the above type. 0 into PostgreSQL...\n",
      "Inserting test sample 954  We investigate a framework where the mediation of supersymmetry breaking is conveyed through an anomalous U(1) gauge symmetry. We propose an innovative strategy to detect invisible charginos and neutralinos. Our method exploits the production of these particles via gauge boson fusion at the Large Hadron Collider (LHC). We explore the sensitivity of the LHC experiments to our signal and quantify the significance of our approach in measuring anomalous U(1) charges. Our findings demonstrate that the proposed methodology opens a novel window for studying anomaly-mediated supersymmetry breaking. 1 into PostgreSQL...\n",
      "Inserting test sample 955  We have investigated the magnetoconductance of semiconducting carbon nanotubes (CNTs) in pulsed, parallel magnetic fields up to 60 T, and report the direct observation of the predicted band-gap closure and the reopening of the gap under variation of the applied magnetic field. We also highlight the important influence of mechanical strain on the magnetoconductance of the CNTs. 0 into PostgreSQL...\n",
      "Inserting test sample 956  In this study, we report the direct observation of the band-gap closure of a semiconducting carbon nanotube under a large parallel magnetic field. By utilizing photoluminescence spectroscopy, we found a significant redshift and broadening of the excitonic transitions, indicating the weakening of the electronic confinement. Our findings provide important insights into the fundamental physics of carbon nanotubes and their potential applications in nanoelectronics. 1 into PostgreSQL...\n",
      "Inserting test sample 957  Nanometric distance measurements with EPR spectroscopy yield crucial information on the structure and interactions of macromolecules in complex systems. The range of suitable spin labels for such measurements was recently expanded with a new class of light-inducible labels: the triplet state of porphyrins. Importantly, accurate distance measurements between a triplet label and a nitroxide have been reported with two distinct light-induced spectroscopy techniques, (light-induced) triplet-nitroxide DEER (LiDEER) and laser-induced magnetic dipole spectroscopy (LaserIMD). In this work, we set out to quantitatively compare the two techniques under equivalent conditions at Q band. Since we find that LiDEER using a rectangular pump pulse does not reach the high modulation depth that can be achieved with LaserIMD, we further explore the possibility of improving the LiDEER experiment with chirp inversion pulses. LiDEER employing a broadband pump pulse results in a drastic improvement of the modulation depth. The relative performance of chirp LiDEER and Laser-IMD in terms of modulation-to-noise ratio is found to depend on the dipolar evolution time: While LaserIMD yields higher modulation-to-noise ratios than LiDEER at short dipolar evolution times ({\\tau}=2 {\\mu}s), the high phase memory time of the triplet spins causes the situation to revert at {\\tau}=6 {\\mu}s. 0 into PostgreSQL...\n",
      "Inserting test sample 958  Light-induced dipolar spectroscopy (LID) is a powerful technique for probing the structural and dynamic properties of macromolecules in solution. Over the years, different variations of LID have been developed, including LiDEER and LaserIMD. In this study, we quantitatively compared the two methods in terms of their sensitivity, precision, and accuracy. Our results showed that LiDEER has a higher sensitivity than LaserIMD, enabling the detection of smaller changes in the dipolar evolution of the sample. Additionally, we found that LiDEER has a superior precision, with a lower experimental error than LaserIMD. On the other hand, LaserIMD appears to be more accurate in terms of the determination of the dipolar distance, especially in complex samples. Overall, both methods have their advantages and limitations, and the choice of the appropriate technique depends on specific experimental requirements. In conclusion, our study provides a comprehensive comparison between LiDEER and LaserIMD, shedding light on their respective strengths and weaknesses, and contributing to the optimization of LID for structural biology applications. 1 into PostgreSQL...\n",
      "Inserting test sample 959  In this article, we study the problem of obtaining Lebesgue space inequalities for the Fourier restriction operator associated to rectangular pieces of the paraboloid and perturbations thereof. We state a conjecture for the dependence of the operator norms in these inequalities on the sidelengths of the rectangles, prove that this conjecture follows from (a slight reformulation of the) restriction conjecture for elliptic hypersurfaces, and prove that, if valid, the conjecture is essentially sharp. Such questions arise naturally in the study of restriction inequalities for degenerate hypersurfaces; we demonstrate this connection by using our positive results to prove new restriction inequalities for a class of hypersurfaces having some additive structure. 0 into PostgreSQL...\n",
      "Inserting test sample 960  Fourier restriction is a fundamental topic in harmonic analysis that has recently gained renewed attention due to its central role in the solution of wave and SchrÃ¶dinger-type equations. One of its core questions is to establish sharp estimates for the Fourier restriction operator to higher dimensional surfaces. In this paper, we examine Fourier restriction above rectangles in the Euclidean space. We establish sharp estimates for the Fourier restriction operator above rectangles by adapting the method of induction on scales. Our results are based on a combination of probabilistic methods, restriction theory, and geometric measure theory. The obtained estimates can be applied in areas such as signal processing, optics, and quantum mechanics. 1 into PostgreSQL...\n",
      "Inserting test sample 961  Let $G$ be a group. Let $X$ be a connected algebraic group over an algebraically closed field $K$. Denote by $A=X(K)$ the set of $K$-points of $X$. We study a class of endomorphisms of pro-algebraic groups, namely algebraic group cellular automata over $(G,X,K)$. They are cellular automata $\\tau \\colon A^G \\to A^G$ whose local defining map is induced by a homomorphism of algebraic groups $X^M \\to X$ where $M\\subset G$ is a finite memory set of $\\tau$. Our first result is that when $G$ is sofic, such an algebraic group cellular automaton $\\tau$ is invertible whenever it is injective and $\\text{char}(K)=0$. As an application, we prove that if $G$ is sofic and the group $X$ is commutative then the group ring $R[G]$, where $R=\\text{End}(X)$ is the endomorphism ring of $X$, is stably finite. When $G$ is amenable, we show that an algebraic group cellular automaton $\\tau$ is surjective if and only if it satisfies a weak form of pre-injectivity called $(\\bullet)$-pre-injectivity.\n",
      "\n",
      "This yields an analogue of the classical Moore-Myhill Garden of Eden theorem.\n",
      "\n",
      "We also introduce the near ring $R(K,G)$ which is $K[X_g: g \\in G]$ as an additive group but the multiplication is induced by the group law of $G$. The near ring $R(K,G)$ contains naturally the group ring $K[G]$ and we extend Kaplansky's conjectures to this new setting. Among other results, we prove that when $G$ is an orderable group, then all one-sided invertible elements of $R(K,G)$ are trivial, i.e., of the form $aX_g+b$ for some $g\\in G$, $a\\in K^*$, $b\\in K$. This allows us to show that when $G$ is locally residually finite and orderable (e.g. $\\mathbb{Z}^d$ or a free group), and $\\text{char}(K)=0$, all injective algebraic cellular automata $\\tau \\colon \\mathbb{C}^G \\to \\mathbb{C}^G$ are of the form $\\tau(x)(h)= a x(g^{-1}h) +b$ for all $x\\in \\mathbb{C}^G, h \\in G$ for some $g\\in G$, $a\\in \\mathbb{C}^*$, $b\\in \\mathbb{C}$. 0 into PostgreSQL...\n",
      "Inserting test sample 962  The study of sofic groups and their properties has been an active area of research for the past few decades. In this paper, we investigate the relationship between sofic groups, Kaplansky's conjectures, and endomorphisms of pro-algebraic groups. \n",
      "\n",
      "Firstly, we introduce the notion of sofic groups, which are countable groups characterized by the existence of an increasing sequence of finite subsets with certain properties. We present several examples of sofic groups, including groups arising from group actions and automata. Furthermore, we prove that a sofic group is amenable if and only if it has the Haagerup property.\n",
      "\n",
      "Next, we discuss Kaplansky's conjectures, which are a series of open problems in the study of rings and their modules. We focus on the conjecture relating to the existence of a non-zero endomorphism of a module which factors through a proper submodule. We establish a connection between this conjecture and the study of sofic groups by showing that it holds true for certain classes of sofic groups.\n",
      "\n",
      "Finally, we examine the endomorphisms of pro-algebraic groups, which are topological groups obtained as the profinite limit of algebraic groups over a fixed field. By studying the sofic properties of these groups, we show that certain endomorphisms can be approximated by homomorphisms from sofic groups. Additionally, we investigate the existence of faithful endomorphisms of pro-algebraic groups and provide conditions under which such endomorphisms exist.\n",
      "\n",
      "Overall, our findings enhance our understanding of the interplay between sofic groups, Kaplansky's conjectures, and endomorphisms of pro-algebraic groups. Furthermore, they provide new insights into the study of algebraic and topological structures, as well as their connections to other areas of mathematics. 1 into PostgreSQL...\n",
      "Inserting test sample 963  Datasets in engineering applications are often limited and contaminated, mainly due to unavoidable measurement noise and signal distortion. Thus, using conventional data-driven approaches to build a reliable discriminative model, and further applying this identified surrogate to uncertainty analysis remains to be very challenging. A deep learning approach is presented to provide predictions based on limited and noisy data. To address noise perturbation, the Bayesian learning method that naturally facilitates an automatic updating mechanism is considered to quantify and propagate model uncertainties into predictive quantities. Specifically, hierarchical Bayesian modeling (HBM) is first adopted to describe model uncertainties, which allows the prior assumption to be less subjective, while also makes the proposed surrogate more robust. Next, the Bayesian inference is seamlessly integrated into the DL framework, which in turn supports probabilistic programming by yielding a probability distribution of the quantities of interest rather than their point estimates. Variational inference (VI) is implemented for the posterior distribution analysis where the intractable marginalization of the likelihood function over parameter space is framed in an optimization format, and stochastic gradient descent method is applied to solve this optimization problem. Finally, Monte Carlo simulation is used to obtain an unbiased estimator in the predictive phase of Bayesian inference, where the proposed Bayesian deep learning (BDL) scheme is able to offer confidence bounds for the output estimation by analyzing propagated uncertainties. The effectiveness of Bayesian shrinkage is demonstrated in improving predictive performance using contaminated data, and various examples are provided to illustrate concepts, methodologies, and algorithms of this proposed BDL modeling technique. 0 into PostgreSQL...\n",
      "Inserting test sample 964  Bayesian deep learning offers an effective approach for making predictions from limited and noisy data. However, incorporating hierarchical prior knowledge into a Bayesian deep learning model adds a new layer of complexity. In this paper, we propose a hierarchical Bayesian deep learning framework that enables flexible modeling of complex data. Our method extends the Bayesian deep learning approach by incorporating a hierarchical prior distribution that can capture high-level features of the data. The framework allows simultaneous modeling of multiple levels of abstraction, thus encouraging modularity and interpretability of the results.\n",
      "\n",
      "We demonstrate the effectiveness of our approach using different datasets. Our experiments show that our method achieves better predictive accuracy than state-of-the-art deep learning models. In particular, our method excels in cases where the amount of training data is limited or the data is noisy. Additionally, we illustrate how our hierarchical Bayesian deep learning framework can be used for transfer learning applications, where prior knowledge from related tasks can be incorporated into the model to improve performance.\n",
      "\n",
      "Overall, our proposed framework provides a powerful tool for modeling complex data with limited or noisy observations. The incorporation of a hierarchical prior encourages more meaningful representations of the data and increases the interpretability of the results. This work presents a significant contribution to the field of Bayesian deep learning, demonstrating how the combination of deep neural networks and Bayesian modeling can provide a flexible and powerful approach for data analysis and prediction. 1 into PostgreSQL...\n",
      "Inserting test sample 965  We show that violation of the null energy condition implies instability in a broad class of models, including classical gauge theories with scalar and fermionic matter as well as any perfect fluid. When applied to the dark energy, our results imply that $w = p / \\rho$ is unlikely to be less than -1. 0 into PostgreSQL...\n",
      "Inserting test sample 966  This paper investigates the relationship between instabilities in gravitational systems and the null energy condition. We analyze the dynamics of spacetimes with unstable null geodesics and show that such systems violate the null energy condition. Our results suggest that the null energy condition may play a crucial role in constraining gravitational instabilities. 1 into PostgreSQL...\n",
      "Inserting test sample 967  Three deep radio continuum surveys of the Andromeda galaxy, M 31, were performed at 11.3, 6.2, and 3.6 cm wavelength with the Effelsberg 100-m telescope. At all wavelengths, the total and polarized emission is concentrated in a ring-like structure between about 7 kpc and 13 kpc radius from the center.\n",
      "\n",
      "Propagation of cosmic rays away from star-forming regions is evident. The ring of synchrotron emission is wider than the ring of the thermal radio emission, and the radial scale length of synchrotron emission is larger than that of thermal emission. The polarized intensity from the ring varies double-periodically with azimuthal angle, indicating that the ordered magnetic field is almost oriented along the ring, with a pitch angle of -14{\\deg} $\\pm$ 2{\\deg}. Faraday rotation measures (RM) show a large-scale sinusoidal variation with azimuthal angle, signature of an axisymmetric spiral (ASS) regular magnetic field, plus a superimposed double-periodic variation of a bisymmetric spiral (BSS) field with about 6x smaller amplitude. The dominating ASS field of M 31 is the most compelling case so far of a field generated by the action of a mean-field dynamo. The RM amplitude between 6.2 cm and 3.6 cm is about 50% larger than between 11.3 cm and 6.2 cm, indicating that Faraday depolarization at 11.3 cm is stronger than at 6.2 cm and 3.6 cm. The phase of the sinusoidal RM variation of -7{\\deg} $\\pm$ 1{\\deg} is interpreted as the average spiral pitch angle of the regular field. The average pitch angle of the ordered field, as derived from the intrinsic orientation of the polarized emission (corrected for Faraday rotation), is significantly smaller: -26{\\deg} $\\pm$ 3{\\deg}. The difference in pitch angle of the regular and the ordered fields indicates that the ordered field contains a significant fraction of an anisotropic turbulent field that has a different pattern than the regular (ASS + BSS) field. 0 into PostgreSQL...\n",
      "Inserting test sample 968  This paper presents an analysis of the magnetic fields and cosmic rays in the Andromeda Galaxy (M 31) using radio observations. We measured the synchrotron spectral index and its spatial variation to search for deviations from the theoretical prediction of -0.5 for a homogeneous cosmic ray population. We found a flattening of the spectral index in the north-east arm of M 31, which suggests the presence of a spatially varying cosmic ray diffusion coefficient. We also determine the magnetic field strength and scale length using the equipartition assumption and found a magnetic field strength of 6 Â± 2 Î¼G and a scale length of 1.8 Â± 0.6 kpc, which is consistent with previous studies. \n",
      "\n",
      "To probe the magnetic field orientation, we computed the Faraday rotation measure (RM), which describes the amount of rotation of linearly polarized radiation as it propagates through a plasma with a magnetic field. We found a coherent RM gradient along the north-west to south-east direction, consistent with a large-scale magnetic field pattern. This is the first detection of a Faraday rotation signal in M 31 and provides constraints on the ordered magnetic field component. \n",
      "\n",
      "We constructed a simple magnetic field model with a large-scale spiral arm and a poloidal halo component. The model is motivated by observations of polarized radio emission and magnetic field structure in other spiral galaxies. We found that the model can reproduce the observed RM distribution and magnetic field morphology reasonably well. The fitted large-scale pattern speed is consistent with the circular rotation speed, suggesting that magnetic fields in M 31 are tightly coupled to the galactic rotation. \n",
      "\n",
      "Overall, our results provide insights into the cosmic ray and magnetic field properties in M 31 and can serve as a benchmark for theoretical models of galaxy evolution and magnetic field generation. Future high-resolution observations will allow us to probe the magnetic field and cosmic ray properties on smaller scales and investigate the connection with the star formation activity and gas dynamics in M 31. 1 into PostgreSQL...\n",
      "Inserting test sample 969  The reconstruction of the initial conditions of the Universe is an important topic in cosmology, particularly in the context of sharpening the measurement of the baryon acoustic oscillation (BAO) peak. Nonlinear reconstruction algorithms developed in recent years, when applied to late-time matter fields, can recover to a substantial degree the initial density distribution, however, when applied to sparse tracers of the matter field, the performance is poorer.\n",
      "\n",
      "In this paper we apply the Shi et al. non-linear reconstruction method to biased tracers in order to establish what factors affect the reconstruction performance. We find that grid resolution, tracer number density and mass assignment scheme all have a significant impact on the performance of our reconstruction method, with triangular-shaped-cloud (TSC) mass assignment and a grid resolution of ${\\sim}1{-}2h^{-1}$ Mpc being the optimal choice. We also show that our method can be easily adapted to include generic tracer biases up to quadratic order in the reconstruction formalism. Applying the reconstruction to halo and galaxy samples with a range of tracer number densities, we find that the linear bias is by far the most important bias term, while including nonlocal and nonlinear biases only leads to marginal improvements on the reconstruction performance. Overall, including bias in the reconstruction substantially improves the recovery of BAO wiggles, down to $k\\sim0.25~h\\text{Mpc}^{-1}$ for tracer number densities between $2\\times10^{-4}$ and $2\\times10^{-3}~(h^{-1}\\text{Mpc})^{-3}$. 0 into PostgreSQL...\n",
      "Inserting test sample 970  The baryon acoustic oscillations (BAO) are a powerful tool for tracing the large-scale structure of the Universe and for constraining cosmological parameters. However, the standard BAO measurements directly probe only the clustering of dark matter and not that of baryons. Furthermore, current galaxy surveys are affected by various observational systematics, including galaxy bias, which can contaminate the BAO signal. In this work, we investigate a new method for reconstructing the BAO signal using biased tracers, which are galaxies that preferentially inhabit high-density regions and are thus more strongly clustered than the underlying dark matter. We demonstrate that this method significantly reduces the impact of galaxy bias on the BAO measurements and improves the sensitivity of the BAO signal, leading to more accurate constraints on the growth rate of cosmic structures and on the nature of dark energy. Specifically, we apply this method to galaxy survey mock catalogs and compare our results with those obtained from standard BAO measurements. We find that our method improves the accuracy of the BAO measurements by a factor of two and leads to tighter cosmological constraints. Our study highlights the potential of using biased tracers to enhance the precision of BAO measurements in current and future galaxy surveys. 1 into PostgreSQL...\n",
      "Inserting test sample 971  We present the measurement of R = B(t->Wb)/B(t->Wq) in ppbar collisions at sqrt(s) = 1.96 TeV, using 230 pb-1 of data collected by the DO experiment at the Fermilab Tevatron Collider. We fit simultaneously R and the number of selected top quark pairs (ttbar), to the number of identified b-quark jets in events with one electron or one muon, three or more jets, and high transverse energy imbalance. To improve sensitivity, kinematical properties of events with no identified b-quark jets are included in the fit. We measure R = 1.03 +0.19 -0.17 (stat+syst), in good agreement with the standard model. We set lower limits of R > 0.61 and |V_tb| > 0.78 at 95% confidence level. 0 into PostgreSQL...\n",
      "Inserting test sample 972  The branching ratio of top quark decays, B(tâ†’Wb)/B(tâ†’Wq), is a fundamental parameter in the study of the Standard Model. We present a measurement of this ratio using data collected with the Collider Detector at Fermilab (CDF II) in proton-antiproton collisions at a center-of-mass energy of 1.96 TeV. The analysis uses events where both top quarks decay to a charged lepton, neutrino and jets. A likelihood-based method is employed to determine the ratio of B(tâ†’ Wb) to B(tâ†’ Wq). The result is measured to be 0.94 Â± 0.21(stat) Â± 0.13(syst) for the dilepton sample and 1.05 Â± 0.23(stat) Â± 0.14(syst) for the lepton+jets sample. This measurement is the most precise determination of this branching ratio at the Tevatron. 1 into PostgreSQL...\n",
      "Inserting test sample 973  [Abridged] With VLT/X-shooter, we obtain optical and NIR spectra of six Ly-alpha blobs at z~2.3. Using three measures --- the velocity offset between the Lya line and the non-resonant [OIII] or H-alpha line (Dv_Lya), the offset of stacked interstellar metal absorption lines, and the spectrally-resolved [OIII] line profile --- we study the kinematics of gas along the line of sight to galaxies within each blob center. These three indicators generally agree in velocity and direction, and are consistent with a simple picture in which the gas is stationary or slowly outflowing at a few hundred km/s from the embedded galaxies. The absence of stronger outflows is not a projection effect: the covering fraction for our sample is limited to <1/8 (13%). The outflow velocities exclude models in which star formation or AGN produce \"super\" or \"hyper\" winds of up to ~1000km/s. The Dv_Lya offsets here are smaller than typical of LBGs, but similar to those of compact LAEs. The latter suggests that outflow speed cannot be a dominant factor in driving extended Lya emission. For one Lya blob (CDFS-LAB14), whose Lya profile and metal absorption line offsets suggest no significant bulk motion, we use a simple radiative transfer model to make the first column density measurement of gas in an embedded galaxy, finding it consistent with a DLA system. Overall, the absence of clear inflow signatures suggests that the channeling of gravitational cooling radiation into Lya is not significant over the radii probed here. However, one peculiar system (CDFS-LAB10) has a blueshifted Lya component that is not obviously associated with any galaxy, suggesting either displaced gas arising from tidal interactions among blob galaxies or gas flowing into the blob center. The former is expected in these overdense regions, and the latter might signify the predicted but elusive cold gas accretion along filaments. 0 into PostgreSQL...\n",
      "Inserting test sample 974  Lyman alpha nebulae are intriguing astrophysical objects that emit strong spectral lines in the Lyman alpha (LyÎ±) transition of atomic hydrogen. The study of their properties, including their gas kinematics, has provided important insights into the formation and evolution of galaxies.\n",
      "\n",
      "This paper aims to investigate the gas kinematics of Lyman alpha nebulae using non-resonant lines. Non-resonant lines are powerful tools for measuring the gas velocity, as they are unaffected by the resonant scattering that often complicates the interpretation of resonant lines. By comparing the non-resonant lines with the resonant LyÎ± line, we can better understand the dynamics of the nebulae and their relationship to the surrounding gas.\n",
      "\n",
      "Our analysis is based on high-resolution spectroscopic observations of a sample of Lyman alpha nebulae at a redshift range of 2 < z < 3. We used the non-resonant OIII] and CIII] lines to derive the systemic and outflow velocities of the gas. Our results reveal a range of kinematic properties, including ordered rotation, turbulent motions, and strong outflows. We also find that the non-resonant lines are consistent with the resonant LyÎ± line, indicating that resonant scattering does not significantly affect the dynamics of these nebulae.\n",
      "\n",
      "Our study sheds light on the complex gas kinematics of Lyman alpha nebulae and provides important constraints on theoretical models of galaxy formation and evolution. By demonstrating the effectiveness of non-resonant lines in probing the dynamics of these objects, our work highlights the potential of future observations to further our understanding of Lyman alpha nebulae and their role in the cosmic story. 1 into PostgreSQL...\n",
      "Inserting test sample 975  We present a detailed structural and morphological study of a large sample of spectroscopically-confirmed galaxies at z >= 6, using deep HST near-IR broad-band images and Subaru optical narrow-band images. The galaxy sample consists of 51 Lyman-alpha emitters (LAEs) at z ~ 5.7, 6.5, and 7.0, and 16 Lyman-break galaxies (LBGs) at 5.9 < z < 6.5. These galaxies exhibit a wide range of rest-frame UV continuum morphology in the HST images, from compact features to multiple component systems. The fraction of merging/interacting galaxies reaches 40% ~ 50% at the brightest end of M_1500 <= -20.5 mag. The intrinsic half-light radii r_{hl,in}, after correction for PSF broadening, are roughly between r_{hl,in} ~ 0.05\" (0.3 kpc) and 0.3\" (1.7 kpc) at M_1500 <= -19.5 mag. The median r_{hl,in} value is 0.16\" (~0.9 kpc). This is consistent with the sizes of bright LAEs and LBGs at z >= 6 in previous studies. In addition, more luminous galaxies tend to have larger sizes, exhibiting a weak size-luminosity relation r_{hl,in} \\propto L^{0.14} at M_1500 <= -19.5 mag. The slope of 0.14 is significantly flatter than those in fainter LBG samples. We discuss the morphology of z >= 6 galaxies with nonparametric methods, including the CAS system and the Gini and M_20 parameters, and demonstrate their validity through simulations. We search for extended Lyman-alpha emission halos around LAEs at z ~ 5.7 and 6.5, by stacking a number of narrow-band images. We do not find evidence of extended halos predicted by cosmological simulations. Such Lyman-alpha halos, if they exist, could be weaker than predicted. Finally, we investigate any positional misalignment between UV continuum and Lyman-alpha emission in LAEs. While the two positions are generally consistent, several merging galaxies show significant positional differences. This is likely caused by a disturbed ISM distribution due to merging activity. 0 into PostgreSQL...\n",
      "Inserting test sample 976  This study aims to investigate the physical properties of galaxies at a redshift of z >= 6, through an analysis of their morphology in the rest-frame UV continuum and Lyman-alpha emission. Our sample consists of 74 spectroscopically-confirmed galaxies in the redshift range of 6.0 â‰¤ z â‰¤ 7.0, which were observed using the Hubble Space Telescope and the Keck Observatory. \n",
      "\n",
      "We present a detailed analysis of the morphologies of these galaxies, using a combination of qualitative and quantitative methods. We find that the majority of the galaxies in our sample are small, compact, and irregular in shape, with no clear evidence of disk-like structures. We also find a strong correlation between the size and luminosity of the galaxies, suggesting that larger galaxies tend to be more luminous.\n",
      "\n",
      "Furthermore, we study the relationship between the morphology of the galaxies and their Lyman-alpha emission properties. We find that galaxies with more extended Lyman-alpha emission tend to have more extended morphologies, while galaxies with compact emission tend to have more compact morphologies. This suggests that the Lyman-alpha emission in these galaxies is closely linked to their underlying morphologies.\n",
      "\n",
      "In addition, we investigate the dependence of galaxy morphology on other physical parameters, such as stellar mass, star formation rate, and continuum color. We find significant correlations between these parameters and the morphology of the galaxies, suggesting that the physical properties of galaxies at high redshift are tightly interconnected.\n",
      "\n",
      "Overall, our study provides new insights into the physical properties of galaxies at z >= 6 and sheds light on the complex interplay between their morphology, Lyman-alpha emission, and other physical parameters. Further observations and modeling efforts will be necessary to fully understand the origins and evolution of these intriguing high-redshift galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 977  Attention-based sequence-to-sequence (seq2seq) models have achieved promising results in automatic speech recognition (ASR). However, as these models decode in a left-to-right way, they do not have access to context on the right. We leverage both left and right context by applying BERT as an external language model to seq2seq ASR through knowledge distillation. In our proposed method, BERT generates soft labels to guide the training of seq2seq ASR. Furthermore, we leverage context beyond the current utterance as input to BERT. Experimental evaluations show that our method significantly improves the ASR performance from the seq2seq baseline on the Corpus of Spontaneous Japanese (CSJ).\n",
      "\n",
      "Knowledge distillation from BERT outperforms that from a transformer LM that only looks at left context. We also show the effectiveness of leveraging context beyond the current utterance. Our method outperforms other LM application approaches such as n-best rescoring and shallow fusion, while it does not require extra inference cost. 0 into PostgreSQL...\n",
      "Inserting test sample 978  Recent advances in deep learning for automatic speech recognition (ASR) have shown that sequence-to-sequence models have the potential to yield significant improvements in transcription performance. However, their complexities have resulted in slow training and inference times, hindering their practical application to ASR. In this paper, we propose a novel approach to leverage the power of BERT, a pre-trained transformer model, to improve sequence-to-sequence ASR performance. Our approach involves distilling the knowledge of BERT into a smaller sequence model which can perform ASR efficiently. We demonstrate the effectiveness of our approach through comprehensive experiments on multiple benchmark datasets and show that it surpasses the state-of-the-art performance on several ASR tasks. In addition, our proposed model reduces the inference time by up to 23% compared to the baseline models. Our findings suggest that distilling the knowledge of pre-trained models can be a viable approach for improving ASR performance and efficiency. 1 into PostgreSQL...\n",
      "Inserting test sample 979  Purpose: Using 4D magnetic particle imaging (MPI), intravascular optical coherence tomography (IVOCT) catheters are tracked in real time in order to compensate for image artifacts related to relative motion. Our approach demonstrates the feasibility for bimodal IVOCT and MPI in-vitro experiments.\n",
      "\n",
      "Material and Methods: During IVOCT imaging of a stenosis phantom the catheter is tracked using MPI. A 4D trajectory of the catheter tip is determined from the MPI data using center of mass sub-voxel strategies. A custom built IVOCT imaging adapter is used to perform different catheter motion profiles: no motion artifacts, motion artifacts due to catheter bending, and heart beat motion artifacts. Two IVOCT volume reconstruction methods are compared qualitatively and quantitatively using the DICE metric and the known stenosis length. Results: The MPI-tracked trajectory of the IVOCT catheter is validated in multiple repeated measurements calculating the absolute mean error and standard deviation. Both volume reconstruction methods are compared and analyzed whether they are capable of compensating the motion artifacts. The novel approach of MPI-guided catheter tracking corrects motion artifacts leading to a DICE coefficient with a minimum of 86% in comparison to 58% for a standard reconstruction approach. Conclusions: IVOCT catheter tracking with MPI in real time is an auspicious method for radiation free MPI-guided IVOCT interventions. The combination of MPI and IVOCT can help to reduce motion artifacts due to catheter bending and heart beat for optimized IVOCT volume reconstructions. 0 into PostgreSQL...\n",
      "Inserting test sample 980  The aim of this study is to present a novel approach for in-vitro MPI-guided IVOCT catheter tracking in real time with motion artifact compensation. The proposed method involves the use of magnetic particle imaging (MPI) for motion detection and catheter localization, as well as intravascular optical coherence tomography (IVOCT) for visualization of the target region. By combining these two techniques, it is possible to accurately track the catheter position in real time and compensate for motion artifacts caused by physiological tremor or respiratory motion. The experimental results demonstrate the feasibility and effectiveness of the proposed method for catheter tracking in a phantom model. The developed catheter tracking system showed a tracking accuracy of less than 0.1 mm and a temporal resolution of 10 ms. The proposed method has the potential to significantly improve the clinical utility of intravascular imaging by providing real-time catheter tracking and motion artifact compensation, which could lead to more accurate diagnosis and treatment of various cardiovascular diseases. Furthermore, the results of this study may have broader implications for the development of other medical imaging systems where motion artifact compensation is crucial for achieving high-quality images. 1 into PostgreSQL...\n",
      "Inserting test sample 981  In Moby Dick, Herman Melville wondered how - or what - whales see with eyes on opposite sides of their heads. \"It is plain that he can never see an object which is exactly ahead... Is his brain so much more comprehensive, combining and subtle than man's that he can at the same moment of time attentively examine two distinct prospects, one on one side of him, and the other in an exactly opposite direction?\" he asked. It's a good question. But if Melville were alive today he might have pondered something perhaps even more intriguing: Can whales see the stars? 0 into PostgreSQL...\n",
      "Inserting test sample 982  The celestial knowledge of whales has long been a subject of fascination among scientists and laypeople alike. Recent studies suggest that some species of whales, operating in both air and water, may in fact be able to perceive stars in the night sky. Specifically, spyhopping whales have been observed looking upwards while at the surface, potentially indicating stargazing behavior. While the exact mechanisms behind this ability remain unclear, certain aspects of whale physiology, including their eyes and melatonin regulation, may be indicative of highly developed visual processing capabilities. Further research is necessary to better understand the extent of this intriguing phenomenon. 1 into PostgreSQL...\n",
      "Inserting test sample 983  The task of object viewpoint estimation has been a challenge since the early days of computer vision. To estimate the viewpoint (or pose) of an object, people have mostly looked at object intrinsic features, such as shape or appearance. Surprisingly, informative features provided by other, extrinsic elements in the scene, have so far mostly been ignored. At the same time, contextual cues have been proven to be of great benefit for related tasks such as object detection or action recognition. In this paper, we explore how information from other objects in the scene can be exploited for viewpoint estimation. In particular, we look at object configurations by following a relational neighbor-based approach for reasoning about object relations. We show that, starting from noisy object detections and viewpoint estimates, exploiting the estimated viewpoint and location of other objects in the scene can lead to improved object viewpoint predictions. Experiments on the KITTI dataset demonstrate that object configurations can indeed be used as a complementary cue to appearance-based viewpoint estimation. Our analysis reveals that the proposed context-based method can improve object viewpoint estimation by reducing specific types of viewpoint estimation errors commonly made by methods that only consider local information. Moreover, considering contextual information produces superior performance in scenes where a high number of object instances occur. Finally, our results suggest that, following a cautious relational neighbor formulation brings improvements over its aggressive counterpart for the task of object viewpoint estimation. 0 into PostgreSQL...\n",
      "Inserting test sample 984  In real-world scenarios, determining the viewpoint of an object is a crucial task for any autonomous perception system. In many cases, such as robotics and computer vision, an accurate estimate of the viewpoint can significantly contribute to the success of applications such as object detection, recognition, tracking, and manipulation. In this paper, we propose a novel context-based 2D relational approach for object viewpoint estimation, which is scalable and converges faster than existing approaches. Our method is based on computing the relationships between the object and its context, which includes the background, other objects, and their respective viewpoints. We model this relationship using a graph-like structure, where each node represents an object and each edge represents a relationship between them. By applying a scalable optimization algorithm on this graph, we jointly estimate the viewpoint of all the objects in the scene. We evaluate our method on benchmark datasets for viewpoint estimation and show that it outperforms state-of-the-art approaches in terms of both accuracy and convergence speed. Furthermore, we demonstrate the practical application of our method in robot grasping and manipulation tasks, where our approach achieves high success rates. In conclusion, our context-based 2D relational approach provides a significant contribution to object viewpoint estimation in real-world scenarios. 1 into PostgreSQL...\n",
      "Inserting test sample 985  The structure and degree of order in soft matter and other materials is intimately connected to the nature of the interactions between the particles.\n",
      "\n",
      "One important research goal is to find suitable control mechanisms, to enhance or suppress different structures. Using dynamical density functional theory, we investigate the interplay between external shear and the characteristic length-scales in the interparticle correlations of a model system. We show that shear can controllably change the characteristic length-scale from one to another quite distinct value. Moreover, with specific small changes in the form of the particle interactions, the applied shear can either selectively enhance or suppress the different characteristic wavelengths of the system, thus showing how to tune these. Our results suggest that the nonlinear response to flow can be harnessed to design novel actively responsive materials. 0 into PostgreSQL...\n",
      "Inserting test sample 986  Soft matter systems, such as polymers and colloids, display unusual viscoelastic behavior when subject to shear flow. A study of these materials reveals an intricate interplay between the intrinsic molecular structure and the external mechanical force, which affects the length-scale distribution of the system. While most current models consider the behavior of these systems on a macroscopic scale, there is a growing interest in understanding the underlying molecular mechanisms that govern this behavior. In this work, we investigate the sensitivity of length-scales in sheared soft matter to intrinsic molecular interactions. Our results demonstrate the importance of such interactions in determining the length-scale distribution under shear, and shed light on the role of molecular structure in the viscoelastic response of these systems. 1 into PostgreSQL...\n",
      "Inserting test sample 987  The interaction of two isotropic turbulent fields of equal integral scale but different kinetic energy generates the simplest kind of inhomogeneous turbulent field. In this paper we present a numerical experiment where two time decaying isotropic fields of kinetic energies $E_1$ and $E_2$ initially match over a narrow region. Within this region the kinetic energy varies as a hyperbolic tangent. The following temporal evolution produces a shearless mixing. The anisotropy and intermittency of velocity and velocity derivative statistics is observed. In particular the asymptotic behavior in time and as a function of the energy ratio $E_1/E_2 \\to \\infty$ is discussed. This limit corresponds to the maximum observable turbulent energy gradient for a given $E_1$ and is obtained through the limit $E_2 \\to 0$. A field with $E_1/E_2 \\to \\infty$ represents a mixing which could be observed near a surface subject to a very small velocity gradient separating two turbulent fields, one of which is nearly quiescent. In this condition the turbulent penetration is maximum and reaches a value equal to 1.2 times the nominal mixing layer width. The experiment shows that the presence of a turbulent energy gradient is sufficient for the appearance of intermittency and that during the mixing process the pressure transport is not negligible with respect to the turbulent velocity transport.\n",
      "\n",
      "These findings may open the way to the hypothesis that the presence of a gradient of turbulent energy is the minimal requirement for Gaussian departure in turbulence. 0 into PostgreSQL...\n",
      "Inserting test sample 988  This paper presents a novel sufficient condition for Gaussian departure in turbulence. By exploring the scaling behavior of the velocity structure functions, we derive a criterion which characterizes the deviation of velocity increments from Gaussian behavior. This criterion, expressed in terms of the kurtosis of the velocity gradient, provides a quantitative measure of the non-Gaussianity of the turbulence statistics. We show that this criterion applies to flows in a variety of regimes, including homogeneous isotropic turbulence, wall-bounded turbulence, and rotating turbulence. Through numerical simulations of the Navier-Stokes equations, we demonstrate that the new criterion is able to capture the non-Gaussian behavior of turbulence statistics more accurately than previous methods. Moreover, our analysis reveals that the degree of non-Gaussianity depends on the Reynolds number and the characteristic length scale of the flow. Finally, we discuss the implications of these results for turbulent mixing and transport, as well as for the development of computational models for turbulence. Overall, our findings contribute to a deeper understanding of the statistical properties of turbulence and pave the way for new avenues of research in this important field of study. 1 into PostgreSQL...\n",
      "Inserting test sample 989  We obtain a relation between right-handed gauge boson mass and soft squark mass in naturally R-parity conserving general supersymmetric left-right models.\n",
      "\n",
      "This relation implies that either ${W_R}$ is lighter than twice the soft squark mass, or a ratio of vacuum expectation values (VEVs) in the model, denoted by $\\tan\\alpha$, is close to its value of unity in the limit of vanishing D-terms.\n",
      "\n",
      "Generally, we find that for heavy $W_R$ $\\tan\\alpha$ is larger than one, and that the right-handed sneutrino VEV, responsible for spontaneous R-parity breaking, is at most of the order $M_{SUSY}/h_{\\Delta_R}$, where $M_{SUSY}$ is supersymmetry breaking scale and $h_{\\Delta_R}$ is the Yukawa coupling in Majorana mass term for right-handed neutrinos. These constraints follow from $SU(3)_c$ and $U(1)_{em}$ gauge invariance of the ground state of the theory. 0 into PostgreSQL...\n",
      "Inserting test sample 990  In this paper, we investigate the size of the $SU(2)_R$ breaking scale in the context of R-parity conserving supersymmetric models. We examine the constraints placed on this scale by precision measurements of electroweak observables and the Higgs boson mass. The effects of the $SU(2)_R$ breaking scale on the lightest supersymmetric particle and its interactions with Standard Model particles are also explored. In addition, we consider the impact of other experimental and theoretical inputs on the allowed range of values for the $SU(2)_R$ breaking scale. Our analysis shows that the $SU(2)_R$ breaking scale typically needs to be relatively high, in the range of a few TeV, in order to satisfy all of the constraints. This study sheds light on the interplay between various aspects of supersymmetry and the electroweak scale. 1 into PostgreSQL...\n",
      "Inserting test sample 991  Digital contact tracing has emerged as a viable tool supplementing manual contact tracing. To date, more than 100 contact tracing applications have been published to slow down the spread of highly contagious Covid-19. Despite subtle variabilities among these applications, all of them achieve contact tracing by manipulating the following three components: a) use a personal device to identify the user while designing a secure protocol to anonymize the user's identity; b) leverage networking technologies to analyze and store the data; c) exploit rich sensing features on the user device to detect the interaction among users and thus estimate the exposure risk. This paper reviews the current digital contact tracing based on these three components. We focus on two personal devices that are intimate to the user: smartphones and wearables. We discuss the centralized and decentralized networking approaches that use to facilitate the data flow. Lastly, we investigate the sensing feature available on smartphones and wearables to detect the proximity between any two users and present experiments comparing the proximity sensing performance between these two personal devices. 0 into PostgreSQL...\n",
      "Inserting test sample 992  As the world continues to grapple with the COVID-19 pandemic, various technological solutions are being explored to help curb its spread. One such solution is the use of personal devices for contact tracing. Smartphones and wearables have the potential to allow for efficient contact tracing while preserving the privacy of individuals. This paper explores the various methods and technologies used in contact tracing, with a focus on personal devices. It examines how these devices can be used to track interactions between individuals and to alert individuals who may have been in close proximity to someone who has tested positive for COVID-19. The paper also considers the ethical implications of using personal devices for contact tracing, including issues of privacy and data security. Ultimately, the use of personal devices for contact tracing could play a vital role in reducing the spread of COVID-19 and returning society to a state of normalcy. 1 into PostgreSQL...\n",
      "Inserting test sample 993  Weak gravitational lensing has emerged as a leading probe of the growth of cosmic structure. However, the shear signal is very small and accurate measurement depends critically on our ability to understand how non-ideal instrumental effects affect astronomical images. WFIRST will fly a focal plane containing 18 Teledyne H4RG-10 near infrared detector arrays, which present different instrument calibration challenges from previous weak lensing observations. Previous work has shown that correlation functions of flat field images are effective tools for disentangling linear and non-linear inter-pixel capacitance (IPC) and the brighter-fatter effect (BFE). Here we present a Fourier-domain treatment of the flat field correlations, which allows us to expand the previous formalism to all orders in IPC, BFE, and classical non-linearity. We show that biases in simulated flat field analyses in Paper I are greatly reduced through the use of this formalism. We then apply this updated formalism to flat field data from three WFIRST flight candidate detectors, and explore the robustness to variations in the analysis. We find that the BFE is present in all three detectors, and that its contribution to the flat field correlations dominates over the non-linear IPC. The magnitude of the BFE is such that the effective area of a pixel is increased by $(3.54\\pm0.03)\\times 10^{-7}$ for every electron deposited in a neighboring pixel. We compare IPC maps from flat field autocorrelation measurements to those obtained from the single pixel reset method and find a median difference of 0.113%. After further diagnosis of this difference, we ascribe it largely to an additional source of cross-talk, the vertical trailing pixel effect, and recommend further work to develop a model for this effect. These results represent a significant step toward calibration of the non-ideal effects in WFIRST detectors. 0 into PostgreSQL...\n",
      "Inserting test sample 994  The Brighter-fatter effect (BFE) is a phenomenon in which pixels in an image become brighter as more light is received due to the non-linear response of the detector. In this paper, we focus on BFE in near-infrared detectors and present a Fourier-domain treatment of flat field correlations. We apply this treatment to Wide Field Infrared Survey Telescope (WFIRST) detectors, which will be used to perform a large survey of the sky starting in the mid-2020s.\n",
      "\n",
      "Our analysis shows that the BFE in WFIRST is caused by a combination of non-linear electronic effects and correlated pixel-to-pixel variations in the detector response. Our Fourier-domain treatment is able to account for both effects, allowing us to better understand and model BFE in WFIRST. We find that the BFE in WFIRST will have a significant impact on the accuracy of sky surveys, particularly for faint sources that would otherwise be undetected.\n",
      "\n",
      "To mitigate the impact of BFE on WFIRST surveys, we propose a new technique for measuring and correcting for BFE using Fourier-domain analysis. This technique is able to accurately correct for BFE in flat-field images, resulting in a more uniform detector response. We demonstrate the effectiveness of this technique using data from the Hubble Space Telescope, which has a similar detector to WFIRST.\n",
      "\n",
      "Overall, our work provides a comprehensive analysis of BFE in near-infrared detectors and its impact on the accuracy of sky surveys. Our proposed Fourier-domain treatment and correction technique will be invaluable for ensuring the success of future wide-field surveys, such as the WFIRST survey, which will provide valuable insights into the nature of dark matter and dark energy. 1 into PostgreSQL...\n",
      "Inserting test sample 995  We explore magnetic field configurations that lead to the formation of magnetic spots on the surface of neutron stars, and to the displacement of the magnetic dipole axis. We find that a toroidally dominated magnetic field is essential for the generation of a single spot with a strong magnetic field.\n",
      "\n",
      "Once a spot forms, it survives for several million years, even after the total magnetic field has decayed significantly. We find that the dipole axis is not stationary with respect to the neutron star's surface and does not in general coincide with the location of the magnetic spot. This is due to non-axisymmetric instabilities of the toroidal field that displace the poloidal dipole axis at rates that may reach $0.4^{\\circ}$ per century. A misaligned poloidal dipole axis with the toroidal field leads to more significant displacement of the dipole axis than the fully aligned case. Finally we discuss the evolution of neutron stars with such magnetic fields on the $P-\\dot{P}$ diagram and the observational implications. We find that neutron stars spend a very short time before they cross the Death-Line of the $P-\\dot{P}$ diagram, compared to their characteristic ages. Moreover, the maximum intensity of their surface magnetic field is substantially higher than the dipole component of the field. We argue that SGR 0418+5729 could be an example of this type of behaviour, having a weak dipole field, yet hosting a magnetic spot responsible for its magnetar behaviour. The evolution on the pulse profile and braking index of the Crab pulsar, which are attributed to an increase of its obliquity, are compatible with the anticipated drift of the magnetic axis. 0 into PostgreSQL...\n",
      "Inserting test sample 996  Neutron stars are a particular class of celestial objects that offer a unique environment for studying matter under extreme gravity and electromagnetic fields. Among the variety of phenomena taking place in these systems, the drift of the magnetic axis and the formation of magnetic spots on the surface are of particular interest. In this work, we investigate the dynamics of neutron stars with toroidal magnetic fields using numerical simulations based on magnetohydrodynamics equations. Our results show that the magnetic axis drifts away from the rotational axis due to the non-axisymmetric structure of the toroidal field. The drift rate and direction correlate with the strength and shape of the toroidal field. Furthermore, we observe the formation of magnetic spots at the poleward regions of the star's surface. The size and location of the spots depend strongly on the initial configuration of the field and the rotation rate of the star. We also find that the magnetic spots generate strong currents and heating in the surrounding plasma. These phenomena have important implications for the observed properties of neutron stars, including their spectra, polarization, and timing behavior. Our simulations provide a theoretical framework for understanding the evolution of magnetic fields in neutron stars and may contribute to the explanation of puzzling observational features such as glitches and anti-glitches. 1 into PostgreSQL...\n",
      "Inserting test sample 997  This paper is devoted to study the circumstances favourable to detect circumstellar and circumbinary planets in well detached binary-star-systems using eclipse timing variations (ETVs). We investigated the dynamics of well detached binary star systems with a star separation from 0.5 to 3~AU, to determine the probability of the detection of such variations with ground based telescopes and space telescopes (like former missions CoRoT and Kepler and future space missions Plato, Tess and Cheops). For the chosen star separations both dynamical configurations (circumstellar and circumbinary) may be observable. We performed numerical simulations by using the full three-body problem as dynamical model. The dynamical stability and the ETVs are investigated by computing ETV maps for different masses of the secondary star and the exoplanet (Earth, Neptune and Jupiter size). In addition we changed the planet's and binary's eccentricities. We conclude that many amplitudes of ETVs are large enough to detect exoplanets in binary star systems. As an application, we prepared statistics of the catalogue of exoplanets in binary star systems which we introduce in this article and compared the statistics with our parameter-space which we used for our calculations. In addition to these statistics of the catalogue we enlarged them by the investigation of well detached binary star systems from several catalogues and discussed the possibility of further candidates. 0 into PostgreSQL...\n",
      "Inserting test sample 998  In recent years, the discovery of exoplanets has been revolutionized by transit and radial-velocity surveys. However, the detection and characterization of planets in binaries remains challenging, particularly for well-detached systems. Despite this difficulty, such systems offer unique opportunities for exoplanet studies, including the chance to study the impact of stellar companions on planet formation and evolution. In this paper, we discuss new prospects for observing and cataloguing exoplanets in well-detached binaries. We review recent progress in detecting and characterizing such exoplanets and explore some of the key questions that can be addressed using these systems. We also examine the challenges and limitations associated with these observations and consider future prospects for expanding this important area of exoplanet research. Ultimately, we argue that well-detached binary systems provide an important avenue for advancing our understanding of exoplanets and their formation, and we provide recommendations for future observations and analyses in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 999  The most spiral galaxies have a flat rotational velocity curve, according to the different observational techniques used in several wavelengths domain. In this work, we show that non-linear terms are able to balance the dispersive effect of the wave, thus reviving the observed rotational curve profiles without inclusion of any other but baryonic matter concentrated in the bulge and disk. In order to prove that the considered model is able to restore a flat rotational curve, Milky Way has been chosen as the best mapped galaxy to apply on. Using the gravitational N-body simulations with up to $10^7$ particles, we test this dynamical model in the case of the Milky Way with two different approaches. Within the direct approach, as an input condition in the simulation runs we set the spiral surface density distribution which is previously obtained as an explicit solution to non-linear Schr\\\"{o}dinger equation (instead of a widely used exponential disk approximation). In the evolutionary approach, we initialize the runs with different initial mass and rotational velocity distributions, in order to capture the natural formation of spiral arms, and to determine their role in the disk evolution. In both cases we are able to reproduce the stable and non-expanding disk structures at the simulation end times of $\\sim10^9$ years, with no halo inclusion. Although the given model doesn't take into account the velocity dispersion of stars and finite disk thickness, the results presented here still imply that non-linear effects can significantly alter the amount of dark matter which is required to keep the galactic disk in stable configuration. 0 into PostgreSQL...\n",
      "Inserting test sample 1000  The spiral structure of the galactic disk has long been a subject of fascination and study among astronomers attempting to understand the complex structure and behavior of our Milky Way. In particular, scientists have looked closely at the influence of this spiral structure on the rotational velocity curve of stars within the disk. \n",
      "\n",
      "In this study, we analyze data from a variety of sources, including observations from the Sloan Digital Sky Survey and the Gaia spacecraft, to investigate the relationship between the spiral structure of the disk and the rotational velocity of stars at various distances from the galactic center. Our findings suggest that the spiral arms have a significant impact on the velocity curve, with stars located in these structures exhibiting faster rotation than those located between arms.\n",
      "\n",
      "We propose a theoretical model to explain this phenomenon that takes into account the influence of gravitational forces on the motion of stars within the disk. Using this model, we are able to reproduce observed rotational velocity curves and predict the behavior of stars in regions of the galaxy where observational data is incomplete. \n",
      "\n",
      "Our results have important implications for our understanding of the large-scale structure of the Milky Way and may help shed new light on the origins of galactic spiral arms. Our findings also have important implications for studies of dark matter, as they suggest that the distribution of this invisible substance may be closely tied to the spiral structure of galaxies. Overall, this study represents an important step forward in our understanding of the dynamics of the galactic disk and the forces that shape it. 1 into PostgreSQL...\n",
      "Inserting test sample 1001  Properties of the mappings \\begin{align*} C&\\mapsto\\frac1{(2\\pi i)^2}\\int_{\\Gamma_1}\\int_{\\Gamma_2}f(\\lambda,\\mu)\\,R_{1,\\,\\lambda}\\,C\\, R_{2,\\,\\mu}\\,d\\mu\\,d\\lambda, C&\\mapsto\\frac1{2\\pi i}\\int_{\\Gamma}g(\\lambda)R_{1,\\,\\lambda}\\,C\\, R_{2,\\,\\lambda}\\,d\\lambda \\end{align*} are discussed; here $R_{1,\\,(\\cdot)}$ and $R_{2,\\,(\\cdot)}$ are pseudo-resolvents, i.~e., resolvents of bounded, unbounded, or multivalued linear operators, and $f$ and $g$ are analytic functions. Several applications are considered: a representation of the impulse response of a second order linear differential equation with operator coefficients, a representation of the solution of the Sylvester equation, and an exploration of properties of the differential of the ordinary functional calculus. 0 into PostgreSQL...\n",
      "Inserting test sample 1002  The topic of analytic functional calculus for two operators has been an area of active research in functional analysis. The paper presents a novel approach to analyzing the two operators which can be used to obtain meaningful results in a wide range of applications, including quantum mechanics and control theory. The study provides a thorough analysis on the fundamental concepts required for understanding the properties of the two operators, and demonstrates how this analysis can be used to deduce certain key properties of the operators. The results obtained in the study form the foundation for further developments in the analytic function theory and related areas of research, paving the way for new and important advances in the field of functional analysis. 1 into PostgreSQL...\n",
      "Inserting test sample 1003  We investigate $B \\to K_0^*(1430) K^*$ decays in the perturbative QCD(pQCD) factorization approach, where $B$ denotes $B_u$, $B_d$ and $B_s$ meson respectively, and the scalar $K_0^*(1430)$ is considered as a meson based on the model of conventional two-quark structure. With the light-cone distribution amplitude of $K_0^*(1430)$ defined in two scenarios, namely Scenario 1 and Scenario 2, we make the first estimation for the branching ratios and CP-violating asymmetries for those concerned decay modes in the pQCD factorization approach. For all considered $B \\to K_0^*(1430) K^*$ decays in this paper, only one preliminary upper limit on the branching ratio of $B^0 \\to {K_0^*(1430)}^0 \\bar{K}^{*0}$ measured at 90% C.L. by Belle Collaboration is available now. It is therefore of great interest to examine the predicted physical quantities at two $B$ factories, Large Hadron Collider experiments, and forthcoming Super-$B$ facility, then test the reliability of the pQCD approach employed to study the considered decay modes involving a $p$-wave scalar meson as one of the final state mesons. Furthermore, these pQCD predictions combined with the future precision measurements are also helpful to explore the complicated QCD dynamics involved in the light scalars. 0 into PostgreSQL...\n",
      "Inserting test sample 1004  In this paper, we investigate the branching ratios and CP violations of B âŸ¶ K*0(1430)K* decays within a perturbative QCD framework. We find non-zero CP asymmetry in some decay channels, indicating the possibility of direct CP violation. Our results suggest that the K* resonance plays a critical role in these decays, leading to a sizeable contribution to the branching ratios. Through a theoretical analysis, we identify the key operators that dominate these processes and provide a detailed exploration of their properties. Our computational framework utilizes a model-independent approach, allowing for a robust prediction of branching ratios and CP violations. We compare our findings with previous experimental measurements and predict the branching ratios of several unobserved decay channels. Our results provide valuable insights into the dynamics of these B meson decays and reveal the importance of the K* resonance in CP violation and branching ratios. These findings will facilitate future experimental measurements and contribute to the ongoing research in particle physics. 1 into PostgreSQL...\n",
      "Inserting test sample 1005  The chiral topological superconductor in two dimensions has a full pairing gap in the bulk and a single chiral Majorana state at the edge. The vortex of the chiral superconducting state carries a Majorana zero mode which is responsible for the non-abelian statistics of the vortices. Despite intensive searches, this novel superconducting state has not yet been identified in nature. In this paper, we consider a quantum Hall or a quantum anomalous Hall state near the plateau transition, and in proximity to a fully gapped s-wave superconductor. We show that this hybrid system realizes the long sought after chiral superconductor state, and propose several experimental methods for its observation. 0 into PostgreSQL...\n",
      "Inserting test sample 1006  The emergence of topological superconductivity in a chiral quantum Hall state represents a significant development in the field of condensed matter physics. In this work, we present a theoretical study of a chiral topological superconductor formed from a quantum Hall state in a two-dimensional electron gas with strong interactions. By investigating the dynamic behavior of vortices and Majorana zero modes, we demonstrate the potential of this system for topological computation and quantum information processing. Our analysis reveals a rich phase diagram, highlighting the role of magnetic field, chemical potential, and superconducting gap on the topological properties of the system. These results offer a promising avenue towards developing robust topological quantum computing devices. 1 into PostgreSQL...\n",
      "Inserting test sample 1007  A new general procedure for a priori selection of more predictable events from a time series of observed variable is proposed. The procedure is applicable to time series which contains different types of events that feature significantly different predictability, or, in other words, to heteroskedastic time series. A priori selection of future events in accordance to expected uncertainty of their forecasts may be helpful for making practical decisions.\n",
      "\n",
      "The procedure first implies creation of two neural network based forecasting models, one of which is aimed at prediction of conditional mean and other - conditional dispersion, and then elaboration of the rule for future event selection into groups of more and less predictable events. The method is demonstrated and tested by the example of the computer generated time series, and then applied to the real world time series, Dow Jones Industrial Average index. 0 into PostgreSQL...\n",
      "Inserting test sample 1008  This research paper investigates the selection of future events based on a time series, with particular emphasis on forecasting uncertainty. The study proposes a method for selecting the most relevant events based on the accuracy and reliability of their forecasted values. The accuracy and uncertainty of forecasts are quantified through statistical analysis of historical data and statistical models. The proposed method is compared with traditional methods of event selection, and it is shown that the proposed method is superior in providing reliable forecasts. The study provides practical recommendations for selecting future events based on a time series, taking into account the level of uncertainty associated with each forecast. This research paper contributes to the field of forecasting and provides a reliable framework for selecting future events from a time series. 1 into PostgreSQL...\n",
      "Inserting test sample 1009  We consider group-valued cocycles over a partially hyperbolic diffeomorphism which is accessible volume-preserving and center bunched. We study cocycles with values in the group of invertible continuous linear operators on a Banach space. We describe properties of holonomies for fiber bunched cocycles and establish their Holder regularity. We also study cohomology of cocycles and its connection with holonomies. We obtain a result on regularity of a measurable conjugacy, as well as a necessary and sufficient condition for existence of a contionuous conjugacy between two cocycles. 0 into PostgreSQL...\n",
      "Inserting test sample 1010  In this paper, we study the holonomies and cohomology of cocycles over partially hyperbolic diffeomorphisms. We prove that the cohomology of the cocycle associated to a partially hyperbolic diffeomorphism is naturally isomorphic to the cohomology of the base dynamics. This result is a consequence of the stable and unstable holonomies, which we study in detail. As an application of our main result, we show the existence of periodic orbits for a large class of partially hyperbolic systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1011  The analysis of modulation schemes for the physical layer network-coded two way relaying scenario is presented which employs two phases: Multiple access (MA) phase and Broadcast (BC) phase. It was shown by Koike-Akino et. al. that adaptively changing the network coding map used at the relay according to the channel conditions greatly reduces the impact of multiple access interference which occurs at the relay during the MA phase. Depending on the signal set used at the end nodes, deep fades occur for a finite number of channel fade states referred as the singular fade states. The singular fade states fall into the following two classes: The ones which are caused due to channel outage and whose harmful effect cannot be mitigated by adaptive network coding are referred as the \\textit{non-removable singular fade states}. The ones which occur due to the choice of the signal set and whose harmful effects can be removed by a proper choice of the adaptive network coding map are referred as the \\textit{removable} singular fade states. In this paper, we derive an upper bound on the average end-to-end Symbol Error Rate (SER), with and without adaptive network coding at the relay, for a Rician fading scenario. It is shown that without adaptive network coding, at high Signal to Noise Ratio (SNR), the contribution to the end-to-end SER comes from the following error events which fall as $\\text{SNR}^{-1}$: the error events associated with the removable singular fade states, the error events associated with the non-removable singular fade states and the error event during the BC phase. In contrast, for the adaptive network coding scheme, the error events associated with the removable singular fade states contributing to the average end-to-end SER fall as $\\text{SNR}^{-2}$ and as a result the adaptive network coding scheme provides a coding gain over the case when adaptive network coding is not used. 0 into PostgreSQL...\n",
      "Inserting test sample 1012  This study analyzed the performance of adaptive physical layer network coding (APLNC) for wireless two-way relaying. APLNC is a promising technique for enhancing the throughput and reliability of wireless communication systems, especially in two-way relaying scenarios. In this paper, we proposed an APLNC scheme that adapts to the channel conditions and packet loss rates to optimize the system performance.\n",
      "\n",
      "To evaluate the performance of our proposed APLNC scheme, we designed an experimental setup with two transceivers and a relay node. We conducted extensive simulations in different channel conditions with varying network parameters. Our results show that the proposed APLNC scheme significantly improves the system throughput compared to conventional network coding schemes. Specifically, our scheme achieved a higher throughput gain of up to 50% in fading channels and 20% in symmetric channels. Furthermore, our proposed scheme outperforms the state-of-the-art APLNC schemes by up to 45% in fading channels and 18% in symmetric channels.\n",
      "\n",
      "We also investigated the impact of packet loss on the system performance. Our simulations indicate that our scheme is more resilient to packet loss compared to conventional network coding and state-of-the-art APLNC techniques. Specifically, our proposed scheme performed up to 40% better than conventional network coding and up to 25% better than state-of-the-art APLNC techniques under high packet loss rates.\n",
      "\n",
      "Overall, our proposed adaptive physical layer network coding scheme demonstrates significant improvements in the system throughput and resilience to packet loss in wireless two-way relaying scenarios. Our findings highlight the potential of APLNC technique for enhancing the performance of wireless communication systems, especially in two-way relaying applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1013  We present time-resolved photometry of the cataclysmic variable (CV) PTF1J2224+17 obtained during 4 nights in October 2018 and January 2019 from Inastars observatory. The object is variable on a period of 103.82 min.\n",
      "\n",
      "Archival Catalina Real-Time Transient Survey (CRTS), PTF, and ZTF-data show frequent changes between high and low states. Based on its photometric properties and the cyclotron humps in the identification spectrum the object is certainly classified as an AM Herculis star (or polar) with a likely magnetic field strength of B ~ 65 MG. Its accretion duty cycle was estimated from nine years of photometric monitoring to be about 35 %. 0 into PostgreSQL...\n",
      "Inserting test sample 1014  PTF1J2224+17 is a highly magnetic white dwarf with an incredibly short rotation period of just over an hour. This makes it a member of the rare class of objects known as high-field polars. The system was discovered via analysis of photometric data collected by the Palomar Transient Factory and follow-up spectroscopic observations. We report on the measurement of the system parameters and model its broadband spectrum, which exhibit strong rotationally modulated accretion signatures in the UV and optical bands. The accreting donor in this system is likely a small, late M-type star. Our findings have important implications for understanding accretion and magnetic field physics in extreme environments. 1 into PostgreSQL...\n",
      "Inserting test sample 1015  Combining two classical notions in extremal combinatorics, the study of Ramsey-Tur\\'an theory seeks to determine, for integers $m\\le n$ and $p \\leq q$, the number $\\mathsf{RT}_p(n,K_q,m)$, which is the maximum size of an $n$-vertex $K_q$-free graph in which every set of at least $m$ vertices contains a $K_p$.\n",
      "\n",
      "Two major open problems in this area from the 80s ask: (1) whether the asymptotic extremal structure for the general case exhibits certain periodic behaviour, resembling that of the special case when $p=2$; (2) constructing analogues of Bollob\\'as-Erd\\H{o}s graphs with densities other than $1/2$.\n",
      "\n",
      "We refute the first conjecture by witnessing asymptotic extremal structures that are drastically different from the $p=2$ case, and address the second problem by constructing Bollob\\'as-Erd\\H{o}s-type graphs using high dimensional complex spheres with all rational densities. Some matching upper bounds are also provided. 0 into PostgreSQL...\n",
      "Inserting test sample 1016  Ramsey-TurÃ¡n theory provides a framework for understanding how large a graph can be without containing certain structures. One of the central ideas of this theory is the use of geometric constructions to prove lower bounds on the size of these structures. In this paper, we explore new geometric constructions that can be used to investigate Ramsey-TurÃ¡n theory. We begin by studying the simplest case of the problem, where we seek to identify the minimum number of edges in a graph that avoids a given set of cliques. Along the way, we develop new techniques for analyzing the structure of graphs, including the use of spectral methods and the theory of extremal graph theory. Our results demonstrate the power of geometric constructions for understanding fundamental problems in graph theory and suggest new avenues for future research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 1017  We study the geometric interpretation of two dimensional rational conformal field theories, corresponding to sigma models on Calabi-Yau manifolds. We perform a detailed study of RCFT's corresponding to T^2 target and identify the Cardy branes with geometric branes. The T^2's leading to RCFT's admit ``complex multiplication'' which characterizes Cardy branes as specific D0-branes. We propose a condition for the conformal sigma model to be RCFT for arbitrary Calabi-Yau n-folds, which agrees with the known cases. Together with recent conjectures by mathematicians it appears that rational conformal theories are not dense in the space of all conformal theories, and sometimes appear to be finite in number for Calabi-Yau n-folds for n>2. RCFT's on K3 may be dense. We speculate about the meaning of these special points in the moduli spaces of Calabi-Yau n-folds in connection with freezing geometric moduli. 0 into PostgreSQL...\n",
      "Inserting test sample 1018  In this paper, we investigate the deep connection between rational conformal field theories and complex multiplication. Rational conformal field theories play a central role in theoretical physics, providing insight into the behavior of systems at critical points. Complex multiplication, on the other hand, is a well-studied topic in number theory, intimately tied to the theory of elliptic curves. We show that there is a rich interplay between these two seemingly unrelated fields, with rational conformal field theories providing a crucial bridge between the two. Our results shed light on the deep connections between geometry and physics and demonstrate the power of a unified perspective in understanding seemingly disparate areas of mathematics. Ultimately, our work lays the groundwork for future developments in the areas of physics, number theory, and their intersections. 1 into PostgreSQL...\n",
      "Inserting test sample 1019  Older GCE models predict [K/Fe] ratios as much as 1 dex lower than those inferred from stellar observations. Abundances of potassium are mainly based on analyses of the 7698 $\\AA$ resonance line, and the discrepancy between models and observations is in part caused by the LTE assumption. We study the statistical equilibrium of KI, focusing on the non-LTE effects on the $7698 \\ \\AA$ line. We aim to determine how non-LTE abundances of K can improve the analysis of its chemical evolution, and help to constrain the yields of models.\n",
      "\n",
      "We construct a model atom that employs the most up-to-date data. In particular, we calculate and present inelastic e+K collisional excitation cross-sections from the convergent close-coupling and the $B$-Spline $R$-matrix methods, and H+K collisions from the two-electron model. We constructed a fine grid of non-LTE abundance corrections that span $4000<\\teff / \\rm{K}<8000$, $0.50<\\lgg<5.00$, $-5.00<\\feh<+0.50$, and applied the corrections to abundances from the literature. In concordance with previous studies, we find severe non-LTE effects in the $7698 \\ \\AA$ line, which is stronger in non-LTE with abundance corrections that can reach $\\sim-0.7\\,\\dex$. We explore the effects of atmospheric inhomogeneity by computing a full 3D non-LTE stellar spectrum of KI for a test star. We find that 3D is necessary to predict a correct shape of the resonance 7698 $\\AA$ line, but the line strength is similar to that found in 1D non-LTE. Our non-LTE abundance corrections reduce the scatter and change the cosmic trends of literature K abundances. In the regime [Fe/H]$\\lesssim-1.0$ the non-LTE abundances show a good agreement with the GCE model with yields from rotating massive stars. The reduced scatter of the non-LTE corrected abundances of a sample of solar twins shows that line-by-line differential analysis techniques cannot fully compensate for systematic modelling errors. 0 into PostgreSQL...\n",
      "Inserting test sample 1020  Non-local thermodynamic equilibrium (non-LTE) analysis of potassium (K I) in late-type stars has been a long-standing challenge in stellar astrophysics due to the complex nature of the atomic structure and non-LTE effects on various spectral features. In this study, we present a comprehensive non-LTE analysis of K I based on high-resolution spectroscopic observations of a sample of late-type stars. Our analysis employs the latest atomic data and extensive model atmospheres to derive accurate K abundances and investigate the non-LTE effects on K spectral lines.\n",
      "\n",
      "We find that the non-LTE effects on K abundances are significant and depend on various atmospheric parameters such as temperature, metallicity, and surface gravity. We also find that the non-LTE effects are more prominent in K lines with low excitation potentials and strong hyperfine structure, which are often used as benchmark indicators in stellar astrophysics.\n",
      "\n",
      "Our results provide new insights into the non-LTE effects on K spectral lines and the underlying atomic physics. This work has important implications for the accurate determination of K abundances in late-type stars, which are crucial for understanding the chemical evolution of galaxies and the formation of planetary systems.\n",
      "\n",
      "In addition, our non-LTE analysis of K I in late-type stars serves as a benchmark for future studies of other heavy elements in similar stellar atmospheres. Overall, this study demonstrates the importance of incorporating non-LTE effects in the analysis of stellar spectra and highlights the need for further improvements in the atomic data and model atmospheres used in such studies. 1 into PostgreSQL...\n",
      "Inserting test sample 1021  Simulated environments are increasingly used by trading firms and investment banks to evaluate trading strategies before approaching real markets.\n",
      "\n",
      "Backtesting, a widely used approach, consists of simulating experimental strategies while replaying historical market scenarios. Unfortunately, this approach does not capture the market response to the experimental agents' actions. In contrast, multi-agent simulation presents a natural bottom-up approach to emulating agent interaction in financial markets. It allows to set up pools of traders with diverse strategies to mimic the financial market trader population, and test the performance of new experimental strategies.\n",
      "\n",
      "Since individual agent-level historical data is typically proprietary and not available for public use, it is difficult to calibrate multiple market agents to obtain the realism required for testing trading strategies. To addresses this challenge we propose a synthetic market generator based on Conditional Generative Adversarial Networks (CGANs) trained on real aggregate-level historical data. A CGAN-based \"world\" agent can generate meaningful orders in response to an experimental agent. We integrate our synthetic market generator into ABIDES, an open source simulator of financial markets. By means of extensive simulations we show that our proposal outperforms previous work in terms of stylized facts reflecting market responsiveness and realism. 0 into PostgreSQL...\n",
      "Inserting test sample 1022  The key to realistic market simulations is generating data that accurately captures the intricacies of financial systems. In this paper, we present a novel approach to achieving this using Generative Adversarial Networks (GANs). GANs are a type of machine learning algorithm that can generate synthetic data that closely resembles real data. We apply GANs to financial market simulation by training them using real market data to create synthetic data that maintains the statistical properties of the original dataset. Our approach allows us to generate new data points that capture the complex relationships and dependencies between different financial variables. We show that our GAN-based market simulator can be used for a variety of applications, including risk management, algorithmic trading, and financial forecasting. Our experiments demonstrate the efficacy of our approach in capturing the underlying market dynamics and generating realistic financial time series. Overall, these results suggest that GANs provide a promising path towards realistic market simulations that can help improve financial decision-making processes. 1 into PostgreSQL...\n",
      "Inserting test sample 1023  This paper presents detailed analysis of large-scale peculiar motions derived from a sample of ~ 700 X-ray clusters and cosmic microwave background (CMB) data obtained with WMAP. We use the kinematic Sunyaev-Zeldovich (KSZ) effect combining it into a cumulative statistic which preserves the bulk motion component with the noise integrated down. Such statistic is the dipole of CMB temperature fluctuations evaluated over the pixels of the cluster catalog (Kashlinsky & Atrio-Barandela 2000). To remove the cosmological CMB fluctuations the maps are Wiener-filtered in each of the eight WMAP channels (Q, V, W) which have negligible foreground component. Our findings are as follows: The thermal SZ (TSZ) component of the clusters is described well by the Navarro-Frenk-White profile expected if the hot gas traces the dark matter in the cluster potential wells. Such gas has X-ray temperature decreasing rapidly towards the cluster outskirts, which we demonstrate results in the decrease of the TSZ component as the aperture is increased to encompass the cluster outskirts. We then detect a statistically significant dipole in the CMB pixels at cluster positions. Arising exclusively at the cluster pixels this dipole cannot originate from the foreground or instrument noise emissions and must be produced by the CMB photons which interacted with the hot intracluster gas via the SZ effect. The dipole remains as the monopole component, due to the TSZ effect, vanishes within the small statistical noise out to the maximal aperture where we still detect the TSZ component. We demonstrate with simulations that the mask and cross-talk effects are small for our catalog and contribute negligibly to the measurements. The measured dipole thus arises from the KSZ effect produced by the coherent large scale bulk flow motion. 0 into PostgreSQL...\n",
      "Inserting test sample 1024  This paper presents a detailed technical account of a measurement of the large-scale peculiar velocities of clusters of galaxies. To achieve this, we employed a methodology based on the so-called \"dipole\" estimator, computed from the redshifts of a large number of galaxy clusters. This estimator allows us to infer the motion of these clusters with respect to the cosmic microwave background radiation, which provides a reference frame for cosmological observations. \n",
      "\n",
      "In order to obtain accurate results, we carefully selected the clusters to be included in our analysis, removing those affected by observational biases or contaminated by foreground structures. We also used a range of statistical tools and simulations to assess the robustness of our measurements and to estimate the uncertainties involved. \n",
      "\n",
      "Our analysis shows that the peculiar velocities of clusters of galaxies are consistent with the predictions of the standard cosmological model, with some variations depending on the specific sample and the redshift range considered. These results offer insights into the large-scale structure of the Universe and the dynamics of its components, contributing to the ongoing effort to understand its origins and evolution. \n",
      "\n",
      "In conclusion, this paper provides a comprehensive description of the technical aspects of a measurement of large-scale peculiar velocities of galaxy clusters, highlighting the challenges and solutions involved in such an endeavor. Our results confirm the validity of the prevailing cosmological paradigm and open up new avenues for further investigations of the cosmos. 1 into PostgreSQL...\n",
      "Inserting test sample 1025  In the redshift range z = 0-1, the gamma ray burst (GRB) redshift distribution should increase rapidly because of increasing differential volume sizes and strong evolution in the star formation rate. This feature is not observed in the Swift redshift distribution and to account for this discrepancy, a dominant bias, independent of the Swift sensitivity, is required. Furthermore, despite rapid localization, about 40-50% of Swift and pre-Swift GRBs do not have a measured redshift. We employ a heuristic technique to extract this redshift bias using 66 GRBs localized by Swift with redshifts determined from absorption or emission spectroscopy. For the Swift and HETE+BeppoSAX redshift distributions, the best model fit to the bias in z < 1 implies that if GRB rate evolution follows the SFR, the bias cancels this rate increase. We find that the same bias is affecting both Swift and HETE+BeppoSAX measurements similarly in z < 1. Using a bias model constrained at a 98% KS probability, we find that 72% of GRBs in z < 2 will not have measurable redshifts and about 55% in z > 2. To achieve this high KS probability requires increasing the GRB rate density in small z compared to the high-z rate. This provides further evidence for a low-luminosity population of GRBs that are observed in only a small volume because of their faintness. 0 into PostgreSQL...\n",
      "Inserting test sample 1026  Gamma-ray bursts (GRBs), the most luminous explosions in the universe, provide vital clues about the high-energy astrophysical processes behind the emission of these events. Redshift measurements have played a critical role in our understanding of GRBs and their host galaxies. However, in recent years, the number of reported GRBs without measured redshifts has grown rapidly. The lack of redshift data severely limits our ability to study the properties of GRBs and their host galaxies and to derive accurate cosmological constraints. In this paper, we review the current state of knowledge on missing GRB redshifts, including the potential biases associated with current observational methods. We also discuss ongoing efforts to obtain redshifts for these events through various techniques, such as follow-up observations of host galaxies and the use of spectral templates. Additionally, we highlight the importance of future missions, including the upcoming James Webb Space Telescope and the Wide Field Infrared Survey Telescope, in providing new opportunities to tackle this outstanding issue and to shed light on the nature of GRBs and their possible connection to other sources of high-energy emission. 1 into PostgreSQL...\n",
      "Inserting test sample 1027  Charge order affects most of the electronic properties but is believed not to alter the spin arrangement since the magnetic susceptibility remains unchanged.\n",
      "\n",
      "We present electron-spin-resonance experiments on quasi-one-dimensional (TMTTF)2X salts (X= PF6, AsF6 and SbF6), which reveal that the magnetic properties are modified below TCO when electronic ferroelectricity sets in. The coupling of anions and organic molecules rotates the g-tensor out of the molecular plane creating magnetically non-equivalent sites on neighboring chains at domain walls. Due to anisotropic Zeeman interaction a novel magnetic interaction mechanism in the charge-ordered state is observed as a doubling of the rotational periodicity of Delta H. 0 into PostgreSQL...\n",
      "Inserting test sample 1028  In this research, we investigate the interplay between charge and magnetic degrees of freedom in molecular spin chains consisting of lithiated copper phthalocyanine species. Our findings reveal that charge order has a significant impact on magnetic symmetry, leading to the formation of domains with differing spin states, which is evidenced by our advanced theoretical simulations and experimental measurements. This novel electronic order in magnetic materials could be exploited in future applications for data storage and transfer, as well as in the development of quantum technologies. Overall, our work provides new insights into the fundamental physics of molecular spin systems and their potential for technological innovation. 1 into PostgreSQL...\n",
      "Inserting test sample 1029  We study the decoherence of a qubit weakly coupled to frustrated spin baths.\n",
      "\n",
      "We focus on spin-baths described by the classical Ising spin glass and the quantum random transverse Ising model which are known to have complex thermodynamic phase diagrams as a function of an external magnetic field and temperature. Using a combination of numerical and analytical methods, we show that for baths initally in thermal equilibrium, the resulting decoherence is highly sensitive to the nature of the coupling to the environment and is qualitatively different in different parts of the phase diagram. We find an unexpected strong non-Markovian decay of the coherence when the random transverse Ising model bath is prepared in an initial state characterized by a finite temperature paramagnet. This is contrary to the usual case of exponential decay (Markovian) expected for spin baths in finite temperature paramagnetic phases, thereby illustrating the importance of the underlying non-trivial dynamics of interacting quantum spinbaths. 0 into PostgreSQL...\n",
      "Inserting test sample 1030  This research paper investigates the decoherence of a qubit under the influence of disordered magnetic environments. Weak coupling analysis is utilized to examine the qubit's behavior in the presence of perturbations arising from the surrounding environment. The impact of the environment on the qubit's coherence is studied by examining the time evolution of the qubit's state. Theoretical calculations are performed to model the expected behavior of the qubit and compare it with experimental observations. The results demonstrate that even weak coupling with the disordered magnetic environments can significantly affect the coherence of the qubit. Careful engineering of the magnetic environment, or suitable error correction methods, can effectively mitigate the decoherence effects. This work provides insight into the vulnerabilities of qubits to environmental perturbations and will help in the development of more robust quantum systems for practical applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1031  Elements with low First Ionization Potential (FIP) are known to be three to four times more abundant in active region loops of the solar corona than in the photosphere. There have been observations suggesting that this observed \"FIP bias\" may be different in other parts of the solar corona and such observations are thus important in understanding the underlying mechanism. The Solar X-ray Monitor (XSM) on board the Chandrayaan-2 mission carried out spectroscopic observations of the Sun in soft X-rays during the 2019-20 solar minimum, considered to be the quietest solar minimum of the past century. These observations provided a unique opportunity to study soft X-ray spectra of the quiescent solar corona in the absence of any active regions. By modelling high resolution broadband X-ray spectra from XSM, we estimate the temperature and emission measure during periods of possibly the lowest solar X-ray intensity.\n",
      "\n",
      "We find that the derived parameters remain nearly constant over time with a temperature around 2 MK, suggesting the emission is dominated by X-ray Bright Points (XBPs). We also obtain the abundances of Mg, Al, and Si relative to H, and find that the FIP bias is ~2, lower than the values observed in active regions. 0 into PostgreSQL...\n",
      "Inserting test sample 1032  In this paper, we present the results of observations made by Chandrayaan-2 XSM towards the quiet sun during the deepest solar minimum of the past century. The XSM (X-ray Spectrometer) instrument onboard the spacecraft has been used to study the elemental abundances in the quiescent corona. Our findings show that the coronal abundances of Ne, Mg, Si, S, Ca, and Fe exhibit a significant enhancement in comparison to their photospheric values. Moreover, we found a strong correlation between the abundance levels of elements with different first ionization potential (FIP) values and their spatial distribution in the quiescent solar corona. We have also studied the spatial distribution of Fe ions in the solar equatorial region and found that the coronal hole and the quiet sun regions have different levels of Fe ionization. Our results have significant implications for understanding the processes responsible for plasma heating in the solar corona and the role of the solar magnetic field in regulating coronal plasma dynamics. This study presents valuable insights into the fundamental properties of the solar atmosphere and provides a unique opportunity to further investigate the coronal heating problem. 1 into PostgreSQL...\n",
      "Inserting test sample 1033  We present the analysis of the inclusive $K^{0}$ production in p+p and p+Nb collisions measured with the HADES detector at a beam kinetic energy of 3.5 GeV. Data are compared to the GiBUU transport model. The data suggest the presence of a repulsive momentum-dependent kaon potential as predicted by the Chiral Perturbation Theory (ChPT). For the kaon at rest and at normal nuclear density, the ChPT potential amounts to $\\approx 35$ MeV. A detailed tuning of the kaon production cross sections implemented in the model has been carried out to reproduce the experimental data measured in p+p collisions. The uncertainties in the parameters of the model were examined with respect to the sensitivity of the experimental results from p+Nb collisions to the in-medium kaon potential. 0 into PostgreSQL...\n",
      "Inserting test sample 1034  This paper investigates the proton-induced production of $K^{0}$ at 3.5 GeV, and the possible medium effects that may influence this process. The experiment was carried out using a K$^{0}$ spectrometer at the Juelich Cooler Synchrotron, with the aim of dissecting the nature of the produced mesons in a nuclear medium. The results of the study were analyzed with respect to their dependence on the kinematic variables, and compared to theoretical predictions based on conventional models of meson production. The obtained data provides evidence for medium effects, which noticeably alter the production yields and kinematics of $K^{0}$ mesons. These findings may have significant impact on the understanding of the mechanisms behind meson production in nuclear processes, and on the interpretation of experimental measurements in this domain. 1 into PostgreSQL...\n",
      "Inserting test sample 1035  In this paper, we proposed the GLHUA double layer cloak; proved the properties of GLHUA double layer cloak; Using GL no scattering inversion and the pre cloak condition 6.1 to 6.4 in paper [1], we create GLHUA outer layer cloak radial relative parameter and angular relative parameter theoretically.\n",
      "\n",
      "We proved theorem 4.1 to theorem 4.4 that the phase velocity of the electromagnetic wave in GLHUA outer layer cloak is less than light speed and tends to zero at the boundary $r=R_1$, the EM wave tends to zero at $r=R1$; When source $r_s > R_2$ in the outside of the cloak and observer $r_o < R_o$ in the concealment, then $\\vec E(\\vec r_o)=0$ and $\\vec H(\\vec r_o)=0$ that is proved in theorem 5.1 and 5.3. We prove that the EM wave excited in outside of the cloak can not propagation penetrate into the concealment. In theorem 5.4 to 5.6, we rigorously proved the EM wave excited in outside of the cloak can not be disturbed by the cloak. In theorem 6.1 to theorem 6.6, we prove that the EM wave excited in the concealment can not propagate to outside of GLHUA inner cloak. we prove that the EM wave excited in the concealment can not be disturbed by the cloak. We theoretically prove that the GLHUA double cloak is invisible cloak with concealment and with relative parameter not less than 1; the GLHUA double layer cloak is practicable. In December of 2016, we submitted 3 papers to arXiv for GLHUA double layer cloak with relative parameters not less than 1. This is the third paper. The task and content of the three paper are different from each other. The second paper [1] and this Paper are theoretical base and proof of the subjects in paper arXiv:1612.02857. We find an exact analytical EM wave field of Maxwell EM equation in GLHUA cloak and mirage bridge wave. Patent of the GLHUA EM cloaks,GLHUA sphere and GL modeling and inversion methods are reserved by authors in GL Geophysical Laboratory. 0 into PostgreSQL...\n",
      "Inserting test sample 1036  In this study, we present a theoretical proof for the GLHUA EM invisible double layer cloak using GL no scattering modeling and inversion. The development of invisibility cloaks has been an important area of research in the field of electromagnetics, and the proposed GLHUA EM cloak offers a new approach to achieving optical invisibility.\n",
      "\n",
      "The GLHUA EM cloak is based on the concept of a double layer, where the inner layer is made of a material with a high refractive index and the outer layer is made of a material with a low refractive index. The two layers are designed to work together to bend light around the object being cloaked, rendering it invisible to the human eye.\n",
      "\n",
      "To prove the effectiveness of the GLHUA EM cloak, we employed the GL no scattering modeling technique. This approach involves solving the Maxwell equations without using the conventional boundary conditions that are typically used in optics. By doing so, we were able to demonstrate that the GLHUA EM cloak can suppress scattering and reflection of electromagnetic waves, thereby enabling the object to remain invisible.\n",
      "\n",
      "Furthermore, we developed an inversion technique to analyze the cloaking performance of the GLHUA EM cloak. This approach involves inverting the scattering parameters to obtain the refractive index profile of the cloak. By applying this technique, we were able to confirm that the GLHUA EM cloak is capable of achieving near-perfect invisibility across a wide range of wavelengths.\n",
      "\n",
      "In conclusion, our study provides a theoretical proof for the GLHUA EM invisible double layer cloak using GL no scattering modeling and inversion. Our findings demonstrate the potential of this innovative approach for achieving optical invisibility in a practical and effective manner. We believe that our work will pave the way for further research in this important area of electromagnetics and lead to the development of new and improved cloaking technologies. 1 into PostgreSQL...\n",
      "Inserting test sample 1037  We consider a system of coupled cubic Schr\\\"odinger equations in one space dimension \\begin{equation*} \\begin{cases} i \\partial_t u + \\partial_x^2 u +(|u|^2 + \\omega |v|^2) u =0\\\\ i \\partial_t v + \\partial_x^2 v+ (|v|^2 + \\omega |u|^2) v=0 \\end{cases}\\quad (t,x)\\in {\\bf R}\\times{\\bf R}, \\end{equation*} in the non-integrable case $0 < \\omega < 1$.\n",
      "\n",
      "First, we justify the existence of a symmetric 2-solitary wave with logarithmic distance, more precisely a solution of the system satisfying \\[ \\lim_{t\\to +\\infty}\\left\\| \\begin{pmatrix} u(t) \\\\ v(t)\\end{pmatrix} - \\begin{pmatrix} e^{it}Q (\\cdot - \\frac{1}{2} \\log (\\Omega t) - \\frac{1}{4} \\log \\log t) \\\\ e^{it}Q (\\cdot + \\frac{1}{2} \\log (\\Omega t) + \\frac{1}{4} \\log \\log t)\\end{pmatrix}\\right\\|_{H^1\\times H^1} = 0\\] where $Q = \\sqrt{2}{\\rm sech}$ is the explicit solution of $ Q'' - Q + Q^3 = 0$ and $\\Omega>0$ is a constant.\n",
      "\n",
      "This result extends to the non-integrable case the existence of symmetric 2-solitons with logarithmic distance known in the integrable case $\\omega=0$ and $\\omega=1$. Such strongly interacting symmetric $2$-solitary waves were also previously constructed for the non-integrable scalar nonlinear Schr\\\"odinger equation in any space dimension and for any energy-subcritical power nonlinearity.\n",
      "\n",
      "Second, under the conditions $0<c<1$ and $0<\\omega < \\frac 12 c(c+1)$, we construct solutions of the system satisfying \\[ \\lim_{t\\to +\\infty}\\left\\| \\begin{pmatrix}u(t) \\\\ v(t)\\end{pmatrix} - \\begin{pmatrix}e^{i c^2 t}Q_c (\\cdot - \\frac{1}{(c+1)c} \\log (\\Omega_c t) ) \\\\ e^{i t} Q (\\cdot + \\frac{1}{c+1} \\log (\\Omega_c t))\\end{pmatrix} \\right\\|_{H^1\\times H^1}=0\\] where $Q_c(x)=cQ(cx)$ and $\\Omega_c>0$ is a constant. Such logarithmic regime with non-symmetric solitons does not exist in the integrable cases $\\omega=0$ and $\\omega=1$ and is still unknown in the non-integrable scalar case. 0 into PostgreSQL...\n",
      "Inserting test sample 1038  This study is concerned with the construction of 2-solitons for the one-dimensional cubic Schrodinger system with logarithmic distance. The investigation of nonlinear wave equations is an important topic in mathematics and has been widely studied in physics, chemistry, and engineering. Solitons are localized waves that have the property of maintaining their shape and speed while interacting with other waves. The cubic Schrodinger equation is a particular type of nonlinear wave equation that models a wide range of physical phenomena, including the propagation of light in optical fibers and the behavior of Bose-Einstein condensates.\n",
      "\n",
      "Our research is based on the study of the integrability properties of the cubic Schrodinger equation, which allows the construction of soliton solutions with explicit formulas. In particular, we focus on the so-called 2-soliton solutions, which are formed by the interaction of two solitons. The logarithmic distance parameter is introduced to model the situation in which the two solitons are close to each other.\n",
      "\n",
      "The main result of our study is the explicit construction of 2-soliton solutions with logarithmic distance for the cubic Schrodinger equation. We derive a formula for the 2-soliton solution and provide its asymptotic expansion. Moreover, we investigate the long-time behavior of the 2-soliton solution and show that it behaves like the sum of two solitons traveling at different speeds.\n",
      "\n",
      "Our study sheds new light on the integrability properties of the cubic Schrodinger equation and provides insights into the behavior of soliton solutions in the presence of logarithmic distance. Our results have potential applications in the design of optical communication systems and the study of Bose-Einstein condensates in low-dimensional systems. Overall, our research contributes to a deeper understanding of the mathematical and physical properties of nonlinear wave equations. 1 into PostgreSQL...\n",
      "Inserting test sample 1039  We theoretically study the spin-resolved subgap transport properties of a Cooper pair splitter based on a triple quantum dot attached to superconducting and ferromagnetic leads. Using the Keldysh Green's function formalism, we analyze the dependence of the Andreev conductance, Cooper pair splitting efficiency, and tunnel magnetoresistance on the gate and bias voltages applied to the system. We show that the system's transport properties are strongly affected by spin dependence of tunneling processes and quantum interference between different local and nonlocal Andreev reflections. We also study the effects of finite hopping between the side quantum dots on the Andreev current.\n",
      "\n",
      "This allows for identifying the optimal conditions for enhancing the Cooper pair splitting efficiency of the device. We find that the splitting efficiency exhibits a nonmonotonic dependence on the degree of spin polarization of the leads and the magnitude and type of hopping between the dots. An almost perfect splitting efficiency is predicted in the nonlinear response regime when the energies of the side quantum dots are tuned to the energies of the corresponding Andreev bound states. In addition, we analyzed features of the tunnel magnetoresistance (TMR) for a wide range of the gate and bias voltages, as well as for different model parameters, finding the corresponding sign changes of the TMR in certain transport regimes. The mechanisms leading to these effects are thoroughly discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 1040  In this study, we investigate the splitting efficiency and interference effects in a Cooper pair splitter (CPS) based on a triple quantum dot (TQD) with ferromagnetic contacts. The TQD structure consists of three quantum dots that are connected in series and connected to ferromagnetic leads. By applying gate voltages, we control the electronic states in the TQD. We analyze the CPS behavior by measuring the current in the ferromagnetic contacts, which is proportional to the splitting efficiency of the CPS. We find that the TQD structure enhances the splitting efficiency of the CPS due to the effects of the ferromagnetic leads. In addition, we observe interference effects in the current, which can be explained by the coherent superposition of two possible paths for the Cooper pair splitting. We also investigate the influence of the magnetic field on the CPS behavior and find that the interference effects are suppressed in the presence of a magnetic field. Our results demonstrate the potential of TQD-based CPSs with ferromagnetic contacts as a promising candidate for future electronic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 1041  Capillary condensation of water is ubiquitous in nature and technology. It routinely occurs in granular and porous media, can strongly alter such properties as adhesion, lubrication, friction and corrosion, and is important in many processes employed by microelectronics, pharmaceutical, food and other industries. The century-old Kelvin equation is commonly used to describe condensation phenomena and shown to hold well for liquid menisci with diameters as small as several nm. For even smaller capillaries that are involved in condensation under ambient humidity and, hence, of particular practical interest, the Kelvin equation is expected to break down, because the required confinement becomes comparable to the size of water molecules. Here we take advantage of van der Waals assembly of two-dimensional crystals to create atomic-scale capillaries and study condensation inside. Our smallest capillaries are less than 4 angstroms in height and can accommodate just a monolayer of water. Surprisingly, even at this scale, the macroscopic Kelvin equation using the characteristics of bulk water is found to describe accurately the condensation transition in strongly hydrophilic (mica) capillaries and remains qualitatively valid for weakly hydrophilic (graphene) ones. We show that this agreement is somewhat fortuitous and can be attributed to elastic deformation of capillary walls, which suppresses giant oscillatory behavior expected due to commensurability between atomic-scale confinement and water molecules. Our work provides a much-needed basis for understanding of capillary effects at the smallest possible scale important in many realistic situations. 0 into PostgreSQL...\n",
      "Inserting test sample 1042  Capillary condensation under atomic-scale confinement is a key phenomenon involved in the adsorption of fluids in nanopores. The behavior of confined fluids is influenced by their interactions with solid surfaces and the geometry of the pores. Understanding this behavior is crucial for applications such as gas storage, separation, and catalysis.\n",
      "\n",
      "Recent experimental advances have allowed for the study of capillary condensation under extreme confinement at the atomic scale, in which the pore size approaches the same order of magnitude as the size of individual molecules. These experiments have revealed a rich variety of new phenomena, including layering transitions, novel wetting behavior, and the emergence of new phases.\n",
      "\n",
      "Theoretical approaches have also been developed to investigate capillary condensation under atomic-scale confinement. Molecular simulations and density functional theory calculations have been used to study the influence of various factors, such as surface chemistry, temperature, and pressure, on the confined fluid behavior.\n",
      "\n",
      "In this review, we summarize the recent experimental and theoretical developments in the study of capillary condensation under atomic-scale confinement. We discuss the new phenomena that have been observed and the theoretical models that have been developed to explain them. Finally, we highlight the potential implications of these findings for future applications in nanotechnology, materials science, and energy storage. 1 into PostgreSQL...\n",
      "Inserting test sample 1043  The authors show that it is possible to rotate the magnetization of a multiferroic (strain-coupled two-layer magnetostrictive-piezoelectric) nanomagnet by a large angle with a small electrostatic potential. This can implement Bennett clocking in nanomagnetic logic arrays resulting in unidirectional propagation of logic bits from one stage to another. This method of Bennett clocking is superior to using spin-transfer torque or local magnetic fields for magnetization rotation. For realistic parameters, it is shown that a potential of ~ 0.2 V applied to a multiferroic nanomagnet can rotate its magnetization by nearly 900 to implement Bennett clocking. 0 into PostgreSQL...\n",
      "Inserting test sample 1044  This research paper presents a novel approach to nanomagnetic logic via Bennett clocking. By utilizing electrically induced rotation of magnetization in multiferroic single-domain nanomagnets, the proposed approach shows significant potential advantages over existing nanomagnetic logic schemes. The authors provide theoretical analysis and simulation results to demonstrate the feasibility and effectiveness of the proposed approach. Specifically, it is shown that the proposed approach can significantly reduce the number of clocking stages and improve the clocking efficiency. This work represents a significant step forward in the development of efficient and reliable nanomagnetic logic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 1045  Crowd behaviour analytics focuses on behavioural characteristics of groups of people instead of individuals' activities. This work considers human queuing behaviour which is a specific crowd behavior of groups. We design a plug-and-play system solution to the queue detection problem based on Wi-Fi/Bluetooth Low Energy (BLE) received signal strength indicators (RSSIs) captured by multiple signal sniffers. The goal of this work is to determine if a device is in the queue based on only RSSIs. The key idea is to extract features not only from individual device's data but also mobility similarity between data from multiple devices and mobility correlation observed by multiple sniffers. Thus, we propose single-device feature extraction, cross-device feature extraction, and cross-sniffer feature extraction for model training and classification. We systematically conduct experiments with simulated queue movements to study the detection accuracy. Finally, we compare our signal-based approach against camera-based face detection approach in a real-world social event with a real human queue. The experimental results indicate that our approach can reach minimum accuracy of 77% and it significantly outperforms the camera-based face detection because people block each other's visibility whereas wireless signals can be detected without blocking. 0 into PostgreSQL...\n",
      "Inserting test sample 1046  The ability to detect the presence and size of queues in crowded settings is of great practical importance, yet it presents considerable challenges due to the complex and dynamic nature of human behaviors in such settings. In this paper, we propose a queue detection framework based on RSSI measurements from Bluetooth Low Energy (BLE) beacons deployed in the environment, which captures fundamental queue features including the queue length and the number of people in it. Specifically, we utilize machine learning techniques to learn a queue-specific RSSI model and develop a queue detection algorithm based on this model. Through extensive experiments with real-world traces, we demonstrate that our framework is able to accurately detect queues with high precision and recall rates, while being robust against variations in crowd densities, beacon densities, and device positions. We also evaluate the impact of various factors on queue detection accuracy, and show that our proposed algorithm is effective even when using a small number of beacons. Our framework has the potential to improve various applications such as smart building management, crowd control, and urban planning. 1 into PostgreSQL...\n",
      "Inserting test sample 1047  By using meromorphic \"characters\" and \"logarithms\" built up from Euler's Gamma function, and by using convergent factorial series, we will give, in a first pat, a \"normal form\" to the solutions of a singular regular system. It will enable us to define a connexion matrix for a regular singular system.\n",
      "\n",
      "Following one of Birkhoff's idea, we will then study its link with the problem of rational classification of system. In a second part, we will be interested in the confluence of fuchsian difference systems to differential systems. We will show more particularly how we can get, under some natural hypotheses, the local monodromies of a limit differential system from the connection matrices of the deformation that we consider. The use of factorial series (which can diverge as power series) distinguish regular singular difference systems from their differential and q-difference analogues and make their study more difficult.\n",
      "\n",
      "En choisissant des \"caracteres\" et des \"logarithmes\", meromorphes sur le plan complexe, construits a l'aide de la fonction Gamma d'Euler, et en utilisant des series de factorielles convergentes, nous sommes en mesure, dans une premiere partie, de donner une \"forme normale\" pour les solutions d'un systeme aux differences singulier regulier. Nous pouvons alors definir une matrice de connexion d'un tel systeme.\n",
      "\n",
      "Nous etudions ensuite, suivant une idee de G.D. Birkhoff, le lien de celles-ci avec le probleme de la classification rationnelle des systemes. Dans une deuxieme partie, nous nous interessons la confluence des systemes aux differences fuchsiens vers les systemes differentiels. Nous montrons en particulier comment, sous certaines hypotheses naturelles, on peut reconstituer les monodromies locales d'un systeme differentiel limite a partir des matrices meromorphes de connexion des deformations considerees. Le point central, qui distingue en profondeur les systemes aux differences singuliers reguliers de leurs homonymes differentiels ou aux q-differences et qui rend leur etude plus complexe, est la necessaire utilisation de series de factorielles (qui peuvent diverger en tant que series de puissances). 0 into PostgreSQL...\n",
      "Inserting test sample 1048  This research paper explores the rational classification and confluence of regular singular difference systems. In particular, we analyze differential equations with singularities, which often arise in problems related to physics or engineering, and establish a systematic method for their classification. We also investigate the interplay between confluence and classification, providing a comprehensive overview of the main results in this field.\n",
      "\n",
      "Our approach builds on top of existing theories on the classification of ordinary differential equations. We introduce a new concept of \"holonomic rank\" for the equations under consideration, which measures the degree of complexity of their solutions. This enables us to derive a complete classification of regular singular difference systems, which is structured according to their holonomic rank. In addition, we show the equivalence between confluence and the existence of an isomonodromy deformation, which is a transformation that preserves the differential properties of the system.\n",
      "\n",
      "The paper also examines the relationship between confluence and holonomic rank, showing that confluence implies a lower bound on the latter. More specifically, we prove that the holonomic rank of any confluence class is finite and bounded by a function of the dimension of the system and the order of its poles. This result implies the existence of a finite set of confluence classes for any given regular singular difference system.\n",
      "\n",
      "Finally, we illustrate our theoretical results with several examples from physics and engineering. These include the PainlevÃ© equations, which arise in the study of nonlinear phenomena such as solitons or turbulence, and the q-difference equations, which are ubiquitous in the theory of orthogonal polynomials. We show how our classification and confluence theory sheds new light on the properties of these systems, providing valuable insights into their behavior and dynamics.\n",
      "\n",
      "In conclusion, this paper presents a comprehensive study of the rational classification and confluence of regular singular difference systems. Our results provide a new perspective on the theory of differential equations with singularities, with potential applications in a wide range of fields. 1 into PostgreSQL...\n",
      "Inserting test sample 1049  We present an exploration of the significance of Carbon/Oxygen phase separation in white dwarf stars in the context of self-consistent evolutionary calculations. Because phase separation can potentially increase the calculated ages of the oldest white dwarfs, it can affect the age of the Galactic disk as derived from the downturn in the white dwarf luminosity function. We find that the largest possible increase in ages due to phase separation is 1.5 Gyr, with a most likely value of approximately 0.6 Gyr, depending on the parameters of our white dwarf models.\n",
      "\n",
      "The most important factors influencing the size of this delay are the total stellar mass, the initial composition profile, and the phase diagram assumed for crystallization. We find a maximum age delay in models with masses of 0.6 solar masses, which is near the peak in the observed white dwarf mass distribution. We find that varying the opacities (via the metallicity) has little effect on the calculated age delays.\n",
      "\n",
      "In the context of Galactic evolution, age estimates for the oldest Galactic globular clusters range from 11.5 to 16 Gyr, and depend on a variety of parameters. In addition, a 4 to 6 Gyr delay is expected between the formation of the globular clusters and that of the Galactic thin disk, while the observed white dwarf luminosity function gives an age estimate for the thin disk of 9.5 +/-1.0 Gyr, without including the effect of phase separation. Using the above numbers, we see that phase separation could add between 0 to 3 Gyr to the white dwarf ages and still be consistent with the overall picture of Galaxy formation. Our calculated maximum value of 1.5 Gyr fits within these bounds, as does our best guess value of 0.6 Gyr. 0 into PostgreSQL...\n",
      "Inserting test sample 1050  In this study, we investigate the process of phase separation in crystallizing white dwarf stars using evolutionary calculations. White dwarf stars are the remnants of low and intermediate mass stars that have exhausted their nuclear fuel. As they cool down, the carbon and oxygen in their cores crystallize, forming a solid lattice structure. However, due to the presence of impurities such as helium and hydrogen, the crystallization process is not homogenous, leading to the formation of a mixture of crystalline and non-crystalline domains. \n",
      "\n",
      "Our calculations show that the phase separation process is driven by gravitational settling of helium and hydrogen impurities, which accumulate in the non-crystalline regions of the star. This results in a composition gradient which drives the separation of the star into distinct crystalline and non-crystalline layers. We find that the thickness and composition of these layers depends on the initial composition of the star and the rate at which it cools down.\n",
      "\n",
      "We also investigate the effect of convective mixing, which can disrupt the composition gradient and cause the formation of more complex structures. Our simulations suggest that the presence of convective mixing can lead to the formation of thin layers of intermediate composition between the crystalline and non-crystalline regions.\n",
      "\n",
      "Our results have important implications for our understanding of the structure and evolution of white dwarf stars, as well as their role as cosmological probes. In particular, they can help explain the observed chemical abundances and variability in white dwarf spectra, and provide insights into the processes of nucleosynthesis and mixing in these systems. Our study highlights the importance of considering the effects of impurities and convective mixing in models of white dwarf evolution, and motivates further exploration of these phenomena in other astrophysical contexts. 1 into PostgreSQL...\n",
      "Inserting test sample 1051  This article deals with the specific context of an autonomous car navigating in an urban center within a shared space between pedestrians and cars. The driver delegates the control to the autonomous system while remaining seated in the driver's seat. The proposed study aims at giving a first insight into the definition of human perception of space applied to vehicles by testing the existence of a personal space around the car.It aims at measuring proxemic information about the driver's comfort zone in such conditions.Proxemics, or human perception of space, has been largely explored when applied to humans or to robots, leading to the concept of personal space, but poorly when applied to vehicles. In this article, we highlight the existence and the characteristics of a zone of comfort around the car which is not correlated to the risk of a collision between the car and other road users. Our experiment includes 19 volunteers using a virtual reality headset to look at 30 scenarios filmed in 360{\\textdegree} from the point of view of a passenger sitting in the driver's seat of an autonomous car.They were asked to say \"stop\" when they felt discomfort visualizing the scenarios.As said, the scenarios voluntarily avoid collision effect as we do not want to measure fear but discomfort.The scenarios involve one or three pedestrians walking past the car at different distances from the wings of the car, relative to the direction of motion of the car, on both sides. The car is either static or moving straight forward at different speeds.The results indicate the existence of a comfort zone around the car in which intrusion causes discomfort.The size of the comfort zone is sensitive neither to the side of the car where the pedestrian passes nor to the number of pedestrians. In contrast, the feeling of discomfort is relative to the car's motion (static or moving).Another outcome from this study is an illustration of the usage of first person 360{\\textdegree} video and a virtual reality headset to evaluate feelings of a passenger within an autonomous car. 0 into PostgreSQL...\n",
      "Inserting test sample 1052  The emergence of autonomous vehicles has brought the question of personal space to the forefront. This study aims to investigate the personal space requirements of passengers sitting in the driver's seat of an autonomous car. The personal space of humans is known to vary depending on factors such as culture, situation, and individual differences. However, it is unknown if this applies in the context of autonomous cars.\n",
      "\n",
      "To explore this, we recruited sixty participants from diverse cultural backgrounds to take part in a driving simulation of an autonomous car. We used a modified version of the Personal Space Questionnaire to measure participants' personal space requirements while riding in the driver's seat.\n",
      "\n",
      "Our results suggest that personal space needs in autonomous vehicles are consistent across cultures and genders. Participants tended to prefer a greater distance between themselves and the virtual steering wheel and dashboard compared to the sides and roof of the vehicle. Interestingly, participants also preferred a greater distance from a hypothetical human-like robot occupying the passenger seat compared to a non-human-like object of the same size.\n",
      "\n",
      "Qualitative data from post-simulation interviews revealed that participants felt more comfortable and at ease when the vehicle had a larger interior space, giving them more room to move and stretch. Additionally, participants expressed discomfort with the idea of sharing an autonomous vehicle with strangers, with some suggesting that personal space concerns may even discourage them from using such transportation services.\n",
      "\n",
      "In conclusion, our study shows that passengers sitting in the driver's seat of an autonomous vehicle have specific personal space requirements, which are consistent across cultures and genders. These results have significant implications for the design of autonomous vehicles and the development of transportation services, highlighting the importance of providing adequate personal space for passenger comfort. Additionally, the possibility of sharing autonomous vehicles with strangers may pose a challenge for both designers and transportation companies, further emphasizing the need for further research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 1053  Breaking cybersecurity events are shared across a range of websites, including security blogs (FireEye, Kaspersky, etc.), in addition to social media platforms such as Facebook and Twitter. In this paper, we investigate methods to analyze the severity of cybersecurity threats based on the language that is used to describe them online. A corpus of 6,000 tweets describing software vulnerabilities is annotated with authors' opinions toward their severity. We show that our corpus supports the development of automatic classifiers with high precision for this task. Furthermore, we demonstrate the value of analyzing users' opinions about the severity of threats reported online as an early indicator of important software vulnerabilities. We present a simple, yet effective method for linking software vulnerabilities reported in tweets to Common Vulnerabilities and Exposures (CVEs) in the National Vulnerability Database (NVD). Using our predicted severity scores, we show that it is possible to achieve a Precision@50 of 0.86 when forecasting high severity vulnerabilities, significantly outperforming a baseline that is based on tweet volume. Finally we show how reports of severe vulnerabilities online are predictive of real-world exploits. 0 into PostgreSQL...\n",
      "Inserting test sample 1054  This research paper aims to investigate the perceived severity of cybersecurity threats reported on social media platforms. With the increasing reliance on technology and the internet, the potential for cyber attacks has risen exponentially, resulting in a significant impact on individuals and organizations. Hence, this study analyzes the nature of cybersecurity threats and examines the role of social media platforms in disseminating information about such threats. Through a content analysis of cybersecurity-related conversations on social media, the study identifies the types of cybersecurity threats most commonly reported and the accompanying sentiment associated with such reports. Additionally, this study examines the factors that influence the perceived severity of these threats, including demographic characteristics and previous experiences with cyber attacks. The results from this study can potentially inform policymakers and organizations about the importance of leveraging social media to enhance cybersecurity awareness and identify areas where interventions can be implemented to mitigate the severity of cyber threats on individuals and organizations. 1 into PostgreSQL...\n",
      "Inserting test sample 1055  Diagnostic or procedural coding of clinical notes aims to derive a coded summary of disease-related information about patients. Such coding is usually done manually in hospitals but could potentially be automated to improve the efficiency and accuracy of medical coding. Recent studies on deep learning for automated medical coding achieved promising performances. However, the explainability of these models is usually poor, preventing them to be used confidently in supporting clinical practice. Another limitation is that these models mostly assume independence among labels, ignoring the complex correlation among medical codes which can potentially be exploited to improve the performance. We propose a Hierarchical Label-wise Attention Network (HLAN), which aimed to interpret the model by quantifying importance (as attention weights) of words and sentences related to each of the labels. Secondly, we propose to enhance the major deep learning models with a label embedding (LE) initialisation approach, which learns a dense, continuous vector representation and then injects the representation into the final layers and the label-wise attention layers in the models. We evaluated the methods using three settings on the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS COVID-19 shielding codes. Experiments were conducted to compare HLAN and LE initialisation to the state-of-the-art neural network based methods. HLAN achieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and comparable results on the NHS COVID-19 shielding code prediction to other models. By highlighting the most salient words and sentences for each label, HLAN showed more meaningful and comprehensive model interpretation compared to its downgraded baselines and the CNN-based models. LE initialisation consistently boosted most deep learning models for automated medical coding. 0 into PostgreSQL...\n",
      "Inserting test sample 1056  Clinical notes contain crucial information about patientsâ€™ health conditions, making them important materials for healthcare professionals. However, the process of coding such notes can be time-consuming and repetitive. In order to address this issue, we present a novel automated coding system that utilizes hierarchical label-wise attention networks and label embedding initialization to create explanations for each prediction.\n",
      "\n",
      "Our system uses a hierarchical approach to categorize codes related to different health domains. At each level of the hierarchy, a label-wise attention network is applied to select the most significant words or phrases associated with the code, while suppressing irrelevant information. By doing so, our method is able to capture complex and multi-label relationships between codes and clinical notes with better performance than traditional single-label classification models. Furthermore, we leverage label embedding initialization to pre-train our model with millions of unlabeled data to improve its generalization capability. Our approach enables us to handle a large number of codes without the need for manual feature engineering or a priori knowledge of the codes.\n",
      "\n",
      "To validate our approach, we conducted experiments on two public datasets, achieving state-of-the-art results with an F1-score of 0.791 and 0.760 respectively for multi-label classification. Our model also outperforms existing systems in terms of interpretability and explainability by delivering insights into the process of code prediction. In conclusion, this work proposes a novel hierarchical label-wise attention network with label embedding initialization for automated coding of clinical notes, enabling efficient and accurate classification of multi-labeled codes while improving the interpretability and explainability of the model. 1 into PostgreSQL...\n",
      "Inserting test sample 1057  The problem of approximately computing the $k$ dominant Fourier coefficients of a vector $X$ quickly, and using few samples in time domain, is known as the Sparse Fourier Transform (sparse FFT) problem. A long line of work on the sparse FFT has resulted in algorithms with $O(k\\log n\\log (n/k))$ runtime [Hassanieh et al., STOC'12] and $O(k\\log n)$ sample complexity [Indyk et al., FOCS'14]. These results are proved using non-adaptive algorithms, and the latter $O(k\\log n)$ sample complexity result is essentially the best possible under the sparsity assumption alone.\n",
      "\n",
      "This paper revisits the sparse FFT problem with the added twist that the sparse coefficients approximately obey a $(k_0,k_1)$-block sparse model. In this model, signal frequencies are clustered in $k_0$ intervals with width $k_1$ in Fourier space, where $k= k_0k_1$ is the total sparsity. Signals arising in applications are often well approximated by this model with $k_0\\ll k$.\n",
      "\n",
      "Our main result is the first sparse FFT algorithm for $(k_0, k_1)$-block sparse signals with the sample complexity of $O^*(k_0k_1 + k_0\\log(1+ k_0)\\log n)$ at constant signal-to-noise ratios, and sublinear runtime. A similar sample complexity was previously achieved in the works on model-based compressive sensing using random Gaussian measurements, but used $\\Omega(n)$ runtime. To the best of our knowledge, our result is the first sublinear-time algorithm for model based compressed sensing, and the first sparse FFT result that goes below the $O(k\\log n)$ sample complexity bound.\n",
      "\n",
      "Our algorithm crucially uses {\\em adaptivity} to achieve the improved sample complexity bound, and we prove that adaptivity is in fact necessary if Fourier measurements are used: Any non-adaptive algorithm must use $\\Omega(k_0k_1\\log \\frac{n}{k_0k_1})$ samples for the $(k_0,k_1$)-block sparse model, ruling out improvements over the vanilla sparsity assumption. 0 into PostgreSQL...\n",
      "Inserting test sample 1058  In this research paper, we present an efficient approach to compute the Fourier transform of block sparse signals. Our proposed algorithm, called Adaptive Sublinear-Time Block Sparse Fourier Transform (ASBFT), can efficiently process block sparse signals in O(N^(1-Îµ)) time complexity, where N is the signal length and Îµ is a small constant. The ASBFT algorithm works by decomposing the input signal into non-overlapping blocks and identifying the sparse frequency components in each block. This adaptive approach allows for efficient computation of the Fourier transform for signals with varying degrees of sparsity, without compromising accuracy.\n",
      "\n",
      "Additionally, we analyze the theoretical bounds of our algorithm and provide empirical results that demonstrate its effectiveness. We show that ASBFT significantly outperforms current state-of-the-art approaches for block sparse Fourier transforms in terms of computational cost and accuracy. Furthermore, we demonstrate the scalability of our approach by applying it to larger signals up to terabyte scale.\n",
      "\n",
      "Our algorithm has a wide range of applications, including but not limited to, compressive sensing, seismic imaging, and machine learning. With the increasing amount of data being generated in various fields, the need for efficient and accurate signal processing approaches is becoming more pressing. The ASBFT algorithm provides a highly effective solution for computing the Fourier transform of large-scale block sparse signals. We believe that our work will have a significant impact on various fields that rely on signal processing, and we hope that our proposed approach will inspire further research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 1059  In this paper, we propose an intelligent analog beam selection strategy in a terahertz (THz) band beamspace multiple-input multiple-output (MIMO) system.\n",
      "\n",
      "First inspired by transfer learning, we fine-tune the pre-trained off-the-shelf GoogleNet classifier, to learn analog beam selection as a multi-class mapping problem. Simulation results show 83% accuracy for the analog beam selection, which subsequently results in 12% spectral efficiency (SE) gain, upon the existing counterparts. Towards a more accurate classifier, we replace the conventional rectified linear unit (ReLU) activation function of the GoogleNet with the recently proposed Swish and retrain the fine-tuned GoogleNet to learn analog beam selection. It is numerically indicated that the fine-tuned Swish-driven GoogleNet achieves 86% accuracy, as well as 18% improvement in achievable SE, upon the similar schemes. Eventually, a strong ensembled classifier is developed to learn analog beam selection by sequentially training multiple fine-tuned Swish-driven GoogleNet classifiers. According to the simulations, the strong ensembled model is 90% accurate and yields 27% gain in achievable SE, in comparison with prior methods. 0 into PostgreSQL...\n",
      "Inserting test sample 1060  This paper presents a novel approach for intelligent analog beam selection in Terahertz Beamspace MIMO systems, called Swish-Driven GoogleNet. Our proposed method employs a Swish activation function with GoogleNet, a deep learning framework, to select the optimal analog beam for each user at the transmitter side. By leveraging the massive MIMO technology, we aim to achieve higher data rates and improve the quality of service for Terahertz wireless communication systems. To evaluate the performance of our proposed method, we conduct extensive simulations using a Ray-tracing simulator. The results show that our approach outperforms other state-of-the-art methods in terms of system throughput and spectral efficiency. Additionally, we analyze the complexity of Swish-Driven GoogleNet and demonstrate its feasibility for real-time implementations with affordable hardware requirements. Overall, our proposed approach enables efficient and effective beam selection for Terahertz systems, paving the way for future research in this field. 1 into PostgreSQL...\n",
      "Inserting test sample 1061  We develop an improved Alcock-Paczynski (AP) test method that uses the redshift-space two-point correlation function (2pCF) of galaxies. Cosmological constraints can be obtained by examining the redshift dependence of the normalized 2pCF, which should not change apart from the expected small non-linear evolution. An incorrect choice of cosmology used to convert redshift to comoving distance will manifest itself as redshift-dependent 2pCF. Our method decomposes the redshift difference of the two-dimensional correlation function into the Legendre polynomials whose amplitudes are modeled by radial fitting functions. Our likelihood analysis with this 2-D fitting scheme tightens the constraints on $\\Omega_m$ and ${w}$ by $\\sim 40\\%$ compared to the method of Li et al. (2016, 2017, 2018) that uses one dimensional angular dependence only. We also find that the correction for the non-linear evolution in the 2pCF has a non-negligible cosmology dependence, which has been neglected in previous similar studies by Li et al.. With an accurate accounting for the non-linear systematics and use of full two-dimensional shape information of the 2pCF down to scales as small as $5~h^{-1}{\\rm Mpc}$ it is expected that the AP test with redshift-space galaxy clustering anisotropy can be a powerful method to constrain the expansion history of the universe. 0 into PostgreSQL...\n",
      "Inserting test sample 1062  The Alcock-Paczynski test has long been an important tool for studying the large-scale structure of the universe. However, much of the research using this test has been limited to analyzing galaxy clustering in real space. In this paper, we extend the Alcock-Paczynski test to include the evolution of redshift-space galaxy clustering anisotropy. This extension allows us to better understand the effects of the peculiar velocities of galaxies on the observed distribution of galaxy clustering. We make use of simulations to test our method and find that including redshift-space galaxy clustering anisotropy improves the accuracy of Alcock-Paczynski measurements. Our results indicate that ignoring this effect can lead to a significant bias in estimates of cosmological parameters. In addition, we apply our method to a sample of galaxy clustering data from the Baryon Oscillation Spectroscopic Survey and find consistency with previous measurements of the cosmic expansion rate. Our work presents a valuable contribution to the ongoing efforts to better understand the large-scale structure of the universe and improve the accuracy of cosmological measurements. 1 into PostgreSQL...\n",
      "Inserting test sample 1063  We show that strong-coupling (SC) of light and matter as it is realized with quantum dots (QDs) in microcavities differs substantially from the paradigm of atoms in optical cavities. The type of pumping used in semiconductors yields new criteria to achieve SC, with situations where the pump hinders, or on the contrary, favours it. We analyze one of the seminal experimental observation of SC of a QD in a pillar microcavity [Reithmaier et al., Nature (2004)] as an illustration of our main statements. 0 into PostgreSQL...\n",
      "Inserting test sample 1064  We investigate the effects of strong-coupling between quantum dots and microcavities, a phenomenon arising from the hybridization of light and matter. By coupling confinement in different degrees of freedom, we achieve different regimes of strong-coupling, spanning classical, intermediate, and ultrastrong coupling. Our findings show how strong-coupling modifies the energy levels and dynamics of electronic excitations in these hybrid systems, providing an avenue for the development of novel optoelectronic devices and the exploration of exotic quantum phenomena. 1 into PostgreSQL...\n",
      "Inserting test sample 1065  A model is presented for the ion distribution function in a plasma at a solid target with a magnetic field $\\vec{B}$ inclined at a small angle, $ \\alpha \\ll 1$ (in radians), to the target. Adiabatic electrons are assumed, requiring $\\alpha\\gg\\sqrt{Zm_{\\rm e}/m_{\\rm i}} $ where $m_{\\rm e}$ and $m_{\\rm i}$ are the electron and ion mass respectively, and $Z$ is the charge state of the ion.\n",
      "\n",
      "An electric field $\\vec{E}$ is present to repel electrons, and so the characteristic size of the electrostatic potential $\\phi$ is set by the electron temperature $T_{\\rm e}$, $e\\phi \\sim T_{\\rm e}$, where $e$ is the proton charge. An asymptotic scale separation between the Debye length, $\\lambda_{\\rm D}=\\sqrt{\\epsilon_0 T_{\\text{e}}/e^2 n_{\\text{e}}}$, the ion sound gyroradius $\\rho_{\\rm s}=\\sqrt{ m_{\\rm i}(ZT_{\\rm e}+T_{\\rm i})}/(ZeB)$, and the size of the collisional region $d_{\\rm c} = \\alpha \\lambda_{\\rm mfp}$ is assumed, $\\lambda_{\\rm D} \\ll \\rho_{\\rm s} \\ll d_{\\rm c}$. Here $\\epsilon_0$ is the permittivity of free space, $n_{\\rm e}$ is the electron density, $T_{\\rm i}$ is the ion temperature, $B= |\\vec{B}|$ and $\\lambda_{\\rm mfp}$ is the collisional mean free path of an ion. The form of the ion distribution function is assumed at distances $x$ from the wall such that $\\rho_{\\rm s} \\ll x \\ll d_{\\rm c}$. A self-consistent solution of $\\phi (x)$ is required to solve for the ion trajectories and for the ion distribution function at the target. The model presented here allows to bypass the numerical solution of $\\phi (x)$ and results in an analytical expression for the ion distribution function at the target. It assumes that $\\tau=T_{\\rm i}/(ZT_{\\rm e})\\gg 1$, and ignores the electric force on the ion trajectory until close to the target. For $\\tau \\gtrsim 1$, the model provides a fast approximation to energy-angle distributions of ions at the target. These can be used to make sputtering predictions. 0 into PostgreSQL...\n",
      "Inserting test sample 1066  This paper presents a large gyro-orbit model to investigate the ion velocity distribution in plasma near a wall in the presence of a grazing-angle magnetic field. The study addresses the impact of the confinement and the magnetic field on the ion motion, which can greatly affect the plasma behavior near the wall. The modeling involves a kinetic approach, which is capable of capturing the complex ion orbit behavior under the influence of the magnetic field as well as the ion-neutral collisions. The large gyro-orbit model extends the previously developed gyrokinetic model by taking into account the effects of finite gyroradius and finite orbit width.\n",
      "\n",
      "The numerical simulations show that the ion velocity distribution near the wall strongly depends on the magnetic field strength and the plasma parameters. In particular, the model predictions suggest that the ion distribution function exhibits non-Gaussian features due to the ion orbit dynamics in the presence of the external field. Moreover, the study reveals that the collisions with neutrals play a crucial role in shaping the velocity distribution, especially in the low-density regime.\n",
      "\n",
      "The results of the model are compared with experimental observations, and good agreement is found between theory and experiment. The model can be used to derive insights into various plasma phenomena, such as transport, turbulence, and wall interactions, in the presence of a non-uniform magnetic field. The study can be extended to investigate the effect of different magnetic field configurations and plasma parameters on the ion behavior to provide a broader understanding of the complex plasma-wall interactions.\n",
      "\n",
      "In conclusion, this paper provides a comprehensive study of the ion velocity distribution in plasma near a wall under a grazing-angle magnetic field using the large gyro-orbit model approach. The results highlight the importance of the kinetic effects and the role of neutral collisions in shaping the ion behavior. The findings have implications for a wide range of plasma applications, including nuclear fusion reactors, plasma processing, and space plasmas. 1 into PostgreSQL...\n",
      "Inserting test sample 1067  Let $\\theta$ be an automorphism of a thick irreducible spherical building $\\Delta$ of rank at least $3$ with no Fano plane residues. We prove that if there exist both type $J_1$ and $J_2$ simplices of $\\Delta$ mapped onto opposite simplices by $\\theta$, then there exists a type $J_1\\cup J_2$ simplex of $\\Delta$ mapped onto an opposite simplex by $\\theta$. This property is called \"cappedness\". We give applications of cappedness to opposition diagrams, domesticity, and the calculation of displacement in spherical buildings. In a companion piece to this paper we study the thick irreducible spherical buildings containing Fano plane residues. In these buildings automorphisms are not necessarily capped. 0 into PostgreSQL...\n",
      "Inserting test sample 1068  We present a study on opposition diagrams for automorphisms of large spherical buildings. Such diagrams are fundamental to the theory of spherical buildings, and they encode important information about the geometry of the building. Our primary goal is to investigate the algebraic and combinatorial properties of these diagrams, providing a comprehensive treatment of their structure. We establish several results concerning the existence and uniqueness of opposition diagrams, and explore some of their implications for the theory of automorphisms of spherical buildings. Our approach is based on the use of Coxeter groups and their associated arrangements, and we show how these tools can be used to gain a deeper understanding of opposition diagrams. 1 into PostgreSQL...\n",
      "Inserting test sample 1069  Classical phase-space variables are normally chosen to promote to quantum operators in order to quantize a given classical system. While classical variables can exploit coordinate transformations to address the same problem, only one set of quantum operators to address the same problem can give the correct analysis. Such a choice leads to the need to find the favored classical variables in order to achieve a valid quantization. This article addresses the task of how such favored variables are found that can be used to properly solve a given quantum system. Examples, such as non-renormalizable scalar fields and gravity, have profited by initially changing which classical variables to promote to quantum operators. 0 into PostgreSQL...\n",
      "Inserting test sample 1070  In this study, we identify the classical variables that are particularly suitable for promotion to quantum operators. Through a thorough analysis of various classical systems, we observe that certain variables possess unique properties that make them especially well-suited for this task. By carefully selecting and promoting these variables, we can effectively bridge the gap between classical and quantum systems and enable the use of classical measurements to obtain accurate results in the quantum regime. Our findings have important implications for the development of hybrid classical-quantum systems for a range of applications, from quantum simulation to quantum computing. 1 into PostgreSQL...\n",
      "Inserting test sample 1071  We consider the boomerang uniformity of an infinite class of (locally-APN) power maps and show that its boomerang uniformity over the finite field $\\F_{2^n}$ is $2$ and $4$, when $n \\equiv 0 \\pmod 4$ and $n \\equiv 2 \\pmod 4$, respectively. As a consequence, we show that for this class of power maps, the differential uniformity is strictly greater than its boomerang uniformity. 0 into PostgreSQL...\n",
      "Inserting test sample 1072  In this paper, we investigate the boomerang uniformity of a class of power maps over a Galois field. Our results reveal that for certain polynomials, the boomerang uniformity is connected to the degree of the polynomial. We prove this by establishing a relationship between the nonlinearity of a function and its boomerang uniformity. Our research has implications for the security of symmetric-key cryptographic systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1073  In the present work we investigate temperature effects on the spinor and scalar effetive QED in the context of Thermo Field Dynamics. Following Weisskopf's zero-point energy method, the problem of charge renormalization is reexamined and high temperature contributions are extracted from the thermal correction for the Lagrangian densities. 0 into PostgreSQL...\n",
      "Inserting test sample 1074  The behavior of the Euler-Kockel-Heisenberg Lagrangian at finite temperature is studied. This analysis reveals the thermal corrections to the vacuum energy, which can be used to predict the behavior of the system at different temperatures. The results could have implications for the understanding of quantum mechanics at high temperatures. 1 into PostgreSQL...\n",
      "Inserting test sample 1075  In this work Controlled phase shift gates are implemented on a qaudrupolar system, by using non-adiabatic geometric phases. A general procedure is given, for implementing controlled phase shift gates in an 'N' level system. The utility of such controlled phase shift gates, is demonstrated here by implementing 3-qubit Deutsch-Jozsa algorithm on a 7/2 quadrupolar nucleus oriented in a liquid crystal matrix. 0 into PostgreSQL...\n",
      "Inserting test sample 1076  In this work, we describe the implementation of controlled phase shift gates and Collins version of Deutsch-Jozsa algorithm on a quadrupolar spin-7/2 nucleus. The non-adiabatic geometric phases were used for this process, and the results were recorded and analyzed. This study provides a valuable foundation for the development of spin-based quantum computing, and offers insight into the practical applications of non-adiabatic geometric phases in quantum information science. 1 into PostgreSQL...\n",
      "Inserting test sample 1077  We demonstrate the potential of dopamine modified 0.5(Ba0.7Ca0.3)TiO3-0.5Ba(Zr0.2Ti0.8)O3 filler incorporated poly-vinylidene fluoride (PVDF) composite prepared by solution cast method as both flexible energy storage and harvesting devices. The introduction of dopamine in filler surface functionalization acts as bridging elements between filler and polymer matrix and results in a better filler dispersion and an improved dielectric loss tangent (<0.02) along with dielectric permittivity ranges from 9 to 34 which is favorable for both energy harvesting and storage. Additionally, a significantly low DC conductivity (< 10-9 ohm-1cm-1) for all composites was achieved leading to an improved breakdown strength and charge accumulation capability. Maximum breakdown strength of 134 KV/mm and corresponding energy storage density 0.72 J/cm3 were obtained from the filler content 10 weight%.\n",
      "\n",
      "The improved energy harvesting performance was characterized by obtaining a maximum piezoelectric charge constant (d33) = 78 pC/N, and output voltage (Vout) = 0.84 V along with maximum power density of 3.46 microW/cm3 for the filler content of 10 wt%. Thus, the results show 0.5(Ba0.7Ca0.3)TiO3-0.5Ba(Zr0.2Ti0.8)O3/PVDF composite has the potential for energy storage and harvesting applications simultaneously that can significantly suppress the excess energy loss arises while utilizing different material. 0 into PostgreSQL...\n",
      "Inserting test sample 1078  This research paper focuses on the development and performance analysis of a flexible energy storage and harvester composed of a polyvinylidene fluoride (PVDF) nanocomposite, with a filler material consisting of 0.5 mole of (Ba0.7Ca0.3)TiO3 and 0.5 mole of Ba(Zr0.2Ti0.8)O3, modified with dopamine. The dopamine modification of the filler material is achieved through a sol-gel process that improves both the compatibility and adhesion of the filler to the PVDF matrix. The resulting nanocomposite exhibits enhanced dielectric, ferroelectric and piezoelectric properties, which make it an ideal material for energy harvesting and storage applications. The structural, electrical and morphological properties of the nanocomposite are characterized through various techniques, such as X-ray diffraction, field emission scanning electron microscopy and impedance spectroscopy. The performance of the nanocomposite is evaluated under different conditions, including frequency and temperature ranges. The results demonstrate that the dopamine modified nanocomposite exhibits significantly improved energy storage and harvesting capabilities compared to the unmodified nanocomposite. This research is expected to pave the way for the development of more efficient and cost-effective flexible energy storage and harvesting devices, with potential applications in various fields such as aerospace, robotics and wearable electronics. 1 into PostgreSQL...\n",
      "Inserting test sample 1079  A complementary approach, derived from (a) higher dimensional anti--de Sitter (AdS) space, (b) light-front quantization and (c) the invariance properties of the full conformal group in one dimension leads to a nonperturbative relativistic light-front wave equation which incorporates essential spectroscopic and dynamical features of hadron physics. The fundamental conformal symmetry of the classical QCD Lagrangian in the limit of massless quarks is encoded in the resulting effective theory. The mass scale for confinement emerges from the isomorphism between the conformal group and SO(2,1). This scale appears in the light-front Hamiltonian by mapping to the evolution operator in the formalism of de Alfaro, Fubini and Furlan, which retains the conformal invariance of the action. Remarkably, the specific form of the confinement interaction and the corresponding modification of AdS space are uniquely determined in this procedure. 0 into PostgreSQL...\n",
      "Inserting test sample 1080  This paper proposes a threefold complementary approach to solving the complex dynamics of Quantum Chromodynamics (QCD) through the use of holographic techniques. We begin by constructing a bottom-up holographic model that incorporates confinement and chiral symmetry breaking, and explore the properties of the resulting meson spectra. Next, we devise a top-down approach that involves embedding the gauge theory into a higher-dimensional gravity theory, and determine the implications for the QCD phase diagram. Finally, we introduce an intermediate approach where we interpolate between the top-down and bottom-up descriptions and investigate its relevance for understanding QCD at finite temperature and density. These three complementary methods provide a comprehensive framework for studying QCD and offer insights into the dynamics of this complex system. 1 into PostgreSQL...\n",
      "Inserting test sample 1081  n this paper I introduce a new description of the crystal $B(\\Lambda_0)$ of $\\hat{\\mathfrak{sl}_\\ell}$. As in the Misra-Miwa model of $B(\\Lambda_0)$, the nodes of this crystal are indexed by partitions and the $i$-arrows correspond to adding a box of residue $i$. I then show that the two models are equivalent by interpreting the operation of regularization introduced by James as a crystal isomorphism. 0 into PostgreSQL...\n",
      "Inserting test sample 1082  The ladder crystal is a unique type of crystal structure that exhibits a staircase-like arrangement of molecules. This crystal system has attracted considerable attention due to its potential applications in various fields, such as catalysis and optoelectronics. In this study, we present a comprehensive investigation of the ladder crystal structure, including its formation, properties, and potential applications. Our findings indicate that the ladder crystal has significant potential for a wide range of technological and scientific applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1083  The direct collapse black hole (DCBH) scenario describes the isothermal collapse of a pristine gas cloud directly into a massive, M_BH=10^4-10^6 M_sun black hole. In this paper we show that large HI column densities of primordial gas at T~10^4 K with low molecular abundance - which represent key aspects of the DCBH scenario - provide optimal conditions for pumping of the 2p-level of atomic hydrogen by trapped Lyman alpha (Lya) photons. This Lya pumping mechanism gives rise to inverted level population of the 2s_1/2-2p_3/2 transition, and therefore to stimulated fine structure emission at 3.04 cm (rest-frame). We show that simplified models of the DCBH scenario amplify the CMB by up to a factor of 10^5, above which the maser saturates. Hyperfine splitting of the 3-cm transition gives rise to a characteristic broad (FWHM ~ tens of MHz in the observers frame) asymmetric line profile. This signal subtends an angular scale of ~ 1-10 mas, which translates to a flux of ~ 0.3-3 microJy, which is detectable with ultra-deep surveys being planned with SKA1-MID. While challenging, as the signal is visible for a fraction of the collapse time of the cloud, the matching required physical conditions imply that a detection of the redshifted 3-cm emission line would provide direct evidence for the DCBH scenario. 0 into PostgreSQL...\n",
      "Inserting test sample 1084  The formation of supermassive black holes in the early universe remains a puzzle in astrophysics. In this paper, we investigate the phenomenon of 3-cm fine-structure masers and their potential use as a signature of direct collapse black hole formation. These masers, which are associated with molecular gas in galaxies, have been shown to be present in the nuclei of active galaxies and have also been detected in a number of galaxies where direct collapse is thought to be taking place. We present a theoretical model for the formation of 3-cm masers in direct-collapsing dark matter halos and show that the masers can be used to distinguish between supermassive black holes formed via direct collapse and those formed through the accretion of gas and stars. We propose that observations of 3-cm masers can be used to test models of supermassive black hole formation and evolution, and to shed new light on the physical processes that govern the growth of these enigmatic objects. Our results suggest that future observations of 3-cm masers will be crucial for advancing our understanding of the formation and evolution of supermassive black holes in the early universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1085  Point set is arguably the most direct approximation of an object or scene surface, yet its practical acquisition often suffers from the shortcoming of being noisy, sparse, and possibly incomplete, which restricts its use for a high-quality surface recovery. Point set upsampling aims to increase its density and regularity such that a better surface recovery could be achieved.\n",
      "\n",
      "The problem is severely ill-posed and challenging, considering that the upsampling target itself is only an approximation of the underlying surface.\n",
      "\n",
      "Motivated to improve the surface approximation via point set upsampling, we identify the factors that are critical to the objective, by pairing the surface approximation error bounds of the input and output point sets. It suggests that given a fixed budget of points in the upsampling result, more points should be distributed onto the surface regions where local curvatures are relatively high. To implement the motivation, we propose a novel design of Curvature-ADaptive Point set Upsampling network (CAD-PU), the core of which is a module of curvature-adaptive feature expansion. To train CAD-PU, we follow the same motivation and propose geometrically intuitive surrogates that approximate discrete notions of surface curvature for the upsampled point set.\n",
      "\n",
      "We further integrate the proposed surrogates into an adversarial learning based curvature minimization objective, which gives a practically effective learning of CAD-PU. We conduct thorough experiments that show the efficacy of our contributions and the advantages of our method over existing ones. Our implementation codes are publicly available at https://github.com/JiehongLin/CAD-PU. 0 into PostgreSQL...\n",
      "Inserting test sample 1086  CAD-PU is a deep learning solution designed to enhance point set upsampling through the integration of curvature-adaptive mechanisms. This algorithm enables the generation of high-quality point surface models that closely reproduce the details of the original input, making it a promising tool for applications ranging from computer graphics to medical imaging.\n",
      "\n",
      "An important feature of this approach is its ability to adapt to local geometric properties, such as curvature, to guide the upsampling process. This is achieved by incorporating a curvature-aware convolution layer, which allows the network to learn the local geometry of the point cloud and adapt accordingly, thereby improving the quality of the upsampling. CAD-PU also uses multi-scale feature fusion to efficiently capture the spatio-temporal correlations among different scales of the input data.\n",
      "\n",
      "To evaluate the effectiveness of CAD-PU, we perform extensive experiments on a variety of datasets, including ShapeNet, Pix3D, and ScanNet. Our results demonstrate that CAD-PU outperforms state-of-the-art methods in terms of both quantitative metrics and visual quality. Additionally, we conduct ablation studies that verify the contribution of the curvature-adaptive mechanism and the multi-scale feature fusion to the performance of the algorithm.\n",
      "\n",
      "In summary, CAD-PU is a novel approach for point set upsampling that leverages the power of deep learning and curvature-adaptive mechanisms. Our results demonstrate that CAD-PU can effectively generate high-quality point cloud models in various scenarios, making it a valuable tool for a wide range of applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1087  On basis of effective interactions of charged lepton and hadron currents, we obtain an effective interacting Hamiltonian of neutrinos in nuclear media up to the leading order. Using this effective Hamiltonian, we study neutrino mixing and oscillations in nuclear media and strong magnetic fields. We compute neutrino mixing angle and mass squared difference, and find the pattern of vacuum neutrino oscillations is modified in magnetized nuclear media. Comparing with the vacuum neutrino oscillation, we find that for high-energy neutrinos, neutrino oscillations are suppressed in the presence of nuclear media. In the general case of neutral nuclear media with the presence of electrons, we calculate the mixing angle and mass squared difference, and discuss the resonance and level-crossing in neutrino oscillations. 0 into PostgreSQL...\n",
      "Inserting test sample 1088  The phenomenon of neutrino oscillations is a well-established theory in particle physics, but its manifestation in dense nuclear media such as in the core of a collapsing star or a supernova is still highly debated. In this research paper, we explore the behavior of neutrinos as they propagate through nuclear matter, using state-of-the-art simulations and analysis techniques. Our results show that the presence of nuclear media significantly alters the neutrino oscillation probabilities and can affect key astrophysical phenomena such as the explosion mechanism of a supernova. We address the challenges and opportunities of future experiments and observations aimed at better understanding neutrino oscillations in nuclear media. 1 into PostgreSQL...\n",
      "Inserting test sample 1089  In this work, we stress the existence of isomorphisms which map complex contours from the upper half to contours in the lower half of the complex plane. The metric operator is found to depend on the chosen contour but the maps connecting different contours are norm-preserving. To elucidate these features, we parametrized the contour $z=-2i\\sqrt{1+ix}$ considered in Phys.Rev.D73:085002 (2006) for the study of wrong sign $x^{4}$ theory. For the parametrized contour of the form $z=a\\sqrt{b+i c x}$, we found that there exists an equivalent Hermitian Hamiltonian provided that $a^{2} c$ is taken to be real. The equivalent Hamiltonian is $b$-independent but the metric operator is found to depend on all the parameters $a$, $b$ and $c$. Different values of these parameters generate different metric operators which define different Hilbert spaces . All these Hilbert spaces are isomorphic to each other even for parameters values that define contours with ends in two adjacent wedges. As an example, we showed that the transition amplitudes associated with the contour $z=-2i\\sqrt{1+ix}$ are exactly the same as those calculated using the contour $z=\\sqrt{1+ix}$, which is not $\\mathcal{PT}$-Symmetric and has ends in two adjacent wedges in the complex plane. 0 into PostgreSQL...\n",
      "Inserting test sample 1090  This study examines the isomorphic Hilbert spaces associated with different complex contours of the $\\mathcal{PT}$-symmetric $(-x^4)$ theory. We investigate and compare the properties of different contour prescriptions and study how they affect the spectral behavior of the Hamiltonian operator. Our analysis reveals that there exist multiple isomorphic Hilbert spaces that are associated with different contour prescriptions. We demonstrate how the isomorphism between these spaces can be established through a unitary transformation of the basis states. Furthermore, we establish that certain contour deformations lead to the emergence of additional eigenvalues in the spectrum, which are not present in other contours. The study thus provides a detailed understanding of the spectral behavior of the $\\mathcal{PT}$-symmetric $(-x^4)$ theory, and highlights the importance of choosing the appropriate contour prescription to accurately depict the spectrum of the system. The results obtained have significant implications for the study of PT-symmetric Hamiltonians in quantum field theory, and contribute to our understanding of the fundamental properties of complex systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1091  This paper is concerned with the large-time behavior of solutions to the Cauchy problem of the one-dimensional viscous radiative and reactive gas. Based on the elaborate energy estimates, we developed a new approach to derive the upper bound of the absolute temperature. Our results have improved the results obtained in Liao and Zhao [{\\it J. Differential Equations} {\\bf 265} (2018), no.5, 2076-2120]. 0 into PostgreSQL...\n",
      "Inserting test sample 1092  This paper presents remarks on the Cauchy problem of the one-dimensional viscous radiative and reactive gas. The problem is formulated in a mathematical framework considering the radiation effects. The solutions for various initial conditions are obtained through analytical and numerical methods. The study reveals that viscosity and radiation significantly impact the dynamics of the gas, leading to the formation of shocks and acceleration waves. The results provide insights for understanding the behavior of actual gas systems in various contexts. 1 into PostgreSQL...\n",
      "Inserting test sample 1093  Determining which small exoplanets have stony-iron compositions is necessary for quantifying the occurrence of such planets and for understanding the physics of planet formation. Kepler-10 hosts the stony-iron world Kepler-10b (K10b), and also contains what has been reported to be the largest solid silicate-ice planet, Kepler-10c (K10c). Using 220 radial velocities (RVs), including 72 precise RVs from Keck-HIRES of which 20 are new from 2014-2015, and 17 quarters of Kepler photometry, we obtain the most complete picture of the Kepler-10 system to date. We find that K10b (Rp=1.47 Re) has mass 3.72$\\pm$0.42 Me and density 6.46$\\pm$0.73 g/cc. Modeling the interior of K10b as an iron core overlaid with a silicate mantle, we find that the iron core constitutes 0.17$\\pm$0.11 of the planet mass. For K10c (Rp=2.35 Re) we measure Mp=13.98$\\pm$1.79 Me and $\\rho$=5.94$\\pm$0.76 g/cc, significantly lower than the mass computed in Dumusque et al. (2014, 17.2$\\pm$1.9 Me). Internal compositional modeling reveals that at least $10\\%$ of the radius of Kepler-10c is a volatile envelope composed of hydrogen-helium ($0.2\\%$ of the mass, $16\\%$ of the radius) or super-ionic water ($28\\%$ of the mass, $29\\%$ of the radius).\n",
      "\n",
      "Analysis of only HIRES data yields a higher mass for K10b and a lower mass for K10c than does analysis of the HARPS-N data alone, with the mass estimates for K10c formally inconsistent by 3$\\sigma$. Splitting the RVs from each instrument leads to inconsistent measurements for the mass of planet c in each data set.\n",
      "\n",
      "This suggests that time-correlated noise is present and that the uncertainties in the planet masses (especially K10c) exceed our formal estimates. Transit timing variations (TTVs) of K10c indicate the likely presence of a third planet in the system, KOI-72.X. The TTVs and RVs are consistent with KOI-72.X having an orbital period of 24, 71, or 101 days, and a mass from 1-7 Me. 0 into PostgreSQL...\n",
      "Inserting test sample 1094  The discovery of exoplanets and their subsequent characterization continues to be of utmost importance in our understanding of the formation and evolution of planetary systems. One such system is Kepler-10, which previously exhibited the smallest exoplanet detected at the time of its discovery. In this paper, we present revised masses and densities of the planets around Kepler-10. \n",
      "\n",
      "Our findings utilize data from NASAâ€™s Kepler spacecraft, which observed the planetary transits of Kepler-10 from 2009 to 2013. The data was then combined with additional observations from the HARPS-North spectrograph in order to accurately determine planetary masses and densities. \n",
      "\n",
      "Our revised calculations reveal that Kepler-10b, the innermost planet in the system, has a mass and density significantly larger than previously thought. The new measurements suggest that Kepler-10b is a rocky planet with an iron core, rather than a low-density gas planet. Our findings also indicate that Kepler-10c, the outer planet in the system, has a higher density than previous estimates, which could imply that its composition is different from that of Kepler-10b. \n",
      "\n",
      "The revised planetary masses and densities have important implications for our understanding of planetary formation and evolution. The revised density of Kepler-10b suggests that it may have formed closer to its host star, where higher temperatures would have caused volatile elements to evaporate. Meanwhile, Kepler-10c's relatively high density could indicate that it formed farther from the host star, where lower temperatures would have facilitated the accretion of heavier elements. \n",
      "\n",
      "In conclusion, our study provides an improved understanding of the planetary system around Kepler-10. The revised measurements of planetary masses and densities have significant implications for our understanding of exoplanetary formation and evolution. Further studies of this system, as well as other exoplanetary systems, will continue to deepen our understanding of the diversity of planetary systems in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1095  We review a method for click-through rate prediction based on the work of Menon et al. [11], which combines collaborative filtering and matrix factorization with a side-information model and fuses the outputs to proper probabilities in [0,1]. In addition we provide details, both for the modeling as well as the experimental part, that are not found elsewhere. We rigorously test the performance on several test data sets from consecutive days in a click-through rate prediction setup, in a manner which reflects a real-world pipeline. Our results confirm that performance can be increased using latent features, albeit the differences in the measures are small but significant. 0 into PostgreSQL...\n",
      "Inserting test sample 1096  This paper proposes a novel approach for predicting clicks in online display advertising using latent features and side-information. We exploit the hidden patterns present in user behavior by leveraging the power of matrix factorization. Our methodology incorporates user and item characteristics, such as geographic location and age, as side-information to improve the accuracy of the predictions. To evaluate our model, we conducted experiments on two real-world datasets. The results demonstrate that the proposed approach outperforms state-of-the-art techniques. Our work contributes to improving the efficiency of recommendation systems in online advertising, which can lead to more effective marketing campaigns and better user engagement. 1 into PostgreSQL...\n",
      "Inserting test sample 1097  Using a hydrodynamic simulation of a LCDM universe, we investigate the \"X-ray forest\" absorption imprinted on the spectra of background quasars by the intervening intergalactic medium (IGM). In agreement with previous studies, we find that OVII and OVIII produce the strongest absorption features. The strong oxygen absorbers that might be detectable with Chandra or XMM-Newton arise in gas with T ~ 10^6 K and overdensities delta >~ 100 that are characteristic of galaxy groups. Future X-ray missions could detect weaker oxygen absorption produced by gas with a wider range of temperatures and the lower densities of unvirialized structures; they could also detect X-ray forest absorption by C, N, Ne, Fe, and possibly Si. If the IGM metallicity is 0.1 solar, then the predicted number of systems strong enough for a ~5\\sigma detection with Chandra or XMM-Newton is extremely low, though scatter in metallicity would increase the number of strong absorbers even if the mean metallicity remained the same.\n",
      "\n",
      "Our simulation reproduces the high observed incidence of OVI absorbers (in the UV), and the most promising strategy for finding the X-ray forest is to search at the redshifts of known OVI systems, thus reducing the signal-to-noise threshold required for a significant detection. However, while many OVI absorbers have associated OVII or OVIII absorption, the OVI systems trace only the low temperature phases of the X-ray forest, and a full accounting of the strong OVII and OVIII systems will require a mission with the anticipated capabilities of Constellation-X. The large effective area of the XEUS satellite would make it an extremely powerful instrument for studying the IGM, measuring X-ray forest absorption by a variety of elements and revealing the shock-heated filaments that may be an important reservoir of cosmic baryons. 0 into PostgreSQL...\n",
      "Inserting test sample 1098  The absorption of X-rays by the intergalactic medium (IGM) provides important insight into the physical properties, distribution, and evolution of the matter within the IGM. In this paper, a numerical study of X-ray absorption by the low-redshift IGM is performed using the Lambda Cold Dark Matter (CDM) model. The analysis is based on a large-scale simulation of the IGM using a state-of-the-art hydrodynamic simulation code that incorporates radiative transfer of ionizing radiation from galaxies and quasars.\n",
      "\n",
      "The results show that the X-ray absorption cross section of the IGM increases rapidly with decreasing X-ray energy, and the absorption is dominated by photoionization of He-like and H-like ions. Furthermore, the IGM temperature and density have a strong impact on the X-ray absorption, with the absorption decreasing at higher temperatures and densities. A comparison of the simulation results with observations of the X-ray background radiation suggests that the contribution of the IGM to the diffuse X-ray background is significant and may account for a significant fraction of the observed excess.\n",
      "\n",
      "The numerical simulation results provide a valuable tool for studying the physical properties and distribution of matter in the low-redshift IGM. They show that the Lambda CDM model is capable of reproducing the observed properties of the IGM and provide a framework for future studies of the IGM using X-ray absorption as a diagnostic. Overall, this work contributes to our understanding of the role of the IGM in the evolution of the universe and the formation of structures. 1 into PostgreSQL...\n",
      "Inserting test sample 1099  Consider the symmetric non-local Dirichlet form $(D,\\D(D))$ given by $$ D(f,f)=\\int_{\\R^d}\\int_{\\R^d}\\big(f(x)-f(y)\\big)^2 J(x,y)\\,dx\\,dy $$with $\\D(D)$ the closure of the set of $C^1$ functions on $\\R^d$ with compact support under the norm $\\sqrt{D_1(f,f)}$, where $D_1(f,f):=D(f,f)+\\int f^2(x)\\,dx$ and $J(x,y)$ is a nonnegative symmetric measurable function on $\\R^d\\times \\R^d$. Suppose that there is a Hunt process $(X_t)_{t\\ge 0}$ on $\\R^d$ corresponding to $(D,\\D(D))$, and that $(L,\\D(L))$ is its infinitesimal generator. We study the intrinsic ultracontractivity for the Feynman-Kac semigroup $(T_t^V)_{t\\ge 0}$ generated by $L^V:=L-V$, where $V\\ge 0$ is a non-negative locally bounded measurable function such that Lebesgue measure of the set $\\{x\\in \\R^d: V(x)\\le r\\}$ is finite for every $r>0$. By using intrinsic super Poincar\\'{e} inequalities and establishing an explicit lower bound estimate for the ground state, we present general criteria for the intrinsic ultracontractivity of $(T_t^V)_{t\\ge 0}$. In particular, if $$J(x,y)\\asymp|x-y|^{-d-\\alpha}\\I_{\\{|x-y|\\le 1\\}}+e^{-|x-y|^\\gamma}\\I_{\\{|x-y|> 1\\}}$$ for some $\\alpha \\in (0,2)$ and $\\gamma\\in(1,\\infty]$, and the potential function $V(x)=|x|^\\theta$ for some $\\theta>0$, then $(T_t^V)_{t\\ge 0}$ is intrinsically ultracontractive if and only if $\\theta>1$. When $\\theta>1$, we have the following explicit estimates for the ground state $\\phi_1$ $$c_1\\exp\\Big(-c_2 \\theta^{\\frac{\\gamma-1}{\\gamma}}|x| \\log^{\\frac{\\gamma-1}{\\gamma}}(1+|x|)\\Big) \\le \\phi_1(x) \\le c_3\\exp\\Big(-c_4 \\theta^{\\frac{\\gamma-1}{\\gamma}}|x| \\log^{\\frac{\\gamma-1}{\\gamma}}(1+|x|)\\Big) ,$$ where $c_i>0$ $(i=1,2,3,4)$ are constants. We stress that, our method efficiently applies to the Hunt process $(X_t)_{t \\ge 0}$ with finite range jumps, and some irregular potential function $V$ such that $\\lim_{|x| \\to \\infty}V(x)\\neq\\infty$. 0 into PostgreSQL...\n",
      "Inserting test sample 1100  This research paper studies the intrinsic ultracontractivity of Feynman-Kac semigroups in relation to symmetric jump processes. More specifically, we examine properties of these semigroups under the influence of two conditions: the curvature dimension inequality and the strong local Dirichlet form. These are fundamental conditions in the field of spectral theory and analysis of semigroups. Our analysis builds on a previous result, which demonstrated that the aforementioned conditions lead to hyperboundedness of the semigroups. We prove that intrinsic ultracontractivity is a logical consequence of hyperboundedness, and thus conclude that these conditions imply the intrinsic ultracontractivity of Feynman-Kac semigroups for symmetric jump processes. Our main result sheds light on the relationship between spectral theory and the stochastic calculus of ItÃ´. In addition to the technical results, we also provide a literature review of previous research in the field. The paper presents a key step towards a thorough understanding of the mathematical foundations of the Feynman-Kac formula and the associated semigroups. Our findings have practical implications in fields such as mathematical finance, probability theory, and quantum mechanics, where these concepts and tools are widely used to model financial derivatives, to study stochastic processes, and to analyze quantum systems, respectively. Through this research paper, we hope to contribute to the development of the theoretical framework underlying these fields, and to inspire new applications and further research. 1 into PostgreSQL...\n",
      "Inserting test sample 1101  Observations of hot Jupiters around solar-type stars with very short orbital periods (~day) suggest that tidal dissipation in such stars is not too efficient so that these planets can survive against rapid orbital decay. This is consistent with recent theoretical works, which indicate that the tidal Q of planet-hosting stars can indeed be much larger than the values inferred from stellar binaries. On the other hand, recent measurements of Rossiter-McLaughlin effects in transiting hot Jupiter systems not only reveal that many such systems have misaligned stellar spin with respect to the orbital axis, but also show that systems with cooler host stars tend to have aligned spin and orbital axes. Winn et al. suggested that this obliquity - temperature correlation may be explained by efficient damping of stellar obliquity due to tidal dissipation in the star. This explanation, however, is in apparent contradiction with the survival of these short-period hot Jupiters. We show that in the solar-type parent stars of close-in exoplanetary systems, the effective tidal Q governing the damping of stellar obliquity can be much smaller than that governing orbital decay. This is because for misaligned systems, the tidal potential contains a Fourier component with frequency equal to the stellar spin frequency (in the rotating frame of the star). This component can excite inertial waves in the convective envelope of the star, and the dissipation of inertial waves then leads to a spin-orbit alignment torque, but not orbital decay. By contrast, for aligned systems, such inertial wave excitation is forbidden since the tidal forcing frequency is much larger than the stellar spin frequency. We derive a general effective tidal evolution theory for misaligned binaries, taking account of different tidal responses and dissipation rates for different tidal forcing components. 0 into PostgreSQL...\n",
      "Inserting test sample 1102  The study of tidal effects in planet-hosting stars is an important field, as it can shed light on the mechanisms driving the dynamics of planetary systems. This paper presents a detailed analysis of tidal dissipation in stars hosting hot Jupiters, and the resulting effects on the spin-orbit misalignment of these planets. We use a combination of analytical and numerical models to simulate the tidal forces acting on the host star, and how they affect the spin and orientation of the planet's orbit.\n",
      "\n",
      "Our results show that tidal dissipation plays a crucial role in damping the spin-orbit misalignment of hot Jupiters. Specifically, we find that the timescales for alignment are strongly dependent on the properties of the star, such as its mass and radius. Additionally, we examine the survival rates of hot Jupiters under different conditions of tidal dissipation, and find that planets with higher levels of dissipation are less likely to survive on long timescales.\n",
      "\n",
      "We also investigate the effects of tidal evolution on the internal structure of the host star. Our models predict that tidal heating can significantly affect the thermal and structural evolution of the star, leading to changes in its luminosity and composition over time.\n",
      "\n",
      "Overall, our study provides insights into the complex interplay between tidal effects and the dynamics of planetary systems. Our findings have important implications for understanding the formation and evolution of hot Jupiters, and for the search for habitable exoplanets. 1 into PostgreSQL...\n",
      "Inserting test sample 1103  Graphs are used in almost every scientific discipline to express relations among a set of objects. Algorithms that compare graphs, and output a closeness score, or a correspondence among their nodes, are thus extremely important.\n",
      "\n",
      "Despite the large amount of work done, many of the scalable algorithms to compare graphs do not produce closeness scores that satisfy the intuitive properties of metrics. This is problematic since non-metrics are known to degrade the performance of algorithms such as distance-based clustering of graphs (Stratis and Bento 2018). On the other hand, the use of metrics increases the performance of several machine learning tasks (Indyk et al. 1999, Clarkson et al. 1999, Angiulli et al. 2002, Ackermann et al. 2010). In this paper, we introduce a new family of multi-distances (a distance between more than two elements) that satisfies a generalization of the properties of metrics to multiple elements. In the context of comparing graphs, we are the first to show the existence of multi-distances that simultaneously incorporate the useful property of alignment consistency (Nguyen et al. 2011), and a generalized metric property. Furthermore, we show that these multi-distances can be relaxed to convex optimization problems, without losing the generalized metric property. 0 into PostgreSQL...\n",
      "Inserting test sample 1104  The problem of computing distances between data points is a fundamental task in machine learning, clustering, and other fields. In the context of multiple graphs, measuring the similarity between their nodes is a major challenge. One way to tackle this problem is to define $n$-metrics, which generalize metrics to multiple graphs. While many such metrics exist, most of them are computationally intractable for large graphs. This paper introduces a new class of $n$-metrics, called tractable $n$-metrics, which can be calculated efficiently for multiple graphs. The proposed metrics are shown to be a generalization of the well-known family of $p$-norms and exhibit desirable properties such as symmetry, positive semi-definiteness and triangle inequality. We present a rigorous mathematical analysis of the proposed metrics, and explain how they can be computed efficiently using algebraic techniques. Empirical evaluation on several datasets demonstrates the effectiveness and efficiency of tractable $n$-metrics in various applications, including graph classification, similarity search, and clustering. We believe that the proposed metrics will be a valuable tool in the analysis of complex and large-scale graphs, and will yield important insights in various domains. 1 into PostgreSQL...\n",
      "Inserting test sample 1105  An ensemble with random n-body interactions is investigated in the presence of symmetries. A striking emergence of regularities in spectra, ground state spins and isospins is discovered in both odd and even-particle systems. Various types of correlations from pairing to spectral sequences and correlations across different masses are explored. A search for interpretation is presented. 0 into PostgreSQL...\n",
      "Inserting test sample 1106  Symmetry arises under certain conditions from seemingly chaotic n-body interactions. Through statistical mechanics, we derive a general framework describing the emergence of symmetry in these complex systems. We show that symmetry can arise from random interactions and depends on the underlying statistical properties of the system. Our findings shed new light on the nature of symmetry and its occurrence in diverse scientific domains. 1 into PostgreSQL...\n",
      "Inserting test sample 1107  Negative thermal expansion (NTE) describes the anomalous propensity of materials to shrink when heated. Since its discovery, the NTE effect has been found in a wide variety of materials with an array of magnetic, electronic and structural properties. In some cases, the NTE originates from phase competition arising from the electronic or magnetic degrees of freedom but we here focus on a particular class of NTE which originates from intrinsic dynamical origins related to the lattice degrees of freedom, a property we term \\textit{structural} negative thermal expansion (SNTE). Here we review some select cases of NTE which strictly arise from anharmonic phonon dynamics, with a focus on open perovskite lattices. We find that NTE is often present close in proximity to competing structural phases, with structural phase transition lines terminating near $T$=0 K yielding the most superlative displays of the SNTE effect. We further provide a theoretical model to make precise the proposed relationship among the signature behavior of SNTE, the proximity of these systems to structural quantum phase transitions and the effects of phase fluctuations near these unique regions of the structural phase diagram. The effects of compositional disorder on NTE and structural phase stability in perovskites are discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 1108  Structural stability plays a crucial role in determining the physical properties of materials. In particular, perovskite materials have been studied extensively due to their numerous applications in technology. Negative thermal expansion (NTE), which is a phenomenon in which volume decreases upon heating, has attracted considerable attention in recent years due to its potential use for various technological applications. In this study, we investigate the NTE properties of open perovskite materials. Our results indicate that these materials exhibit NTE behavior near the precipice of structural stability. We attribute this behavior to the presence of low-energy modes in the crystal structure, which result in a softening response to changes in temperature. We also observe that the NTE effect is more pronounced in materials with a smaller unit cell volume and that it increases with an increase in temperature. Our findings have potential applications in developing materials with tailored thermal expansion properties for specific technological uses. This study contributes to a deeper understanding of the relationship between crystal structure and physical properties and may pave the way for new advances in materials science. 1 into PostgreSQL...\n",
      "Inserting test sample 1109  We present abundances of C, N, O, F, Na, and Fe in six giant stars of the tidally disrupted globular cluster NGC 6712. The abundances were derived by comparing synthetic spectra with high resolution infrared spectra obtained with the Phoenix spectrograph on the Gemini South telescope. We find large star-to-star abundance variations of the elements C, N, O, F, and Na. NGC 6712 and M4 are the only globular clusters in which F has been measured in more than two stars, and both clusters reveal F abundance variations whose amplitude is comparable to, or exceeds, that of O, a pattern which may be produced in M > 5M_sun AGB stars. Within the limited samples, the F abundance in globular clusters is lower than in field and bulge stars at the same metallicity. NGC 6712 and Pal 5 are tidally disrupted globular clusters whose red giant members exhibit O and Na abundance variations not seen in comparable metallicity field stars. Therefore, globular clusters like NGC 6712 and Pal 5 cannot contribute many field stars and/or field stars do not form in environments with chemical enrichment histories like that of NGC 6712 and Pal 5. Although our sample size is small, from the amplitude of the O and Na abundance variations, we infer a large initial cluster mass and tentatively confirm that NGC 6712 was once one of the most massive globular clusters in our Galaxy. 0 into PostgreSQL...\n",
      "Inserting test sample 1110  We present an analysis of the chemical composition of giant stars belonging to the tidally disrupted globular cluster NGC 6712 observed with high-resolution infrared spectroscopy. The obtained spectra allowed us to derive abundances of 29 elements, including both light and heavy elements. We found that the cluster's metallicity is [Fe/H] = -1.43 Â± 0.02, which is consistent with previous estimates. Furthermore, we determined the abundance ratios for several Î±, iron-peak, and neutron-capture elements, revealing the cluster's chemical enrichment history. These ratios showed that NGC 6712 follows the typical trends observed in Galactic globular clusters. Interestingly, our study also revealed a star-to-star variation in the abundance of neutron-capture elements, suggesting the occurrence of an individual nucleosynthesis mechanism in the cluster's star-forming history. Additionally, we identified a likely member of the cluster's blue straggler population, which exhibits a peculiar chemical composition compared to other cluster members. These results increase our understanding of the formation and evolution of tidally disrupted globular clusters and provide important constraints on theoretical models. Finally, we discuss the potential of IR spectroscopy for future studies of globular clusters. 1 into PostgreSQL...\n",
      "Inserting test sample 1111  As the complexity and volume of datasets have increased along with the capabilities of modular, open-source, easy-to-implement, visualization tools, scientists' need for, and appreciation of, data visualization has risen too.\n",
      "\n",
      "Until recently, scientists thought of the \"explanatory\" graphics created at a research project's conclusion as \"pretty pictures\" needed only for journal publication or public outreach. The plots and displays produced during a research project - often intended only for experts - were thought of as a separate category, what we here call \"exploratory\" visualization. In this view, discovery comes from exploratory visualization, and explanatory visualization is just for communication. Our aim in this paper is to spark conversation amongst scientists, computer scientists, outreach professionals, educators, and graphics and perception experts about how to foster flexible data visualization practices that can facilitate discovery and communication at the same time. We present an example of a new finding made using the glue visualization environment to demonstrate how the border between explanatory and exploratory visualization is easily traversed. The linked-view principles as well as the actual code in glue are easily adapted to astronomy, medicine, and geographical information science - all fields where combining, visualizing, and analyzing several high-dimensional datasets yields insight. Whether or not scientists can use such a flexible \"undisciplined\" environment to its fullest potential without special training remains to be seen. We conclude with suggestions for improving the training of scientists in visualization practices, and of computer scientists in the iterative, non-workflow-like, ways in which modern science is carried out. 0 into PostgreSQL...\n",
      "Inserting test sample 1112  This research paper presents new perspectives on data visualization, exploring the potential of emerging tools to unlock the value of complex data sets. Drawing on a range of sources, the paper highlights the transformative power of data visualization for researchers and practitioners in diverse fields, from economics and public health to engineering and computer science. \n",
      "\n",
      "The paper begins by examining the theoretical underpinnings of data visualization, situating it within the broader context of information design and communication. It then goes on to explore the latest tools and techniques for visualizing data, including 3D graphics, interactive dashboards, and machine learning algorithms. \n",
      "\n",
      "One major theme of the paper is the importance of collaboration in data visualization. By engaging with collaborators from diverse backgrounds, researchers and practitioners can leverage their collective expertise to gain new insights and insights. The paper profiles several successful collaborations, demonstrating how strong partnerships can lead to more effective and impactful visualizations. \n",
      "\n",
      "Another key theme is the need to empower users of data visualizations to engage with their data in new and creative ways. The paper argues that data visualization can become even more powerful when users are given the tools to explore their data interactively, rather than simply being presented with pre-determined visualizations. \n",
      "\n",
      "Ultimately, the paper argues that data visualization offers enormous potential for advancing research and practice in diverse fields. However, it also acknowledges the need for continued research and innovation in this rapidly-evolving field, in order to fully realize its transformative potential. 1 into PostgreSQL...\n",
      "Inserting test sample 1113  The circumstellar disk of AB Aurigae has garnered strong attention owing to the apparent existence of spirals at a relatively young stage and also the asymmetric disk traced in thermal dust emission. However, the physical conditions of the spirals are still not well understood. The origin of the asymmetric thermal emission is unclear.\n",
      "\n",
      "We observed the disk at 230 GHz (1.3 mm) in both the continuum and the spectral line ^12CO J=2-1 with IRAM 30-m, the Plateau de Bure interferometer, and the Submillimeter Array to sample all spatial scales from 0.37\" to about 50\". To combine the data obtained from these telescopes, several methods and calibration issues were checked and discussed.\n",
      "\n",
      "The 1.3 mm continuum (dust) emission is resolved into inner disk and outer ring. Molecular gas at high velocities traced by the CO line is detected next to the stellar location. The inclination angle of the disk is found to decrease toward the center. On a larger scale, based on the intensity weighted dispersion and the integrated intensity map of ^12CO J=2-1, four spirals are identified, where two of them are also detected in the near infrared. The total gas mass of the 4 spirals (M_spiral) is 10^-7 < M_spiral < 10^-5 M_sun, which is 3 orders of magnitude smaller than the mass of the gas ring. Surprisingly, the CO gas inside the spiral is apparently counter-rotating with respect to the CO disk, and it only exhibits small radial motion.\n",
      "\n",
      "The wide gap, the warped disk, and the asymmetric dust ring suggest that there is an undetected companion with a mass of 0.03 M_sun at a radius of 45 AU. Although an hypothetical fly-by cannot be ruled out, the most likely explanation of the AB Aurigae system may be inhomogeneous accretion well above or below the main disk plane from the remnant envelope, which can explain both the rotation and large-scale motions detected with the 30-m image. 0 into PostgreSQL...\n",
      "Inserting test sample 1114  AB Aurigae is a young, Herbig Ae star, whose circumstellar disk has been studied by a team of scientists in order to shed light on its formation and evolution. In this paper, we present the results of the analysis of high angular resolution observations obtained with ALMA, which reveal the presence of spiral arms and a gap in the disk that might be indicative of ongoing planet formation. \n",
      "\n",
      "The disk around AB Aurigae is characterized by a complex morphology, with a central cavity and a bright ring of dust at about 100 au from the star, which is likely to host ongoing planet formation based on numerical simulations. The ALMA observations also reveal the presence of multiple spiral arms, which could be caused by the gravitational interaction between the disk and a putative companion, or by the presence of planet-induced gaps that are perturbed by the gravitational interaction with the star. \n",
      "\n",
      "Interestingly, the data provide evidence for envelope accretion onto the disk, which might suggest that AB Aurigae is still undergoing active mass accretion even at late stages of its formation. The accretion rate derived from the observations is about 10^-7 solar masses per year, which is in agreement with previous estimates from other indicators. \n",
      "\n",
      "Overall, our study shows that high angular resolution observations of the circumstellar environment around young stars can provide useful insights into the formation and evolution of planetary systems. Future observations with more advanced ALMA configurations, or with the upcoming next-generation radio telescopes, will allow us to probe even smaller scales and unravel the details of the planet formation process in greater detail. 1 into PostgreSQL...\n",
      "Inserting test sample 1115  Understanding the behaviour of hosts of SARS-CoV-2 is crucial to our understanding of the virus. A comparison of environmental features related to the incidence of SARS-CoV-2 with those of its potential hosts is critical. We examine the distribution of coronaviruses among bats. We analyse the distribution of SARS-CoV-2 in a nine-week period following lockdown in Italy, Spain, and Australia. We correlate its incidence with environmental variables particularly ultraviolet radiation, temperature, and humidity. We establish a clear negative relationship between COVID-19 and ultraviolet radiation, modulated by temperature and humidity. We relate our results with data showing that the bat species most vulnerable to coronavirus infection are those which live in environmental conditions that are similar to those that appear to be most favourable to the spread of COVID-19. The SARS-CoV-2 ecological niche has been the product of long-term coevolution of coronaviruses with their host species. Understanding the key parameters of that niche in host species allows us to predict circumstances where its spread will be most favourable. Such conditions can be summarised under the headings of nocturnality and seasonality. High ultraviolet radiation, in particular, is proposed as a key limiting variable. We therefore expect the risk of spread of COVID-19 to be highest in winter conditions, and in low light environments. Human activities resembling those of highly social cave-dwelling bats (e.g. large nocturnal gatherings or high density indoor activities) will only serve to compound the problem of COVID-19. 0 into PostgreSQL...\n",
      "Inserting test sample 1116  The ongoing COVID-19 pandemic has highlighted the importance of understanding the ecological niche of the causative agent, SARS-CoV-2. One aspect of this niche that has received little attention is the potential influence of the timing of viral transmission, such as nocturnality and seasonality. In this study, we explore the relationship between nocturnality, seasonality, and the transmission dynamics of SARS-CoV-2.\n",
      "\n",
      "We first reviewed the literature on the temporal patterns of SARS-CoV-2 transmission, finding evidence of possible nocturnal transmission, although this remains an open question warranting further investigation. We also examined seasonality, looking at both environmental factors, such as humidity and temperature, and social factors, such as school schedules and vacation patterns, which have been shown to impact disease transmission.\n",
      "\n",
      "To better understand the ecological niche of SARS-CoV-2, we developed a mathematical model to simulate the spread of the virus under different conditions of nocturnality and seasonality. Our results suggest that nocturnal transmission could increase the basic reproduction number (R0) of the virus, and that seasonal changes in human behavior and environmental conditions could have significant impacts on the rate of virus spread.\n",
      "\n",
      "Our findings highlight the need for future research to consider the role of nocturnality and seasonality in the transmission dynamics of SARS-CoV-2 and other infectious diseases. Understanding the ecological niche of this virus is crucial for developing effective prevention and control strategies and mitigating the impact of future pandemics. 1 into PostgreSQL...\n",
      "Inserting test sample 1117  A machine learning configuration refers to a combination of preprocessor, learner, and hyperparameters. Given a set of configurations and a large dataset randomly split into training and testing set, we study how to efficiently select the best configuration with approximately the highest testing accuracy when trained from the training set. To guarantee small accuracy loss, we develop a solution using confidence interval (CI)-based progressive sampling and pruning strategy. Compared to using full data to find the exact best configuration, our solution achieves more than two orders of magnitude speedup, while the returned top configuration has identical or close test accuracy. 0 into PostgreSQL...\n",
      "Inserting test sample 1118  In large datasets, selecting an appropriate machine learning configuration can be a time-consuming and computationally expensive process. This paper introduces ABC, a new technique for efficiently selecting the optimal configuration for machine learning tasks on large datasets. ABC leverages a combination of Bayesian optimization and active learning to intelligently search the configuration space, quickly identifying high-performing configurations and reducing the overall search time. Our experimental evaluation on a range of real-world datasets demonstrates that ABC outperforms existing state-of-the-art techniques in terms of time and accuracy, making it a promising approach for efficient machine learning configuration selection on large datasets. 1 into PostgreSQL...\n",
      "Inserting test sample 1119  The problem of coordinating the charging of electric vehicles gains more importance as the number of such vehicles grows. In this paper, we develop a method for the training of controllers for the coordination of EV charging. In contrast to most existing works on this topic, we require the controllers to preserve the privacy of the users, therefore we do not allow any communication from the controller to any third party.\n",
      "\n",
      "In order to train the controllers, we use the idea of imitation learning -- we first find an optimum solution for a relaxed version of the problem using quadratic optimization and then train the controllers to imitate this solution.\n",
      "\n",
      "We also investigate the effects of regularization of the optimum solution on the performance of the controllers. The method is evaluated on realistic data and shows improved performance and training speed compared to similar controllers trained using evolutionary algorithms. 0 into PostgreSQL...\n",
      "Inserting test sample 1120  Electric vehicles (EVs) are becoming increasingly popular due to their environmental friendliness. However, their widespread adoption is hindered by challenges with infrastructure and charging time. To alleviate these issues, we propose a novel approach using imitation learning to train EV charging controllers. Our method involves learning from expert human drivers to emulate their charging patterns and decision-making processes. We evaluate our approach using real-world EV charging data and show that it outperforms existing approaches in terms of charging time optimization and user satisfaction. Our findings suggest that imitation learning can play a significant role in the advancement of EV charging infrastructure and promote the widespread adoption of electric vehicles, leading to a more sustainable future. 1 into PostgreSQL...\n",
      "Inserting test sample 1121  Four-dimensional N=1 supersymmetric Spin(N) gauge theories with matter in the vector and spinor representations are considered. Dual descriptions are known for some of these theories. It is noted that when masses are given to all fields in the spinor representation, the dual gauge group G breaks to a group H such that \\pi_2(G/H)=Z_2. The quantum numbers of the associated Z_2 monopole and those of the massive spinors are shown to agree, suggesting that the monopole is the image of the massive spinors under duality. It follows that electric sources in the spinor representation, needed as test charges to determine the phase of an SO(N) gauge theory, can be introduced as Z_2-valued magnetic sources in the dual nonabelian gauge theory. This fact is used to study the phases of SO(N) gauge theories with matter in the vector representation. 0 into PostgreSQL...\n",
      "Inserting test sample 1122  This paper explores the duality, phases, spinors, and monopoles in SO(N) and Spin(N) gauge theories. The mathematical framework of these gauge theories is established, with a particular focus on the corresponding gauge fields and their properties. The novel role of spinors and monopoles in these theories is investigated, and their potential implications for applications in high-energy physics are discussed. The concept of duality is analyzed in relation to these gauge theories, and its role in elucidating important physical phenomena is highlighted. Finally, the diverse phases and symmetries exhibited by these gauge theories are explored in detail, providing a better understanding of the complex nature of SO(N) and Spin(N) gauge theories. This paper presents a comprehensive overview of these topics, offering insight into the fundamental properties of these gauge theories that may prove useful in future theoretical and experimental studies. 1 into PostgreSQL...\n",
      "Inserting test sample 1123  Recent observations of the central compact object in the Kesteven 79 supernova remnant show that this neutron star (NS) has a weak dipole magnetic field (a few x 10^{10} G) but an anomalously large (~ 64%) pulse fraction in its surface X-ray emission. We explore the idea that a substantial sub-surface magnetic field exists in the NS crust, which produces diffuse hot spots on the stellar surface due to anisotropic heat conduction, and gives rise to the observed X-ray pulsation. We develop a general-purpose method, termed \"Temperature Template with Full Transport\" (TTFT), that computes the synthetic pulse profile of surface X-ray emission from NSs with arbitrary magnetic field and surface temperature distributions, taking into account magnetic atmosphere opacities, beam pattern, vacuum polarization, and gravitational light bending.\n",
      "\n",
      "We show that a crustal toroidal magnetic field of order a few x 10^{14} G or higher, varying smoothly across the crust, can produce sufficiently distinct surface hot spots to generate the observed pulse fraction in the Kes 79 NS.\n",
      "\n",
      "This result suggests that substantial sub-surface magnetic fields, much stronger than the \"visible\" dipole fields, may be buried in the crusts of some young NSs, and such hidden magnetic fields can play an important role in their observational manifestations. The general TTFT tool we have developed can also be used for studying radiation from other magnetic NSs. 0 into PostgreSQL...\n",
      "Inserting test sample 1124  The study presented in this paper investigates the hidden magnetic field of the young neutron star in Kesteven 79 (K79). Neutron stars, due to their extreme physical properties, are known to possess some of the strongest magnetic fields in the universe, with some reaching intensities of over 10^15 Gauss.\n",
      "\n",
      "Using data obtained from the Chandra X-ray Observatory and the Very Large Array, we analyzed the X-ray and radio emissions from the pulsar in K79. Our findings suggest the presence of a dynamic and highly organized magnetic field structure around the neutron star. The X-rays observed indicate that the magnetic field is concentrated near the surface of the star and is likely responsible for accelerating charged particles to relativistic velocities.\n",
      "\n",
      "We also observed the neutron starâ€™s periodic radio pulses, which allowed us to determine the starâ€™s rotation rate and magnetic field strength. Our results show that the magnetic field is aligned with the starâ€™s rotational axis with a strength of approximately 10^12 Gauss.\n",
      "\n",
      "Our observations and analysis contribute to the understanding of neutron stars and their complex magnetic field structures. The study of pulsars such as the one in K79 provides insights into the fundamental physics of extreme environments and the behavior of matter under extreme physical conditions. 1 into PostgreSQL...\n",
      "Inserting test sample 1125  We study the hadronic decays of $\\Lambda_{c}^{+}$ to the final states $\\Sigma^{+}\\eta$ and $\\Sigma^+\\eta^\\prime$, using an $e^{+}e^{-}$ annihilation data sample of 567 pb$^{-1}$ taken at a center-of-mass energy of 4.6 GeV with the BESIII detector at the BEPCII collider. We find evidence for the decays $\\Lambda_{c}^{+}\\rightarrow\\Sigma^{+}\\eta$ and $\\Sigma^+\\eta^\\prime$ with statistical significance of $2.5\\sigma$ and $3.2\\sigma$, respectively.\n",
      "\n",
      "Normalizing to the reference decays $\\Lambda_c^+\\to\\Sigma^+\\pi^0$ and $\\Sigma^+\\omega$, we obtain the ratios of the branching fractions $\\frac{{\\mathcal B}(\\Lambda_c^+\\to\\Sigma^+\\eta)}{{\\mathcal B}(\\Lambda_c^+\\to\\Sigma^+\\pi^0)}$ and $\\frac{{\\mathcal B}(\\Lambda_c^+\\to\\Sigma^+\\eta^\\prime)}{{\\mathcal B}(\\Lambda_c^+\\to\\Sigma^+\\omega)}$ to be $0.35 \\pm 0.16 \\pm 0.03$ and $0.86 \\pm 0.34 \\pm 0.07$, respectively. The upper limits at the 90\\% confidence level are set to be $\\frac{{\\mathcal B}(\\Lambda_c^+\\to\\Sigma^+\\eta)}{{\\mathcal B}(\\Lambda_c^+\\to\\Sigma^+\\pi^0)}<0.58$ and $\\frac{{\\mathcal B}(\\Lambda_c^+\\to\\Sigma^+\\eta^\\prime)}{{\\mathcal B}(\\Lambda_c^+\\to\\Sigma^+\\omega)}<1.2$. Using BESIII measurements of the branching fractions of the reference decays, we determine $\\mathcal B({\\Lambda_{c}^{+}\\rightarrow\\Sigma^{+}\\eta})=(0.41\\pm0.19\\pm0.05)\\%$ ($<0.68\\%$) and $\\mathcal B({\\Lambda_{c}^{+}\\rightarrow\\Sigma^{+}\\eta'})=(1.34\\pm0.53\\pm0.21)\\%$ ($<1.9\\%$). Here, the first uncertainties are statistical and the second systematic. The obtained branching fraction of $\\Lambda_c^+\\to\\Sigma^+\\eta$ is consistent with the previous measurement, and the branching fraction of $\\Lambda_{c}^{+}\\rightarrow\\Sigma^{+}\\eta^{\\prime}$ is measured for the first time. 0 into PostgreSQL...\n",
      "Inserting test sample 1126  The $\\Lambda^+_{c}$ baryon is a crucial particle in hadron physics and plays a pivotal role in understanding the strong interaction dynamics. In this paper, we present experimental evidence for the decays of $\\Lambda^+_{c}\\to\\Sigma^+\\eta$ and $\\Sigma^+\\eta^\\prime$. The data were collected by the LHCb experiment at the Large Hadron Collider (LHC) in proton-proton collisions at a center-of-mass energy of 13 TeV. Using a data sample consisting of integrated luminosity of 9 fb$^{-1}$, we observed signals at 3.2$\\sigma$ and 4.8$\\sigma$ in the $\\Sigma^+\\eta$ and $\\Sigma^+\\eta^\\prime$ modes, respectively. The measurements are the first experimental evidence for these modes in the charged $\\Lambda^+_{c}$ decay channel.\n",
      "\n",
      "The observed branching ratios are measured to be ${\\mathcal{B}}(\\Lambda^+_{c}\\to\\Sigma^+\\eta)=(3.94\\pm0.84^{+1.13}_{-0.87})\\times10^{-3}$ and ${\\mathcal{B}}(\\Lambda^+_{c}\\to\\Sigma^+\\eta^\\prime)=(10.23\\pm1.93^{+2.85}_{-2.16})\\times10^{-3}$, where the first uncertainties are statistical and the second systematic. The results are compared with theoretical predictions based on different models. The measurements provide new information for future understanding of the strong interaction dynamics in the charm baryon sector.\n",
      "\n",
      "In conclusion, we present the first experimental evidence for the decays of $\\Lambda^+_{c}\\to\\Sigma^+\\eta$ and $\\Sigma^+\\eta^\\prime$ in the charged baryon decay channel. The measurements are based on proton-proton collisions at the LHC and are found to be consistent with theoretical predictions. Our findings contribute to the understanding of the strong interaction dynamics and open doors to further investigations of the charm baryon sector. 1 into PostgreSQL...\n",
      "Inserting test sample 1127  We discuss further our proposed modification of the Susskind-Horowitz-Polchinski scenario in which black hole entropy goes over to string entropy as one scales the string length scale up and the string coupling constant down, keeping Newton's constant unchanged. In our approach, based on our 'matter-gravity entanglement hypothesis', 'string entropy' here should be interpreted as the likely entanglement entropy between (approximately) the single long string and the stringy atmosphere which, as we argue, arise in a pure multistring state of given energy in a (rescaled) box. In a previous simple analysis, we computed this entropy (with promising results) by assuming simple exponentially increasing densities of states for both long string and stringy atmosphere. Here, our starting point is a (more correct) density of states for each single string with the appropriate inverse-power prefactor and a low-energy cutoff. We outline how the relevant entanglement entropy should be calculated for this system and propose a plausible model, which draws on the work of Mitchell and Turok on the multi-string microcanonical ensemble, in which we adopt a similar density of states for long string and stringy atmosphere but now with cutoffs which scale with the total energy, E. With this scaling, we still find our entanglement entropy grows, in leading order, linearly with E and this translates to a black-hole entropy which goes as the square of the black-hole mass, thus retaining most of the promising features of our previous exponential-density-of-states model and providing further evidence for our matter-gravity entanglement hypothesis. 0 into PostgreSQL...\n",
      "Inserting test sample 1128  Black holes are fascinating objects in the universe that continue to challenge physicists' understanding of space and time. One important aspect of black holes is their equilibrium states, where they balance incredibly strong gravitational forces with matter and energy interactions. In recent years, there has been a growing interest in the so-called stringy limit of black hole equilibria, which explores these states using advanced mathematical techniques inspired by string theory.\n",
      "\n",
      "This research paper presents a detailed investigation into the stringy limit of black hole equilibria, building on prior work in the field and introducing new insights and results. The study leverages theoretical tools like effective actions and gauge theories to more deeply understand the intricate relationships between the strings, black holes, and equilibria states. As part of the work, the researchers provide practical formulas and techniques for modeling the dynamics of black hole equilibria under different conditions.\n",
      "\n",
      "The findings of this paper have important implications for our understanding of the behavior of black holes and their surrounding environments. The insights gained from the stringy limit of black hole equilibria may also have practical applications in fields like astrophysics and cosmology, where black holes play a central role in many key questions about the universe's origins and evolution. Overall, the paper represents an important contribution to the ongoing effort to deepen our understanding of black holes and the fundamental physical laws that govern their behaviors. 1 into PostgreSQL...\n",
      "Inserting test sample 1129  Our planet is experiencing an accelerated process of change associated to a variety of anthropogenic phenomena. The future of this transformation is uncertain, but there is general agreement about its negative unfolding that might threaten our own survival. Furthermore, the pace of the expected changes is likely to be abrupt: catastrophic shifts might be the most likely outcome of this ongoing, apparently slow process. Although different strategies for geo-engineering the planet have been advanced, none seem likely to safely revert the large-scale problems associated to carbon dioxide accumulation or ecosystem degradation. An alternative possibility considered here is inspired in the rapidly growing potential for engineering living systems. It would involve designing synthetic organisms capable of reproducing and expanding to large geographic scales with the goal of achieving a long-term or a transient restoration of ecosystem-level homeostasis. Such a regional or even planetary-scale engineering would have to deal with the complexity of our biosphere. It will require not only a proper design of organisms but also understanding their place within ecological networks and their evolvability.\n",
      "\n",
      "This is a likely future scenario that will require integration of ideas coming from currently weakly connected domains, including synthetic biology, ecological and genome engineering, evolutionary theory, climate science, biogeography and invasion ecology, among others. 0 into PostgreSQL...\n",
      "Inserting test sample 1130  The field of bioengineering has been steadily growing over the past few decades, with many researchers exploring the potential applications and implications of manipulating biological systems. One area of interest that has emerged is the possibility of using bioengineering techniques to alter the biosphere itself - the sum total of all biological activity on Earth. This has led to a variety of discussions and debates about the potential benefits and risks of such an endeavor.\n",
      "\n",
      "Some researchers argue that bioengineering the biosphere could lead to significant improvements in areas like agriculture, medicine, and environmental conservation. For example, by altering the genetic makeup of crops, scientists could potentially create strains that are more resistant to pests and require less water to grow. Similarly, by engineering bacteria to break down pollutants more efficiently, we could help mitigate the damage done by industrial processes.\n",
      "\n",
      "However, others are more cautious about the potential consequences of bioengineering the biosphere. They point out that any attempt to manipulate such a complex system is likely to have unintended and unforeseeable consequences. Furthermore, they argue that altering the biosphere in such a way could fundamentally alter the balance of nature and have far-reaching ecological impacts.\n",
      "\n",
      "Overall, the question of bioengineering the biosphere is a complex and multifaceted one that will require careful consideration and analysis. While the potential benefits of such an endeavor are tantalizing, we must also be mindful of the potential risks and unintended consequences. 1 into PostgreSQL...\n",
      "Inserting test sample 1131  We compute the one-loop matching between the Standard Model Effective Field Theory and the low-energy effective field theory below the electroweak scale, where the heavy gauge bosons, the Higgs particle, and the top quark are integrated out. The complete set of matching equations is derived including effects up to dimension six in the power counting of both theories. We present the results for general flavor structures and include both the $CP$-even and $CP$-odd sectors. The matching equations express the masses, gauge couplings, as well as the coefficients of dipole, three-gluon, and four-fermion operators in the low-energy theory in terms of the parameters of the Standard Model Effective Field Theory. Using momentum insertion, we also obtain the matching for the $CP$-violating theta angles. Our results provide an ingredient for a model-independent analysis of constraints on physics beyond the Standard Model.\n",
      "\n",
      "They can be used for fixed-order calculations at one-loop accuracy and represent a first step towards a systematic next-to-leading-log analysis. 0 into PostgreSQL...\n",
      "Inserting test sample 1132  In this paper, we study the low-energy effective field theory below the electroweak scale in the context of matching at one loop. Our main focus is on the matching conditions that relate the parameters of the effective theory at the high-energy scale to those at the low-energy scale. We derive the one-loop matching corrections to the effective operators that contribute to various observables, such as the Higgs boson production cross-section, and investigate their impact on the precision of the predictions. Our analysis includes the renormalization of the effective theory parameters, which is necessary to ensure consistent and accurate predictions. By demonstrating the importance of one-loop matching corrections and renormalization, our results provide a significant advancement in the study of the low-energy effective field theory below the electroweak scale. These findings have important implications for future precision measurements at the LHC and other high-energy experiments. 1 into PostgreSQL...\n",
      "Inserting test sample 1133  The ability to predict the nature and amounts of plutonium emissions in industrial accidents, such as in solvent fires at PUREX nuclear reprocessing facilities, is a key concern of nuclear safety agencies. In accident conditions and in the presence of oxygen and water vapor, plutonium is expected to form the three major volatile species $\\rm{PuO_2}$, $\\rm{PuO_3}$, and $\\rm{PuO_2(OH)_2}$, for which the thermodynamic data necessary for predictions (enthalpies of formation and heat capacities) presently show either large uncertainties or are lacking. In this work we aim to alleviate such shortcomings by obtaining the aforementioned data via relativistic correlated electronic structure calculations employing a two-step multi-reference approach (MS-CASPT2 with SO-RASSI), which is able to describe the multireference character of the ground-state wave functions of $\\rm{PuO_3}$ and $\\rm{PuO_2(OH)_2}$. We benchmark this approach by comparing it to relativistic coupled cluster calculations for the ground, ionized, and excited states of $\\rm{PuO_2}$. Our results allow us to predict enthalpies of formation $\\Delta_fH^\\ominus(\\rm{298.15~K})$ of $\\rm{PuO_2}$, $\\rm{PuO_3}$ and $\\rm{PuO_2(OH)_2}$ to be $\\rm{-449.5\\pm8.8}$, $\\rm{-553.2\\pm27.5}$, and $\\rm{-1012.6\\pm38.1~kJ\\;mol^{-1}}$, respectively, which confirm the predominance of plutonium dioxide, but also reveal the existence of plutonium trioxide in the gaseous phase under oxidative conditions, though the partial pressures of $\\rm{PuO_3}$ and $\\rm{PuO_2(OH)_2}$ are nonetheless always rather low under a wet atmosphere. Our calculations also permit us to reassess prior results for $\\rm{PuO_2}$, establishing that the ground state of the $\\rm{PuO_2}$ molecule is mainly of $\\rm{^{5}\\Sigma_{g}^+}$ character, as well as to confirm the experimental value for the adiabatic ionization energy of $\\rm{PuO_2}$. 0 into PostgreSQL...\n",
      "Inserting test sample 1134  The accurate prediction of the thermodynamic properties of plutonium is of utmost importance to the nuclear industry for various safety and operational reasons. The behavior of plutonium under different environmental and processing conditions is complex, particularly with respect to its volatility. In this study, we present a computational approach for predicting the thermodynamic properties of volatile plutonium.\n",
      "\n",
      "Our methodology involved the use of first-principles techniques, which we validated through comparison with experimental data. We focused on predicting the Gibbs free energy of formation and the enthalpy of sublimation of plutonium. We investigated their dependence on temperature and pressure, and explored the effects of various chemical species and isotopes on these properties.\n",
      "\n",
      "Our results showed that our computational approach is highly accurate and can predict the thermodynamic properties of plutonium with an error of less than 5%. We found that the enthalpy of sublimation of plutonium is strongly influenced by temperature and pressure. Additionally, the presence of impurities, such as oxygen and carbon, can significantly affect the Gibbs free energy of formation.\n",
      "\n",
      "In conclusion, our study provides an accurate and reliable means for predicting the thermodynamic properties of volatile plutonium, while taking into account the effects of various impurities and isotopes. This information is essential for the safe and effective handling and processing of plutonium in various nuclear applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1135  Nonlinear dispersive partial differential equations such as the nonlinear Schr\\\"odinger equations can have solutions that blow-up. We numerically study the long time behavior and potential blowup of solutions to the focusing Davey-Stewartson II equation by analyzing perturbations of the lump and the Ozawa exact solutions. It is shown in this way that the lump is unstable to both blowup and dispersion, and that blowup in the Ozawa solution is generic. 0 into PostgreSQL...\n",
      "Inserting test sample 1136  This numerical study investigates the occurrence of blowup phenomena in the Davey-Stewartson system. We use a combination of numerical simulations and analytical techniques to explore the blow-up mechanism and identify its key features. Our results demonstrate that the blow-up in the Davey-Stewartson system is governed by a complex interplay between nonlinearity, dispersion, and other physical factors. This study contributes to the understanding of blow-up behavior in nonlinear systems and has implications for a wide range of physical applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1137  The identity of a user is permanently lost if biometric data gets compromised since the biometric information is irreplaceable and irrevocable. To revoke and reissue a new template in place of the compromised biometric template, the idea of cancelable biometrics has been introduced. The concept behind cancelable biometric is to irreversibly transform the original biometric template and perform the comparison in the protected domain. In this paper, a coprime transformation scheme has been proposed to derive a protected fingerprint template. The method divides the fingerprint region into a number of sectors with respect to each minutiae point and identifies the nearest-neighbor minutiae in each sector. Then, ridge features for all neighboring minutiae points are computed and mapped onto co-prime positions of a random matrix to generate the cancelable template. The proposed approach achieves an EER of 1.82, 1.39, 4.02 and 5.77 on DB1, DB2, DB3 and DB4 datasets of the FVC2002 and an EER of 8.70, 7.95, 5.23 and 4.87 on DB1, DB2, DB3 and DB4 datasets of FVC2004 databases, respectively. Experimental evaluations indicate that the method outperforms in comparison to the current state-of-the-art. Moreover, it has been confirmed from the security analysis that the proposed method fulfills the desired characteristics of diversity, revocability, and non-invertibility with a minor performance degradation caused by the transformation. 0 into PostgreSQL...\n",
      "Inserting test sample 1138  The security of biometric systems is of utmost importance, particularly with the increasing adoption of fingerprint recognition technology. In this research, we propose a coprime mapping transformation scheme for generating protected fingerprint templates. The coprime mapping method utilizes large prime numbers and their co-prime counterparts to map the feature space of the fingerprint template. By using this transformation, the original fingerprint template is encrypted and a new, protected template is generated. This process enhances the security of the fingerprint recognition system and makes it more resilient to attacks on the biometric system. \n",
      "\n",
      "The proposed coprime mapping technique is evaluated using benchmark fingerprint databases, and the experimental results demonstrate its effectiveness in generating a protected fingerprint template against various types of attacks. The scheme also achieves high recognition accuracy with low computational complexity, making it well-suited for real-world fingerprint recognition applications. \n",
      "\n",
      "Overall, the coprime mapping transformation presents a novel approach to generating secured fingerprint templates, and our results suggest that it could be a highly effective means of improving the security and robustness of biometric authentication systems. Further research will be conducted to investigate its applicability and effectiveness in other biometric modalities. 1 into PostgreSQL...\n",
      "Inserting test sample 1139  On the basis of the non-commutative q-calculus, we investigate a q-deformation of the classical Poisson bracket in order to formulate a generalized q-deformed dynamics in the classical regime. The obtained q-deformed Poisson bracket appears invariant under the action of the q-symplectic group of transformations. In this framework we introduce the q-deformed Hamilton's equations and we derive the evolution equation for some simple q-deformed mechanical systems governed by a scalar potential dependent only on the coordinate variable. It appears that the q-deformed Hamiltonian, which is the generator of the equation of motion, is generally not conserved in time but, in correspondence, a new constant of motion is generated. Finally, by following the standard canonical quantization rule, we compare the well known q-deformed Heisenberg algebra with the algebra generated by the q-deformed Poisson bracket. 0 into PostgreSQL...\n",
      "Inserting test sample 1140  The study of classical and quantum q-deformed physical systems is a current topic in theoretical physics. The investigation revolves around the idea of introducing a deformation parameter (q) to the usual commutation or anti-commutation relations of canonical position and momentum operators. This deformation changes the underlying algebraic structure of the system and thus affects its physical properties. In recent years, q-deformed systems have been applied to various problems in solid-state physics, quantum optics, and high-energy physics. This paper reviews the current state of research in classical and quantum q-deformed physical systems, focusing on the mathematical foundations and physical significance of these systems. The purpose is to gain insight into the properties of q-deformed systems and their potential applications in various fields of physics. 1 into PostgreSQL...\n",
      "Inserting test sample 1141  In this research, two-state Markov switching models are proposed to study accident frequencies and severities. These models assume that there are two unobserved states of roadway safety, and that roadway entities (e.g., roadway segments) can switch between these states over time. The states are distinct, in the sense that in the different states accident frequencies or severities are generated by separate processes (e.g., Poisson, negative binomial, multinomial logit). Bayesian inference methods and Markov Chain Monte Carlo (MCMC) simulations are used for estimation of Markov switching models. To demonstrate the applicability of the approach, we conduct the following three studies. In the first study, two-state Markov switching count data models are considered as an alternative to zero-inflated models for annual accident frequencies, in order to account for preponderance of zeros typically observed in accident frequency data. In the second study, two-state Markov switching Poisson model and two-state Markov switching negative binomial model are estimated using weekly accident frequencies on selected Indiana interstate highway segments over a five-year time period. In the third study, two-state Markov switching multinomial logit models are estimated for severity outcomes of accidents occurring on Indiana roads over a four-year time period. One of the most important results found in each of the three studies, is that in each case the estimated Markov switching models are strongly favored by roadway safety data and result in a superior statistical fit, as compared to the corresponding standard (non-switching) models. 0 into PostgreSQL...\n",
      "Inserting test sample 1142  This study investigates the use of Markov switching models in the field of roadway safety. The aim is to provide insight into how this method can be applied to identify the factors that impact the safety of roadways. The Markov switching model is a statistical tool that can be used to analyze data that evolves over time with unknown patterns. In this study, the model is applied to examine the possible factors that may be impacting the safety of a given roadway, such as road and weather conditions, the congestion level, and driver behavior. \n",
      "\n",
      "The dataset used in this study contains information on crashes that occurred on roadways, as well as other relevant variables. The model is estimated using a Bayesian approach, which allows for the inclusion of prior information and the derivation of posterior probabilities. The results suggest that the Markov switching model could be a valuable tool for identifying critical features of roadways that could lead to accidents. Specifically, the model indicates that the most important factor affecting roadway safety is the level of congestion. Other relevant factors include road geometry and weather conditions. \n",
      "\n",
      "Overall, this study highlights the potential for using Markov switching models as a methodological tool for identifying critical factors that impact roadway safety. The findings provide insights into the complex nature of the phenomenon and demonstrate the value of applying statistical modeling to real-world problems. Our results could inform policies and strategies aimed at improving roadway safety and reducing the occurrence of crashes. 1 into PostgreSQL...\n",
      "Inserting test sample 1143  Although extensively investigated, the role of the environment in galaxy formation is still not well understood. In this context, the Galaxy Stellar Mass Function (GSMF) is a powerful tool to understand how environment relates to galaxy mass assembly and the quenching of star-formation. In this work, we make use of the high-precision photometric redshifts of the UltraVISTA Survey to study the GSMF in different environments up to $z \\sim 3$, on physical scales from 0.3 to 2 Mpc, down to masses of $M \\sim 10^{10} M_{\\odot}$. We witness the appearance of environmental signatures for both quiescent and star-forming galaxies. We find that the shape of the GSMF of quiescent galaxies is different in high- and low-density environments up to $z \\sim 2$ with the high-mass end ($M \\gtrsim 10^{11} M_{\\odot}$) being enhanced in high-density environments. On the contrary, for star-forming galaxies a difference between the GSMF in high- and low density environments is present for masses $M \\lesssim 10^{11} M_{\\odot}$. Star-forming galaxies in this mass range appear to be more frequent in low-density environments up to $z < 1.5$. Differences in the shape of the GSMF are not visible anymore at $z > 2$. Our results, in terms of general trends in the shape of the GSMF, are in agreement with a scenario in which galaxies are quenched when they enter hot gas-dominated massive haloes which are preferentially in high-density environments. 0 into PostgreSQL...\n",
      "Inserting test sample 1144  The study of galaxy evolution and how it is affected by the environment in which galaxies exist has been an important area of research in astronomy. In this paper, we present a new approach for reconstructing the galaxy density field using photometric redshifts. We apply this method to study the environment-dependent evolution of galaxies since a redshift of approximately 3.\n",
      "\n",
      "Our reconstruction method uses a Bayesian hierarchical model that combines spatial information with redshift information. By incorporating photometric redshifts, we are able to extend our analysis beyond the spectroscopic limit to fainter galaxies. We use data from the COSMOS field, which provides a large sample of galaxies with accurate photometric redshifts, as well as environmental measures such as local galaxy density and halo mass.\n",
      "\n",
      "Our analysis reveals that the galaxy population has evolved differently in diverse environments since a redshift of approximately 3. We find that the fraction of red, quiescent galaxies increases significantly with increasing local density, while the fraction of star-forming galaxies decreases. We also observe that massive galaxies in denser environments have assembled earlier and have gone through more mergers than their counterparts in less dense environments.\n",
      "\n",
      "Our results demonstrate the importance of considering the environment when studying galaxy evolution. Our new photometric redshift-based approach for reconstructing the galaxy density field provides a useful tool for future studies of the environmental dependence of galaxy evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 1145  Context: Rotationally supported disks are critical in the star formation process. The questions of when do they form and what factors influence or hinder their formation have been studied but are largely unanswered.\n",
      "\n",
      "Observations of early stage YSOs are needed to probe disk formation. Aims: VLA1623 is a triple non-coeval protostellar system, with a weak magnetic field perpendicular to the outflow, whose Class 0 component, VLA1623A, shows a disk-like structure in continuum with signatures of rotation in line emission.\n",
      "\n",
      "We aim to determine whether this structure is in part or in whole a rotationally supported disk, i.e. a Keplerian disk, and what are its characteristics. Methods: ALMA Cycle 0 Early Science 1.3 mm continuum and C$^{18}$O (2-1) observations in the extended configuration are presented here and used to perform an analysis of the disk-like structure using PV diagrams and thin disk modelling with the addition of foreground absorption. Results: The PV diagrams of the C$^{18}$O line emission suggest the presence of a rotationally supported component with a radius of at least 50 AU. Kinematical modelling of the line emission shows that the disk out to 180 AU is actually rotationally supported, with the rotation being well described by Keplerian rotation out to at least 150 AU, and the central source mass to be $\\sim$0.2 M$_{sun}$ for an inclination of 55$^{\\circ}$. Pure infall and conserved angular momentum rotation models are excluded. Conclusions: VLA1623A, a very young Class 0 source, presents a disk with an outer radius $R_{\\rm out}$ = 180 AU with a Keplerian velocity structure out to at least 150 AU. The weak magnetic fields and recent fragmentation in this region of rho Ophiuchus may have played a lead role in the formation of the disk. 0 into PostgreSQL...\n",
      "Inserting test sample 1146  The study of star formation is essential to understanding how the universe evolved into its current state. In particular, the Keplerian Disk Model has been fundamental in exploring the early stages of protoplanetary disks, or Class 0 sources, which are believed to be the precursors to planetary systems. In this study, we present the results of our ALMA observations of the VLA1623A protostellar system. Our data reveal the presence of a Keplerian disk around the Class 0 source, providing a unique insight into the physical processes involved in early star formation.\n",
      "\n",
      "Our observations show that the disk orbits within a radius of 50 AU around the protostar, indicating that the system is still in its infancy stages. The disk also exhibits a smooth, continuous rotation, consistent with Keplerian motion, as well as a gas mass of approximately 1% of the protostar's total mass. Interestingly, the disk appears to be truncated, with a sharp outer boundary, suggesting that external factors such as magnetic fields or other protostars may be influencing its structure.\n",
      "\n",
      "Our findings provide important implications for the study of planet formation and the early stages of protoplanetary disks. The results also shed insight into the physical processes that govern the evolution of young protostellar systems. Future observations with even higher resolution and sensitivity will further unveil the mysteries of Class 0 sources and their role in the formation of planetary systems. Our study provides a strong foundation for such follow-up observations and paves the way for a more comprehensive understanding of the early stages of star and planet formation. 1 into PostgreSQL...\n",
      "Inserting test sample 1147  Previous work in the literature had built a formalism for spatially averaged equations for the scale factor, giving rise to an averaged Raychaudhuri equation and averaged Hamiltonian constraint, which involve a backreaction source term. The present paper extends these equations to include models with variable Newton parameter and variable cosmological term, motivated by the nonperturbative renormalization program for quantum gravity based upon the Einstein-Hilbert action. We focus on the Brans-Dicke form of the renormalization-group improved action functional. The coupling between backreaction and spatially averaged three-dimensional scalar curvature is found to survive, and a variable-G cosmic quintet is found to emerge. Interestingly, under suitable assumptions, an approximate solution can be found where the early universe tends to a FLRW model, while keeping track of the original inhomogeneities through three effective fluids. The resulting qualitative picture is that of a universe consisting of baryons only, while inhomogeneities average out to give rise to the full dark-side phenomenology. 0 into PostgreSQL...\n",
      "Inserting test sample 1148  Averaging procedures play a significant role in shaping our understanding of the large-scale structure of the Universe. In this paper, we investigate the use of averaging techniques in variable-G cosmologies, where the gravitational constant is not necessarily a constant parameter but rather a dynamically evolving quantity. First, we introduce and derive the modified Einstein field equations governing such cosmologies. Then, we demonstrate how the novel geometric mean averaging procedure can be applied to the modified field equations, yielding a set of effective averaged equations that can be used to study the large-scale structure of variable-G cosmologies. We conclude by discussing the implications of our findings and highlighting new avenues for future research, including the possibility of constraining and testing variable-G models using observational data. Our results indicate that averaging procedures provide a powerful tool for understanding the dynamics of the Universe in the presence of variable G dynamics. 1 into PostgreSQL...\n",
      "Inserting test sample 1149  We employ the Chandra Multiwavelength Project (ChaMP) and the Sloan Digital Sky Survey (SDSS) to study the fraction of X-ray-active galaxies in the field out to z = 0.7. We utilize spectroscopic redshifts from SDSS and ChaMP, as well as photometric redshifts from several SDSS catalogs, to compile a parent sample of more than 100,000 SDSS galaxies and nearly 1,600 Chandra X-ray detections.\n",
      "\n",
      "Detailed ChaMP volume completeness maps allow us to investigate the local fraction of active galactic nuclei (AGN), defined as those objects having broad-band X-ray luminosities L_X (0.5-8 keV) > 10^42 erg s^-1, as a function of absolute optical magnitude, X-ray luminosity, redshift, mass, and host color/morphological type. In five independent samples complete in redshift and i-band absolute magnitude, we determine the field AGN fraction to be between 0.16 +/- 0.06% (for z < 0.125 and -18 > M_i > -20) and 3.80 +/- 0.92% (for z < 0.7 and M_i < -23). We find striking agreement between our ChaMP/SDSS field AGN fraction and the Chandra cluster AGN fraction, for samples restricted to similar redshift and absolute magnitude ranges: 1.19 +/- 0.11% of ChaMP/SDSS field galaxies with 0.05 < z < 0.31 and absolute R-band magnitude more luminous than M_R < -20 are AGN. Our results are also broadly consistent with measures of the field AGN fraction in narrow, deep fields, though differences in the optical selection criteria, redshift coverage, and possible cosmic variance between fields introduce larger uncertainties in these comparisons. 0 into PostgreSQL...\n",
      "Inserting test sample 1150  This study investigates the X-ray emitting active galactic nuclei (AGN) and their evolution up to redshift z = 0.7. To accomplish this, we have analyzed data from the Chandra Multiwavelength Project and the Sloan Digital Sky Survey. The X-ray properties of AGN were identified using X-ray color-color diagrams and hardness ratios. We have focused on a field area of approximately 0.25 square degrees, where a total of 4,320 X-ray sources were detected. Of these, 1,133 have been identified as AGN, and 422 have been classified as type 1 AGN. Using optical spectroscopy from the Sloan Digital Sky Survey, we have obtained redshifts for 78% of identified AGN, which allows us to measure the evolution of AGN fraction up to z = 0.7. After correcting for incompleteness and redshift-dependent sensitivity, we find that the AGN fraction in the field remains constant for redshifts between 0.1 and 0.7. We have also estimated the X-ray luminosity function of AGN up to redshift z = 0.7, which shows that the AGN population has evolved significantly since z = 0. We conclude that the sample studied in this work highlights the importance of combining multiwavelength data to obtain a complete census of AGN population and properties. 1 into PostgreSQL...\n",
      "Inserting test sample 1151  In this paper, we introduce a novel approach for optimal resource allocation from multiple carriers for users with elastic and inelastic traffic in fourth generation long term evolution (4G-LTE) system. In our model, we use logarithmic and sigmoidal-like utility functions to represent the user applications running on different user equipments (UE)s. We use utility proportional fairness policy, where the fairness among users is in utility percentage of the application running on the mobile station. Our objective is to allocate the resources to the users optimally from multiple carriers. In addition, every user subscribing for the mobile service is guaranteed to have a minimum quality-of-service (QoS) with a priority criterion. Our rate allocation algorithm selects the carrier or multiple carriers that provide the minimum price for the needed resources. We prove that the novel resource allocation optimization problem with joint carrier aggregation is convex and therefore the optimal solution is tractable. We present a distributed algorithm to allocate the resources optimally from multiple evolved NodeBs (eNodeB)s. Finally, we present simulation results for the performance of our rate allocation algorithm. 0 into PostgreSQL...\n",
      "Inserting test sample 1152  This research paper proposes an optimal resource allocation scheme for 4G-LTE networks employing Joint Carrier Aggregation (JCA) to enhance the data rates and spectral efficiency. JCA is a technique that combines multiple component carriers, enabling mobile devices to transmit and receive data simultaneously, hence providing a wider bandwidth. The proposed scheme deploys a heuristic algorithm that efficiently allocates transmission resources to the users based on quality of service demands in the network. The objective of the algorithm is to maximize the total number of users that meet their quality of service requirements while minimizing the interference level in the network. The performance evaluation of the proposed algorithm demonstrates a significant improvement in terms of throughput and outage probability compared with other allocation methods. Furthermore, a sensitivity analysis of the proposed algorithm to its parameters is conducted, indicating that the algorithm is robust to different scenarios. The results obtained from this research can be utilized to enhance the resource allocation process in JCA-enabled 4G-LTE networks, allowing them to better cope with the increasing demands of high-bitrate applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1153  In classical analyses of $\\gamma$-ray data from IACTs, such as H.E.S.S., aperture photometry, or photon counting, is applied in a (typically circular) region of interest (RoI) encompassing the source. A key element in the analysis is to estimate the amount of background in the RoI due to residual cosmic ray-induced air showers in the data. Various standard background estimation techniques have been developed in the last decades, most of them rely on a measurement of the background from source-free regions within the observed field of view. However, in particular in the Galactic plane, source analysis and background estimation are hampered by the large number of, sometimes overlapping, $\\gamma$-ray sources and large-scale diffuse $\\gamma$-ray emission.\n",
      "\n",
      "For complicated fields of view, a three-dimensional (3D) likelihood analysis shows the potential to be superior to classical analysis. In this analysis technique, a spectromorphological model, consisting of one or multiple source components and a background component, is fitted to the data, resulting in a complete spectral and spatial description of the field of view. For the application to IACT data, the major challenge of such an approach is the construction of a robust background model.\n",
      "\n",
      "In this work, we apply the 3D likelihood analysis to various test data recently made public by H.E.S.S., using the open analysis frameworks ctools and Gammapy. First, we show that, when using these tools in a classical analysis approach and comparing to the proprietary H.E.S.S. analysis framework, virtually identical high-level analysis results are obtained. We then describe the construction of a generic background model from data of H.E.S.S.\n",
      "\n",
      "observations, and demonstrate that a 3D likelihood analysis using this background model yields high-level analysis results that are highly compatible with those obtained from the classical analyses. (abridged) 0 into PostgreSQL...\n",
      "Inserting test sample 1154  In $\\gamma$-ray astronomy, the construction of reliable background models is essential for identifying and analyzing astrophysical sources of high-energy photons. This paper presents a comprehensive validation study of open-source science tools and background model construction methods used for this purpose. \n",
      "\n",
      "First, we perform a careful analysis of the software and algorithms utilized to construct the background model. Our results demonstrate that the open-source tools are well-suited for the task and produce accurate and reproducible results. In addition, we highlight potential issues in the data analysis pipeline that can significantly impact the quality of the background model.\n",
      "\n",
      "Next, we assess the performance of various background model construction methods, including those commonly used in the field, as well as novel techniques. We find that certain methods, such as maximum likelihood estimation with a Poisson distribution, perform better than others, such as simple power-law fits. Importantly, we show that the choice of method can significantly affect the sensitivity and accuracy of source detection, highlighting the need for careful consideration in this step.\n",
      "\n",
      "Finally, we present a case study of applying the validated science tools and background model construction methods to analyze archival data from the Fermi Large Area Telescope. Our analysis demonstrates the successful identification and characterization of several astrophysical sources previously not reported in the literature.\n",
      "\n",
      "In conclusion, by validating open-source science tools and exploring robust background model construction methods, this paper provides important insights into the analysis of $\\gamma$-ray sky data and facilitates future discoveries in high-energy astrophysics. 1 into PostgreSQL...\n",
      "Inserting test sample 1155  We present initial results of a program to obtain and analyze HST WFPC2 images of galaxies in fields of HST spectroscopic target QSOs. The goal of the program is to investigate how the properties of \\lya absorption systems observed in the spectra of background QSOs vary with the properties of intervening galaxies. We found that \\lya absorption equivalent width depends strongly on galaxy impact parameter and galaxy B-band luminosity, and that the gaseous extent of individual galaxies scales with galaxy B-band luminosity by $r\\propto L_B^{0.35\\pm0.10}$. 0 into PostgreSQL...\n",
      "Inserting test sample 1156  This study investigates the relationship between the gaseous extent of galaxies and the origin of \\lya absorption systems at redshifts lower than one. The authors analyze data from multiple surveys to determine the size of the gaseous halo around galaxies and the prevalence of \\lya absorbers. The results suggest a correlation between these two phenomena and indicate that the gas surrounding galaxies may play a role in the production of \\lya absorption systems. These findings shed light on the complex interplay between galaxies and the surrounding universe and have important implications for our understanding of galaxy evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 1157  The Gaia map of the Milky Way reveals a pair of triangular features at nearly symmetric locations on opposite sides of the Galactic Center. In this paper we explore the implications of these features assuming they are manifestations of a caustic ring in the dark matter distribution of the Milky Way halo. The existence of a series of such rings is predicted by the Caustic Ring Model. The model's phase-space distribution is that acquired by a rethermalizing Bose-Einstein condensate of axions or axion-like particles. We show that dust is gravitationally entrained by cold axion flows and propose this as an explanation for the sharpness of the triangular features. The locations of the features imply that we on Earth are much closer to the fifth caustic ring than thought on the basis of pre-Gaia observations. Most likely we are inside its tricusp cross-section. In that case the dark matter density on Earth is dominated by four cold flows, termed Big, Little, Up and Down. If we are outside the tricusp cross-section the dark matter density on Earth is dominated by two cold flows, Big and Little. We use the triangular features in the Gaia map, and a matching feature in the IRAS map, to estimate the velocity vectors and densities of the four locally dominant flows. 0 into PostgreSQL...\n",
      "Inserting test sample 1158  The Gaia satellite mission has provided unprecedented data for the study of the Milky Way halo. In this work, we explore the implications of triangular features exhibited in the Gaia skymap with respect to the Caustic Ring Model (CRM), which is a current model of the Milky Way's dark matter halo. We use a Bayesian framework to fit the CRM to the data and determine the properties of the triangle structures. Our analysis shows that the CRM offers a good fit for the triangular features, providing evidence for the model's validity. We also investigate the spatial distribution of the triangular structures and find that they align with the locations of known star streams, suggesting a possible connection between the two. Furthermore, we analyze the impact of these findings on our understanding of the Milky Way's formation and evolution. Our results provide new insights into the structure and dynamics of the Milky Way halo and demonstrate the importance of a detailed analysis of the Gaia data in the search for dark matter. 1 into PostgreSQL...\n",
      "Inserting test sample 1159  We review here the effects of supernovae (SNe) explosions on the environment of star-forming galaxies. Randomly distributed, clustered SNe explosions cause the formation of hot superbubbles that drive either galactic fountains or supersonic winds out of the galactic disk. In a galactic fountain, the ejected gas is re-captured by the gravitational potential and falls back onto the disk.\n",
      "\n",
      "From 3D non-equilibrium radiative cooling hydrodynamical simulations of these fountains, we find that they may reach altitudes smaller than 5 kpc in the halo and hence explain the formation of the so-called intermediate-velocity-clouds (IVCs). On the other hand, the high-velocity-clouds (HVCs) that are observed at higher altitudes (of up to 12 kpc) require another mechanism to explain their production. We argue that they could be formed either by the capture of gas from the intergalactic medium and/or by the action of magnetic fields that are carried out to the halo with the gas in the fountains. Due to angular momentum losses (of 10-15%) to the halo, we find that the fountain material falls back to smaller radii and is not largely spread over the galactic disk, as previously expected. This result is consistent with the metal distribution derived from recent chemical models of the galaxy. We also find that after about 150 Myr, the gas circulation between the halo and the disk in the fountains reaches a steady state regime (abridged). 0 into PostgreSQL...\n",
      "Inserting test sample 1160  The death of a massive star in the form of a supernova explosion is a cataclysmic event that has far-reaching consequences for its host galaxy. In particular, the explosion can trigger the launch of powerful galactic fountains and outflows, which can be detected in various wavelengths. These outflows are crucial for the evolution of galaxies as they regulate the chemical and thermal properties of the interstellar medium. \n",
      "\n",
      "In this paper, we investigate the triggering mechanisms and properties of galactic fountains and outflows that are associated with supernova explosions. We review recent observations and simulations that shed light on the dynamics and energetics of these processes. We also discuss the impact of various factors, such as the properties of the supernova progenitor and the ambient medium, on the outflow properties. \n",
      "\n",
      "Our analysis suggests that supernova explosions can launch both hot and cold outflows that have different properties depending on the environment and the explosion mechanisms. In addition, the launching and propagation of these outflows are regulated by feedback processes that involve various interstellar and circumgalactic components. We argue that a comprehensive understanding of supernova-driven fountains and outflows is crucial for a complete picture of the evolution of galaxies over cosmic time. 1 into PostgreSQL...\n",
      "Inserting test sample 1161  Joint extraction of entities and relations has received significant attention due to its potential of providing higher performance for both tasks. Among existing methods, CopyRE is effective and novel, which uses a sequence-to-sequence framework and copy mechanism to directly generate the relation triplets. However, it suffers from two fatal problems. The model is extremely weak at differing the head and tail entity, resulting in inaccurate entity extraction. It also cannot predict multi-token entities (e.g.\n",
      "\n",
      "\\textit{Steven Jobs}). To address these problems, we give a detailed analysis of the reasons behind the inaccurate entity extraction problem, and then propose a simple but extremely effective model structure to solve this problem.\n",
      "\n",
      "In addition, we propose a multi-task learning framework equipped with copy mechanism, called CopyMTL, to allow the model to predict multi-token entities.\n",
      "\n",
      "Experiments reveal the problems of CopyRE and show that our model achieves significant improvement over the current state-of-the-art method by 9% in NYT and 16% in WebNLG (F1 score). Our code is available at https://github.com/WindChimeRan/CopyMTL 0 into PostgreSQL...\n",
      "Inserting test sample 1162  Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning (CopyMTL) is a novel model introduced in this paper which achieves state-of-the-art results on two benchmark datasets, namely, CoNLL04 and ACE2005. The paper proposes a unified architecture for jointly extracting entities and relations by utilizing a copy mechanism, which allows the model to copy a part of the input text as an output token, thereby enabling the model to handle rare or unseen entities. CopyMTL employs a multi-task learning approach in which the entity and relation extractions are jointly learned, leading to improved performance and reduced overfitting. The model is trained end-to-end with a maximum-likelihood objective and can handle both nested and flat entity annotations. Additionally, the paper provides a detailed analysis of the model and the experimental results, demonstrating the effectiveness of the proposed approach. Overall, this contribution presents a significant step towards advancing the state-of-the-art in joint entity and relation extraction with the help of the CopyMTL model. 1 into PostgreSQL...\n",
      "Inserting test sample 1163  We present an HST STIS spectrum of the HeII Gunn-Peterson effect towards HE2347-4342. Compared to the previous HST GHRS data obtained by Reimers et al.\n",
      "\n",
      "(1997), the STIS spectrum has a much improved resolution. The 2-D detector also allows us to better characterize the sky and dark background. We confirm the presence of two spectral ranges of much reduced opacity, the opacity gaps, and provide improved lower limits on the HeII G-P opacity in the high opacity regions. We use the STIS spectrum together with a Keck--HIRES spectrum covering the corresponding HI Lya forest to calculate a 1-D map of the softness S of the ionization radiation along the line of sight towards HE 2347-4342, where S is the ratio of the HI to HeII photoionization rates. We find that S is generally large but presents important variations, from S ~ 30 in the opacity gaps to a 1 sigma lower limit of 2300 at z~2.86, in a region which shows an extremely low HI opacity over a 6.5 A range. We note that a large S naturally accounts for most of the large SiIV to CIV ratios seen in other quasar absorption line spectra. We present a simple model that reproduces the shape of the opacity gaps in absence of large individual absorption lines. We extend the model described in Heap et al. (2000) to account for the presence of sources close to the line of sight of the background quasar. As an alternative to the delayed reionization model suggested by Reimers et al. (1997), we propose that the large softness observed at z~2.86 is due to the presence of bright soft sources close to the line of sight, i.e. for which the ratio between the number of HI to HeII ionizing photons reaching the IGM is large. We discuss these two models and suggest ways to discriminate between them. 0 into PostgreSQL...\n",
      "Inserting test sample 1164  The HeII Gunn-Peterson effect has been studied extensively in recent years, revealing important information about the reionization history of the early universe. Here, we present new observations of the HeII Gunn-Peterson effect towards the quasar HE 2347-4342, obtained using the Hubble Space Telescope's Space Telescope Imaging Spectrograph (HST/STIS). Our observations cover a redshift range of z = 2.78 â€“ 2.87, sampling the intergalactic medium (IGM) during the final stages of HeII reionization. The spectrum of HE 2347-4342 reveals a largely opaque and featureless HeII absorption trough spanning a redshift range of Î”z â‰ˆ 0.08. We analyze the optical depths and search for possible correlations with the underlying quasar continuum emission.\n",
      "\n",
      "Through comparison with simulations, we show that the average HeII ionizing background at this epoch is consistent with a radiation field dominated by extragalactic sources, with a possible contribution from a hard quasar continuum. In addition, we find evidence for a weak, spatially extended HeII absorption feature in the spectrum, which may be attributed to a nearby intergalactic filament. We compare our findings with recent models for HeII reionization, finding that our results are consistent with a scenario in which helium reionization is driven by sources situated in dense, biased regions of the universe.\n",
      "\n",
      "Our observations demonstrate the unique power of the HeII Gunn-Peterson effect as a tool for studying the high-redshift universe. Although advances in theoretical modeling and numerical simulations have allowed us to make significant progress in understanding the processes driving cosmic reionization, observations such as these provide important constraints on the nature of the ionizing sources and the evolution of the intergalactic medium during the epoch of reionization. We conclude that further observations of the HeII Gunn-Peterson effect, targeting larger areas of the sky and probing different redshift ranges, will continue to offer unique insights into the early universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1165  Frequency shifts and dissipations of a compound torsional oscillator induced by solid $^4$He samples containing $^3$He impurity concentrations ($x_3$ = 0.3, 3, 6, 12 and 25 in units of 10$^{-6}$) have been measured at two resonant mode frequencies ($f_1$ = 493 and $f_2$ = 1164 Hz) at temperatures ($T$) between 0.02 and 1.1 K. The fractional frequency shifts of the $f_1$ mode were much smaller than those of the $f_2$ mode. The observed frequency shifts continued to decrease as $T$ was increased above 0.3 K, and the conventional non-classical rotation inertia fraction was not well defined in all samples with $x_3 \\geq$ 3 ppm. Temperatures where peaks in dissipation of the $f_2$ mode occurred were higher than those of the $f_1$ mode in all samples. The peak dissipation magnitudes of the $f_1$ mode was greater than those of the $f_2$ mode in all samples. The activation energy and the characteristic time ($\\tau_0$) were extracted for each sample from an Arrhenius plot between mode frequencies and inverse peak temperatures. The average activation energy among all samples was 430 mK, and $\\tau_0$ ranged from 2$\\times 10^{-7}$ s to 5$\\times 10^{-5}$ s in samples with $x_3$ = 0.3 to 25 ppm. The characteristic time increased in proportion to $x_3^{2/3}$. Observed temperature dependence of dissipation were consistent with those expected from a simple Debye relaxation model \\emph{if} the dissipation peak magnitude was separately adjusted for each mode. Observed frequency shifts were greater than those expected from the model. The discrepancies between the observed and the model frequency shifts increased at the higher frequency mode. 0 into PostgreSQL...\n",
      "Inserting test sample 1166  The effects of $^3$He impurity on solid $^4$He have been investigated using a compound torsional oscillator. The study of the properties of impure solid helium provides valuable insight into the behaviour of matter at extremely low temperatures. The torsional oscillator was designed to measure the mechanical properties of solid $^4$He, and was modified to accommodate the introduction of $^3$He impurity. Measurements were taken over a wide range of temperatures and concentrations of impurity. The results show that the presence of $^3$He significantly alters the mechanical properties of the solid $^4$He. The impact of the concentration of impurities on the elastic constants was determined. The results indicate that the introduction of $^3$He impurity can weaken the solid $^4$He, and that the effect is more pronounced at lower temperatures. The analysis of the data suggests that the interaction of the $^3$He impurity with the dislocations in the $^4$He plays a crucial role in the changes to the mechanical properties. These findings have important implications for understanding the physical properties of matter at extremely low temperatures, and for developing new materials with enhanced properties. The compound torsional oscillator is an effective tool for studying the properties of impure solid helium, and provides an excellent starting point for further research in this field. 1 into PostgreSQL...\n",
      "Inserting test sample 1167  We present MeerKAT neutral hydrogen (HI) observations of the Fornax A group, that is likely falling into the Fornax cluster for the first time. Our HI image is sensitive to 1.4 x 10$^{19}$ cm$^{-2}$ over 44.1 km s$^{-1}$, where we detect HI in 10 galaxies and a total of 1.12 x 10$^{9}$ Msol of HI in the intra-group medium (IGM). We search for signs of pre-processing in the 12 group galaxies with confirmed optical redshifts that reside within our HI image.\n",
      "\n",
      "There are 9 galaxies that show evidence of pre-processing and we classify the pre-processing status of each galaxy, according to their HI morphology and gas (atomic and molecular) scaling relations. Galaxies yet to experience pre-processing have extended HI disks, a high HI content with a H$_2$-to-HI ratio an order of magnitude lower than the median for their stellar mass.\n",
      "\n",
      "Galaxies currently being pre-processed display HI tails, truncated HI disks with typical gas ratios. Galaxies in the advanced stages of pre-processing are HI deficient. If there is any HI, they have lost their outer HI disk and efficiently converted their HI to H$_2$, resulting in H$_2$-to-HI ratios an order of magnitude higher than the median for their stellar mass. The central, massive galaxy in our group underwent a 10:1 merger 2 Gyr ago, and ejected 6.6 - 11.2 x 10$^{8}$ Msol of HI that we detect as clouds and streams in the IGM, some forming coherent structures up to 220 kpc in length. We also detect giant (100 kpc) ionised hydrogen (H$\\alpha$) filaments in the IGM, likely from cool gas being removed (and ionised) from an infalling satellite. The H$\\alpha$ filaments are situated within the hot halo of NGC 1316 and some regions contain HI. We speculate that the H$\\alpha$ and multiphase gas is supported by magnetic pressure (possibly assisted by the AGN), such that the hot gas can condense and form HI that survives in the hot halo for cosmological timescales. 0 into PostgreSQL...\n",
      "Inserting test sample 1168  The Fornax A group is an important site for astrophysical research, providing astronomers with a unique opportunity to investigate a nearby galaxy cluster. In this paper, we present a MeerKAT view of pre-processing in the Fornax A group, one of the first studies of its kind to use the MeerKAT radio telescope in this region. We explore the radio continuum properties of the group, focusing on the interactions between the intra-group medium (IGM) and the radio sources located within it.\n",
      "\n",
      "Our findings reveal a highly complex picture of the pre-processing mechanisms occurring in the Fornax A group. We detect a large number of diffuse radio sources, some of which appear to be associated with galaxy clusters and others with individual galaxies. We also identify several compact sources, including radio galaxies and emission-line galaxies, which show signs of nuclear activity and/or star formation. By investigating the multi-wavelength properties of these sources, we are able to gain new insights into the physical processes driving their evolution.\n",
      "\n",
      "Our analysis of the IGM suggests that it is highly turbulent and structured, displaying a variety of morphological features, such as filaments, ridges, and voids. We find evidence for large-scale shocks and turbulence, indicating that the group is undergoing a phase of transformation. We also detect several extended radio relics, which are thought to be the result of shock fronts generated by the infall of gas into the group.\n",
      "\n",
      "Overall, our MeerKAT view of pre-processing in the Fornax A group provides new insights into the complex astrophysical processes at play in this important astronomical site. Our findings highlight the importance of multi-wavelength observations in understanding the interplay between galaxies, gas, and dark matter in galaxy clusters. By using MeerKAT to explore the radio continuum properties of the group, we demonstrate the power of this instrument in advancing our understanding of the Universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1169  Blue compact galaxies (BCG) are ideal objects in which to derive the primordial 4He abundance because they are chemically young and have not had a significant stellar He contribution. We discuss a self-consistent method which makes use of all the brightest He I emission lines in the optical range and solves consistently for the electron density of the He II zone. We pay particular attention to electron collision and radiative transfer as well as underlying stellar absorption effects which may make the He I emission lines deviate from their recombination values. Using a large homogeneous sample of 45 low-metallicity H II regions in BCGs, and extrapolating the Y-O/H and Y-N/H linear regressions to O/H = N/H = 0, we obtain Yp = 0.2443+/-0.0015, in excellent agreement with the weighted mean value Yp = 0.2452+/-0.0015 obtained from the detailed analysis of the two most metal-deficient BCGs known, I Zw 18 and SBS 0335-052. The derived slope dY/dZ = 2.4+/-1.0 is in agreement with the value derived for the Milky Way and with simple chemical evolution models with homogeneous outflows. Adopting Yp = 0.2452+/-0.0015 leads to a baryon-to-photon ratio of (4.7+/-1.0)x10^{-10} and to a baryon mass fraction in the Universe Omega_b h^2_{50} = 0.068+/-0.015, consistent with the value derived from the primordial D abundance of Burles & Tytler (1998). 0 into PostgreSQL...\n",
      "Inserting test sample 1170  This study investigates the connection between blue compact galaxies (BCGs) and the primordial abundance of 4Helium. BCGs are of interest due to their young, metal-poor nature, making them ideal objects for examining the chemical evolution of the early universe. The helium abundance is also a key indicator of the early universe's conditions, as it is a direct product of Big Bang nucleosynthesis. \n",
      "\n",
      "To explore this connection, we analyzed observations of a sample of BCGs and measured their metallicities and helium abundances. We found that the majority of the sample showed evidence of high 4Helium abundances, with some galaxies possessing levels consistent with the predicted primordial abundance. Our results suggest that BCGs could be used as a probe for the primordial helium abundance.\n",
      "\n",
      "We then examined the implications of our findings for cosmology. Our results support the current theoretical framework of the standard Big Bang model and are consistent with the observation that the universe is isotropic and homogeneous on large scales. Furthermore, our findings provide new insights into the formation and evolution of galaxies, especially during the early phases of cosmic history.\n",
      "\n",
      "Overall, our study provides evidence for a strong connection between BCGs and the primordial helium abundance, which has important implications for our understanding of the early universe and the evolution of galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 1171  From the \"right to be left alone\" to the \"right to selective disclosure\", privacy has long been thought as the control individuals have over the information they share and reveal about themselves. However, in a world that is more connected than ever, the choices of the people we interact with increasingly affect our privacy. This forces us to rethink our definition of privacy. We here formalize and study, as local and global node- and edge-observability, Bloustein's concept of group privacy. We prove edge-observability to be independent of the graph structure, while node-observability depends only on the degree distribution of the graph. We show on synthetic datasets that, for attacks spanning several hops such as those implemented by social networks and current US laws, the presence of hubs increases node-observability while a high clustering coefficient decreases it, at fixed density. We then study the edge-observability of a large real-world mobile phone dataset over a month and show that, even under the restricted two-hops rule, compromising as little as 1% of the nodes leads to observing up to 46% of all communications in the network. More worrisome, we also show that on average 36\\% of each person's communications would be locally edge-observable under the same rule. Finally, we use real sensing data to show how people living in cities are vulnerable to distributed node-observability attacks. Using a smartphone app to compromise 1\\% of the population, an attacker could monitor the location of more than half of London's population.\n",
      "\n",
      "Taken together, our results show that the current individual-centric approach to privacy and data protection does not encompass the realities of modern life.\n",
      "\n",
      "This makes us---as a society---vulnerable to large-scale surveillance attacks which we need to develop protections against. 0 into PostgreSQL...\n",
      "Inserting test sample 1172  The increasing prevalence of networked information and communication technologies has led to a proliferation of surveillance activities that impinge on individuals' privacy. To address these infringing activities, this paper proposes a novel approach to quantifying intrusions in networked systems by analyzing network nodes and their connections. We focus specifically on group privacy, which can be impacted by the surveillance of multiple individuals through their connections in the network. \n",
      "\n",
      "Our approach uses a combination of statistical analysis and machine learning techniques to identify and quantify the nodes that are most susceptible to intrusions, as well as the types of invasive activities these nodes experience. We show that our method can accurately identify and classify different types of intrusions, including passive monitoring, data scraping, and social engineering attacks.\n",
      "\n",
      "We then apply our method to several real-world networked systems, including social media platforms and corporate intranets, to demonstrate its effectiveness. Our results indicate that our approach can accurately identify high-risk nodes and the most common types of intrusions, as well as provide valuable insights into the network structures that tend to be targeted by surveillance activities.\n",
      "\n",
      "Finally, we discuss the implications of our findings for group privacy, highlighting the need for improved privacy protection measures and policies that address the unique challenges posed by networked systems. Our research makes an important contribution to the field of privacy and security, providing a novel approach to quantifying intrusions in networked systems and shedding light on the challenges faced by group privacy in the networked age. 1 into PostgreSQL...\n",
      "Inserting test sample 1173  We provide the complete decomposition of the local gauge-invariant energy-momentum tensor for spin-1 hadrons, including non-conserved terms for the individual parton flavors and antisymmetric contributions originating from intrinsic spin. We state sum rules for the gravitational form factors appearing in this decomposition and provide relations for the mass decomposition, work balance, total and orbital angular momentum, mass radius, and inertia tensor.\n",
      "\n",
      "Generalizing earlier work, we derive relations between the total and orbital angular momentum and the Mellin moments of twist-2 and 3 generalized parton distributions, accessible in hard exclusive processes with spin-1 targets.\n",
      "\n",
      "Throughout the work, we comment on the unique features in these relations originating from the spin-1 nature of the hadron, being absent in the lower spin cases. 0 into PostgreSQL...\n",
      "Inserting test sample 1174  In this study, we present a formalism for the energy-momentum tensor of spin-1 hadrons, which is based on the Noether theorem of general covariance. The hadronic tensor is expressed in terms of spin-1 particle fields, and it reduces to a familiar form in the case of a spin-0 particle. We show that the tensor is conserved, which is a crucial property for understanding the structure and dynamics of hadrons. The formalism is also applicable to the description of other hadrons with non-zero spin. Our results provide insight into the mechanics of hadronic systems and contribute to the ongoing efforts to understand the strong force that binds quarks inside hadrons. 1 into PostgreSQL...\n",
      "Inserting test sample 1175  Atmospheric electrification is not a purely terrestrial phenomenon: all Solar System planetary atmospheres become slightly electrified by cosmic ray ionisation. There is evidence for lightning on Jupiter, Saturn, Uranus and Neptune, and it appears likely to exist on Mars, Venus and Titan. Atmospheric electricity has controversially been implicated in climate on Earth; here, a comparative approach is employed to review the role of electrification in the atmospheres of other planets and their moons. This paper reviews planetary atmospheric electricity including ionisation and ion-aerosol interactions. The conditions necessary for a global electric circuit, and the likelihood of meeting these conditions in other planetary atmospheres are briefly discussed.\n",
      "\n",
      "Atmospheric electrification could be important throughout the Solar System, particularly at the outer planets which receive little solar radiation, increasing the significance of electrical forces. Nucleation onto atmospheric ions has been predicted to affect the evolution and lifetime of haze layers on Titan, Neptune, and Triton. Atmospheric electrical processes on Titan, pre-Huygens, are summarised. Closer to Earth, heating from solar radiation dominates planetary meteorology; however Mars may have a global circuit based on electrical discharges from dust storms. There is a need for direct measurements of planetary atmospheric electrification, in particular on Mars, to assess the risk for future missions. Theoretical understanding could be increased by cross-disciplinary work to modify and update models and parameterisations initially developed for specific planetary atmospheres to make them more broadly applicable. 0 into PostgreSQL...\n",
      "Inserting test sample 1176  Atmospheric electrification is a process that plays an important role in the behavior of planetary atmospheres throughout the Solar System. The electrification process occurs due to the presence of charged particles and electric fields in a planet's atmosphere, leading to a range of phenomena such as lightning, aurorae, and atmospheric breakdowns. An understanding of atmospheric electrification is critical due to its impact on the planetary environment and its underlying physical processes. \n",
      "\n",
      "In this study, we review the current state of research on atmospheric electrification in the Solar System. We discuss the electrification processes and their effects on the planets' atmospheres, including ion drift, charge separation, and current flow. Additionally, we analyze the potential effects of atmospheric electrification on planetary environments, such as the alteration of chemical processes and the formation of complex molecules.\n",
      "\n",
      "Our review highlights that the understanding of atmospheric electrification in the Solar System remains a topic of ongoing research. We also identify key knowledge gaps in the current literature, including the lack of detailed observational data for some planets, and the need to improve our understanding of the coupling between planetary magnetic fields and the electrification process. We conclude that a continued investigation of atmospheric electrification in the Solar System will be essential to advancing our understanding of planetary atmospheres and to support future exploration missions. 1 into PostgreSQL...\n",
      "Inserting test sample 1177  The derivation scheme for hyperspherical harmonics (HSH) with arbitrary arguments is proposed. It is demonstrated that HSH can be presented as the product of HSH corresponding to spaces with lower dimensionality multiplied by the orthogonal (Jacobi or Gegenbauer) polynomial. The relation of HSH to quantum few-body problems is discussed. The explicit expressions for orthonormal HSH in spaces with dimensions from 2 to 6 are given. The important particular cases of four- and six-dimensional spaces are analyzed in detail and explicit expressions for HSH are given for several choices of hyperangles. In the six-dimensional space, HSH representing the kinetic energy operator corresponding to i) the three-body problem in physical space and ii) four-body planar problem are derived. 0 into PostgreSQL...\n",
      "Inserting test sample 1178  The study of hyperspherical harmonics plays a significant role in quantum mechanics and also has applications in other fields such as molecular physics and signal processing. This research paper focuses on the calculation of hyperspherical harmonics with arbitrary arguments. The proposed method for calculation involves the use of the recurrence relation for the associated Legendre polynomials and the Gaussian quadrature rule. The accuracy and efficiency of the method are tested through various numerical experiments, demonstrating its superiority when compared to other numerical methods. The results of the study emphasize the importance of employing efficient techniques for hyperspherical harmonics calculations that can handle arbitrary arguments, leading to better understanding and advancements in various fields of applied mathematics and physics. 1 into PostgreSQL...\n",
      "Inserting test sample 1179  Strong lenses are extremely useful probes of the distribution of matter on galaxy and cluster scales at cosmological distances, but are rare and difficult to find. The number of currently known lenses is on the order of 1,000. We wish to use crowdsourcing to carry out a lens search targeting massive galaxies selected from over 442 square degrees of photometric data from the Hyper Suprime-Cam (HSC) survey. We selected a sample of $\\sim300,000$ galaxies with photometric redshifts in the range $0.2 < z_{phot} < 1.2$ and photometrically inferred stellar masses $\\log{M_*} > 11.2$. We crowdsourced lens finding on this sample of galaxies on the Zooniverse platform, as part of the Space Warps project. The sample was complemented by a large set of simulated lenses and visually selected non-lenses, for training purposes. Nearly 6,000 citizen volunteers participated in the experiment. In parallel, we used YattaLens, an automated lens finding algorithm, to look for lenses in the same sample of galaxies. Based on a statistical analysis of classification data from the volunteers, we selected a sample of the most promising $\\sim1,500$ candidates which we then visually inspected: half of them turned out to be possible (grade C) lenses or better. Including lenses found by YattaLens or serendipitously noticed in the discussion section of the Space Warps website, we were able to find 14 definite lenses, 129 probable lenses and 581 possible lenses. YattaLens found half the number of lenses discovered via crowdsourcing. Crowdsourcing is able to produce samples of lens candidates with high completeness and purity, compared to currently available automated algorithms. A hybrid approach, in which the visual inspection of samples of lens candidates pre-selected by discovery algorithms and/or coupled to machine learning is crowdsourced, will be a viable option for lens finding in the 2020s. 0 into PostgreSQL...\n",
      "Inserting test sample 1180  The Survey of Gravitationally-lensed Objects in HSC Imaging (SuGOHI) project presents its sixth installment in the form of crowdsourced lens finding with Space Warps. The SuGOHI series aims to conduct a systematic search for gravitationally lensed objects in imaging data obtained by the Hyper Suprime-Cam (HSC) on the Subaru Telescope. The latest approach implemented in SuGOHI VI involves collaborative efforts by a citizen science platform called Space Warps. \n",
      "\n",
      "The Space Warps community employs the human brain's superior ability to identify patterns and fill in gaps in image recognition. The team from SuGOHI VI utilized this tool by collecting a total of 189,833 classifications from the Space Warps community. The resulting dataset was analyzed using machine learning methods, allowing the team to identify the image regions where the human classifiers were more likely to have identified lensing features. \n",
      "\n",
      "Through this method, SuGOHI VI was able to identify more than 300 lens candidates. Further vetting of these candidates confirmed 221 previously unknown gravitational lenses, as validated by a suite of metrics, including spectral line identification. The new discoveries probed a range of lensing properties, such as quadruply-imaged, lensed quasars, and broad absorption line objects.\n",
      "\n",
      "The use of a crowdsourcing tool in SuGOHI VI increased the discovery efficiency rate by approximately 30%, relative to previous SuGOHI iterations. Additionally, the new candidates identified by the Space Warps platform increased the total number of confirmed gravitational lenses in the HSC data to over 1,000.\n",
      "\n",
      "The SuGOHI VI project demonstrates the benefits of employing human-in-the-loop approaches alongside machine learning automation. The success of this research opens up possibilities for future collaborations between scientific communities and citizen science platforms to tackle the challenges of large-scale astronomical surveys. 1 into PostgreSQL...\n",
      "Inserting test sample 1181  We show that the gravitational modification of the phase of a neutron beam (the COW experiment) has a classical origin, being due to the time delay which classical particles experience in traversing a background gravitational field.\n",
      "\n",
      "Similarly, we show that classical light waves also undergo a phase shift in traversing a gravitational field. We show that the COW experiment respects the equivalence principle even in the presence of quantum mechanics. 0 into PostgreSQL...\n",
      "Inserting test sample 1182  This work investigates gravitational interactions in the context of quantum interference, building upon the theoretical framework of general relativity and quantum mechanics. We explore the ways in which gravity can affect interference patterns, particularly in the context of matter-wave interference experiments. We show that gravitationally induced phase shifts can lead to observable effects, potentially paving the way for novel experimental setups and new insights into the nature of gravity and quantum mechanics. 1 into PostgreSQL...\n",
      "Inserting test sample 1183  The results of this paper are twofold: In the first part, we prove that for Schr\\\"odinger map flows from hyperbolic planes to Riemannian surfaces with non-positive sectional curvatures, the harmonic maps which are holomorphic or anti-holomorphic of arbitrary size are asymptotically stable. In the second part, we prove that for Schr\\\"odinger map flows from hyperbolic planes into K\\\"ahler manifolds, the admissible harmonic maps of small size are asymptotically stable. The asymptotic stability results stated here contain two types: one is the convergence in $L^{\\infty}_x$ as the previous works, the other is convergence to harmonic maps plus radiation terms in the energy space, which is new in literature of Schr\\\"odinger map flows without symmetry assumptions. 0 into PostgreSQL...\n",
      "Inserting test sample 1184  In this paper, we investigate the global behavior of Schr\\\"odinger map flows on hyperbolic planes close to harmonic maps. We study the long-time existence and asymptotic behavior of solutions to these flows, as well as their stability properties. Our analysis relies on the use of geometric and analytic tools, such as the maximum principle, the Sobolev inequality, and the energy method. To deal with the nonlinearity of the problem, we apply perturbation techniques and establish suitable estimates for the solutions. Our results reveal the intricate interplay between geometry and analysis in understanding the dynamics of Schr\\\"odinger map flows, and provide a deeper insight into the behavior of these flows near harmonic maps on hyperbolic planes. 1 into PostgreSQL...\n",
      "Inserting test sample 1185  It is likely that all stars are born in clusters, but most clusters are not bound and disperse. None of the many protoclusters in our Galaxy are likely to develop into long-lived bound clusters. The Super Star Clusters (SSCs) seen in starburst galaxies are more massive and compact and have better chances of survival. The birth and early development of SSCs takes place deep in molecular clouds, and during this crucial stage the embedded clusters are invisible to optical or UV observations but are studied via the radio-infared supernebulae (RISN) they excite. We review observations of embedded clusters and identify RISN within 10 Mpc whose exciting clusters have a million solar masses or more in volumes of a few cubic parsecs and which are likely to not only survive as bound clusters, but to evolve into objects as massive and compact as Galactic globulars. These clusters are distinguished by very high star formation efficiency eta, at least a factor of 10 higher than the few percent seen in the Galaxy, probably due to violent disturbances their host galaxies have undergone. We review recent observations of the kinematics of the ionized gas in RISN showing outflows through low-density channels in the ambient molecular cloud; this may protect the cloud from feedback by the embedded HII region. 0 into PostgreSQL...\n",
      "Inserting test sample 1186  Globular clusters are among the oldest objects in the universe, but recent observations have revealed that some of them have unexpectedly young ages. In this paper, we present a detailed study of the youngest globular clusters discovered to date, using state-of-the-art telescopes and observational techniques. Our sample comprises ten clusters located in the outskirts of nearby galaxies, each with an estimated age of less than 100 million years. We used multi-wavelength imaging and spectroscopy to derive their properties, such as their sizes, masses, metallicities, and internal dynamics. We find that these young clusters share several characteristics with their older counterparts, but also exhibit some peculiarities, such as extended envelopes and high velocity dispersions. We discuss possible formation scenarios for these clusters, including the accretion of gas-rich clumps and the merging of smaller clusters. Our results have important implications for our understanding of galaxy formation and evolution, as well as the formation and evolution of star clusters in general. Further observations and theoretical modeling will be needed to fully unravel the mysteries of these youngest globular clusters. 1 into PostgreSQL...\n",
      "Inserting test sample 1187  We prove that the category of mixed Tate motives over $\\Z$ is spanned by the motivic fundamental group of $\\Pro^1$ minus three points. We prove a conjecture by M. Hoffman which states that every multiple zeta value is a $\\Q$-linear combination of $\\zeta(n_1,..., n_r)$ where $n_i\\in \\{2,3\\}$. 0 into PostgreSQL...\n",
      "Inserting test sample 1188  This paper studies mixed Tate motives, which provide a way to create new invariants and understand algebraic varieties. We focus on the case of mixed Tate motives over the integers, providing necessary background and highlighting key results. Our main result establishes a useful connection between mixed Tate motives and algebraic cycles on a certain type of variety. 1 into PostgreSQL...\n",
      "Inserting test sample 1189  Recently, Forbes, Kumar and Saptharishi [CCC, 2016] proved that there exists an explicit $d^{O(1)}$-variate and degree $d$ polynomial $P_{d}\\in VNP$ such that if any depth four circuit $C$ of bounded formal degree $d$ which computes a polynomial of bounded individual degree $O(1)$, that is functionally equivalent to $P_d$, then $C$ must have size $2^{\\Omega(\\sqrt{d}\\log{d})}$.\n",
      "\n",
      "The motivation for their work comes from Boolean Circuit Complexity. Based on a characterization for $ACC^0$ circuits by Yao [FOCS, 1985] and Beigel and Tarui [CC, 1994], Forbes, Kumar and Saptharishi [CCC, 2016] observed that functions in $ACC^0$ can also be computed by algebraic $\\Sigma\\mathord{\\wedge}\\Sigma\\Pi$ circuits (i.e., circuits of the form -- sums of powers of polynomials) of $2^{\\log^{O(1)}n}$ size. Thus they argued that a $2^{\\omega(\\log^{O(1)}{n})}$ \"functional\" lower bound for an explicit polynomial $Q$ against $\\Sigma\\mathord{\\wedge}\\Sigma\\Pi$ circuits would imply a lower bound for the \"corresponding Boolean function\" of $Q$ against non-uniform $ACC^0$. In their work, they ask if their lower bound be extended to $\\Sigma\\mathord{\\wedge}\\Sigma\\Pi$ circuits.\n",
      "\n",
      "In this paper, for large integers $n$ and $d$ such that $\\omega(\\log^2n)\\leq d\\leq n^{0.01}$, we show that any $\\Sigma\\mathord{\\wedge}\\Sigma\\Pi$ circuit of bounded individual degree at most $O\\left(\\frac{d}{k^2}\\right)$ that functionally computes Iterated Matrix Multiplication polynomial $IMM_{n,d}$ ($\\in VP$) over $\\{0,1\\}^{n^2d}$ must have size $n^{\\Omega(k)}$. Since Iterated Matrix Multiplication $IMM_{n,d}$ over $\\{0,1\\}^{n^2d}$ is functionally in $GapL$, improvement of the afore mentioned lower bound to hold for quasipolynomially large values of individual degree would imply a fine-grained separation of $ACC^0$ from $GapL$. 0 into PostgreSQL...\n",
      "Inserting test sample 1190  In this paper, we study the functional lower bounds for restricted arithmetic circuits of depth four. Specifically, we investigate circuits consisting of addition and multiplication gates with fan-in two, and we focus on the case where the bottom layer gates are restricted to be multiplication gates while the top layer gates are restricted to be addition gates. Our main result is that the natural polynomial function, which is hard to compute using these restricted circuits, requires exponential size circuits. \n",
      "\n",
      "To prove this result, we first show that the top fan-in-two addition gate model is equivalent, up to a polynomially small error, to the top fan-in-three model. We then use the Raz-Bar-Yehudayoff inner product theorem to obtain an exponential lower bound for the fan-in-three model. Combining these two results yields our main result.\n",
      "\n",
      "Our proof technique uses the polynomial method, and it involves decomposing the target function into a sum of simpler functions that behave well under restriction to low-degree polynomials. Our analysis then relies on combining these simpler functions to obtain the exponential lower bound for the target function.\n",
      "\n",
      "Our results have implications for the study of arithmetic circuit complexity, as well as for the study of geometric complexity theory, which seeks to understand the computational power of algebraic geometry problems. We also expect our techniques to be useful in other areas of complexity theory, such as communication complexity and circuit lower bounds. Overall, this paper provides a deeper understanding of the computational limits of restricted arithmetic circuits of depth four, and it opens up new avenues for future research. 1 into PostgreSQL...\n",
      "Inserting test sample 1191  In the case of nonabelian gauge theories with a complex weight, a controlled exploration of the complexified configuration space during a complex Langevin process requires the use of SL(N,C) gauge cooling, in order to minimize the distance from SU(N). Here we show that adaptive gauge cooling can lead to an efficient implementation of this idea. First results for SU(3) Yang-Mills theory in the presence of a nonzero theta-term are presented as well. 0 into PostgreSQL...\n",
      "Inserting test sample 1192  Complex Langevin dynamics (CLD) is a powerful tool to study systems with complex action. However, its reliability can be limited by gauge noise which causes convergence issues. In this paper, we propose an adaptive gauge cooling method in CLD that reduces the impact of gauge noise and improves convergence. We demonstrate the effectiveness of this method on several examples and provide evidence that it can lead to accurate results in challenging situations. 1 into PostgreSQL...\n",
      "Inserting test sample 1193  BACKGROUND: In object oriented (OO) software systems, class size has been acknowledged as having an indirect effect on the relationship between certain artifact characteristics, captured via metrics, and faultproneness, and therefore it is recommended to control for size when designing fault prediction models. AIM: To use robust statistical methods to assess whether there is evidence of any true effect of class size on fault prediction models. METHOD: We examine the potential mediation and moderation effects of class size on the relationships between OO metrics and number of faults. We employ regression analysis and bootstrapping-based methods to investigate the mediation and moderation effects in two widely-used datasets comprising seventeen systems.\n",
      "\n",
      "RESULTS: We find no strong evidence of a significant mediation or moderation effect of class size on the relationships between OO metrics and faults. In particular, size appears to have a more significant mediation effect on CBO and Fan-out than other metrics, although the evidence is not consistent in all examined systems. On the other hand, size does appear to have a significant moderation effect on WMC and CBO in most of the systems examined. Again, the evidence provided is not consistent across all examined systems CONCLUSION: We are unable to confirm if class size has a significant mediation or moderation effect on the relationships between OO metrics and the number of faults. We contend that class size does not fully explain the relationships between OO metrics and the number of faults, and it does not always affect the strength/magnitude of these relationships. We recommend that researchers consider the potential mediation and moderation effect of class size when building their prediction models, but this should be examined independently for each system. 0 into PostgreSQL...\n",
      "Inserting test sample 1194  This research paper focuses on revisiting the size effect in software fault prediction models. Software fault prediction models play an important role in identifying software defects and allowing developers to take preventive measures. The size effect refers to the relationship between the size of software and the likelihood of faults occurring. \n",
      "\n",
      "Previous studies have shown that software size has a significant impact on the accuracy of fault prediction models. However, there is still much debate on the extent of this impact and the most effective ways to account for it in prediction models. \n",
      "\n",
      "The main objective of this study is to reevaluate the size effect in fault prediction models. To achieve this objective, we conducted an empirical study on data obtained from various open-source software projects. Using different statistical techniques, we analyzed the relationship between software size and the accuracy of fault prediction models.\n",
      "\n",
      "Our findings indicate that the size effect does have a significant impact on the accuracy of fault prediction models. Specifically, our analysis showed that using a simple size metric such as lines of code is not effective in accounting for the size effect. We also found that alternative size metrics, such as cyclomatic complexity and number of methods, perform better in predicting software faults.\n",
      "\n",
      "This study contributes to the ongoing discussion about the size effect in fault prediction models. It provides evidence in support of using alternative size metrics while highlighting the need for further investigation into the most effective ways to account for the size effect in predicting software faults. Our results can provide guidance to developers and researchers in developing more accurate models for software fault prediction. 1 into PostgreSQL...\n",
      "Inserting test sample 1195  Synthesis of multilayer graphene on copper wires by a chemical vapor deposition method is reported. After copper etching, the multilayer tube collapses forming stripes of graphitic films, their electrical conductance as a function of temperature indicate a semiconductor-like behavior. Using the multilayer graphene stripes, a cross junction is built and owing to its electrical behavior we propose that a tunneling process exists in the device. 0 into PostgreSQL...\n",
      "Inserting test sample 1196  In this study, we investigate the electrical conductivity properties of multilayer graphene tubes under compression. Our results show that the collapsed tubes exhibit an increase in conductivity, which can be attributed to enhanced interlayer electronic communication. By analyzing the data, we propose a new model for the conductivity behavior of these structures. These findings shed new light on the potential applications of multilayer graphene tubes in electronic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 1197  A detailed study of correlated scalars, produced in collisions of nuclei and associated with the $\\sigma$-field fluctuations, $(\\delta \\sigma)^2= < \\sigma^2 >$, at the QCD critical point (critical fluctuations), is performed on the basis of a critical event generator (Critical Monte-Carlo) developed in our previous work. The aim of this analysis is to reveal suitable observables of critical QCD in the multiparticle environment of simulated events and select appropriate signatures of the critical point, associated with new and strong effects in nuclear collisions. 0 into PostgreSQL...\n",
      "Inserting test sample 1198  Observational Critical QCD is an active area of research in theoretical physics that aims to understand the behavior of quarks and gluons, the building blocks of matter. This subfield of Quantum Chromodynamics utilizes experimental data from high-energy particle colliders to analyze the properties of strongly interacting matter. Through the development of effective models and computational methods, researchers will gain invaluable insights into the mysterious realm of subatomic physics. In this paper, we will present recent advances in observational critical QCD and explore the implications for our understanding of the Universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1199  Following the spirit of a previous work of ours, we investigate the group of those General Coordinate Transformations (GCTs) which preserve manifest spatial homogeneity. In contrast to the case of Bianchi Type Models we, here, permit an isometry group of motions $G_{4}=SO(3)\\otimes T_{r}$, where $T_{r}$ is the translations group, along the radial direction, while SO(3) acts multiply transitively on each hypersurface of simultaneity $\\Sigma_{t}$. The basis 1-forms, can not be invariant under the action of the entire isometry group and hence produce an Open Lie Algebra. In order for these GCTs to exist and have a non trivial, well defined action, certain integrability conditions have to be satisfied; their solutions, exhibiting the maximum expected ``gauge'' freedom, can be used to simplify the generic, spatially homogeneous, line element. In this way an alternative proof of the generality of the Kantowski-Sachs (KS) vacuum is given, while its most general, manifestly homogeneous, form is explicitly presented. 0 into PostgreSQL...\n",
      "Inserting test sample 1200  This article studies the geometric properties of a class of time-dependent automorphisms that induce diffeomorphisms in open algebras. The Kantowski-Sachs vacuum geometry is highly generalized by these automorphisms, which generate a family of solutions that exhibit diverse behaviors. Specifically, we address the singularity issues that arise in this setting and prove that the time-dependent automorphisms tame the singularities of the Kantowski-Sachs vacuum geometry. We also derive families of explicit solutions that exhibit a range of different asymptotic behaviors and show that these solutions arise naturally in our framework. Finally, we analyze the stability and completeness properties of these solutions and demonstrate that they satisfy the relevant dynamical and geometric conditions. Overall, our work sheds light on the intricate geometric properties of open algebras and provides new insights into the generality of the Kantowski-Sachs vacuum geometry. 1 into PostgreSQL...\n",
      "Inserting test sample 1201  The available data for E2 transition strengths in the region between neutron-deficient Hf and Pt isotopes are far from complete. More and precise data are needed to enhance the picture of structure evolution in this region and to test state-of-the-art nuclear models. In a simple model, the maximum collectivity is expected at the middle of the major shell. However, for actual nuclei, this picture may no longer be the case, and one should use a more realistic nuclear-structure model. We address this point by studying the spectroscopy of Hf. We remeasure the 2^+_1 half-lives of 172,174,176Hf, for which there is some disagreement in the literature. The main goal is to measure, for the first time, the half-lives of higher-lying states of the rotational band. The new results are compared to a theoretical calculation for absolute transition strengths. The half-lives were measured using \\gamma-\\gamma and conversion-electron-\\gamma delayed coincidences with the fast timing method. For the determination of half-lives in the picosecond region, the generalized centroid difference method was applied. For the theoretical calculation of the spectroscopic properties, the interacting boson model is employed, whose Hamiltonian is determined based on microscopic energy-density functional calculations. The measured 2^+_1 half-lives disagree with results from earlier \\gamma-\\gamma fast timing measurements, but are in agreement with data from Coulomb excitation experiments and other methods. Half-lives of the 4^+_1 and 6^+_1 states were measured, as well as a lower limit for the 8^+_1 states. We show the importance of the mass-dependence of effective boson charge in the description of E2 transition rates in chains of nuclei. It encourages further studies of the microscopic origin of this mass dependence. New data on transition rates in nuclei from neighboring isotopic chains could support these studies. 0 into PostgreSQL...\n",
      "Inserting test sample 1202  In this study, we investigate the evolution of E2 transition strength in deformed hafnium isotopes namely $^{172}$Hf, $^{174}$Hf, and $^{176}$Hf. We present new measurements on these isotopes using the bremsstrahlung photon-beam technique and the centroid shift method. Our results reveal that the E2 transition strength increases with deformation in $^{172}$Hf up to a critical point, beyond which it decreases with further deformation. A similar trend is observed in $^{174}$Hf, but with a smaller increase in the transition strength. In $^{176}$Hf, the E2 transition strength increases with deformation up to the highest studied values. We compare our new data with the existing data in this mass region and find good agreement. Our results suggest that the evolution of the E2 transition strength in deformed hafnium isotopes is highly sensitive to their deformation and the location of the neutron number. Moreover, our findings support the idea of shape coexistence among low-lying states in these isotopes. We also discuss the implications of our results for nuclear structure models and astrophysical processes. Overall, this study provides new experimental insights into the evolution of E2 transition strength in deformed hafnium isotopes, which are crucial for a better understanding of nuclear structure and properties. 1 into PostgreSQL...\n",
      "Inserting test sample 1203  Consider a family of Boolean models, indexed by integers $n \\ge 1$, where the $n$-th model features a Poisson point process in ${\\mathbb{R}}^n$ of intensity $e^{n \\rho_n}$ with $\\rho_n \\to \\rho$ as $n \\to \\infty$, and balls of independent and identically distributed radii distributed like $\\bar X_n \\sqrt{n}$, with $\\bar X_n$ satisfying a large deviations principle. It is shown that there exist three deterministic thresholds: $\\tau_d$ the degree threshold; $\\tau_p$ the percolation threshold; and $\\tau_v$ the volume fraction threshold; such that asymptotically as $n$ tends to infinity, in a sense made precise in the paper: (i) for $\\rho < \\tau_d$, almost every point is isolated, namely its ball intersects no other ball; (ii) for $\\tau_d< \\rho< \\tau_p$, almost every ball intersects an infinite number of balls and nevertheless there is no percolation; (iii) for $\\tau_p< \\rho< \\tau_v$, the volume fraction is 0 and nevertheless percolation occurs; (iv) for $\\tau_d< \\rho< \\tau_v$, almost every ball intersects an infinite number of balls and nevertheless the volume fraction is 0; (v) for $\\rho > \\tau_v$, the whole space covered. The analysis of this asymptotic regime is motivated by related problems in information theory, and may be of interest in other applications of stochastic geometry. 0 into PostgreSQL...\n",
      "Inserting test sample 1204  The Boolean model is a popular way to represent complex systems in various fields of science due to its simplicity and effectiveness. In this paper, we explore the behavior of the Boolean model in the Shannon regime, where the number of inputs and outputs is large. We identify three thresholds that affect the performance of the model. The first threshold determines the point at which the system becomes unstable and unpredictable, leading to exponential growth in the number of steady states. The second threshold marks the transition from the regime of high correlation to low correlation among the inputs and outputs, resulting in a change in the system's behavior. The third threshold corresponds to the number of bits needed to represent a steady state, beyond which the model's space complexity becomes impractical. We provide analytical expressions for the asymptotic behavior of the model near each of these thresholds. Our results shed light on the limits of the Boolean model in the Shannon regime and have implications for the modeling of complex systems with large numbers of inputs and outputs. 1 into PostgreSQL...\n",
      "Inserting test sample 1205  We have developed an analytical formulation to calculate the plasmon dispersion relation for a two-dimensional layer which is encapsulated within a narrow spatial gap between two bulk half-space plasmas. This is based on a solution of the inverse dielectric function integral equation within the random-phase approximation (RPA). We take into account the nonlocality of the plasmon dispersion relation for both gapped and gapless graphene as the sandwiched two-dimensional (2D) semiconductor plasma. The associated nonlocal graphene plasmon spectrum coupled to the \"sandwich\" system is exhibited in density plots, which show a linear mode and a pair of depolarization modes shifted from the bulk plasma frequency. 0 into PostgreSQL...\n",
      "Inserting test sample 1206  This study investigates the plasmon excitations of encapsulated graphene by utilizing density-functional theory and a continuum model for substrate screening. The findings reveal that the plasmon dispersion relation in encapsulated graphene is different from that of suspended graphene. Furthermore, the plasmon decay rate in encapsulated graphene is strongly dependent on the interlayer separation, allowing for tunability via external strain. The study also observes that the appearance of the substrate can drastically affect plasmon excitations, leading to additional effects beyond the change in screening. These results shed light on the behavior of plasmon excitations in encapsulated graphene, which can be useful for optoelectronic and plasmonic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 1207  Income and wealth distribution affect stability of a society to a large extent and high inequality affects it negatively. Moreover, in the case of developed countries, recently has been proven that inequality is closely related to all negative phenomena affecting society. So far, Econophysics papers tried to analyse income and wealth distribution by employing distributions such as Fermi-Dirac, Bose-Einstein, Maxwell-Boltzmann, lognormal (Gibrat), and exponential. Generally, distributions describe mostly income and less wealth distribution for low and middle income segment of population, which accounts about 90% of the population. Our approach is based on a totally new distribution, not used so far in the literature regarding income and wealth distribution. Using cumulative distribution method, we find that polynomial functions, regardless of their degree (first, second, or higher), can describe with very high accuracy both income and wealth distribution. Moreover, we find that polynomial functions describe income and wealth distribution for entire population including upper income segment for which traditionally Pareto distribution is used. 0 into PostgreSQL...\n",
      "Inserting test sample 1208  In this paper, we explore the applicability of polynomial distributions for modeling income and wealth distribution. We propose a novel class of polynomial distributions, which generalize the beta and gamma distributions, and analyze their properties. Our results show that the proposed distributions provide a flexible and accurate framework for modeling the distribution of income and wealth. We use empirical data from various countries to validate the model and compare its performance with other commonly used distributions. Additionally, we introduce the concept of inequality functionals based on the proposed distributions, which allow for a more fine-grained analysis of inequality. Our findings suggest that polynomial distributions can offer a versatile and powerful tool for researchers and policymakers interested in understanding and addressing issues related to income and wealth inequality. 1 into PostgreSQL...\n",
      "Inserting test sample 1209  We show that the linear strands of the Tor of determinantal varieties in spaces of symmetric and skew-symmetric matrices are irreducible representations for the periplectic (strange) Lie superalgebra. The structure of these linear strands is explicitly known, so this gives an explicit realization of some representations of the periplectic Lie superalgebra. This complements results of Pragacz and Weyman, who showed an analogous statement for the generic determinantal varieties and the general linear Lie superalgebra. We also give a simpler proof of their result. Via Koszul duality, this is an odd analogue of the fact that the coordinate rings of these determinantal varieties are irreducible representations for a certain classical Lie algebra. 0 into PostgreSQL...\n",
      "Inserting test sample 1210  In this paper, we investigate the derived supersymmetries of determinantal varieties. Utilizing the framework of algebraic geometry, we show that these supersymmetries can be understood in terms of shifts and twists of various derived categories. Our main result is a precise characterization of the derived symmetries of determinantal varieties. We provide examples of how our findings can be used to study the geometry and topology of such varieties, including the computation of their cohomology groups and intersection numbers. Our results shed new light on the relationship between supersymmetry and the geometry of determinantal varieties, and open up new avenues for further research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 1211  Suppose $\\tau$ is a train track on a surface $S$. Let $C(\\tau)$ be the set of isotopy classes of simple closed curves carried by $\\tau$. Masur and Minsky [2004] prove $C(\\tau)$ is quasi-convex inside the curve complex $C(S)$. We prove the complement, $C(S) - C(\\tau)$, is quasi-convex. 0 into PostgreSQL...\n",
      "Inserting test sample 1212  This article presents a study on curves that do not conform to expected patterns. Through the analysis of diverse datasets, we highlight the existence of mathematical forms that are not commonly explored in research. Our findings stress the importance of acknowledging and studying structures that do not conform to established conventions. 1 into PostgreSQL...\n",
      "Inserting test sample 1213  Scanning Transmission Electron Microscopy (STEM) has enabled mapping of atomic structures of solids with sub-pm precision, providing insight to the physics of ferroic phenomena and chemical expansion. However, only a subset of information is available, due to projective nature of imaging in the beam direction. Correspondingly, the analysis often relies on the postulated known form of macroscopic Landau-Ginzburg energy, and some predefined relationship between experimentally determined atomic coordinates and the order parameter field. Here, we propose an approach for exploring the structure of ferroics using reduced order parameter models constructed based on experimental data only. We develop a four sublattices model (FSM) for the analytical description of A-cation displacement in (anti)ferroelectric-antiferrodistortive perovskites of ABO3-type. The model describes the displacements of cation A in four neighboring unit cells and determines the conditions of different structural phases appearance and stability in ABO3. We show that FSM explains the coexistence of rhombohedral (R), orthorhombic (O) and spatially modulated (SM) phases, observed by atomic-resolution STEM in La-doped BiFeO3. Using this approach, we atomically resolve and theoretically model the sublattice asymmetry inherent to the case of the A-site La/Bi cation sublattice in LaxBi1-xFeO3 polymorphs. This approach allows exploring the ferroics behaviors from experimental data only, without additional assumptions on the nature of the order parameter. 0 into PostgreSQL...\n",
      "Inserting test sample 1214  The development of free energy functional models through atomically-resolved imaging techniques has been a significant area of interest for researchers. In this study, we investigate atomic scale phenomena in La-doped BiFeO3 to build a free energy functional. By utilizing advanced imaging techniques, we were able to obtain a detailed understanding of the structural and electronic properties of the material. The La-doping induces complex domain behavior and results in enhanced electrical conductivity. Our findings suggest that the La-doping induces a strain coupling effect, which leads to improved ferroelectric properties. We explore the underlying mechanisms of the enhanced domain behavior, and our results demonstrate unique coupling between structural distortion and electronic property variation at the atomic scale. We further develop a free energy functional model based on our observations, which could potentially be used to optimize device performance in future applications. This study highlights the importance of advanced imaging techniques in providing a detailed understanding of complex materials, and the potential applications of free energy functional models in optimizing device performance. 1 into PostgreSQL...\n",
      "Inserting test sample 1215  The chemical composition of planets is inherited from that of the protoplanetary disk at the time of planet formation. Increasing observational evidence suggests that planet formation occurs in less than 1 Myr. This motivates the need for spatially resolved spectral observations of Class I disks, as carried out by the ALMA chemical survey of Disk-Outflow sources in Taurus (ALMA-DOT). In the context of ALMA-DOT, we observe the edge-on disk around the Class I source IRAS 04302+2247 (the butterfly star) in the 1.3mm continuum and five molecular lines. We report the first tentative detection of methanol (CH$_3$OH) in a Class I disk and resolve, for the first time, the vertical structure of a disk with multiple molecular tracers. The bulk of the emission in the CO 2-1, CS 5-4, and o-H$_2$CO 3(1,2)-2(1,1) lines originates from the warm molecular layer, with the line intensity peaking at increasing disk heights, $z$, for increasing radial distances, $r$. Molecular emission is vertically stratified, with CO observed at larger disk heights (aperture $z/r\\sim0.41-0.45$) compared to both CS and H$_2$CO, which are nearly cospatial ($z/r\\sim0.21-0.28$). In the outer midplane, the line emission decreases due to molecular freeze-out onto dust grains (freeze-out layer) by a factor of >100 (CO) and 15 (CS). The H$_2$CO emission decreases by a factor of only about 2, which is possibly due to H$_2$CO formation on icy grains, followed by a nonthermal release into the gas phase. The inferred [CH$_3$OH]/[H$_2$CO] abundance ratio is 0.5-0.6, which is 1-2 orders of magnitude lower than for Class 0 hot corinos, and a factor ~2.5 lower than the only other value inferred for a protoplanetary disk (in TW Hya, 1.3-1.7). Additionally, it is at the lower edge but still consistent with the values in comets. This may indicate that some chemical reprocessing occurs in disks before the formation of planets and comets. 0 into PostgreSQL...\n",
      "Inserting test sample 1216  The Taurus region provides an ideal laboratory for studying molecular outflows. We present a survey of the vertical stratification of CO, CS, CN, H$_2$CO, and CH$_3$OH in a Class I disk, based on observations obtained with the Atacama Large Millimeter/Submillimeter Array (ALMA) in Band 6 with sub-arcsecond angular resolution. We find that the abundance ratios of various molecular species show a clear dependence on vertical height in the disk. Specifically, the CO and H$_2$CO emission is found to be more abundant towards the disk midplane, while the CS, CN, and CH$_3$OH emission is more abundant in higher regions of the disk. This vertical stratification may be due to changes in physical conditions across the disk, such as temperature and density, or it may be a consequence of gas-phase chemistry driven by UV irradiation from nearby stars. The high angular resolution of our ALMA observations enables us to map the vertical distribution of these complex molecules in fine detail, and our results suggest that the molecular content of protoplanetary disks may be dominated by vertical structures that depend on the height above the disk midplane. Our study provides a solid foundation for future high-resolution studies of disk-outflow interactions in the Taurus region, and for future efforts to understand how molecular abundances evolve over time in protoplanetary disks. Overall, the ALMA-DOT survey sheds new light on the physical and chemical processes at work in disk-outflow systems, and provides a wealth of data for future studies of planet formation and astrochemistry. 1 into PostgreSQL...\n",
      "Inserting test sample 1217  Simulating the dynamics of ions near polarizable nanoparticles (NPs) using coarse-grained models is extremely challenging due to the need to solve the Poisson equation at every simulation timestep. Recently, a molecular dynamics (MD) method based on a dynamical optimization framework bypassed this obstacle by representing the polarization charge density as virtual dynamic variables, and evolving them in parallel with the physical dynamics of ions. We highlight the computational gains accessible with the integration of machine learning (ML) methods for parameter prediction in MD simulations by demonstrating how they were realized in MD simulations of ions near polarizable NPs. An artificial neural network based regression model was integrated with MD simulation and predicted the optimal simulation timestep and optimization parameters characterizing the virtual system with $94.3\\%$ success. The ML-enabled auto-tuning of parameters generated accurate dynamics of ions for $\\approx 10$ million steps while improving the stability of the simulation by over an order of magnitude. The integration of ML-enhanced framework with hybrid OpenMP/MPI parallelization techniques reduced the computational time of simulating systems with thousands of ions and induced charges from thousands of hours to tens of hours, yielding a maximum speedup of $\\approx 3$ from ML-only acceleration and a maximum speedup of $\\approx 600$ from the combination of ML and parallel computing methods. Extraction of ionic structure in concentrated electrolytes near oil-water emulsions demonstrates the success of the method.\n",
      "\n",
      "The approach can be generalized to select optimal parameters in other MD applications and energy minimization problems. 0 into PostgreSQL...\n",
      "Inserting test sample 1218  Molecular dynamics simulations (MD) of ionic systems interacting with polarizable nanoparticles (PNP) are computationally intensive and require the calibration of various parameters, which can be quite challenging. Here, we introduce a novel approach to automating the parameter-tuning process using machine learning (ML) algorithms. We demonstrate the effectiveness of this approach by optimizing the parameters of three commonly-used models: the Drude, Classical, and Adaptive-Integrators Solute-Solvent (AI-SS) models. Our results show that ML-based optimization significantly outperforms other tuning methods, with improvements ranging from 25% to 50%. Additionally, we present a new simulation protocol for the efficient simulation of ionic systems near PNPs, which leverages the advanced features of the optimized models. We applied our protocol to study the dynamics of ions in the vicinity of a polarizable nanoparticle, and obtained new insights into the role of polarization effects in the behavior of ionic solutions. Specifically, we find that polarization results in a significant enhancement of the ion mobility, which has important implications for a wide range of applications, including electrolytes for batteries and capacitors, as well as for the design of novel materials with tunable electrochemical properties. The proposed workflow can help to significantly speed up the simulation of ionic solutions in complex environments, and aid the design of new materials with tailored electrochemical properties. 1 into PostgreSQL...\n",
      "Inserting test sample 1219  This paper presents statistical language and translation models based on collections of small finite state machines we call ``head automata''. The models are intended to capture the lexical sensitivity of N-gram models and direct statistical translation models, while at the same time taking account of the hierarchical phrasal structure of language. Two types of head automata are defined: relational head automata suitable for translation by transfer of dependency trees, and head transducers suitable for direct recursive lexical translation. 0 into PostgreSQL...\n",
      "Inserting test sample 1220  This paper proposes a novel approach to speech translation using head automata. By utilizing machine learning techniques, we train a head automaton to generate accurate translations from raw speech input. Our model outperforms existing approaches on a variety of benchmarks and demonstrates promising results in real-world scenarios. We also provide insights into the underlying mechanisms of our approach through a detailed analysis of the model's activations. These findings demonstrate the effectiveness of using head automata for speech translation. 1 into PostgreSQL...\n",
      "Inserting test sample 1221  The KMOS Redshift One Spectroscopic Survey (KROSS) is an ESO guaranteed time survey of 795 typical star-forming galaxies in the redshift range z=0.8-1.0 with the KMOS instrument on the VLT. In this paper we present resolved kinematics and star formation rates for 584 z~1 galaxies. This constitutes the largest near-infrared Integral Field Unit survey of galaxies at z~1 to date. We demonstrate the success of our selection criteria with 90% of our targets found to be Halpha emitters, of which 81% are spatially resolved. The fraction of the resolved KROSS sample with dynamics dominated by ordered rotation is found to be 83$\\pm$5%. However, when compared with local samples these are turbulent discs with high gas to baryonic mass fractions, ~35%, and the majority are consistent with being marginally unstable (Toomre Q~1). There is no strong correlation between galaxy averaged velocity dispersion and the total star formation rate, suggesting that feedback from star formation is not the origin of the elevated turbulence. We postulate that it is the ubiquity of high (likely molecular) gas fractions and the associated gravitational instabilities that drive the elevated star-formation rates in these typical z~1 galaxies, leading to the ten-fold enhanced star-formation rate density. Finally, by comparing the gas masses obtained from inverting the star-formation law with the dynamical and stellar masses, we infer an average dark matter to total mass fraction within 2.2$r_e$ (9.5kpc) of 65$\\pm$12%, in agreement with the results from hydrodynamic simulations of galaxy formation. 0 into PostgreSQL...\n",
      "Inserting test sample 1222  The KMOS Redshift One Spectroscopic Survey (KROSS) investigates typical star-forming galaxies at redshift z~1 in order to explore their dynamical properties and the fractions of gas and dark matter present in these systems. By utilizing the KMOS instrument on the Very Large Telescope (VLT), KROSS has produced integral-field spectroscopy for over 700 galaxies, allowing for analysis of their rotational properties and velocity dispersions. The results suggest that high rotation velocities and small velocity dispersion are typical for these galaxies, indicating that they are primarily disk-dominated systems. Further analysis reveals a relationship between the dark matter fraction and the specific star formation rate, with more actively star-forming galaxies possessing higher fractions of dark matter. Additionally, the gas fraction is found to be dependent on both the specific star formation rate and the galaxy's size, with larger galaxies at higher redshifts having lower gas fractions. KROSS has thus provided crucial insight into the properties of typical z~1 star-forming galaxies, shedding light on the evolution of these systems over cosmic time. Future analyses of KROSS data will likely yield even more insight into the properties of these galaxies, ultimately leading to a deeper understanding of galaxy evolution and the role of dark matter in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1223  Counterfactual explanations are a prominent example of post-hoc interpretability methods in the explainable Artificial Intelligence research domain. They provide individuals with alternative scenarios and a set of recommendations to achieve a sought-after machine learning model outcome.\n",
      "\n",
      "Recently, the literature has identified desiderata of counterfactual explanations, such as feasibility, actionability and sparsity that should support their applicability in real-world contexts. However, we show that the literature has neglected the problem of the time dependency of counterfactual explanations. We argue that, due to their time dependency and because of the provision of recommendations, even feasible, actionable and sparse counterfactual explanations may not be appropriate in real-world applications.\n",
      "\n",
      "This is due to the possible emergence of what we call \"unfortunate counterfactual events.\" These events may occur due to the retraining of machine learning models whose outcomes have to be explained via counterfactual explanation. Series of unfortunate counterfactual events frustrate the efforts of those individuals who successfully implemented the recommendations of counterfactual explanations. This negatively affects people's trust in the ability of institutions to provide machine learning-supported decisions consistently. We introduce an approach to address the problem of the emergence of unfortunate counterfactual events that makes use of histories of counterfactual explanations. In the final part of the paper we propose an ethical analysis of two distinct strategies to cope with the challenge of unfortunate counterfactual events. We show that they respond to an ethically responsible imperative to preserve the trustworthiness of credit lending organizations, the decision models they employ, and the social-economic function of credit lending. 0 into PostgreSQL...\n",
      "Inserting test sample 1224  Counterfactual explanations play a central role in understanding causality and making inferences about what could have happened if events had occurred differently. However, the temporal dimension of counterfactual reasoning has been relatively understudied. In this paper, we examine the role of time in counterfactual explanations, specifically in situations where multiple counterfactual events could have occurred. We explore how varying the timing of counterfactual events affects judgments of causality and responsibility.\n",
      "\n",
      "We present a series of experiments that investigate how people make sense of counterfactual events that could have occurred at different times. Our results show that temporal distance between causal events affects how people assign responsibility and make judgments of causality. In addition, we find that temporal order of alternative events can also impact counterfactual reasoning and influence judgments in unexpected ways.\n",
      "\n",
      "Furthermore, we examine the interaction of time with other moderators of counterfactual reasoning such as outcome valence and event plausibility. We show that these factors can differentially influence counterfactual reasoning depending on when the alternative events are introduced.\n",
      "\n",
      "Overall, this research highlights the importance of considering the temporal dimension of counterfactual explanations. Our findings have implications for understanding how people reason about causality and responsibility, and provide insights into how counterfactual explanations can be improved in practical settings such as in legal or policymaking contexts. 1 into PostgreSQL...\n",
      "Inserting test sample 1225  We give an approximation algorithm for non-uniform sparsest cut with the following guarantee: For any $\\epsilon,\\delta \\in (0,1)$, given cost and demand graphs with edge weights $C, D$ respectively, we can find a set $T\\subseteq V$ with $\\frac{C(T,V\\setminus T)}{D(T,V\\setminus T)}$ at most $\\frac{1+\\epsilon}{\\delta}$ times the optimal non-uniform sparsest cut value, in time $2^{r/(\\delta\\epsilon)}\\poly(n)$ provided $\\lambda_r \\ge \\Phi^*/(1-\\delta)$. Here $\\lambda_r$ is the $r$'th smallest generalized eigenvalue of the Laplacian matrices of cost and demand graphs; $C(T,V\\setminus T)$ (resp. $D(T,V\\setminus T)$) is the weight of edges crossing the $(T,V\\setminus T)$ cut in cost (resp. demand) graph and $\\Phi^*$ is the sparsity of the optimal cut. In words, we show that the non-uniform sparsest cut problem is easy when the generalized spectrum grows moderately fast. To the best of our knowledge, there were no results based on higher order spectra for non-uniform sparsest cut prior to this work.\n",
      "\n",
      "Even for uniform sparsest cut, the quantitative aspects of our result are somewhat stronger than previous methods. Similar results hold for other expansion measures like edge expansion, normalized cut, and conductance, with the $r$'th smallest eigenvalue of the normalized Laplacian playing the role of $\\lambda_r$ in the latter two cases.\n",
      "\n",
      "Our proof is based on an l1-embedding of vectors from a semi-definite program from the Lasserre hierarchy. The embedded vectors are then rounded to a cut using standard threshold rounding. We hope that the ideas connecting $\\ell_1$-embeddings to Lasserre SDPs will find other applications. Another aspect of the analysis is the adaptation of the column selection paradigm from our earlier work on rounding Lasserre SDPs [GS11] to pick a set of edges rather than vertices. This feature is important in order to extend the algorithms to non-uniform sparsest cut. 0 into PostgreSQL...\n",
      "Inserting test sample 1226  The sparsest cut problem is a fundamental optimization problem that has been widely studied in computer science and related fields. Given a graph and a set of terminals, the objective is to find a cut that minimizes the ratio of the cut size to the number of edges crossing the cut. In this paper, we propose a new algorithm for approximating the sparsest cut problem in graphs with non-uniform degree distributions.\n",
      "\n",
      "Our algorithm is based on a new spectral framework that extends the standard spectral approach to the sparsest cut problem. The key idea is to use generalized eigenvalue decompositions to extract information about the eigenvectors of the Laplacian matrix of the graph. Unlike previous spectral methods, our approach can handle graphs with non-uniform degree distributions, which is important for many real-world applications.\n",
      "\n",
      "We provide a theoretical analysis of our algorithm and show that it achieves a nearly optimal approximation ratio for the sparsest cut problem. Specifically, our algorithm achieves an approximation ratio of O(log n log log n) for graphs with non-uniform degree distributions, where n is the number of nodes in the graph. This matches the best-known approximation ratio for the sparsest cut problem, and in fact improves upon it in some cases.\n",
      "\n",
      "We also provide experimental results that demonstrate the effectiveness of our algorithm on a variety of real-world graphs. Our algorithm consistently outperforms existing methods, particularly on graphs with non-uniform degree distributions. We expect that our new spectral framework will have broad applicability to other optimization problems on graphs, and we plan to explore these applications in future work.\n",
      "\n",
      "In conclusion, we propose a new algorithm based on a generalized spectral approach for approximating the non-uniform sparsest cut problem. Our algorithm achieves a nearly optimal approximation ratio and outperforms existing methods on a variety of real-world graphs. We believe that our new spectral framework has potential for a wide range of other optimization problems on graphs. 1 into PostgreSQL...\n",
      "Inserting test sample 1227  We identify 73 z~7 and 59 z~8 candidate galaxies in the reionization epoch, and use this large 26-29.4 AB mag sample of galaxies to derive very deep luminosity functions to <-18 AB mag and the star formation rate density at z~7 and z~8. The galaxy sample is derived using a sophisticated Lyman-Break technique on the full two-year WFC3/IR and ACS data available over the HUDF09 (~29.4 AB mag, 5 sigma), two nearby HUDF09 fields (~29 AB mag, 14 arcmin) and the wider area ERS (~27.5 AB mag) ~40 arcmin**2). The application of strict optical non-detection criteria ensures the contamination fraction is kept low (just ~7% in the HUDF). This very low value includes a full assessment of the contamination from lower redshift sources, photometric scatter, AGN, spurious sources, low mass stars, and transients (e.g., SNe). From careful modelling of the selection volumes for each of our search fields we derive luminosity functions for galaxies at z~7 and z~8 to <-18 AB mag. The faint-end slopes alpha at z~7 and z~8 are uncertain but very steep at alpha = -2.01+/-0.21 and alpha=-1.91+/-0.32, respectively. Such steep slopes contrast to the local alpha<~-1.4 and may even be steeper than that at z~4 where alpha=-1.73+/-0.05.\n",
      "\n",
      "With such steep slopes (alpha<~-1.7) lower luminosity galaxies dominate the galaxy luminosity density during the epoch of reionization. The star formation rate densities derived from these new z~7 and z~8 luminosity functions are consistent with the trends found at later times (lower redshifts). We find reasonable consistency, with the SFR densities implied from reported stellar mass densities, being only ~40% higher at z<7. This suggests that (1) the stellar mass densities inferred from the Spitzer IRAC photometry are reasonably accurate and (2) that the IMF at very high redshift may not be very different from that at later times. 0 into PostgreSQL...\n",
      "Inserting test sample 1228  This research paper presents an analysis of the ultraviolet (UV) luminosity functions observed from 132 Lyman-break galaxies (LBGs) at redshifts z~7 and z~8. These galaxies are from the ultra-deep HUDF09 and wide-area ERS WFC3/IR observations. The purpose of this study is to investigate the evolution of the UV luminosity function of these galaxies at high redshifts.\n",
      "\n",
      "Our analysis uses a spectral energy distribution fitting method to estimate precise UV luminosities for the sample of galaxies. The resulting UV luminosity functions are then used to study the intrinsic physical properties and star formation rates of LBGs at z~7 and z~8. We also compare the UV luminosity functions of the two samples to investigate possible evolutionary trends between the two redshifts.\n",
      "\n",
      "Our results reveal that the UV luminosity functions of the z~7 and z~8 LBGs are well described by Schechter functions. Furthermore, we find that the faint-end slopes of the UV luminosity functions are similar between the two redshifts, suggesting that there may not be a significant evolution of the faint-end slope over this redshift range. However, we do find possible evidence for luminosity evolution in the bright-end of the UV luminosity function, with the z~8 sample exhibiting a steeper slope than the z~7 sample.\n",
      "\n",
      "Overall, this study provides new insights into the evolution of galaxies at high redshifts. The UV luminosity functions estimated from the large sample of z~7 and z~8 LBGs in this research paper provide important information for modeling galaxy evolution and for interpreting observations of the high-redshift universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1229  Considering rank s fields obey first order equation of motion, we study the dynamics of such fields in a 3 dimensional self-dual space-like warped $AdS_3$ black hole background. We argue that in this background, symmetric conditions and gauge constraint can not be satisfied simultaneously. Using new suitable constraint, we find the exact solutions of equation of motion. Then, we obtain the quasi-normal modes by imposing appropriate boundary condition at horizon and infinity. 0 into PostgreSQL...\n",
      "Inserting test sample 1230  We investigate the behaviour of tensor fields on self-dual warped anti-de Sitter 3 (AdS3) background. We examine their properties under symmetry transformations and provide analysis on their physical interpretation. Our results show the existence of novel tensor-field dynamics with potential implications for further research in the field. 1 into PostgreSQL...\n",
      "Inserting test sample 1231  The estimation of model parameters with uncertainties from observed data is a ubiquitous inverse problem in science and engineering. In this paper, we suggest an inexpensive and easy to implement parameter estimation technique that uses a heteroscedastic Bayesian Neural Network trained using anchored ensembling. The heteroscedastic aleatoric error of the network models the irreducible uncertainty due to parameter degeneracies in our inverse problem, while the epistemic uncertainty of the Bayesian model captures uncertainties which may arise from an input observation's out-of-distribution nature. We use this tool to perform real-time parameter inference in a 6 parameter G-equation model of a ducted, premixed flame from observations of acoustically excited flames. We train our networks on a library of 2.1 million simulated flame videos. Results on the test dataset of simulated flames show that the network recovers flame model parameters, with the correlation coefficient between predicted and true parameters ranging from 0.97 to 0.99, and well-calibrated uncertainty estimates. The trained neural networks are then used to infer model parameters from real videos of a premixed Bunsen flame captured using a high-speed camera in our lab. Re-simulation using inferred parameters shows excellent agreement between the real and simulated flames. Compared to Ensemble Kalman Filter-based tools that have been proposed for this problem in the combustion literature, our neural network ensemble achieves better data-efficiency and our sub-millisecond inference times represent a savings on computational costs by several orders of magnitude. This allows us to calibrate our reduced-order flame model in real-time and predict the thermoacoustic instability behaviour of the flame more accurately. 0 into PostgreSQL...\n",
      "Inserting test sample 1232  This paper presents a novel method for real-time parameter inference in reduced-order flame models using heteroscedastic Bayesian neural network ensembles. Flame models are used to predict the behavior of an often complex process such as combustion. However, flame models typically rely on several parameters that are challenging to measure experimentally. Furthermore, various factors such as noise in data and model form errors can impede accurate predictions. Hence, it is essential to infer the appropriate parameters in real-time to ensure effective control and operation of the process. \n",
      "\n",
      "To address these challenges, this paper proposes the use of Bayesian neural networks (BNNs) that account for heteroscedasticity, which is the property of a material or system having different variances at different points or under different circumstances. Our approach trains an ensemble of BNNs to infer the parameters while also accounting for model errors and noise in the data. The ensemble approach provides a more accurate and reliable prediction of the posterior distribution of the parameters. Additionally, this approach can be applied online, enabling real-time prediction of the parameters.\n",
      "\n",
      "Our results demonstrate that the proposed method outperforms the state-of-the-art alternatives in terms of statistical accuracy and computational efficiency. The experiments were conducted on a range of different flame models, demonstrating the general applicability of our approach. Moreover, we present a sensitivity study to showcase the efficacy of our approach under various conditions.\n",
      "\n",
      "In conclusion, this paper introduces a robust and efficient approach for real-time parameter inference in reduced-order flame models that accounts for heteroscedasticity using Bayesian neural network ensembles. This approach has the potential to significantly enhance the control and operation of several systems reliant on flame models while also providing a means of real-time inferencing. 1 into PostgreSQL...\n",
      "Inserting test sample 1233  We provide algorithms that learn simple auctions whose revenue is approximately optimal in multi-item multi-bidder settings, for a wide range of valuations including unit-demand, additive, constrained additive, XOS, and subadditive. We obtain our learning results in two settings. The first is the commonly studied setting where sample access to the bidders' distributions over valuations is given, for both regular distributions and arbitrary distributions with bounded support. Our algorithms require polynomially many samples in the number of items and bidders. The second is a more general max-min learning setting that we introduce, where we are given \"approximate distributions,\" and we seek to compute an auction whose revenue is approximately optimal simultaneously for all \"true distributions\" that are close to the given ones.\n",
      "\n",
      "These results are more general in that they imply the sample-based results, and are also applicable in settings where we have no sample access to the underlying distributions but have estimated them indirectly via market research or by observation of previously run, potentially non-truthful auctions.\n",
      "\n",
      "Our results hold for valuation distributions satisfying the standard (and necessary) independence-across-items property. They also generalize and improve upon recent works, which have provided algorithms that learn approximately optimal auctions in more restricted settings with additive, subadditive and unit-demand valuations using sample access to distributions. We generalize these results to the complete unit-demand, additive, and XOS setting, to i.i.d.\n",
      "\n",
      "subadditive bidders, and to the max-min setting.\n",
      "\n",
      "Our results are enabled by new uniform convergence bounds for hypotheses classes under product measures. Our bounds result in exponential savings in sample complexity compared to bounds derived by bounding the VC dimension, and are of independent interest. 0 into PostgreSQL...\n",
      "Inserting test sample 1234  Multi-item auctions are a crucial mechanism for allocating resources in many contexts, ranging from online advertising to public procurement. However, designing auctions that optimize the desired objectives, such as revenue or efficiency, is a challenging task, especially when bidders have complex valuations over the items for sale. In recent years, machine learning techniques have been applied to this problem, with the aim of automatically learning auction designs from examples. In this paper, we review the recent literature on learning multi-item auctions, focusing on the use of sample-based methods, which learn by simulating auctions and observing their outcomes. We show how these methods can be used to effectively optimize a wide range of auction objectives, such as revenue, fairness, and social welfare. We also discuss the challenges and limitations of sample-based learning, and compare it to alternative methods that rely on structural assumptions or gradient-based optimization. Finally, we explore some directions for future research, including the use of reinforcement learning and the incorporation of causal reasoning into auction design. Overall, we argue that sample-based learning is a promising approach for tackling the complex problem of designing multi-item auctions, and that it has the potential to lead to more efficient and fair allocation of resources in various real-world settings. 1 into PostgreSQL...\n",
      "Inserting test sample 1235  As the nearest known AGB star (d=64pc) and one of the brightest (mK-2), L2 Pup is a particularly interesting benchmark object to monitor the final stages of stellar evolution. We report new lucky imaging observations of this star with the VLT/NACO adaptive optics system in twelve narrow band filters covering the 1.0-4.0 microns wavelength range. These diffraction limited images reveal an extended circumstellar dust lane in front of the star, that exhibits a high opacity in the J band and becomes translucent in the H and K bands. In the L band, extended thermal emission from the dust is detected. We reproduce these observations using Monte-Carlo radiative transfer modeling of a dust disk with the RADMC-3D code. We also present new interferometric observations with the VLTI/VINCI and MIDI instruments. We measure in the K band an upper limit to the limb-darkened angular diameter of theta_LD = 17.9 +/- 1.6 mas, converting to a maximum linear radius of R = 123 +/- 14 Rsun. Considering the geometry of the extended K band emission in the NACO images, this upper limit is probably close to the actual angular diameter of the star. The position of L2 Pup in the Herzsprung-Russell diagram indicates that this star has a mass around 2 Msun and is probably experiencing an early stage of the asymptotic giant branch. We do not detect any stellar companion of L2 Pup in our adaptive optics and interferometric observations, and we attribute its apparent astrometric wobble in the Hipparcos data to variable lighting effects on its circumstellar material. We however do not exclude the presence of a binary companion, as the large loop structure extending to more than 10 AU to the North-East of the disk in our L band images may be the result of interaction between the stellar wind of L2 Pup and a hidden secondary object. The geometric configuration that we propose, with a large dust disk seen almost edge-on, appears particularly favorable to test and develop our understanding of the formation of bipolar nebulae. 0 into PostgreSQL...\n",
      "Inserting test sample 1236  This paper presents the results of VLT/NACO spectro-imaging observations and VLTI interferometry measurements of the nearest asymptotic giant branch (AGB) star, L2 Puppis. Our observations reveal a previously unknown edge-on translucent dust disk around L2 Puppis, extending from 10 to 200 AU. The disk is highly asymmetric with a brighter southern side and a fainter northern side. The dust disk has a near-infrared spectral index consistent with almost pure astronomical silicates, implying that the opaque grains of the disk have been completely processed by the AGB star.\n",
      "\n",
      "We combined the spectro-imaging data with interferometric measurements to constrain the disk density and geometry. The spectral energy distribution and interferometric data suggest a disk inclined by ~85 degrees with a semi-major axis of ~90 AU. We infer a gas-to-dust mass ratio of 330, indicating a dusty disk which is likely to have formed via the fragmentation of a red giant envelope.\n",
      "\n",
      "The disk around L2 Puppis represents an excellent laboratory to study the late stages of stellar evolution. We present a possible scenario for the formation and evolution of the disk, in which dust is lifted by the interaction of the AGB starâ€™s radiation pressure with the gas in the disk. The dust then becomes trapped at the edge of a cavity, formed by the photoevaporation of the outer disk, a scenario not unlike that which is found for protoplanetary disks around young stars.\n",
      "\n",
      "This study provides a new understanding of AGB star environments and their role in the evolution of circumstellar dust, by revealing a previously unknown, edge-on translucent disk around the nearest AGB star, L2 Puppis, and constraining its density and geometry. Our findings illustrate the importance of high-resolution observations in elucidating the complex and dynamic environments of evolved stars, and provide important implications for the study of planet formation and the origin of dust. 1 into PostgreSQL...\n",
      "Inserting test sample 1237  With an increasingly large number of Chinese tourists in Japan, the hotel industry is in need of an affordable market research tool that does not rely on expensive and time-consuming surveys or interviews. Because this problem is real and relevant to the hotel industry in Japan, and otherwise completely unexplored in other studies, we have extracted a list of potential keywords from Chinese reviews of Japanese hotels in the hotel portal site Ctrip1 using a mathematical model to then use them in a sentiment analysis with a machine learning classifier. While most studies that use information collected from the internet use pre-existing data analysis tools, in our study, we designed the mathematical model to have the highest possible performing results in classification, while also exploring on the potential business implications these may have. 0 into PostgreSQL...\n",
      "Inserting test sample 1238  This study aimed to explore the travel patterns and preferences of Chinese tourists in Japan through text mining of a hotel portal site. Utilizing a sample of over 10,000 reviews written in the Chinese language, we identified key themes and sentiments related to tourist experiences in various regions of Japan. Our findings suggest that Chinese tourists are attracted to traditional culture and natural scenery, but also value modern amenities and convenience. Furthermore, we observed significant differences in travel patterns and hotel preferences based on age and gender. These results have important implications for the tourism industry in Japan, as they provide insights into how to better cater to the specific needs and desires of the growing number of Chinese tourists visiting the country. 1 into PostgreSQL...\n",
      "Inserting test sample 1239  The availability of massive datasets allows for conducting extreme value statistics using more observations drawn from the tail of an underlying distribution. When large datasests are distributedly stored and cannot be combined into one oracle sample, a divide-and-conquer algorithm is often invoked to construct a distributed estimator. If the distributed estimator possesses the same asymptotic behavior as the hypothetical oracle estimator based on the oracle sample, then it is regarded as satisfying the oracle property. In this paper, we introduce a set of tools regarding the asymptotic behavior of the tail empirical and quantile processes under the distributed inference setup. Using these tools, one can easily establish the oracle property for most extreme value estimators based on the peak-over-threshold approach. We provide various examples to show the usefulness of the tools. 0 into PostgreSQL...\n",
      "Inserting test sample 1240  The paper introduces a new framework for distributed statistical inference to estimate tail empirical and quantile processes. Current inference methods often require a complete data set, which can be computationally expensive. The proposed framework allows for data to be decentralized across multiple sources, enabling more efficient and scalable inference. By leveraging the power of modern computing clusters, the authors were able to demonstrate that this approach provides high-quality inference in a distributed computing environment. The authors present both theoretical results and numerical experiments to support their claims, showing that their framework outperforms existing methods in terms of accuracy, efficiency, and scalability. These results are especially promising for applications in large-scale data analysis, where existing inference methods may not be tractable. 1 into PostgreSQL...\n",
      "Inserting test sample 1241  An international group of scientists has begun planning for the Planet Formation Imager (PFI, www.planetformationimager.org), a next-generation infrared interferometer array with the primary goal of imaging the active phases of planet formation in nearby star forming regions and taking planetary system 'snapshots' of young systems to understand exoplanet architectures. PFI will be sensitive to warm dust emission using mid-infrared capabilities made possible by precise fringe tracking in the near-infrared. An L/M band beam combiner will be especially sensitive to thermal emission from young exoplanets (and their circumplanetary disks) with a high spectral resolution mode to probe the kinematics of CO and H2O gas. In this brief White Paper, we summarize the main science goals of PFI, define a baseline PFI architecture that can achieve those goals, and identify key technical challenges that must be overcome before the dreams of PFI can be realized within the typical cost envelope of a major observatory. We also suggest activities over the next decade at the flagship US facilities (CHARA, NPOI, MROI) that will help make the Planet Formation Imager facility a reality. The key takeaway is that infrared interferometry will require new experimental telescope designs that can scale to 8 m-class with the potential to reduce per area costs by x10, a breakthrough that would also drive major advances across astronomy. 0 into PostgreSQL...\n",
      "Inserting test sample 1242  The Planet Formation Imager (PFI) is a cutting-edge instrument that aims to produce the first direct images of planet formation in action around neighboring stars. In this paper, we set the stage for the PFI and review the key scientific questions that it will address. These include the formation and evolution of protoplanetary disks, the physics of planet formation, and the diversity and demographics of planetary systems. We also discuss the technical requirements and challenges facing the PFI, such as the need for high angular resolution, high contrast imaging, and advanced data processing techniques. To address these challenges, we outline a comprehensive instrument development program that leverages the world's leading expertise in adaptive optics, interferometry, and instrumentation. We conclude by highlighting the broad scientific impact of the PFI, both in the areas of planet formation and beyond, and discuss opportunities for community involvement and participation in this exciting scientific endeavor. Overall, the PFI promises to be a game-changing instrument for the study of planet formation and will undoubtedly yield unprecedented insights into the origins and diversity of planetary systems in our galaxy and beyond. 1 into PostgreSQL...\n",
      "Inserting test sample 1243  We describe a variant construction of the unstable Adams spectral the sequence for a space $Y$, associated to any free simplicial resolution of $H^*(Y;R)$ for $R=\\mathbb{F}_p$ or $\\mathbb{Q}$. We use this construction to describe the differentials and filtration in the spectral sequence in terms of appropriate systems of higher cohomology operations. 0 into PostgreSQL...\n",
      "Inserting test sample 1244  We explore higher structures in the unstable Adams spectral sequence. Our main result is an identification of the differential graded modules controlling operations in the cohomology of spaces. This involves a comparison of the spectral sequence with the homotopy version of the Tower spectral sequence associated to a cosimplicial resolution of the base space. 1 into PostgreSQL...\n",
      "Inserting test sample 1245  Phase-resolved spectroscopy of four AM CVn systems obtained with the William Herschel Telescope and the Gran Telescopio de Canarias (GTC) is presented.\n",
      "\n",
      "SDSS\\,J120841.96+355025.2 was found to have an orbital period of 52.96$\\pm$0.40\\,min and shows the presence of a second bright spot in the accretion disc. The average spectrum contains strong Mg\\,{\\sc i} and Si\\,{\\sc i/ii} absorption lines most likely originating in the atmosphere of the accreting white dwarf. SDSS\\,J012940.05+384210.4 has an orbital period of 37.555$\\pm$0.003 min. The average spectrum shows the Stark broadened absorption lines of the DB white dwarf accretor. The orbital period is close to the previously reported superhump period of 37.9\\,min. Combined, this results in a period excess $\\epsilon$=0.0092$\\pm$0.0054 and a mass ratio $q=0.031\\pm$0.018.\n",
      "\n",
      "SDSS\\,J164228.06+193410.0 displays an orbital period of 54.20$\\pm$1.60\\,min with an alias at 56.35\\,min. The average spectrum also shows strong Mg\\,{\\sc i} absorption lines, similar to SDSS\\,J120841.96+355025.2.\n",
      "\n",
      "SDSS\\,J152509.57+360054.50 displays an period of 44.32$\\pm$0.18\\,min. The overall shape of the average spectrum is more indicative of shorter period systems in the 20-35 minute range. The accretor is still clearly visible in the pressure broadened absorption lines most likely indicating a hot donor star and/or a high mass accretor. Flux ratios for several helium lines were extracted from the Doppler tomograms for the disc and bright spot region, and compared with single-slab LTE models with variable electron densities and path lengths to estimate the disc and bright spot temperature. A good agreement between data and the model in three out of four systems was found for the disc region. All three systems show similar disc temperatures of $\\sim$10\\,500 K. In contrast, only weak agreement between observation and models was found for the bright spot region. 0 into PostgreSQL...\n",
      "Inserting test sample 1246  AM CVn systems are a class of cataclysmic variable stars that involve a white dwarf transferring material to a helium-rich companion star, known as donor star. In this paper, we present a detailed analysis of four AM CVn systems, namely SDSS J1240, SDSS J0804, SDSS J1514, and SDSS J1908, with a focus on their orbital periods and accretion disc structures.\n",
      "\n",
      "We measured the radial velocities of the donor stars in each of the four systems through high-resolution spectroscopic observations and analyzed the resulting data with a Lomb-Scargle periodogram. Our analysis allowed us to determine the orbital periods of the donor stars with unprecedented accuracy, which range from 13.8 minutes for SDSS J0804 to 33.2 minutes for SDSS J1240.\n",
      "\n",
      "We then used eclipse mapping techniques to probe the structure of the accretion discs in each of the four systems. We found that the accretion discs are highly non-uniform, with bright spots and an inhomogeneous temperature distribution. The bright spots are indicative of stream-disc impact points, where the material transferred from the donor star collides with the accretion disc.\n",
      "\n",
      "Our observations also revealed that the accretion discs in the four systems have varying degrees of asymmetry, with SDSS J1514 and SDSS J1908 having more asymmetric discs than SDSS J1240 and SDSS J0804. We also found that two of the systems, SDSS J0804 and SDSS J1514, showed evidence of a precessing accretion disc, which is likely caused by the gravitational influence of a third body in the system.\n",
      "\n",
      "In summary, our study provides new insights into the orbital periods and accretion disc structures of four AM CVn systems. Our results reveal that these systems exhibit a wide range of properties, highlighting the complexity of the accretion process in cataclysmic variable stars, and providing valuable input for models of their evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 1247  The VSOP mission is a Japanese-led project to study radio sources with sub-milliarcsecond angular resolution using an orbiting 8-m telescope, HALCA and global arrays of Earth-based telescopes. Approximately 25% of the observing time has been devoted to a survey of compact AGN at 5 GHz which are stronger than 1 Jy -- the VSOP AGN Survey. This paper, the second in a series, describes the data calibration, source detection, self-calibration, imaging and modeling, and gives examples illustrating the problems specific to space VLBI. The VSOP Survey web-site which contains all results and calibrated data is described. 0 into PostgreSQL...\n",
      "Inserting test sample 1248  We present the calibration and imaging results of the Very Long Baseline Interferometry Space Observatory Program at 5 GHz towards 461 active galactic nuclei (AGNs) in the northern hemisphere. The survey utilized the new K4 post-correlation processor for the first time, resulting in enhanced dynamic range and sensitivity. The data were analyzed via the conventional self-calibration approach and manually edited for spurious interference. 80% of the AGNs were imaged with a resolution better than 5 mas, revealing compact, core-jet structures. The remaining sources have lower dynamic range, but still display compact radio cores, indicating the presence of central AGN engines. 1 into PostgreSQL...\n",
      "Inserting test sample 1249  We present ground-based observations of the transiting Neptune-mass planet Gl 436b obtained with the 3.5-meter telescope at Apache Point Observatory and other supporting telescopes. Included in this is an observed transit in early 2005, over two years before the earliest reported transit detection. We have compiled all available transit data to date and perform a uniform modeling using the JKTEBOP code. We do not detect any transit timing variations of amplitude greater than ~1 minute over the ~3.3 year baseline. We do however find possible evidence for a self-consistent trend of increasing orbital inclination, transit width, and transit depth, which supports the supposition that Gl 436b is being perturbed by another planet of < 12 Earth Masses in a non-resonant orbit. 0 into PostgreSQL...\n",
      "Inserting test sample 1250  This study presents new observations of the transits of the exoplanet Gliese 436b, along with a possible detection of parameter variations in these events. By analyzing the transit light curves obtained from three different telescopes, we found that the timing of the transits is not consistent with a constant orbital period. This could be explained by the presence of additional planets in the system or by the influence of a non-uniform density distribution in the planet's atmosphere. Furthermore, we identified a tentative change in the planet's transit depth, which may also be due to atmospheric effects or to a variation in the planet's radius. These findings provide valuable insights into the complex nature of exoplanet atmospheres and dynamics, and highlight the need for further observations and theoretical modeling. 1 into PostgreSQL...\n",
      "Inserting test sample 1251  Knowledge of X-ray shock and radio relic connection in merging galaxy clusters has been greatly extended in terms of both observation and theory over the last decade. ZwCl 2341+0000 is a double-relic merging galaxy cluster; previous studies have shown that half of the S relic is associated with an X-ray surface brightness discontinuity, while the other half not. The discontinuity was believed to be a shock front. Therefore, it is a mysterious case of an only partial shock-relic connection. By using the 206.5 ks deep Chandra observations, we aim to investigate the nature of the S discontinuity.\n",
      "\n",
      "Meanwhile, we aim to explore new morphological and thermodynamical features. In addition, we utilize the GMRT and JVLA images to compute radio spectral index (SI) maps. In the deep observations, the previously reported S surface brightness discontinuity is better described as a sharp change in slope or as a kink, which is likely contributed by the disrupted core of the S subcluster.\n",
      "\n",
      "The radio SI maps show spectral flattening at the SE edge of the S relic, suggesting that the location of the shock front is 640 kpc away from the kink.\n",
      "\n",
      "We update the radio shock Mach number to be $2.2\\pm0.1$ and $2.4\\pm0.4$ for the S and N radio relics, respectively, based on the injection SI. We also put a 3 sigma lower limit on the X-ray Mach number of the S shock to be >1.6.\n",
      "\n",
      "Meanwhile, the deep observations reveal that the N subcluster is in a perfect cone shape, with a ~400 kpc linear cold front on each side. This type of conic subcluster has been predicted by simulations but is observed here for the first time. It represents a transition stage between a blunt-body cold front and a slingshot cold front. Strikingly, we found a 400 kpc long gas trail attached to the apex of the cone, which could be due to the gas stripping. In addition, an over-pressured hot region is found in the SW flank of the cluster. 0 into PostgreSQL...\n",
      "Inserting test sample 1252  We present the results of a deep observation of the merging galaxy cluster ZwCl 2341+0000 using the Chandra Observatory. The galaxy cluster is comprised of two subclusters which are in the process of merging, creating a complex structure with multiple shock fronts and strong gas motions. The Chandra observation enables us to study the morphology of the cluster's hot intracluster medium and the distribution of its dark matter. \n",
      "\n",
      "We detect strong X-ray emission from the intracluster gas and measure its temperature to be around 8 keV. We identify two distinct X-ray bright regions which correspond to the subclusters and confirm their merger status. In addition, we detect a number of point sources within the cluster, many of which exhibit extended emission, suggesting they are associated with the cluster. We perform an optical follow-up of these sources and identify a number of them as galaxies belonging to the cluster. \n",
      "\n",
      "By analyzing the X-ray surface brightness distribution and temperature of the cluster gas, we estimate the mass and gas fraction of the system. We find a total mass of approximately 3.5x10^14 solar masses and a gas fraction of around 20%, consistent with previous studies of galaxy clusters at similar redshifts. \n",
      "\n",
      "Our analysis of the Chandra data also reveals a complex network of shock fronts and gas motions within the cluster, providing insights into the dynamics of the merger process. We identify multiple shock fronts located towards the center of the cluster, where the subclusters are interacting. We estimate the Mach numbers of these shocks to be in the range of 1.5-2.0, indicating they are moderately strong shocks. \n",
      "\n",
      "Overall, our Chandra observation of ZwCl 2341+0000 provides a detailed characterization of the properties and dynamics of a merging galaxy cluster. These results contribute to our understanding of the growth and evolution of galaxy clusters, as well as the physics of mergers between massive structures in the Universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1253  Highlights in a sport video are usually referred as actions that stimulate excitement or attract attention of the audience. A big effort is spent in designing techniques which find automatically highlights, in order to automatize the otherwise manual editing process. Most of the state-of-the-art approaches try to solve the problem by training a classifier using the information extracted on the tv-like framing of players playing on the game pitch, learning to detect game actions which are labeled by human observers according to their perception of highlight. Obviously, this is a long and expensive work. In this paper, we reverse the paradigm: instead of looking at the gameplay, inferring what could be exciting for the audience, we directly analyze the audience behavior, which we assume is triggered by events happening during the game. We apply deep 3D Convolutional Neural Network (3D-CNN) to extract visual features from cropped video recordings of the supporters that are attending the event. Outputs of the crops belonging to the same frame are then accumulated to produce a value indicating the Highlight Likelihood (HL) which is then used to discriminate between positive (i.e. when a highlight occurs) and negative samples (i.e. standard play or time-outs). Experimental results on a public dataset of ice-hockey matches demonstrate the effectiveness of our method and promote further research in this new exciting direction. 0 into PostgreSQL...\n",
      "Inserting test sample 1254  This paper proposes a novel approach to detect indirect match highlights in soccer games using deep convolutional neural networks (DCNNs). The proposed approach is designed to automatically identify and extract important game segments where indirect match highlights occur. The key contribution of this paper is the development of a DCNN architecture, which learns highly discriminative features for indirect match highlights detection. The proposed DCNN architecture integrates multiple convolutional layers with max pooling and non-linear activation functions, followed by several fully connected layers for classification. We evaluate the performance of our approach using a publicly available soccer video dataset and show that our method outperforms state-of-the-art techniques for indirect match highlights detection. To gain insight into the learned features, we provide a comprehensive analysis of feature maps and demonstrate that our DCNN architecture can efficiently capture local motion and texture patterns in soccer videos. Furthermore, we demonstrate the ability of our approach to detect indirect match highlights in challenging scenarios, such as occlusion, camera motion, and cluttered backgrounds. Overall, our proposed approach has the potential to automate the labor-intensive process of indirect match highlights detection and assist soccer analysts in better understanding the game dynamics. 1 into PostgreSQL...\n",
      "Inserting test sample 1255  Unsupervised pattern recognition algorithms support the existence of three gamma-ray burst classes; Class I (long, large fluence bursts of intermediate spectral hardness), Class II (short, small fluence, hard bursts), and Class III (soft bursts of intermediate durations and fluences). The algorithms surprisingly assign larger membership to Class III than to either of the other two classes. A known systematic bias has been previously used to explain the existence of Class III in terms of Class I; this bias allows the fluences and durations of some bursts to be underestimated (Hakkila et al., ApJ 538, 165, 2000). We show that this bias primarily affects only the longest bursts and cannot explain the bulk of the Class III properties. We resolve the question of Class III existence by demonstrating how samples obtained using standard trigger mechanisms fail to preserve the duration characteristics of small peak flux bursts. Sample incompleteness is thus primarily responsible for the existence of Class III. In order to avoid this incompleteness, we show how a new dual timescale peak flux can be defined in terms of peak flux and fluence.\n",
      "\n",
      "The dual timescale peak flux preserves the duration distribution of faint bursts and correlates better with spectral hardness (and presumably redshift) than either peak flux or fluence. The techniques presented here are generic and have applicability to the studies of other transient events. The results also indicate that pattern recognition algorithms are sensitive to sample completeness; this can influence the study of large astronomical databases such as those found in a Virtual Observatory. 0 into PostgreSQL...\n",
      "Inserting test sample 1256  Gamma-ray bursts (GRBs) are one of the most energetic and enigmatic events in the Universe. GRBs are classified based on their spectral properties, which imply different physical processes responsible for the emission. However, incomplete sampling of the prompt gamma-ray emission makes it challenging to precisely determine their spectral properties. In this paper, we investigate the effect of sample completeness on GRB classification using a simulated dataset of prompt gamma-ray emission.\n",
      "\n",
      "Our results show that a lower completeness level leads to a decrease in the accuracy of classifying GRBs based on their spectral properties. We find that the incompleteness of the data affects the classification of a particular subset of GRBs, namely short GRBs. In addition, we show that the impact of the sample completeness is stronger in cases where the data exhibit more significant spectral complexity. We conclude that it is crucial to take into account the incompleteness of the data when performing GRB spectral classification or when comparing different GRB samples.\n",
      "\n",
      "Overall, our study highlights the importance of considering and quantifying the effect of incomplete sampling of the prompt gamma-ray emission on GRB classification, especially for short GRBs. This effect is an important factor to be considered when comparing different GRB samples and when inferring the physical properties of these highly energetic events. We also suggest that future GRB missions should take steps to minimize the incompleteness of their datasets. 1 into PostgreSQL...\n",
      "Inserting test sample 1257  In this study, we propose a new concept, the gammachirp envelope distortion index (GEDI), based on the signal-to-distortion ratio in the auditory envelope, SDRenv to predict the intelligibility of speech enhanced by nonlinear algorithms. The objective of GEDI is to calculate the distortion between enhanced and clean-speech representations in the domain of a temporal envelope extracted by the gammachirp auditory filterbank and modulation filterbank. We also extend GEDI with multi-resolution analysis (mr-GEDI) to predict the speech intelligibility of sounds under non-stationary noise conditions. We evaluate GEDI in terms of speech intelligibility predictions of speech sounds enhanced by a classic spectral subtraction and a Wiener filtering method. The predictions are compared with human results for various signal-to-noise ratio conditions with additive pink and babble noises. The results showed that mr-GEDI predicted the intelligibility curves better than short-time objective intelligibility (STOI) measure, extended-STOI (ESTOI) measure, and hearing-aid speech perception index (HASPI) under pink-noise conditions, and better than HASPI under babble-noise conditions. The mr-GEDI method does not present an overestimation tendency and is considered a more conservative approach than STOI and ESTOI. Therefore, the evaluation with mr-GEDI may provide additional information in the development of speech enhancement algorithms. 0 into PostgreSQL...\n",
      "Inserting test sample 1258  The ability to predict the intelligibility of speech is fundamental in several fields, including telecommunications, medicine, and hearing aid design. The Gammachirp Envelope Distortion Index (GEDI) is a new objective method for assessing speech intelligibility. In this paper, we present the results of a study that evaluated the effectiveness of GEDI in predicting the intelligibility of enhanced speech.\n",
      "\n",
      "We recruited 40 participants with normal hearing to participate in a listening task. Speech samples were degraded with reverberation and noise, and then processed with a speech enhancement algorithm. We calculated GEDI scores for each processed speech sample and observed a strong correlation between GEDI and speech intelligibility. The correlation was stronger than that of traditional intelligibility measures such as the Speech Intelligibility Index and the Short-Time Objective Intelligibility measures.\n",
      "\n",
      "Our findings suggest that GEDI is a reliable and efficient measure for predicting the intelligibility of enhanced speech. GEDI has the potential to become a standard objective method for assessing the quality of speech enhancement algorithms and predicting speech intelligibility in various applications. Future studies should explore the relationship between GEDI and subjective measures of speech intelligibility and investigate the applicability of GEDI in different populations. 1 into PostgreSQL...\n",
      "Inserting test sample 1259  Lyman-Break Galaxy (LBG) samples observed during reionization ($z\\gtrsim6$) with Hubble Space Telescope's Wide Field Camera 3 are reaching sizes sufficient to characterize their clustering properties. Using a combined catalog from the Hubble eXtreme Deep Field and CANDELS surveys, containing $N=743$ LBG candidates at z>6.5 at a mean redshift of $z=7.2$, we detect a clear clustering signal in the angular correlation function (ACF) at $\\sim4\\sigma$, corresponding to a real-space correlation length $r_{0}=6.7^{+0.9}_{-1.0}h^{-1}$cMpc. The derived galaxy bias $b=8.6^{+0.9}_{-1.0}$ is that of dark-matter halos of $M=10^{11.1^{+0.2}_{-0.3}}$M$_{\\odot}$ at $z=7.2$, and highlights that galaxies below the current detection limit ($M_{AB}\\sim-17.7$) are expected in lower-mass halos ($M\\sim10^{8}-10^{10.5}$M$_{\\odot}$). We compute the ACF of LBGs at $z\\sim3.8-z\\sim5.9$ in the same surveys. A trend of increasing bias is found from $z=3.8$ ($b\\sim3.0$) to $z=7.2$ ($b\\sim8.6$), broadly consistent with galaxies at fixed luminosity being hosted in dark-matter halos of similar mass at $4<z<6$, followed by a slight rise in halo masses at $z\\sim7$ ($\\sim2\\sigma$ confidence). Separating the data at the median luminosity of the $z=7.2$ sample ($M_{UV}=-19.4$) shows higher clustering at $z=5.9$ for bright galaxies ($r_{0}=5.5^{+1.4}_{-1.5}h^{-1}$cMpc, $b=6.2^{+1.2}_{-1.5}$) compared to faint galaxies ($r_{0}=1.9^{+1.1}_{-1.0}h^{-1}$cMpc, $b=2.7\\pm1.2$) implying a constant mass-to-light ratio $\\frac{dlogM}{dlogL}\\sim1.2^{+1.8}_{-0.8}$. A similar trend is present in the $z=7.2$ sample with larger uncertainty.\n",
      "\n",
      "Finally, our bias measurements allow us to investigate the fraction of dark-matter halos hosting UV-bright galaxies (the duty-cycle, $\\epsilon_{DC}$).\n",
      "\n",
      "At $z=7.2$ values near unity are preferred, which may be explained by the shortened halo assembly time at high-redshift. 0 into PostgreSQL...\n",
      "Inserting test sample 1260  The large-scale distribution of galaxies in the universe reveals information about the physical processes that govern their formation and evolution. In this study, we aim to measure the clustering of galaxies at a redshift of z~7.2 and investigate the evolution of galaxy bias within the redshift range of 3.8<z<8. Our analysis uses data from the XDF, GOODS-S, and GOODS-N surveys, which provide a comprehensive picture of the distant universe. \n",
      "\n",
      "We use a statistical approach to estimate the clustering of galaxies based on their spatial distribution, which is characterized by the two-point correlation function. We find that the clustering amplitude of galaxies at z~7.2 is lower than at lower redshifts, indicating that the large-scale structure of the universe was still in the process of formation at this epoch. Our analysis of the evolution of galaxy bias indicates that galaxies become increasingly clustered with time, as gravity amplifies small initial density fluctuations, leading to the formation of large-scale structures.\n",
      "\n",
      "To investigate the evolution of galaxy bias, we model the bias factor, which describes how the clustering of galaxies differs from that of the underlying dark matter distribution. We find that the bias factor increases with decreasing redshift, indicating that galaxies become more strongly clustered as they evolve over time. Our results suggest that the physical processes that govern the formation and evolution of galaxies have a significant impact on their large-scale distribution.\n",
      "\n",
      "Furthermore, we investigate uncertainties in our measurements, including systematic errors and cosmic variance. We find that these uncertainties can have a significant impact on our results, highlighting the need for more precise observational data and improved theoretical models.\n",
      "\n",
      "In conclusion, our study provides important insights into the large-scale distribution of galaxies at high redshifts. We find that the clustering of galaxies at z~7.2 is lower than at lower redshifts, indicating that the large-scale structure of the universe was still in formation at this epoch. Our analysis of the evolution of galaxy bias suggests that galaxies become increasingly clustered with time, reflecting the complex physical processes that govern their formation and evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 1261  Images obtained by the Cassini spacecraft between 2012 and 2015 reveal a periodic brightness variation in a region of Saturn's D ring that previously appeared to be rather featureless. Furthermore, the intensity and radial wavenumber of this pattern have decreased steadily with time since it was first observed. Based on analogies with similar structures elsewhere in the D ring, we propose that this structure was created by some event that disturbed the orbital motions of the ring particles, giving them finite orbital eccentricities and initially aligned pericenters. Differential orbital precession then transformed this structure into a spiral pattern in the ring's optical depth that became increasingly tightly wound over time. The observed trends in the pattern's radial wavenumber are roughly consistent with this basic model, and also indicate that the ring-disturbing event occurred in early December 2011. Similar events in 1979 may have generated the periodic patterns seen in this same region by the Voyager spacecraft. The 2011 event could have been caused by debris striking the rings, or by a disturbance in the planet's electromagnetic environment. The rapid reduction in the intensity of the brightness variations over the course of just a few years indicates that some process is either damping orbital eccentricities in this region or causing the orbital pericenters of particles with the same semi-major axis to become misaligned. 0 into PostgreSQL...\n",
      "Inserting test sample 1262  In late 2011, a new pattern in Saturn's D ring was observed and recorded. This intriguing discovery has caused much speculation and interest among the scientific community. The unusual pattern that emerged has never been seen before in this area of the planet's ring system, suggesting a unique formation process. Despite extensive study of Saturn's rings, the D ring continues to present mysteries that have yet to be fully understood. The new pattern observed appears to have a well-defined radial width of approximately 40 km. The pattern is also continuous in azimuth spanning at least 2,400 km and contains an undulation that varies by around 20 km per wavelength. These unusual features may have been caused by an impact, a gravitational perturbation, or a transfer of material from another ring. The data gathered from this discovery provides new opportunities to study Saturn's D ring in further detail. The investigation of this unique phenomenon may also shed light on the complex processes that form the rings of gas giant planets in our solar system and beyond. 1 into PostgreSQL...\n",
      "Inserting test sample 1263  It is known that every surface-link, which is a closed surface embedded in $4$-space, can be described as the closure of a $2$-dimensional braid providing it is orientable. In this paper, we introduce a new method of describing a surface-link using a braided surface, which we call a plat form. We prove that every surface-link, not necessarily orientable, can be described in a plat form. The plat index for a surface-link is defined. In classical knot theory, the plat index of a link coincides with the bridge index. The plat index of a surface-link we introduce here is an analogy of them. 0 into PostgreSQL...\n",
      "Inserting test sample 1264  This paper presents a novel plat form for creating surface-links. Surface-links are an important class of links that allow objects to move and interact with surfaces. However, existing methods for creating surface-links are often complex and difficult to implement. Our plat form simplifies the process of creating surface-links by providing a modular framework that allows developers to easily customize and combine different linking techniques. We demonstrate the effectiveness of our plat form through experiments on various surfaces and objects. These experiments show that our plat form is capable of creating stable and efficient surface-links, and has the potential to facilitate the design of a wide range of interactive systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1265  Catastrophic forgetting is a notorious issue in deep learning, referring to the fact that Deep Neural Networks (DNN) could forget the knowledge about earlier tasks when learning new tasks. To address this issue, continual learning has been developed to learn new tasks sequentially and perform knowledge transfer from the old tasks to the new ones without forgetting. While recent structure-based learning methods show the capability of alleviating the forgetting problem, these methods start from a redundant full-size network and require a complex learning process to gradually grow-and-prune or search the network structure for each task, which is inefficient. To address this problem and enable efficient network expansion for new tasks, we first develop a learnable sparse growth method eliminating the additional pruning/searching step in previous structure-based methods. Building on this learnable sparse growth method, we then propose GROWN, a novel end-to-end continual learning framework to dynamically grow the model only when necessary. Different from all previous structure-based methods, GROWN starts from a small seed network, instead of a full-sized one. We validate GROWN on multiple datasets against state-of-the-art methods, which shows superior performance in both accuracy and model size. For example, we achieve 1.0\\% accuracy gain on average compared to the current SOTA results on CIFAR-100 Superclass 20 tasks setting. 0 into PostgreSQL...\n",
      "Inserting test sample 1266  This paper introduces a new training approach, called GROWN, for machine learning models. GROWN stands for \"GRow Only When Necessary\" and is designed to promote continual learning, where a model can adapt to new data without forgetting previously learned information. \n",
      "\n",
      "Traditional machine learning methods often require large amounts of data to be retrained from scratch to incorporate new information, which can be computationally expensive and time-consuming. GROWN, on the other hand, dynamically grows the model capacity only when necessary, allowing the model to capture new features and patterns while maintaining its previous knowledge.\n",
      "\n",
      "We propose an adaptive threshold to determine when the network should grow and a novel node insertion algorithm to efficiently increase network capacity. We also introduce a novel memory management approach to prevent catastrophic forgetting, ensuring that previous knowledge is not lost as new data is added.\n",
      "\n",
      "To evaluate the effectiveness of GROWN, we conduct experiments on various datasets, demonstrating its ability to achieve state-of-the-art performance while requiring much less computation compared to other continual learning methods. Our results suggest that GROWN is a promising approach for training machine learning models that can adapt to continuously changing environments. 1 into PostgreSQL...\n",
      "Inserting test sample 1267  When spreadsheets are filled freely by knowledge workers, they can contain rather unstructured content. For humans and especially machines it becomes difficult to interpret such data properly. Therefore, spreadsheets are often converted to a more explicit, formal and structured form, for example, to a knowledge graph. However, if a data maintenance strategy has been missing and user-generated data becomes \"messy\", the construction of knowledge graphs will be a challenging task. In this paper, we catalog several of those challenges and propose an interactive approach to solve them. Our approach includes a graphical user interface which enables knowledge engineers to bulk-annotate spreadsheet cells with extracted information. Based on the cells' annotations a knowledge graph is ultimately formed. Using five spreadsheets from an industrial scenario, we built a 25k-triple graph during our evaluation. We compared our method with the state-of-the-art RDF Mapping Language (RML) attempt. The comparison highlights contributions of our approach. 0 into PostgreSQL...\n",
      "Inserting test sample 1268  This research paper presents a novel method for constructing knowledge graphs from user-generated spreadsheets. Our approach utilizes an interactive process that leverages natural language processing techniques to identify the relationships and mappings between data elements in the spreadsheet. The focus of this work is on addressing the challenges posed by the messy and unstructured nature of user-generated data. Our method not only improves the quality of the knowledge graph but also the user experience by providing a visual interface for users to interact with. Additionally, we describe a case study in which our approach is used to construct a knowledge graph for a real-world dataset. Our evaluation demonstrates the effectiveness of our approach in extracting and organizing information from spreadsheets with a high degree of accuracy. This work has significant implications for information management and knowledge discovery in diverse fields such as business intelligence, healthcare, and education. 1 into PostgreSQL...\n",
      "Inserting test sample 1269  Automatic microblog hashtag generation can help us better and faster understand or process the critical content of microblog posts.\n",
      "\n",
      "Conventional sequence-to-sequence generation methods can produce phrase-level hashtags and have achieved remarkable performance on this task. However, they are incapable of filtering out secondary information and not good at capturing the discontinuous semantics among crucial tokens.\n",
      "\n",
      "A hashtag is formed by tokens or phrases that may originate from various fragmentary segments of the original text.\n",
      "\n",
      "In this work, we propose an end-to-end Transformer-based generation model which consists of three phases: encoding, segments-selection, and decoding. The model transforms discontinuous semantic segments from the source text into a sequence of hashtags.\n",
      "\n",
      "Specifically, we introduce a novel Segments Selection Mechanism (SSM) for Transformer to obtain segmental representations tailored to phrase-level hashtag generation.\n",
      "\n",
      "Besides, we introduce two large-scale hashtag generation datasets, which are newly collected from Chinese Weibo and English Twitter.\n",
      "\n",
      "Extensive evaluations on the two datasets reveal our approach's superiority with significant improvements to extraction and generation baselines. The code and datasets are available at \\url{https://github.com/OpenSUM/HashtagGen}. 0 into PostgreSQL...\n",
      "Inserting test sample 1270  In the era of social media, hashtags play a significant role in content categorization and retrieval. Automatically generating relevant hashtags for microblog posts has become an increasing research interest. In this paper, we propose a novel segment attention based selection mechanism for microblog hashtag generation, called Attend and Select. Specifically, we first segment the microblog into multiple meaningful segments and extract segment features. Then, we leverage an attention mechanism to assign importance weights to different segments. Finally, we employ a selection mechanism to choose representative segments and generate hashtags accordingly. Experimental results demonstrate that our method significantly outperforms existing state-of-the-art approaches in terms of both accuracy and diversity. Additionally, we conduct in-depth analysis and visualize the attention distributions to interpret the internal mechanisms of our model. To the best of our knowledge, this is the first work that introduces a segment attention based selection mechanism for microblog hashtag generation. 1 into PostgreSQL...\n",
      "Inserting test sample 1271  We show that most gravitational lenses lie on the passively evolving fundamental plane for early-type galaxies. For burst star formation models (1 Gyr of star formation, then quiescence) in low Omega_0 cosmologies, the stellar populations of the lens galaxies must have formed at z_f > 2. Typical lens galaxies contain modest amounts of patchy extinction, with a median differential extinction for the optical (radio) selected lenses of E(B-V) = 0.04 (0.07) mag. The dust can be used to determine both extinction laws and lens redshifts. For example, the z_l=0.96 elliptical lens in MG0414+0534 has an R_V=1.7 +/- 0.1 mean extinction law. Arc and ring images of the quasar and AGN source host galaxies are commonly seen in NICMOS H band observations. The hosts are typically blue, L < L_* galaxies. 0 into PostgreSQL...\n",
      "Inserting test sample 1272  The Cosmic Lens All-Sky Survey (CASTLES) is a comprehensive survey of gravitational lenses in the sky. These lenses are formed when the light from a distant source is bent by the gravitational pull of an intervening object. The CASTLES survey has identified numerous gravitational lenses in the sky and has provided valuable insights into the nature of dark matter and the distribution of matter in the universe. In this paper, we present the results from the CASTLES survey, including the properties of the lensing objects, the sources, and the lensed images. We discuss the implications of these results for cosmology and astrophysics, and we explore the potential of gravitational lenses as probes of the universe's structure and evolution. Our findings demonstrate that gravitational lenses are powerful tools for studying the universe and understanding the fundamental laws of physics. 1 into PostgreSQL...\n",
      "Inserting test sample 1273  We show a method resulting in the improvement of several polynomial-space, exponential-time algorithms.\n",
      "\n",
      "An instance of the problem Max (r,2)-CSP, or simply Max 2-CSP, is parametrized by the domain size r (often 2), the number of variables n (vertices in the constraint graph G), and the number of constraints m (edges in G). When G is cubic, and omitting sub-exponential terms here for clarity, we give an algorithm running in time r^((1/5)n) = r^((2/15)m); the previous best was r^((1/4)n) = r^((1/6)m). By known results, this improvement for the cubic case results in an algorithm running in time r^((9/50)m) for general instances; the previous best was r^((19/100)m). We show that the analysis of the earlier algorithm was tight: our improvement is in the algorithm, not just the analysis. The new algorithm, like the old, extends to Polynomial and Ring CSP.\n",
      "\n",
      "We also give faster algorithms for #Dominating Set, counting the dominating sets of every cardinality 0,...,n for a graph G of order n. For cubic graphs, our algorithm runs in time 3^((1/6)n); the previous best was 2^((1/2)n). For general graphs, we give an unrelated algorithm running in time 1.5183^n; the previous best was 1.5673^n.\n",
      "\n",
      "The previous best algorithms for these problems all used local transformations and were analyzed by the \"Measure and Conquer\" method. Our new algorithms capitalize on the existence of small balanced separators for cubic graphs - a non-local property - and the ability to tailor the local algorithms always to \"pivot\" on a vertex in the separator. The new algorithms perform much as the old ones until the separator is empty, at which point they gain because the remaining vertices are split into two independent problem instances that can be solved recursively. It is likely that such algorithms can be effective for other problems too, and we present their design and analysis in a general framework. 0 into PostgreSQL...\n",
      "Inserting test sample 1274  This paper presents improved algorithms for two important problems in theoretical computer science: Max 2-CSP and Counting Dominating Sets. Max 2-CSP, a variant of the Constraint Satisfaction Problem (CSP), involves finding the maximum number of constraints that can be satisfied in a given set of equations. Counting Dominating Sets is a combinatorial optimization problem that requires counting the number of dominating sets in a graph. Both problems are known to be NP-hard, which means that they are computationally difficult to solve. \n",
      "\n",
      "In this work, we present novel techniques that separately handle each constraint in Max 2-CSP, significantly reducing the time complexity of the problem. Our algorithm is based on a divide-and-conquer approach that partitions the equations and solves them separately. We demonstrate experimentally that our algorithm outperforms previous approaches and achieves state-of-the-art results.\n",
      "\n",
      "For Counting Dominating Sets, we use an iterative algorithm that exploits the properties of the graph to count the number of dominating sets. Our approach iteratively removes vertices from the graph until there are no dominating sets left. We prove that the algorithm is efficient and provide experimental evidence that it outperforms previous algorithms.\n",
      "\n",
      "Overall, our proposed algorithms achieve faster runtimes in both Max 2-CSP and Counting Dominating Sets, and provide improved solutions for these challenging optimization problems. In addition, our techniques have the potential to be adapted to other CSP and graph-based problems, opening up new avenues for research in theoretical computer science. 1 into PostgreSQL...\n",
      "Inserting test sample 1275  We present continuum high resolution Submillimeter Array (SMA) observations of the transition disk object RX J1633.9-2442, which is located in the Ophiuchus molecular cloud and has recently been identified as a likely site of ongoing giant planet formation. The observations were taken at 340 GHz (880 micron) with the SMA in its most extended configuration, resulting in an angular resolution of 0.3\" (35 AU at the distance of the target). We find that the disk is highly inclined (i ~50 deg) and has an inner cavity ~25 AU in radius, which is clearly resolved by our observations. We simultaneously model the entire optical to millimeter wavelength spectral energy distribution (SED) and SMA visibilities of RX J1633.9-2442 in order to constrain the structure of its disk. We find that an empty cavity ~25 AU in radius is inconsistent with the excess emission observed at 12, 22, and 24 micron. Instead, the mid-IR excess can be modeled by either a narrow, optically thick ring at ~10 AU or an optically thin region extending from ~7 AU to ~25 AU. The inner disk (r < 5 AU) is mostly depleted of small dust grains as attested by the lack of detectable near-IR excess. We also present deep Keck aperture masking observations in the near-IR, which rule out the presence of a companion up to 500 times fainter than the primary star (in K-band) for projected separations in the 5-20 AU range. We argue that the complex structure of the RX J1633.9-2442 disk is best explained by multiple planets embedded within the disk. We also suggest that the properties and incidence of objects such as RX J1633.9-2442, T Cha, and LkCa 15 (and those of the companions recently identified to these two latter objects) are most consistent with the runaway gas accretion phase of the core accretion model, when giant planets gain their envelopes and suddenly become massive enough to open wide gaps in the disk. 0 into PostgreSQL...\n",
      "Inserting test sample 1276  The RX J1633.9-2442 Transition Disk has long been a subject of interest among astronomers and astrophysicists, and Submillimeter Array (SMA) observations reveals an exciting opportunity to shed new light on the dynamics of this transitional system. This study examines the gas and dust content of the RX J1633.9-2442 using SMA observations with an aim to explore the planet formation process. The SMA is a cutting-edge interferometer that provides high angular resolution observations of the spectral lines, making it an invaluable tool in mapping out the physical characteristics of the disk. \n",
      "\n",
      "Our study identified several substructures and features in the SMA observations, which indicate the presence of significant planet formation activities within the disk. Among the most compelling features, two pressure bumps are found around 40 and 70 astronomical units from the star with an azimuthal separation of 180Â°, which may suggest the presence of two forming planets. These structures are identified by combining SMA images at different wavelengths, suggesting central gas depletion, and enabling us to map the dust and gas density distributions.\n",
      "\n",
      "The SMA spectral line observations also show that the 13CO emission arises mainly from the outer disk regions, while the C18O emission stems from the disk midplane, suggesting that the molecular gas has decoupled and settled towards the midplane. We also find evidence of gaps and asymmetries in the dust and gas distributions of the disk, which further confirm the ongoing planet formation. \n",
      "\n",
      "In conclusion, our SMA observations of RX J1633.9-2442 Transition Disk provide compelling evidence for ongoing planet formation. We identified substructures and features in the SMA images that suggest the presence of planet formation around 40 and 70 astronomical units from the star, confirming the long-held theory that planets form in circumstellar disks. Our results further show the importance of combining high angular resolution and spectral mapping techniques for a comprehensive study of the disk's physical properties. Future observations of this system using the Atacama Large Millimeter/submillimeter Array (ALMA) and other high-resolution telescopes will help unravel the mysteries of planet formation and disk evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 1277  This lecture discusses the physics implemented by Monte Carlo event generators for hadron colliders. It details the construction of parton showers and the matching of parton showers to fixed-order calculations at higher orders in perturbative QCD. It also discusses approaches to merge calculations for a varying number of jets, the interface to the underlying event and hadronization. 0 into PostgreSQL...\n",
      "Inserting test sample 1278  Parton-shower event generators are widely used in high-energy physics to simulate the production and evolution of colliding particles. These generators employ iterative procedures in order to construct the showering process, which allows for a realistic approximation of the fragmentation and radiation of partons. In this paper, we present an introduction to the fundamentals of parton-shower event generators and their applications in contemporary particle physics research. 1 into PostgreSQL...\n",
      "Inserting test sample 1279  Biological networks have two modes. The first mode is static: a network is a passage on which something flows. The second mode is dynamic: a network is a pattern constructed by gluing functions of entities constituting the network.\n",
      "\n",
      "In this paper, first we discuss that these two modes can be associated with the category theoretic duality (adjunction) and derive a natural network structure (a path notion) for each mode by appealing to the category theoretic universality. The path notion corresponding to the static mode is just the usual directed path. The path notion for the dynamic mode is called lateral path which is the alternating path considered on the set of arcs. Their general functionalities in a network are transport and coherence, respectively. Second, we introduce a betweenness centrality of arcs for each mode and see how the two modes are embedded in various real biological network data. We find that there is a trade-off relationship between the two centralities: if the value of one is large then the value of the other is small. This can be seen as a kind of division of labor in a network into transport on the network and coherence of the network. Finally, we propose an optimization model of networks based on a quality function involving intensities of the two modes in order to see how networks with the above trade-off relationship can emerge through evolution. We show that the trade-off relationship can be observed in the evolved networks only when the dynamic mode is dominant in the quality function by numerical simulations. We also show that the evolved networks have features qualitatively similar to real biological networks by standard complex network analysis. 0 into PostgreSQL...\n",
      "Inserting test sample 1280  The study of complex systems has increasingly become inter-disciplinary, with researchers and scientists across different sub-fields pooling their concepts to better understand their respective areas of study. In this regard, the interface between category theory and directed networks has revealed new insights into the evolution of biological networks. In this paper, we present a study on how category theory and directed networks can be applied to the understanding of the evolution of biological networks. Specifically, we explore how directed networks can be modeled as categories and how these categories can be used to analyze the evolution of biological networks. We also discuss the implications of our findings for our understanding of biological systems.\n",
      "\n",
      "Our results show that directed networks can provide a useful framework for understanding the evolution of biological networks. By modeling these networks as categories, we are able to more easily define and understand the relationships between different elements of these systems. Our research also reveals that the structure of these networks play a significant role in the evolution of biological systems. We observed that certain types of networks tend to undergo more drastic changes than others during evolution, and we propose that this may be due to a combination of factors including the nature of interactions within the network and the selective pressures acting on the system.\n",
      "\n",
      "Overall, our study highlights the potential of using category theory and directed networks to investigate the evolution of biological networks. We believe that this approach can lead to new insights into the structure and function of these complex systems. Our findings may also have implications for other areas of research, such as the study of social systems and technological networks. 1 into PostgreSQL...\n",
      "Inserting test sample 1281  We report a measurement on the temporal response of a plasmonic antenna at the femtosecond time scale. The antenna consists of a square array of nanometer-size gold rods. We find that the far-field dispersion of light reflected from the plasmonic antenna is less than that of a 1.2 mm thick glass slide. Assuming a simple oscillating dipole model this implies that the near-field of the antenna may be used as an electron switch that responds faster than 20 fs. Alternatively, ultrafast electron diffraction may be used to investigate the near-field dynamics of the plasmonic antenna. 0 into PostgreSQL...\n",
      "Inserting test sample 1282  This paper presents an investigation of the ultrafast temporal response of a plasmonic antenna using time-resolved experiments. The plasmonic antenna was excited by a femtosecond laser pulse, and the response was measured using a time-delayed probe pulse. By varying the length of the probe pulse, the spectral and temporal response of the plasmonic antenna was examined. The results show that the plasmonic antenna exhibits a fast response time on the femtosecond timescale, making it a promising candidate for applications in ultrafast optics and optoelectronics. These findings contribute to the understanding of plasmonic antennas and their potential uses. 1 into PostgreSQL...\n",
      "Inserting test sample 1283  Automatic modulation classification (AMC) has been studied for more than a quarter of a century; however, it has been difficult to design a classifier that operates successfully under changing multipath fading conditions and other impairments. Recently, deep learning (DL)-based methods are adopted by AMC systems and major improvements are reported. In this paper, a novel convolutional neural network (CNN) classifier model is proposed to classify modulation classes in terms of their families, i.e., types. The proposed classifier is robust against realistic wireless channel impairments and in relation to that when the data sets that are utilized for testing and evaluating the proposed methods are considered, it is seen that RadioML2016.10a is the main dataset utilized for testing and evaluation of the proposed methods. However, the channel effects incorporated in this dataset and some others may lack the appropriate modeling of the real-world conditions since it only considers two distributions for channel models for a single tap configuration. Therefore, in this paper, a more comprehensive dataset, named as HisarMod2019.1, is also introduced, considering real-life applicability.\n",
      "\n",
      "HisarMod2019.1 includes 26 modulation classes passing through the channels with 5 different fading types and several numbers of taps for classification. It is shown that the proposed model performs better than the existing models in terms of both accuracy and training time under more realistic conditions. Even more, surpassed their performance when the RadioML2016.10a dataset is utilized. 0 into PostgreSQL...\n",
      "Inserting test sample 1284  Automatic Modulation Classification (AMC) is an essential step in modern wireless communication systems, especially when deploying heterogeneous communication environments with multiple modulation schemes. This paper proposes a robust and fast AMC method based on Convolutional Neural Network (CNN) modeling under multipath fading channels. The proposed method capitalizes on the strengths of the CNN model and offers improved performance compared to traditional AMC methods. Specifically, the proposed approach is designed to solve the problem of accurate modulation classification in the presence of channel impairments due to multipath fading, which is a common occurrence in wireless communication system. The proposed method's performance is evaluated using a well-known wireless communication dataset of different modulation schemes corrupted by fading channels. The results show that the proposed CNN-based method achieves superior classification accuracy compared to several state-of-the-art techniques. Additionally, the proposed solution achieves faster convergence during the training process, reducing the computational time required for classification. This work contributes to the development of robust and fast AMC methods that can potentially have applications in cognitive radio and radar systems, where accurate modulation classification is vital. 1 into PostgreSQL...\n",
      "Inserting test sample 1285  The tangent dynamics of the Lyapunov modes and their dynamics as generated numerically - {\\it the numerical dynamics} - is considered. We present a new phenomenological description of the numerical dynamical structure that accurately reproduces the experimental data for the quasi-one-dimensional hard-disk system, and shows that the Lyapunov mode numerical dynamics is linear and separate from the rest of the tangent space. Moreover, we propose a new, detailed structure for the Lyapunov mode tangent dynamics, which implies that the Lyapunov modes have well-defined (in)stability in either direction of time.\n",
      "\n",
      "We test this tangent dynamics and its derivative properties numerically with partial success. The phenomenological description involves a time-modal linear combination of all other Lyapunov modes on the same polarization branch and our proposed Lyapunov mode tangent dynamics is based upon the form of the tangent dynamics for the zero modes. 0 into PostgreSQL...\n",
      "Inserting test sample 1286  The study of Lyapunov mode dynamics in hard-disk systems has been an area of active research in recent years. This paper presents a thorough analysis of Lyapunov modes, which are collective excitations of the system, and their behavior in hard-disk systems. We have investigated the evolution of Lyapunov modes in time and space, and their dependence on system parameters such as density and temperature. Our results show that Lyapunov modes play an important role in the dynamics of hard-disk systems, particularly in the early stages of relaxation. Furthermore, we have observed the presence of so-called \"persistent modes\" which persist long after the initial excitation has died down. This study provides valuable insights into the behavior of hard-disk systems and may have applications in fields such as materials science and computer simulation. 1 into PostgreSQL...\n",
      "Inserting test sample 1287  The Landauer approach to diffusive transport is mathematically related to the solution of the Boltzmann transport equation, and expressions for the thermoelectric parameters in both formalisms are presented. Quantum mechanical and semiclassical techniques to obtain from a full description of the bandstructure, E(k), the number of conducting channels in the Landauer approach or the transport distribution in the Boltzmann solution are developed and compared. Thermoelectric transport coefficients are evaluated from an atomistic level, full band description of a crystal. Several example calculations for representative bulk materials are presented, and the full band results are related to the more common effective mass formalism. Finally, given a full E(k) for a crystal, a procedure to extract an accurate, effective mass level description is presented. 0 into PostgreSQL...\n",
      "Inserting test sample 1288  This research paper presents a comparison study between the Landauer and Boltzmann approaches for evaluating thermoelectric transport coefficients. In particular, we investigate the impact of full band and effective mass approximations for calculating these transport properties. We demonstrate that the full band approach provides a more accurate description of transport behavior in certain materials, particularly those with complex electronic structures. However, we also find that the effective mass approximation remains a useful tool for estimating transport coefficients in simpler materials or those with weaker anharmonic scattering. Overall, our results provide important insights into the limitations and strengths of these two commonly used models for evaluating thermoelectric transport behavior, which could inform development of more efficient and effective thermoelectric materials. 1 into PostgreSQL...\n",
      "Inserting test sample 1289  This article is concerned with the fluctuations and the concentration properties of a general class of discrete generation and mean field particle interpretations of nonlinear measure valued processes. We combine an original stochastic perturbation analysis with a concentration analysis for triangular arrays of conditionally independent random sequences, which may be of independent interest. Under some additional stability properties of the limiting measure valued processes, uniform concentration properties, with respect to the time parameter, are also derived. The concentration inequalities presented here generalize the classical Hoeffding, Bernstein and Bennett inequalities for independent random sequences to interacting particle systems, yielding very new results for this class of models. We illustrate these results in the context of McKean-Vlasov-type diffusion models, McKean collision-type models of gases and of a class of Feynman-Kac distribution flows arising in stochastic engineering sciences and in molecular chemistry. 0 into PostgreSQL...\n",
      "Inserting test sample 1290  This paper presents concentration inequalities for mean field particle models. Mean field models provide a way to approximate the behavior of a complex system by considering the collective behavior of its individual components. Such models are widely used in statistical physics, machine learning, and neuroscience, among other fields. In this work, we provide analytical tools to estimate the deviations of the empirical measures obtained from the mean field particle models from their theoretical counterparts. Specifically, we derive exponential tail bounds on the deviations, which provide a quantitative measure of the concentration of the empirical measures. We demonstrate the usefulness of the derived concentration inequalities on several examples, including a mean field interacting particle system and a neural network model. Our results pave the way for more precise and robust analysis of mean field approximations and their applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1291  There are many factors that can influence the outcome for students in a mathematics PhD program: bachelor's GPA (BGPA), bachelor's major, GRE scores, gender, Under-Represented Minority (URM) status, institution tier, etc. Are these variables equally important predictors of a student's likelihood of succeeding in a math PhD program? In this paper, we present and analyze admission data of students from different groups entering a math PhD program at a southern California university. We observe that some factors correlate with success in the PhD program (defined as obtaining a PhD degree within a time-limit). According to our analysis, GRE scores correlate with success, but interestingly, the verbal part of the GRE score has a higher predictive power compared to the quantitative part. Further, we observe that undergraduate student GPA does not correlate with success (there is even a slight negative slope in the relationship between GPA and the probability of success). This counterintuitive observation is explained once undergraduate institutions are separated by tiers: students from \"higher tiers\" have undergone a more rigorous training program; they on average have a slightly lower GPA but run a slightly higher probability to succeed. Finally, a gender gap is observed in the probability to succeed with female students having a lower probability to finish with a PhD despite the same undergraduate performance, compared to males. This gap is reversed if we only consider foreign graduate students. It is our hope that this study will encourage other universities to perform similar analyses, in order to design better admission and retention strategies for Math PhD programs. 0 into PostgreSQL...\n",
      "Inserting test sample 1292  This study aimed to identify admission predictors for success in a mathematics graduate program. A sample of incoming students in a large public university's mathematics graduate program was followed over a period of three years. Data was collected on students' undergraduate academic records, standardized test scores, and demographic information. Success in the program was determined by grade point average (GPA) and completion rates. Logistic regression analysis was used to identify predictors of success. Results showed that undergraduate GPA was the strongest predictor of success in the program, followed by scores on the Graduate Record Examination (GRE) Mathematics Subject Test. Gender and race were not significant predictors of success, nor were other indicators such as age, citizenship status, and previous work experience. These findings suggest that admissions committees should place greater weight on undergraduate GPA and GRE Mathematics Subject Test scores when evaluating applicants for mathematics graduate programs. Additionally, these findings suggest that efforts to increase diversity in mathematics graduate programs should focus on outreach and recruitment efforts to underrepresented groups, rather than changes to admissions criteria. Overall, this study provides valuable insights into the factors that contribute to success in mathematics graduate programs and can inform admissions decisions and program policies. 1 into PostgreSQL...\n",
      "Inserting test sample 1293  The literature on \"mechanism design from samples,\" which has flourished in recent years at the interface of economics and computer science, offers a bridge between the classic computer-science approach of worst-case analysis (corresponding to \"no samples\") and the classic economic approach of average-case analysis for a given Bayesian prior (conceptually corresponding to the number of samples tending to infinity). Nonetheless, the two directions studied so far are two extreme and almost diametrically opposed directions: that of asymptotic results where the number of samples grows large, and that where only a single sample is available. In this paper, we take a first step toward understanding the middle ground that bridges these two approaches: that of a fixed number of samples greater than one. In a variety of contexts, we ask what is possibly the most fundamental question in this direction: \"are two samples really better than one sample?\". We present a few surprising negative results, and complement them with our main result: showing that the worst-case, over all regular distributions, expected-revenue guarantee of the Empirical Revenue Maximization algorithm given two samples is greater than that of this algorithm given one sample. The proof is technically challenging, and provides the first result that shows that some deterministic mechanism constructed using two samples can guarantee more than one half of the optimal revenue. 0 into PostgreSQL...\n",
      "Inserting test sample 1294  This paper examines the problem of empirical revenue maximization in the context of two samples, and inquires whether using two samples leads to better performance compared to using just one sample. We focus on non-asymptotic analyses that consider finite data samples, which capture the effects of sampling errors and can be relevant in practical settings. We provide a comprehensive theoretical analysis of the sample complexity and estimation error bounds of empirical maximization under a general class of revenue functions, which includes a number of widely used functions such as the generalized second-price and Vickrey-Clarke-Groves auctions. Our results reveal that the benefit of using two samples instead of one can be situation-dependent, and essentially hinges on the degree of dependence and correlation between the two samples. In particular, we find that in many cases using two independent samples could potentially lead to more accurate estimation and better revenue performance guarantees than using a single sample. We verify our results through simulations and experiments on real-world datasets. Our findings highlight the importance of considering the non-asymptotic performance of empirical revenue maximization in practice, and can offer guidance to practitioners on when and how to use two samples for better performance. 1 into PostgreSQL...\n",
      "Inserting test sample 1295  Events with one lepton, one photon and missing energy are the subject of recent searches at the Fermilab Tevatron. We compute possible contributions to these type of events from the process p pbar --> photon l nu_l nu_tau nubar_tau, where l=e,mu in the context of a Low Scale Technicolor Model. We find that with somewhat tighter cuts than the ones used in the CDF search, it could be possible to either confirm or exclude this model in a small region of its parameter space. 0 into PostgreSQL...\n",
      "Inserting test sample 1296  We study the Technicolor (TC) contribution to lepton plus photon plus missing energy final states at the Tevatron in light of the recent CDF and D0 results. We find that TC models predict much smaller cross sections in this channel in comparison with the standard model. However, a deviation from the standard model could appear in the tails of kinematic distributions, such as the photon transverse momentum and the di-lepton invariant mass, which should be scrutinized in current and future Tevatron data. 1 into PostgreSQL...\n",
      "Inserting test sample 1297  We report on the redshift of the BL Lac object 3FGL J0909.0+2310 based on observations obtained with the OSIRIS Multi Object Spectrograph (MOS) mounted on the 10.4-m Gran Telescopio Canarias. A redshift of 0.432+/-0.002 was obtained by the identification of three absorption features (CaII K&H and G-band) detected in the spectrum of the BL Lac host galaxy. The closest object to the BL Lac at an angular separation of 3.8\" (~21 kpc at this distance) has a similar redshift of 0.431+/-0.002. This companion galaxy could be the most likely cause of the nuclear activity as postulated by studies based on more extended data sets and cosmological models. MOS allows us to study the object's neighbourhood within a field of view of approximately 7'x2' and we find two small groups of galaxies at redshifts 0.28 and 0.39 which are probably not related to the activity of 3FGL~J0909.0+2310. 0 into PostgreSQL...\n",
      "Inserting test sample 1298  The BL Lac 3FGL J0909.0+2310 is a blazar located at a redshift of 0.902 and known for its intense and variable electromagnetic emission. In this paper, we report on the redshift measurements of the BL Lac and its close companion, as well as the identification of their host galaxies through high-resolution imaging and spectroscopy techniques. Our findings reveal that the two objects are gravitationally bound and likely interact with each other. We also investigate the X-ray and gamma-ray properties of the system, providing evidence of an enhanced emission region close to the companion object. Our results suggest that the BL Lac system is a valuable astrophysical laboratory to investigate how gravitational interactions affect the properties of active galactic nuclei. Finally, we discuss the implications of our findings for the study of high-energy astrophysics and the formation history of massive objects in the Universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1299  Relying upon the equivalence between a gauge theory for the translation group and general relativity, a teleparallel version of the original Kaluza-Klein theory is developed. In this model, only the internal space (fiber) turns out to be five dimensional, spacetime being kept always four dimensional. A five-dimensional translational gauge theory is obtained which unifies, in the sense of Kaluza-Klein theories, gravitational and electromagnetic interactions. 0 into PostgreSQL...\n",
      "Inserting test sample 1300  In this work, we investigate the teleparallel equivalent of the Kaluza-Klein theory. We begin by reviewing the mathematical formalism of both theories and then establishing a correspondence between them. We analyze the geometrical and physical implications of this correspondence, and we demonstrate that the teleparallel theory can be used to describe the same phenomena described by the Kaluza-Klein theory, while providing a simpler and more natural framework. 1 into PostgreSQL...\n",
      "Inserting test sample 1301  Strangelets with non-zero entropy are studied within the MIT bag model.\n",
      "\n",
      "Explicit account is taken of the constraints that strangelets must be color neutral and have a fixed total momentum. In general, masses increase with increasing entropy per baryon, and the constraints work so as to increase masses further. This has an important destabilizing effect on strangelets produced in ultrarelativistic heavy ion collisions. 0 into PostgreSQL...\n",
      "Inserting test sample 1302  This paper investigates the properties of strangelets with finite entropy using the framework of statistical mechanics and thermodynamics. We show that these strangelets exhibit interesting behaviors due to their non-trivial topology, including a phase transition between different stable states. By analyzing the thermodynamic potentials and the microstructure of these strangelets, we provide insights into their fundamental properties. Our results contribute to a better understanding of the physics of exotic matter. 1 into PostgreSQL...\n",
      "Inserting test sample 1303  Using the XMM-Newton Slew Survey, we construct a hard-band selected sample of low-luminosity Galactic X-ray sources. Two source populations are represented, namely coronally-active stars and binaries (ASBs) and cataclysmic variables (CVs), with X-ray luminosities collectively spanning the range 10^(28-34) erg/s (2-10 keV). We derive the 2-10 keV X-ray luminosity function (XLF) and volume emissivity of each population. Scaled to the local stellar mass density, the latter is found to be 1.08 +/- 0.16 x 10^28 erg/s/M and 2.5 +/- 0.6 x 10^27 erg/s/M, for the ASBs and CVs respectively, which in total is a factor 2 higher than previous estimates. We employ the new XLFs to predict the X-ray source counts on the Galactic plane at l = 28.5 deg and show that the result is consistent with current observational constraints. The X-ray emission of faint, unresolved ASBs and CVs can account for a substantial fraction of the Galactic ridge X-ray emission (GRXE). We discuss a model in which roughly 80 per cent of the 6-10 keV GRXE intensity is produced in this way, with the remainder attributable to X-ray scattering in the interstellar medium and/or young Galactic source populations. Much of the hard X-ray emission attributed to the ASBs is likely to be produced during flaring episodes. 0 into PostgreSQL...\n",
      "Inserting test sample 1304  The Galactic ridge X-ray emission (GRXE) constitutes a diffuse X-ray radiation that permeates the disk of our galaxy. Low-luminosity X-ray sources, such as cataclysmic variables, pre-main-sequence stars, and active binaries, are believed to be the primary contributors to the GRXE. In this study, we carefully examine the relationship between low-luminosity X-ray sources and the GRXE.\n",
      "\n",
      "We use data obtained from the Chandra X-ray Observatory and the XMM-Newton observatory to identify and analyze low-luminosity X-ray sources in the Galactic ridge region. Our results show that these sources are indeed correlated with GRXE, and suggest that they contribute significantly to its emission.\n",
      "\n",
      "We also investigate the spectral properties of the low-luminosity X-ray sources to better understand the physical processes that govern the production of the GRXE. Our analysis reveals strong correlations between the spectral properties of the sources and their luminosities, which provides important insights into the mechanisms responsible for the production and propagation of the GRXE.\n",
      "\n",
      "Overall, our findings demonstrate the importance of taking into account low-luminosity X-ray sources in models of the GRXE. Our results provide a valuable contribution to the understanding of both the sources of X-ray radiation in our galaxy and the properties of the Galactic ridge X-ray emission itself. 1 into PostgreSQL...\n",
      "Inserting test sample 1305  Results of Fe K-, As K-, and La L3-edge x-ray absorption near edge structure (XANES) measurements on LaO1-xFxFeAs compounds are presented. The Fe K- edge exhibits a chemical shift to lower energy, near edge feature modifications, and pre-edge feature suppression as a result of F substitution for O. The former two changes provide evidence of electron charge transfer to the Fe sites and the latter directly supports the delivery of this charge into the Fe-3d orbitals. The As K- edge measurements show spectral structures typical of compounds with planes of transition-metal tetrahedrally coordinated to p-block elements as is illustrated by comparison to other such materials. The insensitivity of the As-K edge to doping, along with the strong Fe-K doping response, is consistent with band structure calculations showing essentially pure Fe-d character near the Fermi energy in these materials. The energy of the continuum resonance feature above the La-L3 edge is shown to be quantitatively consistent with the reported La-O inter-atomic separation and with other oxide compounds containing rare earth elements. 0 into PostgreSQL...\n",
      "Inserting test sample 1306  The properties of the LaO1-xFxFeAs system have been studied using X-ray absorption spectroscopy (XAS) measurements. In this research, we report the results of our investigation into the electronic structure and local coordination environment of the system. The measurements were carried out at the Fe K-edge and F K-edge to explore the changes in the electronic and atomic structures upon doping with fluorine. The results show that there is a significant difference in the local coordination environment around the Fe atom and the electronic structure of the system upon fluorine doping. The data collected is consistent with the formation of an FeAs4 tetrahedral network, suggesting that the doping agent has a direct impact on the superconducting properties of this material. The findings presented in this study provide valuable insight into the physics of the LaO1-xFxFeAs system, which is essential for the development of high-temperature superconductors. 1 into PostgreSQL...\n",
      "Inserting test sample 1307  The influence of an external magnetic field on the static shear strain and the effective shear modulus of a magnetoactive elastomer (MAE) is studied theoretically in the framework of a recently introduced approach to the single-particle magnetostriction mechanism [V. M. Kalita et al, Phys. Rev. E 93, 062503 (2016)]. The planar problem of magnetostriction in an MAE with soft magnetic inclusions in the form of a thin disk (platelet) having the magnetic anisotropy in the plane of this disk is solved analytically. An external magnetic field acts with torques on magnetic filler particles, creates mechanical stresses in the vicinity of inclusions, induces shear strain and increases the effective shear modulus of these composite materials. It is shown that the largest effect of the magnetic field on the effective shear modulus should be expected in MAEs with soft elastomer matrices, where the shear modulus of the matrix is less than the magnetic anisotropy constant of inclusions. It is derived that the effective shear modulus is non-linearly dependent on the external magnetic field and approaches the saturation value in magnetic fields exceeding the field of particle anisotropy. It is shown that model calculations of the effective shear modulus correspond to a phenomenological definition of effective elastic moduli and magnetoelastic coupling constants. Obtained theoretical results compare well with known experimental data. Determination of effective elastic coefficients in MAEs and their dependence on magnetic field is discussed. The concentration dependence of the effective shear modulus at higher filler concentrations has been estimated using the method of Pad\\'e approximants, which correctly predicts that both the absolute and relative changes of the magnetic-field dependent effective shear modulus will significantly increase with the growing concentration of filler particles. 0 into PostgreSQL...\n",
      "Inserting test sample 1308  This research paper investigates the impact of single-particle magnetostriction on the shear modulus of compliant magnetoactive elastomers. The study aims to deepen the understanding of the behavior of these materials under magnetic fields and provide insights for their potential applications in fields such as soft robotics and biomedicine.\n",
      "\n",
      "Experimental tests were carried out on compliant magnetoactive elastomer samples with different particle sizes and volume fractions under varying magnetic fields. The results showed that single-particle magnetostriction can significantly affect the shear modulus of these elastomers. In particular, the shear modulus of the elastomer increased with increasing magnetic field strength, particle volume fraction, and decreasing particle size. Moreover, the shear modulus was found to be strongly dependent on the orientation of magnetic particles relative to the shear direction.\n",
      "\n",
      "To explain these observations, a theoretical model was developed based on the micromechanics of magnetoactive elastomers. The model was able to capture the effects of single-particle magnetostriction on the shear modulus of the material, as well as the dependence on particle size, volume fraction, and magnetic field strength and orientation. The model provides a valuable framework for understanding the complex behavior of compliant magnetoactive elastomers and their response to external magnetic fields.\n",
      "\n",
      "The findings of this study have important implications for the design and optimization of magnetoactive elastomers for practical applications. By controlling the particle size, volume fraction, and magnetic field strength and orientation, the shear modulus of these elastomers can be tuned to match specific requirements for soft robotics and biomedicine. Overall, this research provides fundamental insights into the behavior of magnetoactive elastomers and paves the way for their future development as smart and adaptive materials. 1 into PostgreSQL...\n",
      "Inserting test sample 1309  Understanding the mechanical behaviour of bones up to failure is necesary for diagnosis and prevention of accident and trauma. As far as we know, no authors have yet studied the tensile behaviour of compact bone including failure under dynamic loadings (1m/s). The originality of this study comes from not only the analysis of compact bone failure under dynamic loadings, the results of which are compared to those obtained under quasi static loadings but also the development of a statistical model. We developed a protocol using three different devices. Firstly, an X-ray scanner to analyse bone density, secondly, a common tensile device to perform quasi static experiments and thirdly, a special device based upon a hydraulic cylinder to perform dynamic tests. For all the tests, we used the same sample shape which took into account the brittleness of the compact bone. We first performed relaxation and hysteresis tests followed by tensile tests up to failure. Viscous and plastic effects were not relevant to the compact bone behaviour so its behaviour was considered elastic and brittle. The bovine compact bone was three to four times more brittle under a dynamic load than under a quasi static one. Numerically, a statistical model, based upon the Weibull theory is used to predict the failure stress in compact bone. 0 into PostgreSQL...\n",
      "Inserting test sample 1310  This study investigates the failure behavior of compact bone under two different loading rates, using both experimental and modeling approaches. The first loading rate is the quasi-static compression, while the second rate is the dynamic compression. The results indicate that the quasi-static compression leads to a localized shear deformation and produces a distinct fracture pattern, while the dynamic compression leads to a diffuse damage pattern and a more complex fracture pattern. Moreover, the stress-strain response of compact bone under quasi-static and dynamic compression has been characterized using experimental data. The proposed numerical model captures the observed biomechanical response and is validated using the experimental data. The results of this study provide insights into the understanding of the mechanisms of bone fracture under different loading conditions. This research has significant implications in the medical field, such as understanding the cause and prevention of bone fractures in aging or diseased individuals, or designing safer implants for patients with bone disorders. 1 into PostgreSQL...\n",
      "Inserting test sample 1311  In this paper, we provide a compositional approach for constructing finite abstractions (a.k.a. finite Markov decision processes (MDPs)) of interconnected discrete-time stochastic switched systems. The proposed framework is based on a notion of stochastic simulation functions, using which one can employ an abstract system as a substitution of the original one in the controller design process with guaranteed error bounds on their output trajectories. To this end, we first provide probabilistic closeness guarantees between the interconnection of stochastic switched subsystems and that of their finite abstractions via stochastic simulation functions. We then leverage sufficient small-gain type conditions to show compositionality results of this work. Afterwards, we show that under standard assumptions ensuring incremental input-to-state stability of switched systems (i.e., existence of common incremental Lyapunov functions, or multiple incremental Lyapunov functions with dwell-time), one can construct finite MDPs for the general setting of nonlinear stochastic switched systems.\n",
      "\n",
      "We also propose an approach to construct finite MDPs for a particular class of nonlinear stochastic switched systems. To demonstrate the effectiveness of our proposed results, we first apply our approaches to a road traffic network in a circular cascade ring composed of 200 cells, and construct compositionally a finite MDP of the network. We employ the constructed finite abstractions as substitutes to compositionally synthesize policies keeping the density of the traffic lower than 20 vehicles per cell. We then apply our proposed techniques to a fully interconnected network of 500 nonlinear subsystems (totally 1000 dimensions), and construct their finite MDPs with guaranteed error bounds. We compare our proposed results with those available in the literature. 0 into PostgreSQL...\n",
      "Inserting test sample 1312  The synthesis of controllers for large-scale dynamical systems, such as networks of stochastic switched systems, is a challenging problem in control theory. In practice, such systems are subject to uncertainties, noise, and limited information. A promising approach for addressing these challenges is compositional abstraction-based synthesis, which relies on constructing a system-level controller by composing local controllers for subsystems. This approach allows breaking the overall control problem into smaller, more manageable parts.\n",
      "\n",
      "In this paper, we describe a framework for compositional abstraction-based synthesis of controllers for networks of stochastic switched systems. Our framework includes methods for computing abstractions of individual subsystems, as well as for composing the abstractions into a controller for the entire network. We show that the framework is scalable and can handle large-scale systems, and that it can effectively deal with uncertainties and noise.\n",
      "\n",
      "We illustrate the effectiveness of our framework with a numerical case study involving a network of interconnected aircraft systems. We show that our compositional abstraction-based synthesis approach produces controllers that lead to improved stability and performance of the network, compared to existing controller synthesis methods. Our approach is also computationally efficient and can handle systems with complex dynamics.\n",
      "\n",
      "Overall, our work demonstrates the promise and effectiveness of compositional abstraction-based synthesis for networks of stochastic switched systems. The framework presented here can be applied to a wide range of control problems in complex, uncertain systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1313  Materials with high dielectric constant are in great demand for the miniaturization of electronic devices. More specifically, high dielectric constant polymer-ceramic composites are useful for embedded capacitor applications. A composite consisting of giant dielectric CaCu3Ti4O12 (CCTO) incorporated into the Poly(methyl methacrylate) (PMMA) polymer matrix has been fabricated by melt mixing followed by hot pressing. The composites thus fabricated has been characterised for structural, morphological and dielectric properties. The composites dielectric constants had increased when the CCTO content increased in the PMMA matrix. The dielectric constant of PMMA is around 4.9 @ 100Hz which has increased to 15.7 @ 100Hz when the ceramic content has increased to 40 Vol %. At low frequencies, space charge polarisation is dominant. This composite also exhibited remarkably low dielectric loss at high frequency, which makes this composite a suitable candidate for the capacitors in high frequency application. 0 into PostgreSQL...\n",
      "Inserting test sample 1314  This work investigates the dielectric properties of Poly(methyl methacrylate) (PMMA)/CaCu3Ti4O12 composites. The composites were prepared through a simple solution-casting method and characterized by X-ray diffraction (XRD) and scanning electron microscopy (SEM). The dielectric properties of the composites were studied using a broadband dielectric spectrometer with a frequency range of 10^-2-10^6 Hz. It was found that the dielectric constant and dielectric loss of the composites increased with increasing CaCu3Ti4O12 content. The maximum dielectric constant of 61 was obtained for the composite containing 30 wt% of CaCu3Ti4O12 at 10^6 Hz. The dielectric properties of the PMMA/CaCu3Ti4O12 composites can be attributed to the interfacial polarization and space charge polarization. These results provide useful information for the preparation and application of PMMA/CaCu3Ti4O12 composites in the field of electronic materials. 1 into PostgreSQL...\n",
      "Inserting test sample 1315  The best outer bound on the capacity region of the two-user Gaussian Interference Channel (GIC) is known to be the intersection of regions of various bounds including genie-aided outer bounds, in which a genie provides noisy input signals to the intended receiver. The Han and Kobayashi (HK) scheme provides the best known inner bound. The rate difference between the best known lower and upper bounds on the sum capacity remains as large as 1 bit per channel use especially around $g^2=P^{-1/3}$, where $P$ is the symmetric power constraint and $g$ is the symmetric real cross-channel coefficient. In this paper, we pay attention to the \\emph{moderate interference regime} where $g^2\\in (\\max(0.086, P^{-1/3}),1)$. We propose a new upper-bounding technique that utilizes noisy observation of interfering signals as genie signals and applies time sharing to the genie signals at the receivers. A conditional version of the worst additive noise lemma is also introduced to derive new capacity bounds. The resulting upper (outer) bounds on the sum capacity (capacity region) are shown to be tighter than the existing bounds in a certain range of the moderate interference regime. Using the new upper bounds and the HK lower bound, we show that $R_\\text{sym}^*=\\frac{1}{2}\\log \\big(|g|P+|g|^{-1}(P+1)\\big)$ characterizes the capacity of the symmetric real GIC to within $0.104$ bit per channel use in the moderate interference regime at any signal-to-noise ratio (SNR). We further establish a high-SNR characterization of the symmetric real GIC, where the proposed upper bound is at most $0.1$ bit far from a certain HK achievable scheme with Gaussian signaling and time sharing for $g^2\\in (0,1]$. In particular, $R_\\text{sym}^*$ is achievable at high SNR by the proposed HK scheme and turns out to be the high-SNR capacity at least at $g^2=0.25, 0.5$. 0 into PostgreSQL...\n",
      "Inserting test sample 1316  The Gaussian interference channel (GIC) is a fundamental communication system which can model various interference scenarios. One of the key objectives in the study of the GIC is to understand its capacity, which imposes the fundamental limit on the rate at which information can be reliably transmitted over the channel. In this paper, we investigate the high signal-to-noise ratio (SNR) regime of the GIC, which is of interest due to its practical importance in wireless communication systems. Specifically, we focus on the symmetric GIC scenario where both transmitters send messages to their respective receivers simultaneously.\n",
      "\n",
      "We first present a new set of capacity bounds for the GIC based on a combination of non-asymptotic information-theoretic tools and approximation techniques. These bounds improve upon the existing bounds in the literature and are shown to be tight in certain regimes. We then derive the high-SNR capacity of the symmetric GIC using a combination of scale-invariant and large deviation analysis. Our analysis reveals a surprising dichotomy in the high-SNR behavior of the symmetric GIC, depending on the strength of the interference. In particular, we show that in the presence of weak interference, the capacity scales linearly with the SNR, whereas in the presence of strong interference, the capacity is fundamentally limited by the interference power.\n",
      "\n",
      "Finally, we demonstrate the practical relevance of our results by applying them to the design of coding schemes that achieve the high-SNR capacity of the GIC. We show that our codes are capable of achieving the derived capacity bounds, and outperform existing coding schemes in the literature. Our analysis and results provide new insights into the high-SNR behavior of the GIC and advance the state-of-the-art in the capacity analysis of this classical communication system. 1 into PostgreSQL...\n",
      "Inserting test sample 1317  Clear, persuasive arguments are brought forward to motivate the need for highly precise measurements of the electron/muon orbital g, i.e. gL. First, we briefly review results obtained using an extended Dirac equation, which conclusively showed that, as a consequence of quantum relativistic corrections arising from the time-dependence of the rest-energy, the electron gyromagnetic factors are corrected. It is next demonstrated, using the data of Kusch & Foley on the measurement of deltaS minus 2 deltaL together with the modern precise measurements of the electron deltaS where deltaS identically equal to gS minus 2, that deltaL may be a small, non-zero quantity, where we have assumed Russel-Saunders LS coupling and proposed, along with Kusch and Foley, that gS = 2 plus deltaS and gS = 1 plus deltaL. Therefore, there is probable evidence from experimental data that gS is not exactly equal to 1; the expectation that quantum effects will significantly modify the classical value of the orbital g is therefore reasonable. Finally, we show that if, as suggested by the results obtained from the modified Dirac theory, deltaS and deltaL depend linearly on a dimensionless parameter DELTA such that the gyromagnetic factors are considered corrected as follows; gS = 2 plus 2 DELTA and gL = 1 minus DELTA, then the Kusch-Foley data implies that the correction DELTA approximately equals 1.0 times 10-3 . Modern, high precision measurements of the electron and muon orbital gL are therefore required, in order to properly determine by experiments the true value of gL minus 1, perhaps to about one part in a trillion as was recently done for gS minus 2. 0 into PostgreSQL...\n",
      "Inserting test sample 1318  The electron and muon exhibit a property called the gyromagnetic factor, which relates their magnetic field to their angular momentum. While the theoretical value for these factors can be calculated via quantum electrodynamics (QED), experimental verifications are often difficult due to the smallness of their values. In this paper, we report on a precision measurement of the electron and muon gyromagnetic factors using the muon-electron scattering experiment (MUSE). MUSE is a unique experiment that allows for non-destructive measurements of the magnetic moment of the muon and the electron. Our results show agreement with the QED calculations, verifying their theoretical predictions to unparalleled accuracy. We obtain a precision of 0.28 parts per million for the electron and 0.23 parts per million for the muon, the most precise measurements to date. Our experimental technique employs the principle of spin precession in a magnetic field to determine the gyromagnetic factors. By subjecting the particles to a magnetic field, we can measure their motion and thereby calculate the strength of their magnetic field. Our findings have implications for the search for new physics beyond the standard model, as inconsistencies between experimental and theoretical predictions could indicate the existence of previously unknown particles or interactions. This measurement provides a new benchmark for testing the predictions of QED and for exploring the fundamental properties of matter. 1 into PostgreSQL...\n",
      "Inserting test sample 1319  We combine the Density Matrix Technique (DMRG) with Green Function Monte Carlo (GFMC) simulations. The DMRG is most successful in 1-dimensional systems and can only be extended to 2-dimensional systems for strips of limited width.\n",
      "\n",
      "GFMC is not restricted to low dimensions but is limited by the efficiency of the sampling. This limitation is crucial when the system exhibits a so-called sign problem, which on the other hand is not a particular obstacle for the DMRG. We show how to combine the virtues of both methods by using a DMRG wavefunction as guiding wave function for the GFMC. This requires a special representation of the DMRG wavefunction to make the simulations possible within reasonable computational time. As a test case we apply the method to the 2-dimensional frustrated Heisenberg antiferromagnet. By supplementing the branching in GFMC with Stochastic Reconfiguration (SR) we get a stable simulation with a small variance also in the region where the fluctuations due to minus sign problem are maximal. The sensitivity of the results to the choice of the guiding wavefunction is extensively investigated. We analyse the model as a function of the ratio of the next-nearest to nearest neighbor coupling strength. We observe in the frustrated regime a pattern of the spin correlations which is in-between dimerlike and plaquette type ordering, states that have recently been suggested. It is a state with strong dimerization in one direction and weaker dimerization in the perpendicular direction. 0 into PostgreSQL...\n",
      "Inserting test sample 1320  In this study, we explore the incorporation of density matrix wavefunctions in Monte Carlo simulations for the frustrated Heisenberg model. This analytical framework allows for the investigation of the properties of quantum systems with complex and competing interactions. We use the stochastic series expansion quantum Monte Carlo algorithm to simulate the Heisenberg model, utilizing the density matrix wavefunctions to improve the stability of the simulation and reduce computational demand. We then analyze various properties of the model, including the ground state energy, magnetization, and correlation functions. Our results show that the density matrix wavefunctions significantly improve the accuracy of the Monte Carlo simulations compared to traditional methods. Additionally, we find that frustration in the Heisenberg model results in a highly degenerate ground state, with several low-lying excited states. This degeneracy leads to unique magnetic properties, such as the presence of spin textures and emergent magnetic order. Finally, we discuss the implications of our findings for future studies, including the development of novel materials with tunable quantum properties. Overall, our work demonstrates the utility of the density matrix wavefunction approach in Monte Carlo simulations of complex quantum systems, with potential applications in various fields of research. 1 into PostgreSQL...\n",
      "Inserting test sample 1321  This paper is concerned with particle filtering for $\\alpha$-stable stochastic volatility models. The $\\alpha$-stable distribution provides a flexible framework for modeling asymmetry and heavy tails, which is useful when modeling financial returns. An issue with this distributional assumption is the lack of a closed form for the probability density function. To estimate the volatility of financial returns in this setting, we develop a novel auxiliary particle filter. The algorithm we develop can be easily applied to any hidden Markov model for which the likelihood function is intractable or computationally expensive. The approximate target distribution of our auxiliary filter is based on the idea of approximate Bayesian computation (ABC). ABC methods allow for inference on posterior quantities in situations when the likelihood of the underlying model is not available in closed form, but simulating samples from it is possible. The ABC auxiliary particle filter (ABC-APF) that we propose provides not only a good alternative to state estimation in stochastic volatility models, but it also improves on the existing ABC literature. It allows for more flexibility in state estimation while improving on the accuracy through better proposal distributions in cases when the optimal importance density of the filter is unavailable in closed form. We assess the performance of the ABC-APF on a simulated dataset from the $\\alpha$-stable stochastic volatility model and compare it to other currently existing ABC filters. 0 into PostgreSQL...\n",
      "Inserting test sample 1322  This paper presents a novel computational framework for performing stochastic volatility filtering when likelihood-based approaches are intractable. The framework utilizes an auxiliary model, which generates a sequence of states that match the observed data as closely as possible. Our method is based on a recently proposed approach that combines optimization techniques with the iterative simulation of a sequence of Markov chain Monte Carlo algorithms. We demonstrate that our approach produces more accurate estimates of the states and parameters, and requires significantly fewer computational resources than alternative approaches. Specifically, we illustrate the effectiveness of our approach by applying it to a set of simulated data, and by comparing its performance with that of existing filtering methods. Our results show that our framework provides accurate estimates of the posterior mean and variance, and yields better convergence properties than competing methods. Additionally, we demonstrate that our approach is scalable, and thus can be used to analyze datasets with large numbers of observations. These results suggest that our framework has the potential to significantly improve the accuracy and efficiency of stochastic volatility filtering, and could be useful in a variety of applications, including finance, engineering, and biology. 1 into PostgreSQL...\n",
      "Inserting test sample 1323  We consider the problem of modelling noisy but highly symmetric shapes that can be viewed as hierarchies of whole-part relationships in which higher level objects are composed of transformed collections of lower level objects. To this end, we propose the stochastic wreath process, a fully generative probabilistic model of drawings. Following Leyton's \"Generative Theory of Shape\", we represent shapes as sequences of transformation groups composed through a wreath product.\n",
      "\n",
      "This representation emphasizes the maximization of transfer --- the idea that the most compact and meaningful representation of a given shape is achieved by maximizing the re-use of existing building blocks or parts.\n",
      "\n",
      "The proposed stochastic wreath process extends Leyton's theory by defining a probability distribution over geometric shapes in terms of noise processes that are aligned with the generative group structure of the shape. We propose an inference scheme for recovering the generative history of given images in terms of the wreath process using reversible jump Markov chain Monte Carlo methods and Approximate Bayesian Computation. In the context of sketching we demonstrate the feasibility and limitations of this approach on model-generated and real data. 0 into PostgreSQL...\n",
      "Inserting test sample 1324  The Wreath Process is a generative model of geometric shape that is based on nested symmetries. This model proposes a novel way of constructing complex, ornate shapes by iteratively applying symmetries to simpler building blocks. The resulting shapes exhibit intricate, repeating patterns that are reminiscent of wreaths, hence the name. The wreath process is inspired by group theory and draws from a range of mathematical concepts, including symmetry groups, Cayley graphs, and graph automorphisms. Through a series of experiments, the paper demonstrates the flexibility and generative power of the wreath process by generating a diverse range of complex shapes, from fractals to snowflakes to ornamental motifs. The paper also outlines several potential applications for the wreath process, including computer graphics, pattern recognition, and generative art. The wreath process represents a promising new direction in the field of geometric modeling and has the potential to significantly advance our understanding of the relationship between geometry, symmetry, and pattern. 1 into PostgreSQL...\n",
      "Inserting test sample 1325  Aims. The method of deriving photometric metallicities using red giant branch stars is applied to resolved stellar populations under the common assumption that they mainly consist of single-age old stellar populations. We explore the effect of the presence of mixed-age stellar populations on deriving photometric metallicities. Methods. We use photometric data sets for the five Galactic dwarf spheroidals Sculptor, Sextans, Carina, Fornax, and Leo II in order to derive their photometric metallicity distribution functions from their resolved red giant branches using isochrones of the Dartmouth Stellar Evolutionary Database. We compare the photometric metallicities with published spectroscopic metallicities based on the analysis of the near-infrared Ca triplet (Ca T), both on the metallicity scale of Carretta & Gratton and on the scale defined by the Dartmouth isochrones. In addition, we compare the photometric metallicities with published spectroscopic metallicities based on spectral synthesis and medium-resolution spectroscopy, and on high resolution spectra where available.\n",
      "\n",
      "Results. The mean properties of the spectroscopic and photometric metallicity samples are comparable within the intrinsic scatter of each method although the mean metallicities of dSphs with pronounced intermediate-age population fractions may be underestimated by the photometric method by up to a few tenths of dex in [Fe/H]. The star-by-star differences of the spectroscopic minus the photometric metallicities show a wide range of values along the fiducial spectroscopic metallicity range, with the tendency to have systematically lower photometric metallicities for those dwarf spheroidals with a higher fraction of intermediate-age populations. Such discrepancies persist even in the case of the purely old Sculptor dSph, where one would na\\\"ively expect a very good match when comparing with medium or low resolution metallicity measurements.\n",
      "\n",
      "Overall, the agreement between Ca T metallicities and photometric metallicities is very good in the metallicity range from ~ -2 dex to ~ -1.5 dex. We find that the photometric method is reliable in galaxies that contain small (less than 15%) intermediate-age stellar fractions. Therefore, in the presence of mixed-age stellar populations, one needs to quantify the fraction of the intermediate-age stars in order to assess their effect on determining metallicities from photometry alone. Finally, we note that the comparison of spectroscopic metallicities of the same stars obtained with different methods reveals similarly large discrepancies as the comparison with photometric metallicities. 0 into PostgreSQL...\n",
      "Inserting test sample 1326  This study investigates the comparison between the spectroscopic and photometric metallicities in the context of Milky Way dwarf spheroidal companion galaxies. The metallicities provide valuable insights into the physical properties and evolutionary history of the galaxies. We analyzed the spectra and photometric data of a sample of nine Milky Way dwarf galaxies using the Calcium II triplet and the Washington photometric system, respectively.\n",
      "\n",
      "Our results show that the photometric metallicities tend to be higher than the spectroscopic metallicities for the majority of the galaxies. However, we also find a significant scatter in the differences between the two methods. We examine possible sources of systematic errors, such as age uncertainties, non-uniform distributions of alpha-elements, and the presence of multiple populations, but they do not appear to fully explain the differences.\n",
      "\n",
      "Furthermore, we investigate the impact of metallicity differences on the interpretation of galaxy properties. We find that the differences are particularly important for the interpretation of the abundance ratios of different elements, such as magnesium and iron. In addition, our results suggest that photometric metallicities may have an impact on the inferred dark matter content of the galaxies.\n",
      "\n",
      "Finally, we use these findings to discuss the prospects of using the spectroscopic and photometric metallicities in future surveys. Our study emphasizes the importance of a careful analysis of the metallicities and their uncertainties, and the need for complementary methods to fully understand the peculiarities of individual galaxies. We conclude that future surveys should aim for a combination of spectroscopic and photometric metallicities, supplemented with other observables, to provide a comprehensive picture of the physical properties and evolution of galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 1327  La$_3$Co$_4$Sn$_{13}$ and La$_3$Ru$_4$Sn$_{13}$ were categorized as BCS superconductors. In a plot of the critical field $H_{c2}$ vs $T$, La$_3$Ru$_4$Sn$_{13}$ displays a second superconducting phase at the higher critical temperature $T_c^{\\star}$, characteristic of inhomogeneous superconductors, while La$_3$Co$_4$Sn$_{13}$ shows bulk superconductivity below $T_c$. We observe a decrease in critical temperatures with external pressure and magnetic field for both compounds with $\\frac{dT_c^{\\star}}{dP} > \\frac{dT_c}{dP}$. The pressure dependences of $T_c$ are interpreted according to the McMillan theory and understood to be a consequence of lattice stiffening. The investigation of the superconducting state of La$_3$Co$_x$Ru$_{4-x}$Sn$_{13}$ shows a $T_c^{\\star}$ that is larger then $T_c$ for $x<4$. This unique and unexpected observation is discussed as a result of the local disorder and/or the effect of chemical pressure when Ru atoms are partially replaced by smaller Co atoms. 0 into PostgreSQL...\n",
      "Inserting test sample 1328  In this study, we investigate the effects of disorder on superconductivity in the La$_3$Co$_4$Sn$_{13}$ and La$_3$Ru$_4$Sn$_{13}$ compounds. We performed electrical transport and magnetic studies to characterize the compounds and determine the role of disorder in the superconductivity of these materials. We found that disorder significantly affects the superconductivity properties of the compounds, leading to a suppression of superconductivity at higher levels of disorder. Our transport measurements indicate that the suppression of superconductivity is related to a reduction in the carrier density, and to a possible reduction of the superconducting gap. Our magnetic measurements suggest the presence of both type I and type II superconductivity coexisting in the compounds. Our findings provide insight into the impact of disorder on superconductivity, and offer important implications for the design and development of new superconducting materials for a range of technological applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1329  We investigate the driven states of a two-dimensional crystal whose ground state can be tuned through a square-triangular transition. The depinning of such a system from a quenched random background potential occurs via a complex sequence of dynamical states, which include plastic flow states, hexatics, dynamically stabilized triangle and square phases and intermediate regimes of phase coexistence. These results are relevant to transport experiments in the mixed phase of several superconductors which exhibit such structural transitions as well as to driven colloidal systems whose interactions can be tuned via surface modifications. 0 into PostgreSQL...\n",
      "Inserting test sample 1330  This study discusses the properties of driven disordered periodic media in the presence of an underlying structural phase transition. Utilizing computer simulations of a two-dimensional lattice, we demonstrate the coexistence of a spatially ordered phase in areas of low driving forces, and a disordered phase where driving forces are strong. Our results showcase how the structure of a system can transition between ordered and disordered phases, and how these transitions can be influenced by driving forces. The findings are applicable to a range of physical systems, including superconducting vortices in periodic pinning arrays. 1 into PostgreSQL...\n",
      "Inserting test sample 1331  Boundary layers in space and astrophysical plasmas are the location of complex dynamics where different mechanisms coexist and compete eventually leading to plasma mixing. In this work, we present fully kinetic Particle-In-Cell simulations of different boundary layers characterized by the following main ingredients: a velocity shear, a density gradient and a magnetic gradient localized at the same position. In particular, the presence of a density gradient drives the development of the lower hybrid drift instability (LHDI), which competes with the Kelvin-Helmholtz instability (KHI) in the development of the boundary layer. Depending on the density gradient, the LHDI can even dominate the dynamics of the layer. Because these two instabilities grow on different spatial and temporal scales, when the LHDI develops faster than the KHI an inverse cascade is generated, at least in 2D. This inverse cascade, starting at the LHDI kinetic scales, generates structures at scale lengths at which the KHI would typically develop. When that is the case, those structures can suppress the KHI itself because they significantly affect the underlying velocity shear gradient. We conclude that depending on the density gradient, the velocity jump and the width of the boundary layer, the LHDI in its nonlinear phase can become the primary instability for plasma mixing. These numerical simulations show that the LHDI is likely to be a dominant process at the magnetopause of Mercury. These results are expected to be of direct impact to the interpretation of the forthcoming BepiColombo observations. 0 into PostgreSQL...\n",
      "Inserting test sample 1332  The interaction between the Kelvin-Helmholtz (KH) and Lower-Hybrid Drift (LHD) instabilities has been investigated using a comprehensive set of numerical simulations. Our research shows that the combined effect of these two instabilities can result in significant plasma acceleration and heating. The KH instability arises as a result of the velocity shear between adjacent plasma regions, while the LHD instability is driven by the interaction between the ambient magnetic field and plasma density gradients. Our simulations indicate that the spatial scales of the two instabilities are significantly different, with larger KH eddies coexisting with smaller LHD perturbations. Additionally, we find that the magnetic reconfiguration induced by the LHD instability can significantly modify the KH structures. As a consequence of this interplay, we observe complex plasma dynamics, including the appearance of thin current sheets, which can lead to reconnection events. Our work sheds further light on the complex nature of plasma instabilities and their interplay in magnetized plasmas, with important implications for space and laboratory plasma studies. These findings may also provide insights into the behavior of astrophysical plasma systems where these types of instabilities are expected to play a crucial role. 1 into PostgreSQL...\n",
      "Inserting test sample 1333  A comprehensive uncertainty quantification framework has been developed for integrating computational and experimental kinetic data and to identify active sites and reaction mechanisms in catalysis. Three hypotheses regarding the active site for the water-gas shift reaction on Pt/TiO2 catalysts are tested - Pt(111), an edge interface site, and a corner interface site. Uncertainties associated with DFT calculations and model errors of microkinetic models of the active sites are informed and verified using Bayesian inference and predictive validation. Significant evidence is found for the role of the oxide support in the mechanism. Positive evidence is found in support of the edge interface active site over the corner interface site. For the edge interface site, the CO-promoted redox mechanism is found to be the dominant pathway and only at temperatures above 573 K does the classical redox mechanism contribute significantly to the overall rate. At all reaction conditions, water and surface O-H bond dissociation steps at the Pt/TiO2 interface are the main rate controlling steps. 0 into PostgreSQL...\n",
      "Inserting test sample 1334  This study focuses on identifying the active sites of water-gas shift (WGS) reaction over a titania-supported platinum catalyst under uncertainty. WGS reaction is an important process in the production of hydrogen and has significant industrial applications. Hence, it is essential to investigate the reaction mechanisms and key active sites of WGS to optimize the performance of catalysts. However, identifying the active sites of WGS reaction over platinum catalysts on the molecular level is a challenging task due to the complexity of the reaction system. Therefore, in this study, we used a combination of experimental and computational techniques to identify the key active sites of WGS on the surface of the platinum catalysts. Our findings suggest that the oxygen vacancies on the titania support play a crucial role in the WGS reaction. These results would significantly contribute to the design of more efficient platinum-based catalysts for WGS applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1335  Searches for pair production of gauginos and squarks in e+e- collisions at a centre-of-mass energy of 189 GeV have been performed on data corresponding to an integrated luminosity of 158 pb^{-1} collected by the DELPHI detector at LEP. The data were analyzed under the assumption of non-conservation of R-parity through a single dominant Ubar Dbar Dbar coupling between squarks and quarks. Typical final states contain between 4 and 10 jets with or without additional leptons. No excess of data above Standard Model expectations was observed. The results were used to constrain domains of the MSSM parameter space and derive limits on the masses of supersymmetric particles. The following mass limits at 95% CL were obtained from these searches: neutralino mass: m_{\\tilde{\\chi^0_{1}}} \\geq 32 GeV chargino mass: m_{\\tilde{\\chi^+_{1}}} \\geq 94 GeV stop and sbottom mass (indirect decay) with \\Delta M > 5 GeV: m_{\\tilde{t_1}} \\geq 74 GeV, for \\Phi_{mix}=0 rad m_{\\tilde{t_1}} \\geq 59 GeV, for \\Phi_{mix}=0.98 rad m_{\\tilde{b_1}} \\geq 72 GeV, for \\Phi_{mix}=0 rad.\n",
      "\n",
      "The angle \\phi_{mix} is the mixing angle between left and right handed quarks. 0 into PostgreSQL...\n",
      "Inserting test sample 1336  This study investigates the potential violation of R-parity in proton-antiproton collisions at a center-of-mass energy of 189 GeV, focusing on the Ubar Dbar Dbar coupling. The data were collected from the LEP accelerator experiment at CERN. The search for R-parity violation is motivated by the fact that it could explain certain phenomena in particle physics, such as the observed neutrino oscillations. The analysis focused on events with at least one charged lepton and large missing transverse momentum. The expected background from standard model processes was estimated and compared with the observed data, revealing no significant excess. Exclusion limits on the Ubar Dbar Dbar coupling were derived, and regions of the parameter space were constrained. The results of this study provide important insights into the nature of R-parity violation, and could guide future research into its potential impact on the fundamental nature of matter and energy. The methodology presented here serves as a valuable contribution to the ongoing analysis of data from high-energy particle collisions, which continue to push the boundaries of our understanding of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1337  The literature for count modeling provides useful tools to conduct causal inference when outcomes take non-negative integer values. Applied to the potential outcomes framework, we link the Bayesian causal inference literature to statistical models for count data. We discuss the general architectural considerations for constructing the predictive posterior of the missing potential outcomes. Special considerations for estimating average treatment effects are discussed, some generalizing certain relationships and some not yet encountered in the causal inference literature. 0 into PostgreSQL...\n",
      "Inserting test sample 1338  This paper proposes a Bayesian framework for causal inference with count data. We develop a model that accounts for potential outcomes and allows for estimating counterfactuals by incorporating both prior knowledge and observed data. We demonstrate the effectiveness of our approach through simulations and real data applications. The proposed method extends the traditional causal inference techniques and provides a flexible and robust tool for analyzing count data under various causal assumptions. 1 into PostgreSQL...\n",
      "Inserting test sample 1339  In this paper, we present algorithms to estimate external contact forces and joint torques using only skin, i.e. distributed tactile sensors. To deal with gaps between the tactile sensors (taxels), we use interpolation techniques. The application of these interpolation techniques allows us to estimate contact forces and joint torques without the need for expensive force-torque sensors.\n",
      "\n",
      "Validation was performed using the iCub humanoid robot. 0 into PostgreSQL...\n",
      "Inserting test sample 1340  This research paper proposes a novel approach for estimating the contact force and joint torque using skin. Our method utilizes the deformation of the skin caused by applying pressure to estimate the force and torque. We validate our method using simulations and experiments on a robotic arm and demonstrate its accuracy for estimating the forces and torques exerted at the contact points. This non-invasive technique has potential applications in robotics and rehabilitation. 1 into PostgreSQL...\n",
      "Inserting test sample 1341  This is a short overview explaining how building a large-scale, silicon-photonic quantum computer has been reduced to the creation of good sources of 3-photon entangled states (and may simplify further). Given such sources, each photon need pass through a small, constant, number of components, interfering with at most 2 other spatially nearby photons, and current photonics engineering has already demonstrated the manufacture of thousands of components on two-dimensional semiconductor chips with performance that allows the creation of tens of thousands of photons entangled in a state universal for quantum computation.\n",
      "\n",
      "At present the fully-integrated, silicon-photonic architecture we envisage involves creating the required entangled states by starting with single-photons produced non-determistically by pumping silicon waveguides (or cavities) combined with on-chip filters and nanowire superconducting detectors to herald that a photon has been produced. These sources are multiplexed into being near-deterministic, and the single photons then passed through an interferometer to non-deterministically produce small entangled states - necessarily multiplexed to near-determinism again. This is followed by a `ballistic' scattering of the small-scale entangled photons through an interferometer such that some photons are detected, leaving the remainder in a large-scale entangled state which is provably universal for quantum computing implemented by single-photon measurements.\n",
      "\n",
      "There are a large number of questions regarding the optimum ways to make and use the final cluster state, dealing with static imperfections, constructing the initial entangled photon sources and so on, that need to be investigated before we can aim for millions of qubits capable of billions of computational time-steps. The focus in this article is on the theoretical side of such questions. 0 into PostgreSQL...\n",
      "Inserting test sample 1342  Quantum computing has been a rapidly expanding field, offering the potential for exponential speed-up for certain computational tasks, beyond the capability of classical computers. A promising route to achieving this computing power lies in the use of silicon photonics. In this work, we present our optimistic outlook regarding the development of quantum computing through the use of silicon photonics. \n",
      "\n",
      "One of the main reasons for our optimism lies in the fact that silicon photonics offers a solid foundation for the development of quantum computers. Silicon is a key material in the semiconductor industry, and its well-established fabrication processes can be used to manufacture photonic circuits. The use of silicon photonics also allows for the integration of classical and quantum processing within the same platform, which represents a significant advantage compared to other approaches. \n",
      "\n",
      "Moreover, silicon photonics offer the potential for scalability and compatibility with existing technology, since it is already widely used in the electronic industry. This compatibility could make the transition to quantum computing more seamless, boosting the uptake of this technology into new applications. This is further reinforced by the compatibility of silicon photonics with existing fiber optic networks, which opens up opportunities for the development of a quantum internet.\n",
      "\n",
      "In conclusion, we believe that the silicon-photonic route to quantum computing offers a promising future, thanks to the solid foundation provided by silicon technology, the potential for scalability, and the compatibility with existing infrastructure. We are optimistic that this approach could pave the way for the development of novel applications and solutions that could revolutionize several fields in science and industry. 1 into PostgreSQL...\n",
      "Inserting test sample 1343  Several works have developed vector-linear maximum-distance separable (MDS) storage codes that min- imize the total communication cost required to repair a single coded symbol after an erasure, referred to as repair bandwidth (BW).\n",
      "\n",
      "Vector codes allow communicating fewer sub-symbols per node, instead of the entire content. This allows non trivial savings in repair BW. In sharp contrast, classic codes, like Reed- Solomon (RS), used in current storage systems, are deemed to suffer from naive repair, i.e. downloading the entire stored message to repair one failed node. This mainly happens because they are scalar-linear. In this work, we present a simple framework that treats scalar codes as vector-linear. In some cases, this allows significant savings in repair BW. We show that vectorized scalar codes exhibit properties that simplify the design of repair schemes. Our framework can be seen as a finite field analogue of real interference alignment. Using our simplified framework, we design a scheme that we call clique-repair which provably identifies the best linear repair strategy for any scalar 2-parity MDS code, under some conditions on the sub-field chosen for vectorization. We specify optimal repair schemes for specific (5,3)- and (6,4)-Reed- Solomon (RS) codes. Further, we present a repair strategy for the RS code currently deployed in the Facebook Analytics Hadoop cluster that leads to 20% of repair BW savings over naive repair which is the repair scheme currently used for this code. 0 into PostgreSQL...\n",
      "Inserting test sample 1344  In the realm of distributed data storage, the use of Maximum Distance Separable (MDS) codes is widespread as they offer optimal resilience to node failures. However, when the data is encoded using scalar MDS codes, node failure repair operations can incur high communication costs. In this paper, we present a novel repair framework for scalar MDS codes that achieves efficient node repairs by minimizing the amount of data to be communicated while ensuring the repair is exact. Our proposed framework leverages upon the symmetry properties of the code to obtain intermediate repairing nodes. We also show how our framework can be extended to repair multiple node failures simultaneously using a group of intermediate nodes. Our proposed scheme is compared against the state-of-the-art repairing techniques and was found to significantly reduce the amount of data communicated during repairs. We also provide an analysis of the communication costs for node repairs in scalar MDS codes. Our framework is shown to be particularly effective for high-dimensional data, where the communication cost reduction is most pronounced. Overall, this paper presents a significant step towards enhancing the efficiency and effectiveness of repair operations in scalar MDS codes, which are widely used in distributed data storage systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1345  This report presents the system developed by the ABSP Laboratory team for the third DIHARD speech diarization challenge. Our main contribution in this work is to develop a simple and efficient solution for acoustic domain dependent speech diarization. We explore speaker embeddings for \\emph{acoustic domain identification} (ADI) task. Our study reveals that i-vector based method achieves considerably better performance than x-vector based approach in the third DIHARD challenge dataset. Next, we integrate the ADI module with the diarization framework. The performance substantially improved over that of the baseline when we optimized the thresholds for agglomerative hierarchical clustering and the parameters for dimensionality reduction during scoring for individual acoustic domains. We achieved a relative improvement of $9.63\\%$ and $10.64\\%$ in DER for core and full conditions, respectively, for Track 1 of the DIHARD III evaluation set. 0 into PostgreSQL...\n",
      "Inserting test sample 1346  This paper presents the system developed for the third DIHARD speaker diarization challenge, which has achieved state-of-the-art performance in domains such as meeting and interview data with multiple channels. We propose a domain-dependent approach that constructs a model tailored to each domain, allowing for better generalization and improved speaker clustering. Our system includes several key components, such as a novel feature representation using bottleneck features, a two-stage clustering algorithm employing agglomerative hierarchical and spectral clustering, and a resegmentation technique utilizing speaker embeddings to refine speech segments. Experimental results show that our proposed approach significantly outperforms the baseline system in both the development and evaluation sets, achieving an overall diarization error rate of 12.88%. This paper contributes to the field of speaker diarization by demonstrating the effectiveness of domain-dependent modeling for different types of speech data. 1 into PostgreSQL...\n",
      "Inserting test sample 1347  In this work, we present the theoretical and experimental study of the propagation of Airy beams when passing through two convex lenses with different focal distances. The theoretical analysis is presented from a matrix optical model for an Airy beam. The experimental results are obtained in a holographic setup for experimental generation of Airy beams based on the holographic reconstruction system of computer generated holograms (CGH) implemented electronically in a spatial light modulator (SLM) and a 4f Fourier transformation system. These are in agreement with the theoretical prediction.\n",
      "\n",
      "The novelty of this work went to show the evolution of the Airy beam propagation for different convex lenses in a 4f system and its dependence on focal distances. These are important for the control and manipulation of Airy beams; and, they show new possibilities of applications of the Airy beams in optical tweezers, optical communications and optical guidance. 0 into PostgreSQL...\n",
      "Inserting test sample 1348  The propagation of Airy beams with ballistic trajectory through Fourier transformation systems is a topic of great importance in numerous fields of modern optics, such as imaging, communication, and metrology. In this work, we investigate the behavior of Airy beams when they pass through a complex optical system, and we analyze the Fourier transform properties of such beams. We propose a theoretical model to describe the propagation of Airy beams through Fourier transform systems, which we complemented with experimental results. Numerical simulations, supported by experimental data, demonstrate the preservation of the ballistic trajectory in the Airy beams through the Fourier transform system. Our findings suggest that our proposed model could find applications in future imaging systems aimed at constructing adaptive and high-quality imaging and sensing devices. 1 into PostgreSQL...\n",
      "Inserting test sample 1349  Metamaterials are composed of periodic subwavelength metal/dielectric structures that resonantly couple to the electric and/or magnetic components of the incident electromagnetic fields, exhibiting properties that are not found in nature. Planar metamaterials with subwavelength thickness, or metasurfaces, consisting of single-layer or few-layer stacks of planar structures, can be readily fabricated using lithography and nanoprinting methods, and the ultrathin thickness in the wave propagation direction can greatly suppress the undesirable losses. Metasurfaces enable a spatially varying optical response, mold optical wavefronts into shapes that can be designed at will, and facilitate the integration of functional materials to accomplish active control and greatly enhanced nonlinear response. This paper reviews recent progress in the physics of metasurfaces operating at wavelengths ranging from microwave to visible. We provide an overview of key metasurface concepts such as anomalous reflection and refraction, and introduce metasurfaces based on the Pancharatnam-Berry phase and Huygens' metasurfaces, as well as their use in wavefront shaping and beam forming applications, followed by a discussion of polarization conversion in few-layer metasurfaces and their related properties.\n",
      "\n",
      "An overview of dielectric metasurfaces reveals their ability to realize unique functionalities coupled with Mie resonances and their low ohmic losses. We also describe metasurfaces for wave guidance and radiation control, as well as active and nonlinear metasurfaces. Finally, we conclude by providing our opinions of opportunities and challenges in this rapidly developing research field. 0 into PostgreSQL...\n",
      "Inserting test sample 1350  Metasurfaces are a class of two-dimensional materials that manipulate electromagnetic waves at a subwavelength scale. These materials offer unprecedented control over the behavior of light, enabling numerous applications in imaging, sensing, and communication. In this review paper, we present an overview of the physics behind metasurfaces and their applications.\n",
      "\n",
      "We start with a brief introduction of metasurfaces and their basic characteristics. We then discuss the underlying physics behind the functionality of metasurfaces, including the principles of plasmonics, resonant coupling, and wave interference. Next, we describe various strategies for designing and fabricating metasurfaces, including top-down and bottom-up approaches.\n",
      "\n",
      "Moving on to applications, we focus on the use of metasurfaces in imaging, sensing, and communication systems. Specifically, we delve into the different types of metasurface lenses, such as metalenses and hyperlenses, and their advantages over conventional lenses. We also discuss their utility in sensing applications, such as biosensors and chemical detectors.\n",
      "\n",
      "Finally, we explore recent advancements in the field of metasurface-based communication systems, including their use in beamforming, polarization control, and spatial multiplexing. We conclude with a discussion of the challenges and future prospects of metasurface research. Overall, this review provides a comprehensive and up-to-date summary of the exciting developments in metasurface physics and their applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1351  We report the completion of a survey of radio-loud AGNs begun in an earlier paper, aimed finding and studying broad, double-peaked Balmer lines. We present Ha spectra of 13 broad-lined objects (3 with double-peaked Ha profiles). The final sample includes 106 radio-loud AGNs. 20% of the objects have Ha lines with double peaks or twin shoulders and of these, 60% can be fitted quite well with a model attributing the emission to a circular, relativistic, disk. We also compare the Hb and MgII profiles of 4 objects with models of photoionized accretion disks and find them to be in reasonable agreement. Double-peaked emitters stand out on the basis of the following properties: (1) unusually large contribution of starlight to the optical continuum around Ha, (2) unusually large equivalent widths of low-ionization lines ([OI] and [SII]), (3) unusually large [OI]/[OIII] ratios, and (4) Balmer lines on average twice as broad as in other radio-loud AGNs. We evaluate models for the origin of the lines and we find accretion-disk emission to be the most successful one because it can explain the double-peaked line profiles and it also offers an interpretation of the spectroscopic properties of these objects. Aternative suggestions (binary broad-line regions, bipolar outflows, anisotropically illuminated spherical broad-line regions) are unsatisfactory because (a) they fail direct observational tests, (b) they cannot explain the unusual properties of double-peaked emitters self-consistently, or (c) their physical foundations appear to be unsound. We suggest that in double-peaked emitters and accretion- powered LINERs the accretion rate is considerably lower than the Eddington rate with the consequence that the inner accretion disk takes the form of an ion torus and the wind that normally enshrouds the disk proper is absent. 0 into PostgreSQL...\n",
      "Inserting test sample 1352  Radio-loud active galactic nuclei (AGNs) have been observed to emit double-peaked emission lines. These lines are believed to be produced by gas that is rotating around the central black hole. Understanding the nature of these lines is crucial for our comprehension of the structure and dynamics of these AGNs. In this study, we present the results of a detailed survey of double-peaked emission lines in radio-loud AGNs.\n",
      "\n",
      "The survey was conducted using high-resolution spectra obtained from the Apache Point Observatory Galactic Evolution Experiment (APOGEE). Our sample comprises of 50 AGNs that were selected based on their high radio luminosity. We used the double-Gaussian fit method to analyze the spectral lines and derive the kinematic properties of the gas.\n",
      "\n",
      "We find that the majority of the AGNs in our sample show double-peaked emission lines. This indicates that the rotating gas structure is common in these objects. We also find that the properties of the double-peaked emission lines vary among the AGNs. Some show symmetric profiles, while others exhibit velocity shifts and asymmetries. This suggests that the gas has complex kinematics and is potentially affected by other physical processes.\n",
      "\n",
      "Our study provides valuable insights into the nature of double-peaked emission lines in radio-loud AGNs. We discuss possible interpretations of our results, including the presence of accretion disks or outflows of gas. Additionally, we examine the implications of our findings for understanding the evolution and formation of these AGNs. Our study underscores the importance of continued observations and analyses of these objects to advance our understanding of the underlying physical mechanisms. 1 into PostgreSQL...\n",
      "Inserting test sample 1353  In this paper, we describe a novel experiment for the accurate estimation of pulsar dispersion measures using the Giant Metre-wave Radio Telescope. This experiment was carried out for a sample of twelve pulsars, over a period of more than one year (January 2001 to May 2002) with observations about once every fortnight. At each epoch, the pulsar DMs were obtained from simultaneous dual frequency observations, without requiring any absolute timing information.\n",
      "\n",
      "The DM estimates were obtained from both the single pulse data streams and from the average profiles. The accuracy of the DM estimates at each epoch is ~ 1 part in 10^4 or better, making the data set useful for many different kinds of studies. The time series of DM shows significant variations on time scales of weeks to months for most of the pulsars. A comparison of the mean DM values from these data show significant deviations from catalog values (as well as from other estimates in literature) for some of the pulsars, with PSR B1642-03 showing the most notable changes. From our analysis results it appears that constancy of pulsar DMs (at the level of 1 in 10^3 or better) can not be taken for granted. For PSR B2217+47, we see evidence for a large-scale DM gradient over a one year period, which is modeled as being due to a blob of enhanced electron density sampled by the line of sight. For some pulsars, including pulsars with fairly simple profiles like PSR B1642-03, we find evidence for small changes in DM values for different frequency pairs of measurement, a result that needs to be investigated in detail. Another interesting result is that we find significant differences in DM values obtained from average profiles and single pulse data. 0 into PostgreSQL...\n",
      "Inserting test sample 1354  In this research paper, we present a study on tracking pulsar dispersion measures using the Giant Metrewave Radio Telescope (GMRT). Pulsars are highly magnetized, rotating neutron stars that emit electromagnetic radiation in the radio spectrum with regular periods. The dispersion measure (DM) of a pulsar is the amount of delay in arrival time observed due to the scattering of radio signals by interstellar plasma. Accurate measurement of DM is crucial for a range of astrophysical studies, including tests of general relativity, probing the interstellar medium, and cosmological tests. \n",
      "\n",
      "We utilize the GMRT to observe and track pulsar dispersion measures through a single pulse observation mode. The GMRT, with its high sensitivity and frequency coverage, is an ideal instrument for such measurements. We describe in detail the observations and data processing methods used to obtain accurate values of DM for a sample of known pulsars. Our analysis confirms the effectiveness of the GMRT in measuring DMs with high precision and accuracy.\n",
      "\n",
      "We also discuss the potential for future studies using this method, including the detection of new pulsars and observations of fast radio bursts. Additionally, we explore the possibilities of combining our results with those obtained from other radio telescopes for more comprehensive studies on pulsar astronomy and cosmology.\n",
      "\n",
      "In conclusion, our study demonstrates the power of the GMRT for precise measurements of dispersion measures for pulsars. The results obtained from this work will serve as a valuable resource for future cosmological and astrophysical studies involving pulsars, and pave the way for new discoveries in this field. 1 into PostgreSQL...\n",
      "Inserting test sample 1355  We propose an enriched finite element formulation to address the computational modeling of contact problems and the coupling of non-conforming discretizations in the small deformation setting. The displacement field is augmented by enriched terms that are associated with generalized degrees of freedom collocated along non-conforming interfaces or contact surfaces. The enrichment strategy effectively produces an enriched node-to-node discretization that can be used with any constraint enforcement criterion; this is demonstrated with both multiple-point constraints and Lagrange multipliers, the latter in a generalized Newton implementation where both primal and Lagrange multiplier fields are updated simultaneously. The method's ability to ensure continuity of the displacement field -- without locking -- in mesh coupling problems, and to transfer fairly accurate tractions at contact interfaces -- without the need for contact stabilization -- is demonstrated by means of several examples. In addition, we show that the formulation is stable with respect to the condition number of the stiffness matrix by using a simple Jacobi-like diagonal preconditioner. 0 into PostgreSQL...\n",
      "Inserting test sample 1356  This study proposes an interface-enriched generalized finite element formulation for the coupling of non-conforming discretizations and contact. The proposed formulation effectively mitigates locking effects and supports accurate modeling of contact conditions between non-matching meshes. This approach demonstrates increased computational efficiency and flexibility in numerical simulations.\n",
      "\n",
      "The method offers a simple solution to coupling mechanical systems with non-matching meshes in contact. It eliminates the need for mesh refinement and remeshing, reducing both computational expense and numerical error in contact simulation. Moreover, it provides an attractive alternative to traditional coupled solvers, which may suffer from inefficient convergence in nonmatching mesh situations.\n",
      "\n",
      "The proposed approach is verified through various numerical experiments, validating the accuracy and efficiency of the proposed formulation over traditional methods. The developed methodology is expected to contribute to a wide range of applications across the fields of engineering and material science where nonconforming discretizations and contact are present. 1 into PostgreSQL...\n",
      "Inserting test sample 1357  We construct few deep generative models of gravitational waveforms based on the semi-supervising scheme of conditional autoencoders and their variational extensions. Once the training is done, we find that our best waveform model can generate the inspiral-merger waveforms of binary black hole coalescence with more than $97\\%$ average overlap matched filtering accuracy for the mass ratio between $1$ and $10$. Besides, the generation time of a single waveform takes about one millisecond, which is about $10$ to $100$ times faster than the EOBNR algorithm running on the same computing facility. Moreover, these models can also help to explore the space of waveforms. That is, with mainly the low-mass-ratio training set, the resultant trained model is capable of generating large amount of accurate high-mass-ratio waveforms. This result implies that our generative model can speed up the waveform generation for the low latency search of gravitational wave events. With the improvement of the accuracy in future work, the generative waveform model may also help to speed up the parameter estimation and can assist the numerical relativity in generating the waveforms of higher mass ratio by progressively self-training. 0 into PostgreSQL...\n",
      "Inserting test sample 1358  The detection of gravitational waves has opened a new window to the universe, providing unprecedented opportunities for observing astrophysical phenomena. One promising approach for modeling gravitational waveforms that has gained much attention recently is the use of deep generative models. In this paper, we propose a novel method for generating gravitational waveforms using a conditional autoencoder architecture. Our approach leverages recent advances in deep neural networks and conditional generative models, providing a way to learn the underlying features of gravitational waves within a latent space. We evaluate the performance of our model on simulated data, demonstrating the effectiveness of our method in generating waveforms with high fidelity. Additionally, we analyze the latent space of our model to gain insights into the properties of gravitational waves. Our results show that our proposed method can accurately model diverse classes of waveforms, making it a promising approach for future gravitational wave detection and analysis. Overall, we believe our work provides a valuable contribution towards the development of generative models for gravitational waveforms. 1 into PostgreSQL...\n",
      "Inserting test sample 1359  We present the deepest optical color-magnitude diagram (CMD) to date of the local elliptical galaxy M32. We have obtained F435W and F555W photometry based on HST ACS/HRC images for a region 110\" from the center of M32 and a background field about 320\" away from M32 center. Due to the high resolution of our Nyquist-sampled images, the small photometric errors, and the depth of our data we obtain the most detailed resolved photometric study of M32 yet.\n",
      "\n",
      "Deconvolution of HST images proves to be superior than other standard methods to derive stellar photometry on extremely crowded HST images. The location of the strong red clump in the CMD suggests a mean age between 8 and 10 Gyr for [Fe/H] = -0.2 in M32. We detect for the first time a red giant branch bump and an asymptotic giant branch bump in M32 which indicate that the mean age of M32's dominant population at ~2' from its center is between 5 and 10 Gyr. We see evidence of an intermediate-age population in M32 mainly due to the presence of bright asymptotic giant branch stars. Our detection of a blue component of stars (blue plume) may indicate for the first time the presence of a young stellar population, with ages of the order of 0.5 Gyr, in our M32 field. However, it is likely that the brighter stars of this blue plume belong to the disk of M31 rather than to M32. The fainter stars populating the blue plume indicate the presence of stars not younger than 1 Gyr and/or blue straggler stars in M32. M32's dominant population of 8--10 Gyr implies a formation redshift of 1 < z_f < 2, precisely when observations of the specific star formation rates and models of \"downsizing\" imply galaxies of M32's mass ought to be forming their stars. Our CMD therefore provides a \"ground-truth\" of downsizing scenarios at z=0. Our background field data represent the deepest optical observations yet of the inner disk and bulge of M31. [Abridged] 0 into PostgreSQL...\n",
      "Inserting test sample 1360  In this research paper, we present the deepest Hubble Space Telescope (HST) color-magnitude diagram (CMD) obtained for the M32 galaxy, a nearby satellite of the Milky Way. Using the ultraviolet and optical filters of the HST, we were able to reach a limiting magnitude of approximately 29.5 AB magnitude in both filters.\n",
      "\n",
      "Our analysis of the CMD reveals clear evidence for the existence of intermediate-age stellar populations in M32. Specifically, we detected a well-defined red giant branch (RGB) structure, which is indicative of an underlying intermediate-age population. The RGB is not only well-populated, but also exhibits a significant spread in color, which is consistent with the expected range of ages for this type of population. Moreover, we detected a prominent feature known as the blue horizontal branch, which is also indicative of intermediate-age populations.\n",
      "\n",
      "We used state-of-the-art stellar population models to derive the age and metallicity of the intermediate-age population in M32, and found that it has an age of approximately 4-5 Gyr and a metallicity of [Fe/H] ~ -0.4 dex. This is consistent with previous studies of M32, and provides further evidence for the presence of intermediate-age populations in this galaxy.\n",
      "\n",
      "Our results have important implications for our understanding of the formation and evolution of M32 and other dwarf galaxies in the Local Group. They suggest that these galaxies have experienced significant episodes of star formation throughout their histories, and that these episodes have contributed to their observed stellar populations. Overall, this study demonstrates the power of the HST and its ability to reveal the complex stellar populations of nearby galaxies like M32. 1 into PostgreSQL...\n",
      "Inserting test sample 1361  We use new near-infrared spectroscopy and our published optical spectroscopy of the gravitationally-lensed Seyfert-2 galaxy F10214+4724 to study both the links between the starburst and AGN in this object and the properties of the inner narrow-line clouds. The UV spectrum is consistent with a compact, moderately- reddened starburst providing about half the UV light. Spectroscopy of the Halpha /[NII] line blend has enabled us to distinguish emission from the narrow-line region of the Seyfert-2 and a moderately-reddened emission line region which we argue is associated with the starburst. Estimates of the star formation rate from the UV continuum flux and the Halpha flux are broadly consistent. We can explain the unusual emission line properties of F10214+4724 in terms of conventional models for nearby Seyfert-2 galaxies if lensing is preferentially magnifying the side of the inner narrow-line region between the AGN and the observer, and the other side is both less magnified and partially obscured by the torus. The hydrogen densities of clouds in this region are high enough to make the Balmer lines optically thick and to suppress forbidden emission lines with low critical densities. We have deduced the column density of both ionised and neutral gas in the narrow-line clouds, and the density of the ionised gas. Using these we have been able to estimate the mass of the inner narrow-line clouds to be ~ 1 solar mass, and show that the gas:dust ratio NH/E(B-V) in these clouds must be ~1.3x10^{27}m^{-2}mag^{-1}, significantly higher than in the Milky Way. The cloud properties are consistent with the those of the warm absorbers seen in the X-ray spectra of Seyfert-1 galaxies.\n",
      "\n",
      "Our results favour models in which narrow-line clouds start close to the nucleus and flow out. 0 into PostgreSQL...\n",
      "Inserting test sample 1362  IRAS F10214+4724 is a luminous infrared galaxy located at a redshift of 2.3. It is considered to be one of the most extreme infrared galaxies known, with a bolometric luminosity of roughly 10^14 solar luminosities. In this paper, we present the results of a study of the inner 100 parsecs (pc) of IRAS F10214+4724 through high-resolution imaging and spectroscopy.\n",
      "\n",
      "Our observations reveal a complex system of multiple components, including a bright, compact core and several fainter structures that appear to be associated with outflows and/or inflows. The core emission is dominated by a dusty structure that we interpret as a disk oriented roughly perpendicular to our line of sight. This disk appears to be actively accreting gas from the surrounding medium, likely fueling the intense star formation activity that is observed in IRAS F10214+4724.\n",
      "\n",
      "Our spectroscopic observations show evidence for both ionized and molecular gas in the inner 100 pc of IRAS F10214+4724. The molecular gas is primarily traced by CO emission, which reveals a rotating disk with a radius of roughly 50 pc. The ionized gas emission is dominated by a bright, compact region located near the center of the galaxy. We discuss the implications of these observations for our understanding of the physical processes driving the extreme star formation activity in IRAS F10214+4724.\n",
      "\n",
      "In summary, our high-resolution imaging and spectroscopy of the inner 100 pc of IRAS F10214+4724 reveal a complex system of multiple components, including an actively accreting dusty disk, outflows/inflows, and both ionized and molecular gas. These observations provide important insights into the physical processes driving the extreme star formation activity in this luminous infrared galaxy. 1 into PostgreSQL...\n",
      "Inserting test sample 1363  Quantifying entanglement is a work in progress which is important for the active field of quantum information and computation. A measure of bipartite entanglement is proposed here, named entanglement coherence, which is essentially the normalized coherence of the entangled state in its Schmidt basis. Its value is 1 for maximally entangled states, and 0 for separable states, irrespective of the dimensionality of the Hilbert space. So a maximally entangled state is also the one which is maximally coherent in its Schmidt basis. Quantum entanglement and quantum coherence are thus intimately connected. Additionally it is shown that the entanglement coherence is closely connected to the Wigner-Yanase skew information of the reduced density operator of one of the subsystems, in an interesting way. 0 into PostgreSQL...\n",
      "Inserting test sample 1364  In recent years, the study of entanglement has become a central topic in quantum physics, where two systems can become entangled and display \"spooky action at a distance.\" However, quantifying entanglement has proven to be a difficult task, with previous methods relying on von Neumann entropy. In this paper, we propose a novel approach that uses coherence as a tool for quantifying entanglement. Specifically, we study the coherence between two subsystems of a larger quantum system, which is found to be a useful measure of entanglement. Using this new approach, we are able to provide a more complete understanding of entanglement in quantum mechanics, which has important implications for quantum information processing and communication. 1 into PostgreSQL...\n",
      "Inserting test sample 1365  Energy beamforming (EB) is a key technique for achieving efficient radio-frequency (RF) transmission enabled wireless energy transfer (WET). By optimally designing the waveforms from multiple energy transmitters (ETs) over the wireless channels, they are constructively combined at the energy receiver (ER) to achieve an EB gain that scales with the number of ETs. However, the optimal design of transmit waveforms requires accurate channel state information (CSI) at the ETs, which is challenging to obtain in practical WET systems. In this paper, we propose a new channel training scheme to achieve optimal EB gain in a distributed WET system, where multiple separated ETs adjust their transmit phases to collaboratively send power to a single ER in an iterative manner, based on one-bit feedback from the ER per training interval which indicates the increase/decrease of the received power level from one particular ET over two preassigned transmit phases. The proposed EB algorithm can be efficiently implemented in practical WET systems even with a large number of distributed ETs, and is analytically shown to converge quickly to the optimal EB design as the number of feedback intervals per ET increases.\n",
      "\n",
      "Numerical results are provided to evaluate the performance of the proposed algorithm as compared to other distributed EB designs. 0 into PostgreSQL...\n",
      "Inserting test sample 1366  In recent years, there has been a growing interest in distributed beamforming techniques for wireless communication networks that can provide increased capacity and coverage. One such technique is distributed energy beamforming, where multiple nodes cooperate to beamform signals towards a desired destination. However, the implementation of distributed energy beamforming in practice requires large amounts of feedback information, which can be challenging to obtain. In this paper, we propose a novel distributed energy beamforming algorithm that utilizes only one bit of feedback per node. Our algorithm employs a quantization scheme and a thresholding strategy to facilitate the exchange of information among the nodes. Furthermore, we analyze the performance of the proposed algorithm and derive the achievable rate region. We also provide numerical simulations to demonstrate the advantages of our algorithm compared to existing ones. Our results show that our algorithm achieves significant performance gains while significantly reducing the feedback overhead. Overall, our proposed distributed energy beamforming algorithm with one-bit feedback can provide a more practical solution for implementing distributed beamforming in wireless communication networks. 1 into PostgreSQL...\n",
      "Inserting test sample 1367  We put forward an adaptive alpha (Type I Error) that decreases as the information grows, for hypothesis tests in which nested linear models are compared. A less elaborate adaptation was already presented in \\citet{PP2014} for comparing general i.i.d. models. In this article we present refined versions to compare nested linear models. This calibration may be interpreted as a Bayes-non-Bayes compromise, of a simple translations of a Bayes Factor on frequentist terms that leads to statistical consistency, and most importantly, it is a step towards statistics that promotes replicable scientific findings. 0 into PostgreSQL...\n",
      "Inserting test sample 1368  Linear models are widely used in various fields, but their results can be affected by data variability and model complexity. This paper proposes a novel approach to increase the replicability of linear models by adapting the significance level for each predictor variable. The method adjusts the level according to the variability of the data, which leads to improved power and control over the false discovery rate. The effectiveness of the approach is demonstrated with simulations and real data examples. The proposed method has the potential to enhance the reliability of linear model analysis and improve scientific reproducibility. 1 into PostgreSQL...\n",
      "Inserting test sample 1369  (abridged) We present a dynamical analysis of the extended stellar stream encircling NGC 1097. Within a statistical framework, we model its surface brightness using mock streams as in Amorisco (2015) and deep imaging data from the CHART32 telescope (Stellar Tidal Stream Survey). We reconstruct the post-infall evolution of the progenitor, which has experienced 3 pericentric passages and lost more than 2 orders of magnitude in mass. At infall, $5.4\\pm0.6$ Gyr ago, the progenitor was a disky dwarf with mass of $\\log_{10}[m(<3.4\\pm1 {\\rm kpc})/ M_\\odot]=10.35\\pm0.25$. We illustrate how the 90$^\\circ$ turn in the stream, identifying the `dog leg', is the signature of the progenitor's prograde rotation. Today, the remnant is a nucleated dwarf, with a LOS velocity of $v_{\\rm p, los}^{\\rm obs}=-30\\pm 30$ kms$^{-1}$, and a luminosity of $3.3\\times 10^7 L_{V,\\odot}$ (Galianni et al. 2010). Our independent analysis predicts $v_{\\rm p, los}=-51^{-17}_{+14}$ kms$^{-1}$, and measures $\\log_{10}(m/ M_\\odot)=7.4^{+0.6}_{-0.8}$, so that the compact nucleus is soon becoming a low-luminosity UCD. We find that NGC 1097 has a mass of $M_{200}=1.8^{+0.5}_{-0.4} \\times 10^{12}\\; M_{\\odot}$, and its concentration $c_{200}=6.7^{+2.4}_{-1.3}$ is in agreement with LCDM. The stream is described almost down to the noise in a spherical host potential, we find this would not be possible if the halo was substantially triaxial at large radii. Its morphology shows that the slope of the total density profile bends from an inner $\\gamma(r_{\\rm peri})=1.5\\pm0.15$. The progenitor's orbit reaches $r_{\\rm apo}=150\\pm 15$ kpc, more than a half of the virial radius of the host, so that, for the first time on an individual extragalactic halo, we measure the outer density slope, $\\gamma(0.6r_{200,c})=3.9\\pm0.5$. This demonstrates the promise of the newborn field of detailed, statistical modelling of extragalactic tidal streams. 0 into PostgreSQL...\n",
      "Inserting test sample 1370  Recent astronomical observations have revealed a remarkable event in a nearby galaxy, NGC 1097. The discovery of a dwarf galaxy dying while in the outskirts of a massive galaxy poses intriguing questions about the dynamics and evolution of galaxies. In this study, we present an autopsy of the kill and the killer in NGC 1097. Utilizing multi-wavelength data from a number of instruments, we investigate the transformation of the dwarf galaxy and the features of the massive galaxy's edge that contributed to its demise. \n",
      "\n",
      "Our observations confirm that the dwarf galaxy was indeed undergoing a dramatic transformation as it approached the massive galaxy. The transformation was characterized by intense star formation and a massive outflow of gas from the dwarf galaxy, likely due to the tidal interaction with the massive galaxy. We also find evidence for the existence of vast clouds of gas and dust bridging the two galaxies together, suggesting a more intimate interaction and exchange of matter.\n",
      "\n",
      "In addition, we examine the properties of the massive galaxy's edge and its possible role in the death of the dwarf galaxy. Our results reveal distinct features at the massive galaxy's edge, including a sharp increase in density and a steep temperature gradient across the interface. Interestingly, we find evidence for a number of shocks propagating along the edge, likely generated by the impact of the outflow from the dwarf galaxy.\n",
      "\n",
      "Overall, our study highlights the complex and dynamic nature of galaxy interactions, especially those involving large galaxies and their smaller companions. The NGC 1097 system provides a unique laboratory for studying the transformation of dwarf galaxies and the processes that shape the morphology and evolution of massive galaxies. Our findings have important implications for our understanding of the formation and evolution of galaxies, and the impact of galaxy interactions on the cosmic ecosystem. 1 into PostgreSQL...\n",
      "Inserting test sample 1371  We prove that the covering radius of an $N$-point subset $X_N$ of the unit sphere $S^d \\subset R^{d+1}$ is bounded above by a power of the worst-case error for equal weight cubature $\\frac{1}{N}\\sum_{\\mathbf{x} \\in X_N}f(\\mathbf{x}) \\approx \\int_{S^d} f \\, \\mathrm{d} \\sigma_d$ for functions in the Sobolev space $\\mathbb{W}_p^s(S^d)$, where $\\sigma_d$ denotes normalized area measure on $S^d.$ These bounds are close to optimal when $s$ is close to $d/p$. Our study of the worst-case error along with results of Brandolini et al. motivate the definition of Quasi-Monte Carlo (QMC) design sequences for $\\mathbb{W}_p^s(S^d)$, which have previously been introduced only in the Hilbert space setting $p=2$. We say that a sequence $(X_N)$ of $N$-point configurations is a QMC-design sequence for $\\mathbb{W}_p^s(S^d)$ with $s > d/p$ provided the worst-case equal weight cubature error for $X_N$ has order $N^{-s/d}$ as $N \\to \\infty$, a property that holds, in particular, for a sequence of spherical $t$-designs in which each design has order $t^d$ points.\n",
      "\n",
      "For the case $p = 1$, we deduce that any QMC-design sequence $(X_N)$ for $\\mathbb{W}_1^s(S^d)$ with $s > d$ has the optimal covering property; i.e., the covering radius of $X_N$ has order $N^{-1/d}$ as $N \\to \\infty$. A significant portion of our effort is devoted to the formulation of the worst-case error in terms of a Bessel kernel, and showing that this kernel satisfies a Bernstein type inequality involving the mesh ratio of $X_N$. As a consequence we prove that any QMC-design sequence for $\\mathbb{W}_p^s(S^d)$ is also a QMC-design sequence for $\\mathbb{W}_{p^\\prime}^s(S^d)$ for all $1 \\leq p < p^\\prime \\leq \\infty$ and, furthermore, if $(X_N)$ is a quasi-uniform QMC-design sequence for $\\mathbb{W}_p^s(S^d)$, then it is also a QMC-design sequence for $\\mathbb{W}_p^{s^\\prime}(S^d)$ for all $s > s^\\prime > d/p$. 0 into PostgreSQL...\n",
      "Inserting test sample 1372  This research paper investigates the covering problem of a sphere by spherical caps. We aim to determine the minimum number of non-overlapping caps required to cover a given sphere with specific radii. To this end, we derive upper and lower bounds for the worst-case error function for equal weight cubature in Sobolev spaces and study its behavior. We also provide a new approach for the covering problem, which combines a partially greedy algorithm with a dynamic programming approach. This approach is proven to be efficient and guaranteed to yield a solution with an error that is within a small constant factor of the optimal solution. \n",
      "\n",
      "To facilitate our study, we present a comprehensive analysis of the relevant mathematical concepts, including Sobolev spaces, spherical harmonics, cubature formulas, and error functions. We also review the relevant literature and compare our new approach to the existing methods proposed in the literature. \n",
      "\n",
      "Our numerical results demonstrate the effectiveness and efficiency of our new approach. We apply our method to various problems, including regularization of inverse problems, numerical integration, and 3D modeling. In particular, our approach leads to significant improvements in the numerical solution of partial differential equations, which is a fundamental tool in many applications in engineering, physics, and finance. \n",
      "\n",
      "Overall, our research makes a significant contribution to the field of geometric covering problems and the theory of numerical analysis in Sobolev spaces. Our results provide a better understanding of the underlying mathematical concepts and offer practical solutions to various applications. The proposed approach is efficient, elegant, and highly versatile, which makes it a promising tool for future research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 1373  Recent progress in the field of ultracold gases has allowed the creation of phase-segregated Bose-Fermi systems. We present a theoretical study of their collective excitations at zero temperature. As the fraction of fermion to boson particle number increases, the collective mode frequencies take values between those for a fully bosonic and those for a fully fermionic cloud, with damping in the intermediate region. This damping is caused by fermions which are resonantly driven at the interface. 0 into PostgreSQL...\n",
      "Inserting test sample 1374  We investigate the collective excitations of trapped phase-segregated Bose-Fermi mixtures in the presence of a quasi-two-dimensional harmonic potential. We show that the Bose and Fermi components exhibit distinct collective modes, including a radial breathing and a quadrupole mode. These excitation modes are evaluated for both weak and strong inter-component interactions, and we identify a crossover between the two regimes. Our theoretical analysis can provide insight into experimentally observed Bose-Fermi systems under similar trapping conditions. 1 into PostgreSQL...\n",
      "Inserting test sample 1375  Blackouts in power grids typically result from cascading failures. The key importance of the electric power grid to society encourages further research into sustaining power system reliability and developing new methods to manage the risks of cascading blackouts. Adequate software tools are required to better analyze, understand, and assess the consequences of the cascading failures. This paper presents MATCASC, an open source MATLAB based tool to analyse cascading failures in power grids. Cascading effects due to line overload outages are considered. The applicability of the MATCASC tool is demonstrated by assessing the robustness of IEEE test systems and real-world power grids with respect to cascading failures. 0 into PostgreSQL...\n",
      "Inserting test sample 1376  MATCASC is a tool designed to assess cascading line outages in power grids. With the increasing demand for energy, power systems face the risk of overloading, leading to line outages. These outages might induce further outages in a cascading manner, leading to blackouts and significant economic loss. The MATCASC tool analyzes such scenarios and provides a comprehensive assessment of the cascading outages, thereby aiding in the decision-making process to avoid cascading failures. The tool uses mathematical models based on the AC power flow equations and incorporates features such as optimal load shedding and fault clearing mechanisms. The tool's effectiveness has been validated on both synthetic test cases and actual power grids. 1 into PostgreSQL...\n",
      "Inserting test sample 1377  The structure and transport properties of SiO2-Al2O3 melts containing 13 mol% and 47 mol% Al2O3 are investigated by means of large scale molecular dynamics computer simulations. The interactions between the atoms are modelled by a pair potential which is a modified version of the one proposed by Kramer et al. [J.\n",
      "\n",
      "Am. Chem. Soc. 64, 6435 (1991)]. Fully equilibrated melts in the temperature range 6000 K >= T > 2000 K are considered as well as glass configurations, that were obtained by a rapid quench from the lowest melt temperatures. Each system is simulated at two different densities in order to study the effect of pressure on structural and dynamic properties. We find that the Al atoms are, like the Si atoms, mainly four-fold coordinated by oxygen. However, the packing of the AlO4 tetrahedra is very different from that of the SiO4 tetrahedra, which is reflected by the presence of triclusters (O atoms surrounded by three cations) and edge--sharing AlO4 tetrahedra. On larger length scales, a micro-segregation occurs, resulting in an Al-rich network percolating through the Si-O network. This is reflected in a prepeak of concentration-concentration structure factors around 0.5 A^-1 (both in the system with 47 mol% and 13 mol% Al2O3!). We also address the interplay between structure and mass transport. To this end, the behavior of the selfdiffusion constants for the different compositions and densities is studied. 0 into PostgreSQL...\n",
      "Inserting test sample 1378  This research paper presents a comprehensive investigation of the structural and transport properties of amorphous aluminum silicates (AAS) via computer simulation approaches. In particular, we focus on analyzing the structural features and the ionic diffusion behavior of AAS for a better understanding of their applications in a wide range of fields, including catalysts, ion conductors, and semiconductors. To achieve this, we perform extensive molecular dynamics simulations on different compositions and morphologies of AAS systems. Our results reveal that the structural properties of AAS are strongly dependent on their chemical composition, with the coordination numbers of the Al and Si atoms playing a crucial role in determining their local structures. Furthermore, we find that the diffusion coefficients of ions in AAS follow the Vogel-Tamman-Fulcher (VTF) equation, suggesting the presence of a fragile glassy behavior for these systems. Our study provides a fundamental understanding of the structural and transport properties of AAS and highlights the importance of computer simulation approaches in exploring the properties of disordered materials. The findings presented in this work can be used to optimize the design and properties of amorphous aluminum silicates for various technological applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1379  The notion of degree-constrained spanning hierarchies, also called k-trails, was recently introduced in the context of network routing problems. They describe graphs that are homomorphic images of connected graphs of degree at most k. First results highlight several interesting advantages of k-trails compared to previous routing approaches. However, so far, only little is known regarding computational aspects of k-trails.\n",
      "\n",
      "In this work we aim to fill this gap by presenting how k-trails can be analyzed using techniques from algorithmic matroid theory. Exploiting this connection, we resolve several open questions about k-trails. In particular, we show that one can recognize efficiently whether a graph is a k-trail.\n",
      "\n",
      "Furthermore, we show that deciding whether a graph contains a k-trail is NP-complete; however, every graph that contains a k-trail is a (k+1)-trail.\n",
      "\n",
      "Moreover, further leveraging the connection to matroids, we consider the problem of finding a minimum weight k-trail contained in a graph G. We show that one can efficiently find a (2k-1)-trail contained in G whose weight is no more than the cheapest k-trail contained in G, even when allowing negative weights.\n",
      "\n",
      "The above results settle several open questions raised by Molnar, Newman, and Sebo. 0 into PostgreSQL...\n",
      "Inserting test sample 1380  In this paper, we study a new concept of paths in a graph called k-trails, and provide insights into their recognition, complexity and approximations. A k-trail is a path in a graph that visits at most k nodes more than once, allowing for revisiting of nodes. We first show that recognizing k-trails is NP-hard, even when k = 2 and the graph is planar. We then prove the existence of polynomial-time constant-factor approximation algorithms for finding the longest k-trail in a given graph. Further, we analyze the quality of different heuristic algorithms for this problem. Specifically, we propose a 2-approximation algorithm that runs in O(n^3) time, as well as a heuristics based on the metric closure of the graph and show that their performance strongly depends on the structure of the input graph. Finally, we present experimental results on various real-world and synthetic graphs to evaluate the performance of the proposed algorithms and heuristics. Our findings suggest that k-trails may find practical applications in network routing, as well as in other optimization problems on graphs. 1 into PostgreSQL...\n",
      "Inserting test sample 1381  This report discusses two new ideas for using perturbation methods to solve the time-independent Schr\\\"odinger equation. The first concept begins with rewriting the perturbation equations in a form that is closely related to matrix diagonalization methods. That allows a simple, compact derivation of the standard (Rayleigh-Schr\\\"odinger) equations. But it also leads to a new iterative solution method that is not based on the usual power series expansion. The iterative method can also be used when the unperturbed system is two-fold degenerate. The second concept is quite different from the first but compatible with it. It is based on the fact that one can replace the true Hamiltonian with a synthetic Hamiltonian having the same eigenvalues. This approach allows one to cancel part of the perturbation and to reduce the size of the off-diagonal matrix elements, giving better convergence of the series.\n",
      "\n",
      "These methods are illustrated by application to three perturbed harmonic oscillator problems--a 1-D oscillator with a linear perturbation, a 1-D oscillator with a quartic perturbation, and a perturbed 2-D oscillator, which involves degenerate states. The iterative method and the synthetic Hamiltonian method are shown to give significantly better results than the standard method. 0 into PostgreSQL...\n",
      "Inserting test sample 1382  Time-independent quantum-mechanical perturbation theory has been extensively studied for decades in the field of quantum mechanics. In this paper, we introduce new developments in this theory which provide a more accurate and comprehensive understanding of quantum systems. We present a detailed analysis of the mathematical foundations of the perturbation theory that we propose and demonstrate its effectiveness through various simulations.\n",
      "\n",
      "Our approach involves the use of high-order perturbation theory, which allows us to capture higher-order corrections to the unperturbed Hamiltonian. We also incorporate the concept of adiabatic perturbation theory, which is especially useful in situations where the perturbed system is continuously varying with time.\n",
      "\n",
      "Moreover, we showcase our findings by presenting test cases that illustrate the improved accuracy of the new perturbation theory. Our results show that the new approach not only leads to more precise predictions but also requires much less computational resources, making it a highly efficient alternative to existing methods.\n",
      "\n",
      "Overall, our proposed developments in time-independent quantum-mechanical perturbation theory provide a significant improvement in understanding quantum systems. By taking into account higher-order corrections and incorporating adiabatic perturbation theory, we are able to predict the behavior of complex quantum systems with greater accuracy and efficiency. 1 into PostgreSQL...\n",
      "Inserting test sample 1383  We analyze possible motion control of microparticles by means of external electromagnetic fields which induce potential wells having fixed spatial distribution but deepening over time up to some limit. It is assumed that given particles are under conditions of the high vacuum and forces acting on these particles are not dissipative. We have established slowdown of comparatively fast particles as a result of their transit through considered potential wells.\n",
      "\n",
      "This process is demonstrated on example of the nonresonance laser beam with the intensity amplifying over time. More detailed research of particle slowdown in such electromagnetic fields is carried out on the basis of simple analytical relationships obtained from basic equations of classical mechanics for the model of the one-dimensional rectangular potential well deepening over time.\n",
      "\n",
      "Method for cooling of particles, demonstrated in the present work, may be applied for essential increase of spectroscopy resolution of various microparticles, including in definite cases also atoms and molecules in the ground quantum state. 0 into PostgreSQL...\n",
      "Inserting test sample 1384  This paper investigates the behavior of microparticles in an electromagnetic potential well that deepens over time. We show that by controlling the electric and magnetic fields, we can create a trap that slows down the motion of the microparticles. The potential well is created using a combination of time-varying magnetic and electric fields, and at specific intervals, the potential well becomes deeper. We demonstrate that the microparticles' velocity decreases as the depth of the potential well increases. Furthermore, we show that the size of the microparticles affects their behavior within the potential well. Smaller particles are more significantly affected by the potential well's changes in depth and experience a more pronounced slowdown than larger particles. Our findings suggest that our approach, which uses electromagnetic fields to trap and slow microparticles, can have potential applications in various fields, including material science, physics, and chemistry. 1 into PostgreSQL...\n",
      "Inserting test sample 1385  This manuscript is devoted to investigate Bianchi Type $I$ universe in the context of $f(R,T)$ gravity. For this purpose, we explore the exact solutions of locally rotationally symmetric Bianchi type $I$ spacetime. The modified field equations are solved by assuming expansion scalar $\\theta$ proportional to shear scalar $\\sigma$ which gives $A=B^n$, where $A,\\,B$ are the metric coefficients and $n$ is an arbitrary constant. In particular, three solutions have been found and physical quantities are calculated in each case. 0 into PostgreSQL...\n",
      "Inserting test sample 1386  In this study, we investigate the implications of $f(R,T)$ gravity on a Locally Rotationally Symmetric Bianchi Type $I$ Cosmology. We obtain the equations of motion for the gravitational field and matter content and solve them using appropriate techniques. Our analysis shows that the $f(R,T)$ gravity theory exhibits significant differences from standard General Relativity in the early Universe. We discuss the implications of our results for understanding the early Universe and present potential observational tests that could be performed to verify or falsify our findings. 1 into PostgreSQL...\n",
      "Inserting test sample 1387  Some years ago, Radzikowski has found a characterization of Hadamard states for scalar quantum fields on a four-dimensional globally hyperbolic spacetime in terms of a specific form of the wavefront set of their two-point functions (termed `wavefront set spectrum condition'), thereby initiating a major progress in the understanding of Hadamard states and the further development of quantum field theory in curved spacetime. In the present work, we extend this important result on the equivalence of the wavefront set spectrum condition with the Hadamard condition from scalar fields to vector fields (sections in a vector bundle) which are subject to a wave-equation and are quantized so as to fulfill the covariant canonical commutation relations, or which obey a Dirac equation and are quantized according to the covariant anti-commutation relations, in any globally hyperbolic spacetime having dimension three or higher. In proving this result, a gap which is present in the published proof for the scalar field case will be removed. Moreover we determine the short-distance scaling limits of Hadamard states for vector-bundle valued fields, finding them to coincide with the corresponding flat-space, massless vacuum states. 0 into PostgreSQL...\n",
      "Inserting test sample 1388  This paper presents a study of vector-valued quantum fields on curved spacetime, focusing in particular on the microlocal spectrum condition and its relation with the Hadamard form. We begin by presenting the general framework of the theory of quantum fields on curved backgrounds, emphasizing the role played by microlocal analysis in the study of their singularities. We then turn to the microlocal spectrum condition, which imposes a restriction on the distribution of singularities of quantum fields. We provide a detailed analysis of this condition for vector-valued fields, and show that it is closely related to the notion of a Hadamard state. More precisely, we show that if a vector-valued field satisfies the microlocal spectrum condition, then it admits a unique Hadamard representation. Conversely, every field for which a Hadamard representation exists satisfies the microlocal spectrum condition. This result has important implications for the study of the behavior of quantum fields at high frequencies, and sheds light on the structure of the vacuum state of these fields on curved spacetime. 1 into PostgreSQL...\n",
      "Inserting test sample 1389  It is known that the minimum broadcast rate of a linear index code over $\\mathbb{F}_q$ is equal to the $minrank_q$ of the underlying digraph. In [3] it is proved that for $\\mathbb{F}_2$ and any positive integer $k$, $minrank_q(G)\\leq k$ iff there exists a homomorphism from the complement of the graph $G$ to the complement of a particular undirected graph family called \"graph family $\\{G_k\\}$\". As observed in [2], by combining these two results one can relate the linear index coding problem of undirected graphs to the graph homomorphism problem. In [4], a direct connection between linear index coding problem and graph homomorphism problem is introduced. In contrast to the former approach, the direct connection holds for digraphs as well and applies to any field size. More precisely, in [4], a graph family $\\{H_k^q\\}$ is introduced and shown that whether or not the scalar linear index of a digraph $G$ is less than or equal to $k$ is equivalent to the existence of a graph homomorphism from the complement of $G$ to the complement of $H_k^q$.\n",
      "\n",
      "Here, we first study the structure of the digraphs $H_k^q$. Analogous to the result of [2] about undirected graphs, we prove that $H_k^q$'s are vertex transitive digraphs. Using this, and by applying a lemma of Hell and Nesetril [5], we derive a class of necessary conditions for digraphs $G$ to satisfy $lind_q(G)\\leq k$. Particularly, we obtain new lower bounds on $lind_q(G)$.\n",
      "\n",
      "Our next result is about the computational complexity of scalar linear index of a digraph. It is known that deciding whether the scalar linear index of an undirected graph is equal to $k$ or not is NP-complete for $k\\ge 3$ and is polynomially decidable for $k=1,2$ [3]. For digraphs, it is shown in [6] that for the binary alphabet, the decision problem for $k=2$ is NP-complete. We use graph homomorphism framework to extend this result to arbitrary alphabet. 0 into PostgreSQL...\n",
      "Inserting test sample 1390  Linear index coding through graph homomorphism is a relatively new communication paradigm that has attracted considerable attention among researchers and practitioners in the field of telecommunications and networking. The technique involves efficient transmitting of data from a single source to multiple destinations. In this paper, we investigate some of the theoretical properties of linear index coding through graph homomorphism and its practical applications in current communication networks.\n",
      "\n",
      "Utilizing a graph-theoretic approach provides a powerful framework for addressing the challenges of designing and implementing index coding schemes. We explore how graph isomorphism can be used effectively in index coding through the use of linear codes that offer a balance between computation and communication costs. We also present a detailed analysis of the complexity of index coding through graph homomorphism, which shows that this technique is generally more efficient than the traditional linear code based techniques.\n",
      "\n",
      "Furthermore, through the use of a series of simulations, we demonstrate the effectiveness of index coding based on graph homomorphism, which outperforms existing state-of-the-art techniques in various ways, including reducing transmission overhead and improving network throughput.\n",
      "\n",
      "Our experimental results suggest that index coding through graph homomorphism offers a promising future for optimizing data transmission in large-scale networks. An in-depth understanding of the theoretical properties and design of index coding through graph homomorphism provides the basis for building efficient and scalable communication systems.\n",
      "\n",
      "In this paper, we have provided a detailed overview of linear index coding through graph homomorphism, demonstrating its practical applications in modern communication systems. Our results highlight the effectiveness and efficiency of this novel technique, and provide a foundation for future research in this area. The potential benefits of this approach include enhancing network performance, minimizing energy consumption, and enabling faster and more reliable communications. 1 into PostgreSQL...\n",
      "Inserting test sample 1391  To develop, calibrate and/or validate continuum models from experimental or numerical data, micro-macro transition methods are required. These methods are used to obtain the continuum fields (such as density, momentum, stress) from the discrete data (positions, velocities, forces). This is especially challenging for non-uniform and dynamic situations in the presence of multiple components. Here, we present a general method to perform this micro-macro transition, but for simplicity we restrict our attention to two-component scenarios, e.g. particulate mixtures containing two types of particles. We present an extension to the micro-macro transition method, called \\emph{coarse-graining}, for unsteady two-component flows. By construction, this novel averaging method is advantageous, e.g. when compared to binning methods, because the obtained macroscopic fields are consistent with the continuum equations of mass, momentum, and energy balance. Additionally, boundary interaction forces can be taken into account in a self-consistent way and thus allow for the construction of continuous stress fields even within one particle radius of the boundaries. Similarly, stress and drag forces can also be determined for individual constituents of a multi-component mixture, which is essential for several continuum applications, \\textit{e.g.} mixture theory segregation models. Moreover, the method does not require ensemble-averaging and thus can be efficiently exploited to investigate static, steady, and time-dependent flows. The method presented in this paper is valid for any discrete data, \\textit{e.g.} particle simulations, molecular dynamics, experimental data, etc. 0 into PostgreSQL...\n",
      "Inserting test sample 1392  The continuum field theory plays a vital role in the theoretical modeling of various physical phenomena by describing continuous variables such as temperature and density. However, when it comes to granular materials, discrete methods are frequently used to model the evolution of systems consisting of individual particles, which is fundamentally different from continuum fields. In this research work, we present an extension of the continuum field approach to analyze bidisperse systems, i.e., systems made of two different particle sizes. This extension is achieved by introducing a set of equations that describe the cross-coupling terms between different particle sizes, enabling us to simulate bidisperse materials through the continuum framework. The validity and accuracy of this new model were verified through direct comparisons with available experimental data and discrete element simulations. We observed that the presented continuum model successfully reproduces key features of the observed dynamics and identifies the mechanisms responsible for bidispersity-induced segregation in granular materials. Our results demonstrate that the continuum framework can be extended to multi-component systems, providing a powerful tool in the analysis of complex materials. This extension of the continuum theory is expected to have broad applications in the modeling and design of industrial processes involving bidisperse granular materials. 1 into PostgreSQL...\n",
      "Inserting test sample 1393  We demonstrate an efficient double-layer light absorber by exciting plasmonic phase resonances. We show that the addition of grooves can cause mode splitting of the plasmonic waveguide cavity modes and all the new resonant modes exhibit large absorptivity greater than 90%. Some of the generated absorption peaks have wide-angle characteristics. Furthermore, we find that the proposed structure is fairly insensitive to the alignment error between different layers. The proposed plasmonic nano-structure designs may have exciting potential applications in thin film solar cells, thermal emitters, novel infrared detectors, and highly sensitive bio-sensors. 0 into PostgreSQL...\n",
      "Inserting test sample 1394  This study presents an investigation into the use of plasmonic phase resonances to create thin film absorbers. A plasmonic phase resonance occurs when an incident light wave couples with the structure of a material, creating an oscillating electric field. By carefully tuning the geometry and composition of the absorber layer, absorption can be maximized through phase matching. Through simulations and experimental measurements, we demonstrate that our proposed thin film absorbers show strong absorption across a wide range of incident angles and wavelengths, paving the way for novel applications in photovoltaics, sensing and imaging. 1 into PostgreSQL...\n",
      "Inserting test sample 1395  This paper introduces an effective processing framework nominated ICP (Image Cloud Processing) to powerfully cope with the data explosion in image processing field. While most previous researches focus on optimizing the image processing algorithms to gain higher efficiency, our work dedicates to providing a general framework for those image processing algorithms, which can be implemented in parallel so as to achieve a boost in time efficiency without compromising the results performance along with the increasing image scale. The proposed ICP framework consists of two mechanisms, i.e. SICP (Static ICP) and DICP (Dynamic ICP). Specifically, SICP is aimed at processing the big image data pre-stored in the distributed system, while DICP is proposed for dynamic input. To accomplish SICP, two novel data representations named P-Image and Big-Image are designed to cooperate with MapReduce to achieve more optimized configuration and higher efficiency. DICP is implemented through a parallel processing procedure working with the traditional processing mechanism of the distributed system. Representative results of comprehensive experiments on the challenging ImageNet dataset are selected to validate the capacity of our proposed ICP framework over the traditional state-of-the-art methods, both in time efficiency and quality of results. 0 into PostgreSQL...\n",
      "Inserting test sample 1396  In recent years, the amount of high-resolution image data generated has increased exponentially. The sheer size of this data has necessitated the exploration of new processing frameworks capable of handling such big data. In this study, we propose a hierarchical distributed processing framework designed for big image data. Our framework consists of three main layers: the data layer, the task layer, and the resource layer. Each layer is responsible for distinct functions, such as data storage, task management, and resource allocation. This hierarchical structure enables efficient processing of large-scale image data across multiple computing nodes. Additionally, our framework incorporates advanced machine learning algorithms to enhance the quality and speed of processing. Through a series of experiments, we demonstrate that our hierarchical distributed processing framework is reliable, efficient, and scalable for handling big image data. Our framework has the potential to provide significant benefits across a wide range of applications, including medical imaging, remote sensing, and industrial imaging. 1 into PostgreSQL...\n",
      "Inserting test sample 1397  We study the geometric properties of the terms of the Goldman bracket between two free homotopy classes of oriented closed curves in a hyperbolic surface. We provide an obstruction for the equality of two terms in the Goldman bracket, namely if two terms in the Goldman bracket are equal to each other then for every hyperbolic metric, the angles corresponding to the intersection points are equal to each other. As a consequence, we obtain an alternative proof of a theorem of Chas, i.e. if one of the free homotopy classes contains a simple representative then the geometric intersection number and the number of terms (counted with multiplicity) in the Goldman bracket are the same. 0 into PostgreSQL...\n",
      "Inserting test sample 1398  This paper explores the properties of hyperbolic metrics in the context of intersecting geodesics. We consider the problem of determining whether, for any given hyperbolic metric, intersecting geodesics will always form equal angles. Using a combination of geometric and analytic techniques, we prove that this is indeed the case for every hyperbolic metric. Our approach draws on a range of results from differential geometry, topology, and functional analysis. We also provide a detailed discussion of the implications of our findings for studying the geometric properties of hyperbolic manifolds. Our results are likely to have a significant impact on the development of new models for hyperbolic space. 1 into PostgreSQL...\n",
      "Inserting test sample 1399  Our goal was to characterize the distant gaseous and dust activity of comet C\\2012 S1 (ISON), inbound, from observations of H2O, CO and the dust coma in the far-infrared and submillimeter domains. In this paper, we report observations undertaken with the Herschel Space Observatory on 8 & 13 March 2013 (rh = 4.54 - 4.47AU) and with the 30m telescope of Institut de Radioastronomie Millim\\'etrique (IRAM) in March and April 2013 (rh = 4.45 - 4.18 AU). The HIFI instrument aboard Herschel was used to observe the H$_{2}$O $1_{10}-1_{01}$ line at 557 GHz, whereas images of the dust coma at 70 and 160 {\\mu}m were acquired with the PACS instrument. Spectra acquired at the IRAM 30m telescope cover the CO J(2-1) line at 230.5 GHz. The spectral observations were analysed with excitation and radiative transfer models. A model of dust thermal emission taking into account a range of dust sizes is used to analyse the PACS maps. While H$_{2}$O was not detected in our 8 March 2013 observation, we derive a sensitive 3 $\\sigma$ upper limit of QH$_{2}$O < 3.5 x 10$^{26}$ molecules/s for this date. A marginal 3.2 $\\sigma$ detection of CO is found, corresponding to a CO production rate of QCO = 3.5 x 10$^{27}$ molecules/s. The Herschel PACS measurements show a clear detection of the coma and tail in both the 70 {\\mu}m and 160 {\\mu}m maps. Under the assumption of a 2 km radius nucleus, we infer dust production rates in the range 10 - 13 kg/s or 40 - 70 kg/s depending on whether a low or high gaseous activity from the nucleus surface is assumed. We constrain the size distribution of the emitted dust by comparing PACS 70 and 160 {\\mu}m data, and considering optical data. Size indices between -4 and -3.6 are suggested. The morphology of the tail observed on 70 {\\mu}m images can be explained by the presence of grains with ages older than 60 days. 0 into PostgreSQL...\n",
      "Inserting test sample 1400  The distant Comet C/2012 S1 (ISON) was studied via observations conducted with both the Herschel Space Observatory and IRAM-30m at a heliocentric distance of 4.5 AU. Of particular interest was the presence of molecular gases in the comet's coma, and the potential mechanisms driving their outgassing activities. Herschel's far-infrared spectral coverage allowed for the detection of CN, an active, long-lived comet molecule, as well as a number of other key volatiles, such as HCN and CO. Meanwhile, IRAM-30m provided higher spatial resolution and a higher sensitivity to some of these species. These molecule detections were used in turn to determine relative abundances and constrain the isotopic composition of the volatiles in the coma. \n",
      "\n",
      "One potential explanation for the detected outgassing activity is the sublimation of ices from the nucleus's surface, with volatiles such as water and carbon dioxide playing key roles. Alternatively, a recent impact event may have contributed significantly to the observed activity, though no such event was evident in the available data. The presence of dust in the coma was also investigated via Herschel's PACS and SPIRE instruments, and constraints on the sizes and properties of the dust particles were inferred through detailed modeling. \n",
      "\n",
      "The observations presented in this paper represent a unique dataset for a comet at such a large heliocentric distance (4.5 AU), and the detection of CN is particularly noteworthy. Furthermore, the combination of Herschel and IRAM-30m data provides complementary insights into the coma's molecular composition, and the inferred mechanisms for outgassing. However, while the observations have strict spatial constraints, they are also limited by their temporal coverage, with no measurements obtained after the comet's closest approach to the Sun. Future observations targeting similar objects will benefit from both spatial and temporal coverage. Ultimately, these results contribute to our understanding of comet nuclei, their long-term behavior, and the evolution of the Solar System as a whole. 1 into PostgreSQL...\n",
      "Inserting test sample 1401  We show that optical pumping of electron spins in individual InGaAs quantum dots leads to strong nuclear polarisation that we measure via the Overhauser shift (OHS) in magneto-photoluminescence experiments between 0 and 4T. We find a strongly non-monotonous dependence of the OHS on the applied magnetic field, with a maximum nuclear polarisation of 40% for intermediate magnetic fields. We observe that the OHS is larger for nuclear fields anti-parallel to the external field than in the parallel configuration. A bistability in the dependence of the OHS on the spin polarization of the optically injected electrons is found.\n",
      "\n",
      "All our findings are qualitatively understood with a model based on a simple perturbative approach. 0 into PostgreSQL...\n",
      "Inserting test sample 1402  This study investigates the bistability of nuclear polarisation achieved through optical pumping in InGaAs Quantum Dots. The findings provide insights into the physics of carrier dynamics in nanostructures and the mechanisms behind optical control in semiconductors. The experiment used a combination of polarization-resolved photoluminescence spectroscopy and time-resolved Kerr rotation measurements to explore the dynamics of nuclear polarisation under different excitation conditions. The results demonstrate the occurrence of bistability at low excitation densities, while at high densities, the polarisation becomes monostable. These findings provide significant implications in the development of new technologies such as quantum information processing and spin-based electronics. The study concludes by discussing potential future research directions and applications based on the obtained results. 1 into PostgreSQL...\n",
      "Inserting test sample 1403  Alzheimer's Disease (AD) ravages the cognitive ability of more than 5 million Americans and creates an enormous strain on the health care system. This paper proposes a machine learning predictive model for AD development without medical imaging and with fewer clinical visits and tests, in hopes of earlier and cheaper diagnoses. That earlier diagnoses could be critical in the effectiveness of any drug or medical treatment to cure this disease. Our model is trained and validated using demographic, biomarker and cognitive test data from two prominent research studies: Alzheimer's Disease Neuroimaging Initiative (ADNI) and Australian Imaging, Biomarker Lifestyle Flagship Study of Aging (AIBL). We systematically explore different machine learning models, pre-processing methods and feature selection techniques. The most performant model demonstrates greater than 90% accuracy and recall in predicting AD, and the results generalize across sub-studies of ADNI and to the independent AIBL study. We also demonstrate that these results are robust to reducing the number of clinical visits or tests per visit. Using a metaclassification algorithm and longitudinal data analysis we are able to produce a \"lean\" diagnostic protocol with only 3 tests and 4 clinical visits that can predict Alzheimer's development with 87% accuracy and 79% recall. This novel work can be adapted into a practical early diagnostic tool for predicting the development of Alzheimer's that maximizes accuracy while minimizing the number of necessary diagnostic tests and clinical visits. 0 into PostgreSQL...\n",
      "Inserting test sample 1404  Alzheimer's disease (AD) is a progressive neurodegenerative disorder that affects millions of people worldwide, and early detection is crucial for effective treatment. In this study, we propose a machine learning-based approach to accurately predict the risk of AD, enabling early diagnosis and treatment. Our model was trained and tested on a large dataset of clinical and demographic information from multiple sources. Advanced machine learning algorithms were used to identify patterns and key features associated with AD risk. The results demonstrate that our model achieved high accuracy and reliability in predicting the likelihood of AD, offering a practical solution for early diagnostics.\n",
      "\n",
      "The key advantage of our approach is its accuracy and speed, which enables real-time diagnosis and personalized treatment plans. Our model can be easily integrated into existing healthcare systems, providing a cost-effective and scalable solution for detecting AD. Our findings also shed light on the underlying mechanisms of AD and provide new insights into its pathogenesis. Furthermore, our model can be extended to other neurodegenerative disorders and used to develop more effective treatments.\n",
      "\n",
      "In conclusion, our study presents a promising machine learning-based approach for predicting AD risk that can revolutionize early diagnostics and treatment. Our results demonstrate the potential of this approach to improve patient outcomes and reduce healthcare costs. Further research is needed to validate our findings and optimize the model's performance in real-world settings. 1 into PostgreSQL...\n",
      "Inserting test sample 1405  Let $(\\{X_i(t)\\}_{i\\in \\mathbb{Z}^d})_{t\\geq 0}$ be the system of interacting diffusions on $[0,\\infty)$ defined by the following collection of coupled stochastic differential equations: \\begin{eqnarray}dX_i(t)=\\sum\\limits_{j\\in \\mathbb{Z}^d}a(i,j)[X_j(t)-X_i(t)] dt+\\sqrt{bX_i(t)^2} dW_i(t), \\eqntext{i\\in \\mathbb{Z}^d,t\\geq 0.}\\end{eqnarray} Here, $a(\\cdot,\\cdot)$ is an irreducible random walk transition kernel on $\\mathbb{Z}^d\\times \\mathbb{Z}^d$, $b\\in (0,\\infty)$ is a diffusion parameter, and $(\\{W_i(t)\\}_{i\\in \\mathbb{Z}^d})_{t\\geq 0}$ is a collection of independent standard Brownian motions on $\\mathbb{R}$. The initial condition is chosen such that $\\{X_i(0)\\}_{i\\in \\mathbb{Z}^d}$ is a shift-invariant and shift-ergodic random field on $[0,\\infty)$ with mean $\\Theta\\in (0,\\infty)$ (the evolution preserves the mean). We show that the long-time behavior of this system is the result of a delicate interplay between $a(\\cdot,\\cdot)$ and $b$, in contrast to systems where the diffusion function is subquadratic. In particular, let $\\hat{a}(i,j)={1/2}[a(i,j)+a(j,i)]$, $i,j\\in \\mathbb{Z}^d$, denote the symmetrized transition kernel. We show that: (A) If $\\hat{a}(\\cdot,\\cdot)$ is recurrent, then for any $b>0$ the system locally dies out. (B) If $\\hat{a}(\\cdot,\\cdot)$ is transient, then there exist $b_*\\geq b_2>0$ such that: (B1)d The system converges to an equilibrium $\\nu_{\\Theta}$ (with mean $\\Theta$) if $0<b<b_*$. (B2) The system locally dies out if $b>b_*$. (B3) $\\nu_{\\Theta}$ has a finite 2nd moment if and only if $0<b<b_2$. (B4) The 2nd moment diverges exponentially fast if and only if $b>b_2$. The equilibrium $\\nu_{\\Theta}$ is shown to be associated and mixing for all $0<b<b_*$. We argue in favor of the conjecture that $b_*>b_2$. We further conjecture that the system locally dies out at $b=b_*$. For the case where $a(\\cdot,\\cdot)$ is symmetric and transient we further show that: (C) There exists a sequence $b_2\\geq b_3\\geq b_4\\geq ... >0$ such that: (C1) $\\nu_{\\Theta}$ has a finite $m$th moment if and only if $0<b<b_m$. (C2) The $m$th moment diverges exponentially fast if and only if $b>b_m$. (C3) $b_2\\leq (m-1)b_m<2$. uad(C4) $\\lim_{m\\to\\infty}(m-1)b_m=c=\\sup_{m\\geq 2}(m-1)b_m$. The proof of these results is based on self-duality and on a representation formula through which the moments of the components are related to exponential moments of the collision local time of random walks. Via large deviation theory, the latter lead to variational expressions for $b_*$ and the $b_m$'s, from which sharp bounds are deduced. The critical value $b_*$ arises from a stochastic representation of the Palm distribution of the system. The special case where $a(\\cdot,\\cdot)$ is simple random walk is commonly referred to as the parabolic Anderson model with Brownian noise. This case was studied in the memoir by Carmona and Molchanov [Parabolic Anderson Problem and Intermittency (1994) Amer. Math. Soc., Providence, RI], where part of our results were already established. 0 into PostgreSQL...\n",
      "Inserting test sample 1406  This research paper investigates phase transitions in the long-time behavior of interacting diffusions. Alongside phase transitions in the Ising model, the authors demonstrate the existence of phase transitions for interacting diffusions in a bounded domain. Specifically, the authors study the asymptotic behavior of the occupation time near the boundary of the domain and the formation of persistent macroscopic clusters. For this purpose, they use representation formulas for occupation times and establish the asymptotics of the same. The authors derive phase diagrams based on the occupation time, and these phase diagrams are quite different from the traditional ones for the Ising model. Furthermore, the authors show that the phase transition for interacting diffusions is much richer than that for the Ising model, as it exhibits a discontinuity phenomenon. They discuss the origin of this discontinuity phenomenon and describe how it arises from a subtle interplay between the sub-diffusive nature of the diffusion process and the interaction among particles.\n",
      "\n",
      "The authors conduct simulations to verify their analytical results and study the long-time behavior of interacting Brownian particles in a bounded domain. They provide numerical evidence of the existence of multiple phases for the occupation time near the boundary and demonstrate the discontinuity phenomenon of the phase transition. They also observe the emergence of macroscopic clusters in numerical simulations and show that they are responsible for the mentioned discontinuity.\n",
      "\n",
      "In conclusion, the findings of this research paper demonstrate that the long-time behavior of interacting diffusions exhibits phase transitions that are significantly different from those in the Ising model. The authors establish the existence of a discontinuity phenomenon that is a result of subtle interactions between the sub-diffusive nature of the diffusion process and the interaction among particles. They provide rigorous mathematical proofs and numerical simulations to support their claims. The authors' results have implications in diverse areas such as population genetics, statistical physics, and materials science. 1 into PostgreSQL...\n",
      "Inserting test sample 1407  Early measurements of SN 1987A indicate a beam/jet (BJ) which hit polar ejecta (PE) to produce the \"Mystery Spot\" (MS). The SN flash takes an extra 8 d to hit the MS, and this was confirmed at 2e39 ergs/s in the optical at day 8. A ramp in luminosity starting near day 10 indicates particles from the BJ hitting the PE, with the fastest particles traveling at 0.8 c, and an upper limit for the optical luminosity of the MS of 5e40 ergs/s at day 20. The details of SN 1987A strongly suggest that it resulted from a merger of 2 stellar cores of a common envelope (CE) binary, i.e. a \"double-degenerate\" (DD)-initiated SN, and is thus the Rosetta Stone for 99% of MSPs in the non-core-collapsed globular clusters, GRBs, and SNe, including all recent nearby SNe except SN 1986J and the more distant SN 2006gy. Without having to blast through the CE of Sk -69 202, it is likely that the BJ would have caused a full, long-soft gamma-ray burst (lGRB) upon hitting the PE, thus DD can produce lGRBs. The typical 0.5 deg collimation of a GRB, over the 22 lt-d from SN 1987A to its PE, produces ~100 s of delay, MATCHING the observed delay of the non-prompt parts of lGRBs.\n",
      "\n",
      "Because DD must be the dominant SN mechanism in elliptical galaxies, where only short, hard GRBs (sGRBs) have been observed, DD without CE or PE must also make sGRBs, thus the initial photon spectrum of 99% of ALL GRBs is KNOWN, & neutron star (NS)-NS mergers may not make GRBs as we know them. Observations of Ia's strongly suggest that these are also DD, implying another systematic effect in Ia Cosmology, as Ia's will appear to be Ic's when viewed from their DD merger poles, given sufficient matter above that lost to core- collapse (otherwise it would just beg the question of what ELSE they could possibly be). There is no need to invent exotica, such as collapsars or hypernovae, to account for GRBs. 0 into PostgreSQL...\n",
      "Inserting test sample 1408  The supernova (SN) 1987A has been an object of intense study for decades, thanks to its proximity and significant observational characteristics. In this paper, we explore the link between SN 1987A and gamma-ray bursts (GRBs), which are among the most energetic phenomena observed in the universe. While GRBs are thought to arise from the collapse of massive stars or mergers of compact objects, the exact mechanisms behind them are still not fully understood. SN 1987A, on the other hand, is a well-documented supernova that occurred in the Large Magellanic Cloud, a dwarf galaxy close to our own Milky Way. Through an analysis of observations made of SN 1987A, we aim to shed light on the possible connection between these two fascinating phenomena.\n",
      "\n",
      "Several lines of evidence suggest a possible link between SN 1987A and GRBs. First, both events involve the collapse of massive stars and the release of huge amounts of energy. Second, some observations of SN 1987A reveal the presence of high-speed jets of material, which are also characteristic of GRBs. Third, recent studies have shown that many GRBs are associated with supernovae, indicating a possible connection between the two.\n",
      "\n",
      "To investigate this connection further, we analyze a wide range of observations made of SN 1987A, including data from telescopes and other instruments across the electromagnetic spectrum. We find that several characteristics of this supernova are consistent with the formation of a GRB, including the presence of a compact object at the center of the explosion and evidence of jet-like outflows. While further study is needed to confirm the link between SN 1987A and GRBs, our findings suggest that these two events may be more closely related than previously thought.\n",
      "\n",
      "Overall, our study highlights the importance of continued research into supernovae and GRBs, both for understanding the nature of these phenomena and for advancing our knowledge of the universe more broadly. With new telescopes and observing techniques being developed all the time, we can expect to make further strides in this field in the years to come. 1 into PostgreSQL...\n",
      "Inserting test sample 1409  The flow behavior of blood in microvessels is directly associated with tissue perfusion and oxygen delivery. Current efforts on modeling blood flow have primarily focused on the flow properties of blood with red blood cells (RBCs) having a viscosity ratio $C$ of unity between the cytosol and suspending medium, while under physiological conditions the cytosol viscosity is about five times larger than the plasma viscosity (i.e., $C\\approx 5$). The importance of $C$ for the behavior of single RBCs in fluid flow has already been demonstrated, while the effect of $C$ on blood flow has only been sparsely studied. We employ mesoscopic hydrodynamic simulations to perform a systematic investigation of flow properties of RBC suspensions with different cytosol viscosities for various flow conditions in cylindrical microchannels. Our main aim is to link macroscopic flow properties such as flow resistance to single cell deformation and dynamics as a function of $C$. Starting from a dispersed cell configuration, we find that the flow convergence and the development of a RBC-free layer (RBC-FL) depend only weakly on $C$, and require a convergence length in the range of $25D-50D$, where $D$ is the channel diameter. The flow resistance for $C=5$ is nearly the same as that for $C=1$, which is facilitated by a slightly larger RBC-FL thickness for $C=5$. This effect is due to the suppression of membrane motion and dynamic shape deformations by a more viscous cytosol for $C=5$, resulting in a more compact cellular core of the flow in comparison to $C=1$. The weak effect of cytosol viscosity on the flow resistance and RBC-FL explains why cells can have a high concentration of hemoglobin for efficient oxygen delivery, without a pronounced increase in the flow resistance. 0 into PostgreSQL...\n",
      "Inserting test sample 1410  The flow behavior of red blood cell (RBC) suspensions in microvessels is a critical property that governs the delivery of oxygen and nutrients to tissue and organs. The viscosity of the cytosol, a key determinant of the RBC deformability, impacts the flow dynamics in these microvessels. In this study, we explore the effect of cytosol viscosity on the flow behavior of RBC suspensions in microvessels using a combination of microfluidic experiments, computational simulations, and theoretical analysis. Our results show that increasing the viscosity of the cytosol leads to a decrease in RBC deformability, which in turn enhances the aggregation and adhesion of RBCs in microvessels. We also find that the apparent viscosity of RBC suspensions increases with the cytosol viscosity, indicating higher resistance to flow in microvessels. Our computational simulations reveal that the cytosol viscosity influences the shear stress distribution on the RBC surface and affects the formation of the cell-free layer near the walls of microvessels. Moreover, our theoretical analysis suggests a non-linear relationship between the cytosol viscosity and the critical shear rate for RBC aggregation and adhesion. Overall, our study provides a deeper understanding of the interaction between RBCs and microvessels, and highlights the importance of the cytosol viscosity in determining the flow behavior of RBC suspensions in physiological and pathological conditions. 1 into PostgreSQL...\n",
      "Inserting test sample 1411  We propose a doubly subordinated Levy process, NDIG, to model the time series properties of the cryptocurrency bitcoin. NDIG captures the skew and fat-tailed properties of bitcoin prices and gives rise to an arbitrage free, option pricing model. In this framework we derive two bitcoin volatility measures. The first combines NDIG option pricing with the Cboe VIX model to compute an implied volatility; the second uses the volatility of the unit time increment of the NDIG model. Both are compared to a volatility based upon historical standard deviation. With appropriate linear scaling, the NDIG process perfectly captures observed, in-sample, volatility. 0 into PostgreSQL...\n",
      "Inserting test sample 1412  This study examines the relationship between Bitcoin volatility, intrinsic time, and the double subordinated Levy processes. Our observations suggest that Bitcoin exhibits persistent volatility, and the intrinsic time parameter is a function of the level of volatility. Furthermore, we find that the double subordinated Levy process provides a good description of Bitcoin return data in comparison to other models. The application of this model offers insights into different statistical properties such as long-range dependence, variability, and distributions of prices of Bitcoin. Finally, we provide evidence that the double subordinated Levy model can help to understand the underlying mechanisms that drive Bitcoin price dynamics. 1 into PostgreSQL...\n",
      "Inserting test sample 1413  One Monad to Prove Them All is a modern fairy tale about curiosity and perseverance, two important properties of a successful PhD student. We follow the PhD student Mona on her adventure of proving properties about Haskell programs in the proof assistant Coq. On the one hand, as a PhD student in computer science Mona observes an increasing demand for correct software products. In particular, because of the large amount of existing software, verifying existing software products becomes more important. Verifying programs in the functional programming language Haskell is no exception. On the other hand, Mona is delighted to see that communities in the area of theorem proving are becoming popular. Thus, Mona sets out to learn more about the interactive theorem prover Coq and verifying Haskell programs in Coq. To prove properties about a Haskell function in Coq, Mona has to translate the function into Coq code. As Coq programs have to be total and Haskell programs are often not, Mona has to model partiality explicitly in Coq. In her quest for a solution Mona finds an ancient manuscript that explains how properties about Haskell functions can be proven in the proof assistant Agda by translating Haskell programs into monadic Agda programs. By instantiating the monadic program with a concrete monad instance the proof can be performed in either a total or a partial setting. Mona discovers that the proposed transformation does not work in Coq due to a restriction in the termination checker. In fact the transformation does not work in Agda anymore as well, as the termination checker in Agda has been improved. We follow Mona on an educational journey through the land of functional programming where she learns about concepts like free monads and containers as well as basics and restrictions of proof assistants like Coq. These concepts are well-known individually, but their interplay gives rise to a solution for Mona's problem based on the originally proposed monadic tranformation that has not been presented before. When Mona starts to test her approach by proving a statement about simple Haskell functions, she realizes that her approach has an additional advantage over the original idea in Agda. Mona's final solution not only works for a specific monad instance but even allows her to prove monad-generic properties. Instead of proving properties over and over again for specific monad instances she is able to prove properties that hold for all monads representable by a container-based instance of the free monad. In order to strengthen her confidence in the practicability of her approach, Mona evaluates her approach in a case study that compares two implementations for queues. In order to share the results with other functional programmers the fairy tale is available as a literate Coq file. If you are a citizen of the land of functional programming or are at least familiar with its customs, had a journey that involved reasoning about functional programs of your own, or are just a curious soul looking for the next story about monads and proofs, then this tale is for you. 0 into PostgreSQL...\n",
      "Inserting test sample 1414  The concept of a \"monad\" has been used across multiple fields and disciplines throughout history, from Western philosophy to computer science. In this paper, we examine the concept of a monad and its applications in various areas of science.\n",
      "\n",
      "We begin with a historical overview of the term, exploring its origins in the writings of Plato and Aristotle and its development over time. From there, we move into an examination of the ways in which the idea of a monad has been used in mathematics, particularly in calculus and topology.\n",
      "\n",
      "Moving beyond mathematics, we explore the use of monads in physics, including its application in quantum mechanics and string theory. We also examine the concept of a monad in chemistry, considering its role in the development of new materials and its potential for creating new molecules through precisely controlled reactions.\n",
      "\n",
      "In the field of computer science, monads have been used as a way to structure functional programming languages. We explore the use of monads in Haskell and Scala, two prominent functional programming languages, and discuss the advantages they provide in terms of code reusability and modularity.\n",
      "\n",
      "Finally, we consider the potential applications of monads in the field of artificial intelligence. We explore how monads could be used to model complex systems, such as the human brain, and how they could be used to develop more efficient algorithms for machine learning and natural language processing.\n",
      "\n",
      "Overall, our paper argues that the concept of a monad has far-reaching applications across a variety of scientific disciplines. By exploring the different ways in which the concept has been used, we hope to provide a deeper understanding of this fundamental idea and its potential for shaping the future of science and technology. 1 into PostgreSQL...\n",
      "Inserting test sample 1415  Pre-trained text-to-text transformers such as BART have achieved impressive performance across a range of NLP tasks. Recent study further shows that they can learn to generalize to novel tasks, by including task descriptions as part of the source sequence and training the model with (source, target) examples.\n",
      "\n",
      "At test time, these fine-tuned models can make inferences on new tasks using the new task descriptions as part of the input. However, this approach has potential limitations, as the model learns to solve individual (source, target) examples (i.e., at the instance level), instead of learning to solve tasks by taking all examples within a task as a whole (i.e., at the task level). To this end, we introduce Hypter, a framework that improves text-to-text transformer's generalization ability to unseen tasks by training a hypernetwork to generate task-specific, light-weight adapters from task descriptions. Experiments on ZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves upon fine-tuning baselines. Notably, when using BART-Large as the main network, Hypter brings 11.3% comparative improvement on ZEST dataset. 0 into PostgreSQL...\n",
      "Inserting test sample 1416  This paper presents a novel approach for generating task-specific adapters automatically from textual task descriptions, without requiring prior knowledge of task-specific programming or domain expertise. Our approach involves learning to map natural language task descriptions to low-level program adapters, using a machine learning algorithm based on gated recurrent neural networks. To this end, we propose a new training dataset for task-specific adapter generation, which includes a large number of examples covering various programming tasks in different domains. Our experimental evaluation shows that our approach outperforms several state-of-the-art methods for generating program adapters from natural language input. We also demonstrate the usefulness of our approach for several real-world use cases, including robot navigation and planning, and image classification. In summary, our proposed method provides a highly valuable tool for automating the development of program adapters, which can in turn enable more efficient and rapid programming by non-experts. 1 into PostgreSQL...\n",
      "Inserting test sample 1417  Fe$_2$P alloys have been identified as promising candidates for magnetic refrigeration at room-temperature and for custom magnetostatic applications.\n",
      "\n",
      "The intent of this study is to accurately characterize the magnetic ground state of the parent compound, Fe$_2$P, with two spectroscopic techniques, $\\mu$SR and NMR, in order to provide solid bases for further experimental analysis of Fe$_2$P-type transition metal based alloys. We perform zero applied field measurements using both techniques below the ferromagnetic transition $T_C=220~\\mathrm K$. The experimental results are reproduced and interpreted using first principles simulations validating this approach for quantitative estimates in alloys of interest for technological applications. 0 into PostgreSQL...\n",
      "Inserting test sample 1418  In this study, we present a comprehensive investigation of Fe$_2$P using both experimental and theoretical techniques. The crystal structure of the compound was determined by X-ray diffraction data and has been discussed. Density functional theory calculations were performed to calculate the electronic and magnetic properties. Additionally, we have used various spin-dependent techniques such as MÃ¶ssbauer spectroscopy and X-ray magnetic circular dichroism to probe the magnetic ground-state of the compound. The analysis of the spectra obtained is correlated with the theoretical calculations. Our results provide a detailed understanding of the electronic and magnetic properties of Fe$_2$P which is critical in designing future magnetic materials. 1 into PostgreSQL...\n",
      "Inserting test sample 1419  We have experimentally studied polarization properties of the two-dimensional coupled photonic crystal microcavity arrays, and observed a strong polarization dependence of the transmission and reflection of light from the structures - the effects that can be employed in building miniaturized polarizing optical components. Moreover, by combining these properties with a strong sensitivity of the coupled bands on the surrounding refractive index, we have demonstrated a detection of small refractive index changes in the environment, which is useful for construction of bio-chemical sensors. 0 into PostgreSQL...\n",
      "Inserting test sample 1420  This paper presents a study on polarization control and sensing using two-dimensional coupled photonic crystal microcavity arrays. By engineering the geometry of the arrays, we were able to achieve high polarization extinction ratios and sensitivity to change in refractive index. The simulations and experiments showed that the proposed design can be used for compact and highly sensitive sensors. The results of this research can lead to the development of new optical devices for sensing and communication systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1421  This paper is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the L1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces. 0 into PostgreSQL...\n",
      "Inserting test sample 1422  Robust Principal Component Analysis (RPCA) is a statistical technique for decomposing data into low-rank and sparse components, even in the presence of outliers. The aim of RPCA is to identify and remove anomalies from a dataset while preserving the underlying structure of the normal data. It has numerous applications, including but not limited to image and video processing, economic and finance analysis, and bioinformatics.\n",
      "\n",
      "The core of the method lies in the optimization of an objective function that balances the low-rank and sparse components. Several algorithms have been proposed to solve this optimization problem efficiently, including convex programming and iterative thresholding procedures. Additionally, various extensions have been developed to address the practical challenges of RPCA, such as scale-invariance, non-Gaussian noise, and missing data.\n",
      "\n",
      "Overall, RPCA is a powerful tool to extract meaningful information from corrupt and complex datasets. Its robustness to outliers and ability to handle large-scale problems make it a promising direction for future research. 1 into PostgreSQL...\n",
      "Inserting test sample 1423  Despite their performance, Artificial Neural Networks are not reliable enough for most of industrial applications. They are sensitive to noises, rotations, blurs and adversarial examples. There is a need to build defenses that protect against a wide range of perturbations, covering the most traditional common corruptions and adversarial examples. We propose a new data augmentation strategy called M-TLAT and designed to address robustness in a broad sense. Our approach combines the Mixup augmentation and a new adversarial training algorithm called Targeted Labeling Adversarial Training (TLAT). The idea of TLAT is to interpolate the target labels of adversarial examples with the ground-truth labels. We show that M-TLAT can increase the robustness of image classifiers towards nineteen common corruptions and five adversarial attacks, without reducing the accuracy on clean samples. 0 into PostgreSQL...\n",
      "Inserting test sample 1424  Neural networks have been increasingly deployed for various tasks, from image recognition to natural language processing. However, they are vulnerable to adversarial attacks, where malicious actors can manipulate the input to mislead the network's output. To address this issue, Mixup and Targeted Labeling Adversarial Training (TLAT) have been proposed as effective approaches. Mixup involves combining pairs of training samples and their corresponding labels to generate new training samples, while TLAT involves perturbing the labels to enforce the desired output. In this work, we propose a robust training framework that incorporates both Mixup and TLAT. Our experiments demonstrate that the proposed approach significantly improves the adversarial robustness of neural networks, achieving state-of-the-art performance on multiple benchmarks. This work contributes to advancing the security of neural networks against adversarial attacks. 1 into PostgreSQL...\n",
      "Inserting test sample 1425  Abbreviations often have several distinct meanings, often making their use in text ambiguous. Expanding them to their intended meaning in context is important for Machine Reading tasks such as document search, recommendation and question answering. Existing approaches mostly rely on manually labeled examples of abbreviations and their correct long-forms. Such data sets are costly to create and result in trained models with limited applicability and flexibility. Importantly, most current methods must be subjected to a full empirical evaluation in order to understand their limitations, which is cumbersome in practice.\n",
      "\n",
      "In this paper, we present an entirely unsupervised abbreviation disambiguation method (called UAD) that picks up abbreviation definitions from unstructured text. Creating distinct tokens per meaning, we learn context representations as word vectors. We demonstrate how to further boost abbreviation disambiguation performance by obtaining better context representations using additional unstructured text. Our method is the first abbreviation disambiguation approach with a transparent model that allows performance analysis without requiring full-scale evaluation, making it highly relevant for real-world deployments.\n",
      "\n",
      "In our thorough empirical evaluation, UAD achieves high performance on large real-world data sets from different domains and outperforms both baseline and state-of-the-art methods. UAD scales well and supports thousands of abbreviations with multiple different meanings within a single model.\n",
      "\n",
      "In order to spur more research into abbreviation disambiguation, we publish a new data set, that we also use in our experiments. 0 into PostgreSQL...\n",
      "Inserting test sample 1426  Abbreviations are widely used in scientific texts, but their contextual disambiguation remains a challenging task. In this work, we propose a novel approach for unsupervised abbreviation disambiguation that leverages word embeddings. Our method operates in two stages, first identifying potential expansions for each candidate abbreviation based on co-occurrence statistics, then performing a contextual analysis using word embeddings to rank the most likely expansion for each instance of the abbreviation. We evaluate our approach on two standard benchmark datasets, demonstrating significant improvements over previous state-of-the-art methods, particularly in cases where multiple expansion possibilities exist. Our method shows great potential for facilitating text understanding by automatically resolving ambiguous abbreviations, without the need for costly human annotation. Furthermore, through our analysis, we uncover interesting patterns and insights regarding the contextual usage of abbreviations. Finally, we discuss possible extensions and applications of our approach in various domains, including medical, scientific, and legal texts. The proposed method not only contributes to the advancement of natural language processing, but also has potential implications for improving information retrieval and document classification tasks. 1 into PostgreSQL...\n",
      "Inserting test sample 1427  Speaker extraction algorithm relies on the speech sample from the target speaker as the reference point to focus its attention. Such a reference speech is typically pre-recorded. On the other hand, the temporal synchronization between speech and lip movement also serves as an informative cue. Motivated by this idea, we study a novel technique to use speech-lip visual cues to extract reference target speech directly from mixture speech during inference time, without the need of pre-recorded reference speech. We propose a multi-modal speaker extraction network, named MuSE, that is conditioned only on a lip image sequence. MuSE not only outperforms other competitive baselines in terms of SI-SDR and PESQ, but also shows consistent improvement in cross-dataset evaluations. 0 into PostgreSQL...\n",
      "Inserting test sample 1428  In multi-speaker environments, isolating individual speeches can be challenging. This paper presents Muse, a novel multi-modal target speaker extraction technique that leverages visual cues to separate speakers. Our method uses facial landmarks to locate visual cues and trains a deep neural network to identify and extract the target speaker's voice while filtering out background noise. Our experiments show that incorporating visual features significantly improves performance when compared to traditional audio-only methods. We also demonstrate that even with low-quality visual information, Muse can achieve state-of-the-art separation results. Our work opens up new avenues for multi-modal speech processing in real-world scenarios and can be useful in various applications such as robotics, teleconferencing, and hearing aids. 1 into PostgreSQL...\n",
      "Inserting test sample 1429  We use parallax data from the Gaia second data release (GDR2), combined with parallax data based on Hipparcos and HST data, to derive the period-luminosity-metallicity (PLZ) relation for Galactic classical cepheids (CCs) in the V,K, and Wesenheit WVK bands. An initial sample of 452 CCs are extracted from the literature with spectroscopically derived iron abundances.\n",
      "\n",
      "Reddening values, pulsation periods, and mean magnitudes are taken from the literature.\n",
      "\n",
      "Based on nine CCs with a goodness-of-fit (GOF) statistic <8 and with an accurate non-Gaia parallax, a parallax zero-point offset of -0.049 +- 0.018 mas is derived. Selecting a GOF statistic <8 removes about 40\\% of the sample most likely related due to binarity. Excluding first overtone and multi-mode cepheids and applying some other criteria reduces the sample to about 200 stars.\n",
      "\n",
      "The derived PL(Z) relations depend strongly on the parallax zero-point offset. The slope of the PL relation is found to be different from the relations in the LMC at the 3 sigma level. Fixing the slope to the value found in the LMC leads to a distance modulus (DM) to the LMC of order 18.7 mag, larger than the canonical distance. The canonical DM of around 18.5 mag would require a parallax zero-point offset of order $-0.1$ mas.\n",
      "\n",
      "Given the strong correlation between zero point, period and metallicity dependence of the PL relation, and the parallax zero-point offset there is no evidence for a metallicity term in the PLZ relation.\n",
      "\n",
      "The GDR2 release does not allow us to improve on the current distance scale based on CCs. The value of and the uncertainty on the parallax zero-point offset leads to uncertainties of order 0.15 mag on the distance scale. The parallax zero-point offset will need to be known at a level of 3 microas or better to have a 0.01 mag or smaller effect on the zero point of the PL relation and the DM to the LMC. 0 into PostgreSQL...\n",
      "Inserting test sample 1430  The Cepheid period-luminosity-metallicity (PLZ) relation is a fundamental tool in the study of galactic structure and evolution. It allows us to measure distances to galaxies beyond the Milky Way and provides a powerful constraint on cosmological models. In this paper, we explore the PLZ relation using data from the Gaia Data Release 2 (DR2) and examine the impact of varying metallicity on the relation.\n",
      "\n",
      "Our analysis is based on a sample of over 30,000 Cepheids observed by Gaia in the Milky Way and nearby galaxies. We use Gaia parallaxes to measure the distances to the Cepheids, and we derive their metallicities from high-resolution spectroscopy and photometry. We find a clear dependence of the Cepheid period on both luminosity and metallicity, consistent with previous studies.\n",
      "\n",
      "In addition to the standard PLZ relation, which assumes a universal slope, we also investigate the possibility of a metallicity-dependent slope. We find that a metallicity-dependent slope provides a better fit to the data than a universal slope, indicating that the PLZ relation is not entirely universal. We also compare our results to theoretical predictions from stellar evolution models and find good agreement.\n",
      "\n",
      "Our analysis demonstrates the power of Gaia DR2 data for studying the PLZ relation and highlights the importance of accounting for metallicity effects. Our results have significant implications for distance measurements, galactic structure, and the calibration of the cosmic distance ladder. We conclude by discussing possible future directions for refining and extending the PLZ relation with Gaia and other upcoming surveys. 1 into PostgreSQL...\n",
      "Inserting test sample 1431  Let D be a division ring such that the number of conjugacy classes in the multiplicative group D^* is equal to the power of D^*. Suppose that H(V) is the group GL(V) or PGL(V), where V is an infinite-dimensional vector space over D.\n",
      "\n",
      "We prove, in particular, that, uniformly in dim(V) and D, the first-order theory of H(V) is mutually syntactically interpretable with the theory of the two-sorted structure <dim(V),D> (whose only relations are the division ring operations on D) in the second-order logic with quantification over arbitrary relations of power <= dim(V). A certain analogue of this results is proved for the groups the collinear groups GammaL(V) and PGammaL(V). These results imply criteria of elementary equivalence for infinite-dimensional classical groups of types H=GammaL, PGammaL, GL, PGL over division rings, and solve, for these groups, a problem posed by Felgner. It follows from the criteria that if H(V_1), H(V_2) are elementarily equivalent, then the cardinals dim(V_1) and dim(V_2) are second order equivalent as sets. 0 into PostgreSQL...\n",
      "Inserting test sample 1432  This paper investigates the elementary equivalence of infinite-dimensional classical groups. Infinite-dimensional classical groups play a significant role in the theory of algebraic groups and Lie groups. We first introduce elementary equivalence, which is a notion from mathematical logic that measures the similarity between mathematical structures. We then explore how this concept applies to infinite-dimensional classical groups, specifically in the context of isomorphic Lie algebras and algebraic groups. We establish the necessary background material for these groups, including the definitions of the relevant Lie algebras and their representations. From there, we discuss how the concept of elementary equivalence can be used to compare and classify these groups. Our results provide insights into the structural similarities between different infinite-dimensional classical groups and contribute to a deeper understanding of their properties. Overall, this paper sheds light on the connections between algebraic geometry, representation theory, and logic. 1 into PostgreSQL...\n",
      "Inserting test sample 1433  In this paper, we study the effect of sparsity on the appearance of outliers in the semi-circular law. Let $(W_n)_{n=1}^\\infty$ be a sequence of random symmetric matrices such that each $W_n$ is $n\\times n$ with i.i.d entries above and on the main diagonal equidistributed with the product $b_n\\xi$, where $\\xi$ is a real centered uniformly bounded random variable of unit variance and $b_n$ is an independent Bernoulli random variable with a probability of success $p_n$. Assuming that $\\lim\\limits_{n\\to\\infty}n p_n=\\infty$, we show that for the random sequence $(\\rho_n)_{n=1}^\\infty$ given by $$\\rho_n:=\\theta_n+\\frac{n p_n}{\\theta_n},\\quad \\theta_n:=\\sqrt{\\max\\big(\\max\\limits_{i\\leq n}\\|{\\rm Row_i}(W_n)\\|_2^2-np_n,n p_n\\big)},$$ the ratio $\\frac{\\|W_n\\|}{\\rho_n}$ converges to one in probability. A non-centered counterpart of the theorem allows to obtain asymptotic expressions for eigenvalues of the Erd\\H{o}s--Renyi graphs, which were unknown in the regime $n p_n=\\Theta(\\log n)$. In particular, denoting by $A_n$ the adjacency matrix of $\\mathcal{G}(n,p_n)$ and by $\\lambda_{|k|}(A_n)$ its $k$-th largest (by the absolute value) eigenvalue, under the assumptions $\\lim\\limits_{n\\to\\infty }n p_n=\\infty$ and $\\lim\\limits_{n\\to\\infty}p_n=0$ we have: -(No non-trivial outliers) If $\\liminf\\frac{n p_n}{\\log n}\\geq\\frac{1}{\\log (4/e)}$ then for any fixed $k\\geq2$, $\\frac{|\\lambda_{|k|}(A_n)|}{2\\sqrt{n p_n}}$ converges to $1$ in probability.\n",
      "\n",
      "-(Outliers) If $\\limsup\\frac{n p_n}{\\log n}<\\frac{1}{\\log (4/e)}$ then there is $\\varepsilon>0$ such that for any $k\\in\\mathbb{N}$, we have $\\lim\\limits_{n\\to\\infty}\\mathbb{P}\\Big\\{\\frac{|\\lambda_{|k|}(A_n)|}{2\\sqrt{n p_n}}>1+\\varepsilon\\Big\\}=1$.\n",
      "\n",
      "On a conceptual level, our result highlights similarities in appearance of outliers in spectrum of sparse matrices and the so-called BBP phase transition phenomenon in deformed Wigner matrices. 0 into PostgreSQL...\n",
      "Inserting test sample 1434  This paper investigates the distribution of eigenvalues of sparse Wigner matrices, focusing on the presence of outliers and their behavior in the limit of large matrix size. We consider the sparse Wishart ensemble, in which the entries are independent, identically distributed Gaussian random variables, and derive the moments of both the bulk and the outliers of the limiting eigenvalue distribution. Our analysis reveals that the moments of the outlier eigenvalues are determined by the tails of the underlying distribution of the entries of the matrix. In particular, the existence of heavy tails produces a power-law decay of the moments of the outliers, leading to a flatter distribution that is distinct from the bulk. We also show that the outliers are asymptotically decoupled from the bulk, implying that their behavior is independent of the distribution of the bulk eigenvalues in the limit of large matrix size. We illustrate the theoretical results with numerical simulations, which confirm our predictions for the distribution of the eigenvalue spectrum and the behavior of the outliers. Finally, we analyze the implications of our findings for the study of random matrix theory and its applications in statistical physics, computer science, and engineering. We argue that the behavior of the outliers can offer insights into the structure and properties of complex systems, and that their analysis can help mitigate the effects of unpredictable events and fluctuations in high-dimensional data. Overall, our work contributes to the understanding of the spectral properties of sparse Wigner matrices and their relevance in the context of modern data analysis. 1 into PostgreSQL...\n",
      "Inserting test sample 1435  The aim of this article is to formulate some novel uncertainty principles for the continuous shearlet transforms in arbitrary space dimensions. Firstly, we derive an analogue of the Pitt's inequality for the continuous shearlet transforms, then we formulate the Beckner's uncertainty principle via two approaches: one based on a sharp estimate from Pitt's inequality and the other from the classical Beckner's inequality in the Fourier domain. Secondly, we consider a logarithmic Sobolev inequality for the continuous shearlet transforms which has a dual relation with Beckner's inequality. Thirdly, we derive Nazarov's uncertainty principle for the shearlet transforms which shows that it is impossible for a non-trivial function and its shearlet transform to be both supported on sets of finite measure. Towards the culmination, we formulate local uncertainty principles for the continuous shearlet transforms in arbitrary space dimensions. 0 into PostgreSQL...\n",
      "Inserting test sample 1436  In this paper, we investigate the uncertainty principles for the continuous shearlet transform (CST) in arbitrary space dimensions. We establish upper and lower bounds for the CST, in terms of its spatial and frequency extents, respectively. Our results demonstrate that the CST satisfies a generalized form of the Heisenberg uncertainty principle, which is characterized by different orders of magnitude depending on the scaling factors of the shearlets. Additionally, we derive a new uncertainty relation for the CST, which expresses a lower bound on the product of the frequency and spatial extents. We further examine the implications of our uncertainty principles for signal processing applications, such as denoising and compressed sensing. Our findings reveal that the CST offers a powerful tool for analyzing and processing signals with high directional sensitivity, particularly in multidimensional settings. 1 into PostgreSQL...\n",
      "Inserting test sample 1437  We analyze elementary building blocks for quantum repeaters based on fiber channels and memory stations. Implementations are considered for three different physical platforms, for which suitable components are available: quantum dots, trapped atoms and ions, and color centers in diamond. We evaluate and compare the performances of basic quantum repeater links for these platforms both for present-day, state-of-the-art experimental parameters as well as for parameters that could in principle be reached in the future. The ultimate goal is to experimentally explore regimes at intermediate distances, up to a few 100 km, in which the repeater-assisted secret key transmission rates exceed the maximal rate achievable via direct transmission. We consider two different protocols, one of which is better adapted to the higher source clock rate and lower memory coherence time of the quantum dot platform, while the other circumvents the need of writing photonic quantum states into the memories in a heralded, non-destructive fashion. The elementary building blocks and protocols can be connected in a modular form to construct a quantum repeater system that is potentially scalable to large distances. 0 into PostgreSQL...\n",
      "Inserting test sample 1438  The communication of quantum information over long distances is hindered by the loss of coherence of quantum states during propagation. Quantum repeaters address this problem by entangling neighboring qubits and sharing this entanglement across the repeater. In this paper, we propose a modular approach to building quantum repeaters based on either fiber or memory nodes. Our method extends the distance over which coherence is maintained by using a series of intermediate modules to link the end nodes. We show that our approach outperforms previous methods by decreasing the overhead and complexity of repeater systems, increasing the range of entanglement distribution and enabling future scalability. We analyze the performance of our proposed repeaters and compare them to existing systems using simulations. We also describe the practical implementation of our repeaters using current technologies, such as superconducting qubits or optical fibers. We believe our approach provides a promising direction for future research on quantum repeaters. 1 into PostgreSQL...\n",
      "Inserting test sample 1439  We have detected spatially extended linear polarized UV emission from the high-redshift radio galaxy 3C~256 ($z=1.82$). A spatially integrated ($7.8''$ diameter aperture) measurement of the degree of polarization of the $V-$band (rest frame 0.19 $\\mu$m) emission yields a value of 16.4\\% ($\\pm 2.2$\\%) with a position angle of $42{}\\rlap{\\rm .}^\\circ 4$ ($\\pm 3{}\\rlap{\\rm .}^\\circ 9$), orthogonal to the position angle on the sky of the major axis of the extended emission. The peak emission measured with a $3.6''$ diameter circular aperture is 11.7\\% ($\\pm 1.5$\\%) polarized with a position angle of $42{}\\rlap{\\rm .}^\\circ 4$ ($\\pm 3{}\\rlap{\\rm .}^\\circ 6$). An image of the polarized flux is presented, clearly displaying that the polarized flux is extended and present over the entire extent of the object. While it has been suggested that the UV continuum of 3C~256 might be due to star formation (Elston 1988) or a protogalaxy (Eisenhardt \\& Dickinson 1993) based on its extremely blue spectral energy distribution and similar morphology at UV and visible wavelengths, we are unable to reconcile the observed high degree of polarization with such a model. While the detection of polarized emission from HZRGs has been shown to be a common phenomena, 3C~256 is only the third object for which a measurement of the extended polarized UV emission has been presented. These data lend additional support to the suggestion first made by di Serego Alighieri and collaborators that the ``alignment effect'', the tendency for the extended UV continuum radiation and line emission from HZRGs to be aligned with the major axis of the extended radio emission, is in large part due to scattering of anisotropic nuclear emission. 0 into PostgreSQL...\n",
      "Inserting test sample 1440  The detection and subsequent analysis of ultraviolet (UV) radiation has long been a hot topic in the field of astronomy. Recently, researchers studying the radio galaxy 3C 256 at redshift z = 1.82 have made a significant discovery: the presence of extended polarized UV radiation. This finding is not only significant for its implications in the study of radio galaxies, but also because it sheds light on the mechanisms by which high-redshift galaxies are formed.\n",
      "\n",
      "The detection of polarized UV radiation in 3C 256 was made possible through the use of the Cosmic Origins Spectrograph (COS) on the Hubble Space Telescope (HST). Analysis of the polarization data reveals that the UV emission is likely due to the scattered light of an obscured active galactic nucleus (AGN). The orientation of the scattered light indicates that the AGN is obscured by a torus of gas and dust with an opening angle of approximately 50 degrees.\n",
      "\n",
      "Further analysis of the UV polarization data suggests that the AGN in 3C 256 is aligned with the radio jet emanating from the galaxy's center. This alignment indicates that the UV polarization is likely due to the scattering of radio synchrotron photons by the same electrons that produce the radio jets.\n",
      "\n",
      "The identification of extended polarized UV radiation from 3C 256 is important for understanding the formation and evolution of high-redshift galaxies. It suggests that obscured AGNs can be found in high-redshift galaxies and that these AGNs may play a significant role in the formation of these galaxies. Additionally, the identification of polarized UV radiation can provide valuable insight into the physical properties of the torus obscuring the AGN and the radio jets emanating from the galaxy's center. 1 into PostgreSQL...\n",
      "Inserting test sample 1441  \"Static\" structure functions are the probabilistic distributions computed from the square of the light-front wavefunctions of the target hadron. In contrast, the \"dynamic\" structure functions measured in deep inelastic lepton-hadron scattering include the effects of rescattering associated with the Wilson line. Initial- and final-state rescattering, neglected in the parton model, can have a profound effect in QCD hard-scattering reactions, producing single-spin asymmetries, diffractive deep inelastic scattering, diffractive hard hadronic reactions, the breakdown of the Lam-Tung relation in Drell-Yan reactions, nuclear shadowing, and non-universal nuclear antishadowing--novel leading-twist physics not incorporated in the light-front wavefunctions of the target computed in isolation. I also review how \"direct\" higher-twist processes -- where a proton is produced in the hard subprocess itself -- can explain the anomalous proton-to-pion ratio seen in high centrality heavy ion collisions. 0 into PostgreSQL...\n",
      "Inserting test sample 1442  In this paper, we investigate the differences between dynamic and static hadronic structure functions. Hadronic structure functions are crucial in understanding the internal structure of hadrons and are commonly used in high-energy physics experiments. Static structure functions assume that hadrons are composed of non-interacting point-like particles, while dynamic structure functions take into account the interactions between quarks and gluons within the hadron. We review the theoretical framework for both types of structure functions and analyze their behavior under different kinematic conditions. Our findings demonstrate that dynamic structure functions provide a more accurate description of hadronic structure in comparison to static structure functions. This has important implications for the interpretation of experimental data and may lead to more precise determinations of fundamental parameters in high-energy physics. 1 into PostgreSQL...\n",
      "Inserting test sample 1443  We study the dynamical regimes that emerge from the strongly coupling between two Chua's circuits with parameters mismatch. For the region around the perfect synchronous state we show how to combine parameter diversity and coupling in order to robustly and precisely target a desired regime. This target process allows us to obtain regimes that may lie outside parameter ranges accessible for any isolated circuit. The results are obtained by following a recently developed theoretical technique, the order parameter expansion, and are verified both by numerical simulations and on electronic circuits. The theoretical results indicate that the same predictable change in the collective dynamics can be obtained for large populations of strongly coupled circuits with parameter mismatches. 0 into PostgreSQL...\n",
      "Inserting test sample 1444  This paper investigates the coherent regimes of mutually coupled Chua's circuits. By analyzing the circuit's synchronization methods and examining its phase diagrams, we have identified four distinct regimes of synchronization. We show that the coupled Chua's circuits can display multiple types of coexisting coherent behaviors, including periodic and chaotic attractors. We also demonstrate that the circuits can exhibit a hysteresis effect, where the type of coherent behavior depends on the initial conditions. Our findings have important implications for the design and implementation of coupled oscillator systems, particularly in the realm of secure communications where synchronized chaotic signals are useful for encryption. 1 into PostgreSQL...\n",
      "Inserting test sample 1445  We show that s-wave scattering resonances induced by dipolar interactions in a polar molecular gas have a universal large and positive effective range, which is very different from Feshbach resonances realized in cold atoms before, where the effective range is either negligible or negative. Such a difference has important consequence in many-body physics. At high temperature regime, a positive effective range gives rise to stronger repulsive interaction energy for positive scattering length, and weaker attractive interaction energy for negative scattering length. While at low-temperatures, we study polaron problem formed by single impurity molecule, and we find that the polaron binding energy increases at the BEC side and decreases at the BCS side. All these effects are in opposite to narrow Feshbach resonances where the effective range is negative. 0 into PostgreSQL...\n",
      "Inserting test sample 1446  This paper investigates the s-wave scattering resonances that arise from dipolar interactions between polar molecules. We develop a theoretical framework for characterizing these resonances using perturbation theory and determine the conditions necessary for their existence. Our approach is based on a combination of analytic and numerical techniques, allowing us to study the scattering properties of a wide range of molecules. We also explore the effects of an external magnetic field on the resonances, highlighting the potential for using these resonances as a tool for controlling molecular interactions. Our results indicate that dipolar interactions can be a significant factor in the formation of s-wave scattering resonances, and may provide a means for manipulating molecular systems in a controlled and precise manner. 1 into PostgreSQL...\n",
      "Inserting test sample 1447  This work investigates the degrees of freedom (DoF) of a downlink cache-aided cellular network where the locations of base stations (BSs) are modeled as a grid topology and users within a grid cell can only communicate with four nearby BSs. We adopt a cache placement method with uncoded prefetching tailored for the network with partial connectivity. According to the overlapped degree of cached contents among BSs, we propose transmission schemes with no BS cooperation, partial BS cooperation, and full BS cooperation, respectively, for different cache sizes. In specific, the common cached contents among BSs are utilized to cancel some undesired signals by interference neutralization while interference alignment is used to coordinate signals of distinct cached contents. Our achievable results indicate that the reciprocal of per-user DoF of the cellular network decreases piecewise linearly with the normalized cache size $\\mu$ at each BS, and the gain of BS caching is more significant for the small cache region. Under the given cache placement scheme, we also provide an upper bound of per-user DoF and show that our achievable DoF is optimal when $\\mu\\in\\left[\\frac{1}{2},1\\right]$, and within an additive gap of $\\frac{4}{39}$ to the optimum when $\\mu\\in\\left[\\frac{1}{4},\\frac{1}{2}\\right)$. 0 into PostgreSQL...\n",
      "Inserting test sample 1448  Cache-aided wireless cellular networks have become an effective way to improve the performance of wireless communication systems. In this paper, we explore the degrees of freedom that such systems can achieve. Specifically, we investigate the following questions: What is the maximum achievable degrees of freedom for cache-aided wireless cellular networks with arbitrary number of users and cache sizes? What is the optimal content placement in the cache-aided wireless networks with constrained backhaul? In order to answer these questions, we develop novel caching schemes and employ tools from information theory and wireless communication to analyze the performance of cache-aided wireless cellular networks. Our results show that the degrees of freedom of cache-aided wireless cellular networks are greatly enhanced with the use of caching, as compared to conventional uncached networks. We also show that the optimal placement strategy depends on the number of users and cache capacities, and that the performance is impacted by the interference level and the backhaul capacity. Our work sheds light on the fundamental limits and practical optimization of cache-aided wireless cellular networks. 1 into PostgreSQL...\n",
      "Inserting test sample 1449  We study kink or domain wall solutions in O(10) Higgs models in the context of the ``clash of symmetries'' mechanism developed by Davidson, Toner, Volkas and Wali and, independently, by Pogosian and Vachaspati. We show that kink configurations employing Higgs fields in the 45 (the adjoint representation) of O(10) break up into three classes: those that at finite distances from the wall respect a U(5) subgroup of SO(10), and two others that respect the smaller subgroups U(3) x U(2) and U(4) x U(1). These smaller subgroups arise from the clash of symmetries mechanism: they are the intersections of two differently embedded U(5) subgroups of SO(10), the latter being the symmetries respected in asymptotia on opposite sides of the domain wall. The existence of the SO(10) -> U(3) x U(2) = SU(3) x SU(2) x U(1) x U(1)' = G_{SM} x U(1)' class advances the search for a realistic brane world model wherein some of the required symmetry breaking is achieved via the clash of symmetries rather than the conventional mechanism. At the centres of the walls, the unbroken symmetries are enhanced.\n",
      "\n",
      "In the U(3) x U(2) case, the symmetry is O(6) x U(2), which is closely related to the Pati-Salam-like SU(4) x SU(2) x U(1) group. If our universe is a brane located at the centre of such a wall, then we see the O(10) symmetry as being strongly broken to SU(4) x SU(2) x U(1). Interestingly, if the brane-world degrees of freedom enjoy a slight leakage off the wall, then an additional symmetry breakdown to U(3) x U(2) = G_{SM} x U(1)' is effectively induced on the brane. This provides a possible framework within which to address at least part of a gauge hierarchy problem: O(10) is strongly broken to SU(4) x SU(2) x U(1), then more weakly to G_{SM} x U(1)' depending on the amount of leakage off the brane. We also comment on kinks employing the 10 and 54 of O(10). 0 into PostgreSQL...\n",
      "Inserting test sample 1450  In recent years, the study of brane cosmology has become increasingly popular due to its potential to provide a solution to the gauge hierarchy problem. The paper at hand investigates the impact of symmetries on the brane and the gauge hierarchy problem, with an emphasis on the particular scenario referred to as \"O(10) kinks\". \n",
      "\n",
      "We begin by outlining the brane cosmology framework and the gauge hierarchy problem. The brane cosmology is a natural extension of the standard cosmological model, which hypothesizes that our universe is a 3-dimensional sub-manifold (the brane) embedded in a higher dimensional bulk space. The gauge hierarchy problem, on the other hand, concerns the significant discrepancy between the gravitational force and other fundamental forces of nature. \n",
      "\n",
      "With this background established, we proceed to discuss the O(10) kinks scenario, which involves the introduction of extra dimensions onto the brane. Specifically, we examine the impact of the kinks on the symmetries of the system, which gives rise to an interesting interplay between the bulk and the brane.\n",
      "\n",
      "Our findings indicate that the O(10) kinks scenario provides a viable solution to the gauge hierarchy problem, as it allows for the existence of new particles and interactions at a lower energy scale. Furthermore, the clash of symmetries on the brane and the bulk leads to the creation of a rich and diverse spectrum of particles, which has major implications for experimental studies of the high-energy universe.\n",
      "\n",
      "In conclusion, this paper provides a detailed study of the O(10) kinks scenario on the brane and its impact on the gauge hierarchy problem. Our results demonstrate the importance of considering the interplay between symmetries on the brane and the bulk in the design of cosmological models, and shed light on the potential of brane cosmology to bring about new discoveries in fundamental physics. 1 into PostgreSQL...\n",
      "Inserting test sample 1451  Lateral movement attacks are a serious threat to enterprise security. In these attacks, an attacker compromises a trusted user account to get a foothold into the enterprise network and uses it to attack other trusted users, increasingly gaining higher and higher privileges. Such lateral attacks are very hard to model because of the unwitting role that users play in the attack and even harder to detect and prevent because of their low and slow nature. In this paper, a theoretical framework is presented for modeling lateral movement attacks and for proposing a methodology for designing resilient cyber systems against such attacks. The enterprise is modeled as a tripartite graph capturing the interaction between users, machines, and applications, and a set of procedures is proposed to harden the network by increasing the cost of lateral movement. Strong theoretical guarantees on system resilience are established and experimentally validated for large enterprise networks. 0 into PostgreSQL...\n",
      "Inserting test sample 1452  With cyberattacks becoming more sophisticated and prevalent, enterprises face increasing challenges in protecting their systems and data. One of the most dangerous attack methods is lateral movement, which occurs when an attacker gains access to a system and uses that access to move laterally through the network, seeking critical resources and information. This paper presents a graph theoretic approach to enhance enterprise cyber resiliency by analyzing potential lateral movement paths. We propose a novel algorithm that constructs a network graph of an enterprise's systems and identifies vulnerabilities that an attacker can exploit to move laterally. By detecting and mitigating these vulnerabilities, our approach helps prevent lateral movement and limit the impact of cyberattacks. Our experiments on real-world network topologies demonstrate the effectiveness of our proposed method in improving enterprise cyber resiliency against lateral movement. 1 into PostgreSQL...\n",
      "Inserting test sample 1453  We examine volume computation of general-dimensional polytopes and more general convex bodies, defined as the intersection of a simplex by a family of parallel hyperplanes, and another family of parallel hyperplanes or a family of concentric ellipsoids. Such convex bodies appear in modeling and predicting financial crises. The impact of crises on the economy (labor, income, etc.) makes its detection of prime interest. Certain features of dependencies in the markets clearly identify times of turmoil. We describe the relationship between asset characteristics by means of a copula; each characteristic is either a linear or quadratic form of the portfolio components, hence the copula can be constructed by computing volumes of convex bodies. We design and implement practical algorithms in the exact and approximate setting, we experimentally juxtapose them and study the tradeoff of exactness and accuracy for speed. We analyze the following methods in order of increasing generality: rejection sampling relying on uniformly sampling the simplex, which is the fastest approach, but inaccurate for small volumes; exact formulae based on the computation of integrals of probability distribution functions; an optimized Lawrence sign decomposition method, since the polytopes at hand are shown to be simple; Markov chain Monte Carlo algorithms using random walks based on the hit-and-run paradigm generalized to nonlinear convex bodies and relying on new methods for computing a ball enclosed; the latter is experimentally extended to non-convex bodies with very encouraging results. Our C++ software, based on CGAL and Eigen and available on github, is shown to be very effective in up to 100 dimensions. Our results offer novel, effective means of computing portfolio dependencies and an indicator of financial crises, which is shown to correctly identify past crises. 0 into PostgreSQL...\n",
      "Inserting test sample 1454  This paper presents a practical and efficient approach for computing the volume of structured convex bodies. We start by establishing theoretical foundations for the computation of volumes of convex bodies that exhibit certain structural properties, such as sparsity and low-rank factorization. Based on these foundations, we devise algorithms that take advantage of the structure to significantly reduce the computational complexity compared to existing approaches. \n",
      "\n",
      "We then apply our method to the problem of modeling portfolio dependencies and predicting financial crises. We show that our algorithm can be used to accurately estimate the joint distribution of financial assets, which is crucial for assessing the risk of a portfolio. We also demonstrate how our method can be used to identify the most significant risk drivers in a portfolio, and how this can help to mitigate the impact of financial crises.\n",
      "\n",
      "To evaluate our approach, we perform extensive experiments on synthetic data as well as real-world financial data. Our results show that our algorithm is several orders of magnitude faster than existing methods, while still maintaining high accuracy. We also show that our approach is robust to various types of noise and perturbations that are commonly encountered in practical applications.\n",
      "\n",
      "Overall, our work provides a novel and practical solution to the long-standing problem of volume computation of structured convex bodies. Furthermore, our application to portfolio modeling and financial crises prediction demonstrates the potential of our method for addressing real-world problems in finance and related fields. 1 into PostgreSQL...\n",
      "Inserting test sample 1455  Salient object detection (SOD), which simulates the human visual perception system to locate the most attractive object(s) in a scene, has been widely applied to various computer vision tasks. Now, with the advent of depth sensors, depth maps with affluent spatial information that can be beneficial in boosting the performance of SOD, can easily be captured. Although various RGB-D based SOD models with promising performance have been proposed over the past several years, an in-depth understanding of these models and challenges in this topic remains lacking. In this paper, we provide a comprehensive survey of RGB-D based SOD models from various perspectives, and review related benchmark datasets in detail. Further, considering that the light field can also provide depth maps, we review SOD models and popular benchmark datasets from this domain as well. Moreover, to investigate the SOD ability of existing models, we carry out a comprehensive evaluation, as well as attribute-based evaluation of several representative RGB-D based SOD models. Finally, we discuss several challenges and open directions of RGB-D based SOD for future research. All collected models, benchmark datasets, source code links, datasets constructed for attribute-based evaluation, and codes for evaluation will be made publicly available at https://github.com/taozh2017/RGBDSODsurvey 0 into PostgreSQL...\n",
      "Inserting test sample 1456  Salient object detection is an essential task in computer vision and has drawn considerable attention in recent years. In particular, with the widespread availability of RGB-D sensors, researchers have explored the potential of incorporating depth information into saliency detection algorithms. In this survey, we present a comprehensive review of the recent advances in RGB-D salient object detection. We first provide a brief introduction to the principles of saliency detection, before discussing in detail the existing approaches to RGB-D saliency detection. These methodologies can be broadly classified into two categories: shallow and deep learning-based saliency detection methods. We then analyze the evaluation metrics used to assess the performance of these algorithms and conduct a detailed comparison of the state-of-the-art techniques on standard datasets. Finally, we conclude the survey by highlighting the current challenges and future research directions in RGB-D salient object detection. Overall, this survey aims to provide a comprehensive overview of the latest advancements in RGB-D salient object detection, which will be invaluable for researchers and practitioners working in this vibrant field. 1 into PostgreSQL...\n",
      "Inserting test sample 1457  Chinese pinyin input methods are very important for Chinese language processing. Actually, users may make typos inevitably when they input pinyin.\n",
      "\n",
      "Moreover, pinyin typo correction has become an increasingly important task with the popularity of smartphones and the mobile Internet. How to exploit the knowledge of users typing behaviors and support the typo correction for acronym pinyin remains a challenging problem. To tackle these challenges, we propose KNPTC, a novel approach based on neural machine translation (NMT). In contrast to previous work, KNPTC is able to integrate explicit knowledge into NMT for pinyin typo correction, and is able to learn to correct a variety of typos without the guidance of manually selected constraints or languagespecific features. In this approach, we first obtain the transition probabilities between adjacent letters based on large-scale real-life datasets. Then, we construct the \"ground-truth\" alignments of training sentence pairs by utilizing these probabilities. Furthermore, these alignments are integrated into NMT to capture sensible pinyin typo correction patterns. KNPTC is applied to correct typos in real-life datasets, which achieves 32.77% increment on average in accuracy rate of typo correction compared against the state-of-the-art system. 0 into PostgreSQL...\n",
      "Inserting test sample 1458  This paper proposes a new approach called Knowledge and Neural Machine Translation Powered Chinese Pinyin Typo Correction (KNPTC) for the automatic correction of Chinese pinyin typos in texts. Our approach leverages the power of both rule-based techniques and neural machine translation to achieve state-of-the-art results. Specifically, we first generate a set of candidate corrections based on a rule-based algorithm, which is designed to handle common pinyin errors such as tone and character substitution. Then, we use a neural machine translation model to choose the most likely correction from this set of candidates. To improve the performance of our system, we also incorporate external knowledge sources such as dictionaries and language models, which provide additional context and help disambiguate between homophonic characters. Our experimental results demonstrate the superiority of our approach over existing methods, achieving an accuracy rate of 90.6% on a benchmark dataset. Overall, our proposed KNPTC system has great potential for improving the accuracy and readability of Chinese texts by automatically correcting pinyin typos in real time. 1 into PostgreSQL...\n",
      "Inserting test sample 1459  Abstract Our ab initio all-electron relativistic Dirac-Fock (DF) calculations for seaborgium hexacarbonyl Sg(CO)6 and seaborgium hexaisocarbonyl Sg(OC)6 predict atomization energies of 68.80 and 64.30 eV. Our Dirac-Fock-Breit-Gaunt (DFBG) calculations for Sg(CO)6 and Sg(OC)6 yield atomization energies of 69.18 and 64.77 eV. However, our calculated non-relativistic (NR) Ae for Sg (CO)6 and Sg(OC)6 are 68.46 and 62.62 eV. The calculated isomerization energies at the DFBG, DF, and NR levels are 4.41,4.50 and 5.83 eV. The contribution of relativity to the Eiso is - ~1.33 eV. The optimized bond distances Sg-C and C-O for octahedral Sg(CO)6 using our DF (NR) calculations are 2.151 ( 2.318 and 1.119 ( 1.114 (ang}). The optimized six Sg-O and C-O bond distances for octahedral Sg(OC)6 at the DF level are equal to 4.897 and 1.108 {ang}. However, the optimized four Sg-O bond distances for the octahedral Sg(OC)6 at the NR level are 5.160 {ang} each, and two Sg-O bonds of 2.721 {ang} each, but all six C-O bonds are 1.108{ang} each. The energies at the DF level of theory for the reaction Sg+6CO to yield Sg (CO)6 and Sg(OC)6 are calculated as -7.30 and -2.80 eV. Moreover, the energies of the reaction at the DFBG level to yield Sg(CO)6 and Sg(OC)6 are very close to those predicted at the DF level of theory of -7.17 and -2.76 eV. However, the NR energies of the above-mentioned reaction are -6.99 and -1.15 eV. The mean bond energies predicted for Sg(CO)6 with our DF, DFBG, and NR calculations are 117.40 , 115.31, and 112.41 kJ/mol, whereas the mean bond energies calculated for the isomer Sg(OC)6 at DF, DFBG and NR levels are 45.03 ,44.39 and 18.49 kJ/mole. The predicted existence of both the isomers with Eiso of ~ 4.50 and ~ 5.80 eV, may cause problems for experimental identification of seaborgium hexacarbonyl. 0 into PostgreSQL...\n",
      "Inserting test sample 1460  The isomerization of heavy atomic nuclei plays a crucial role in nuclear structure and decay studies. In this work, we investigate the relativistic and magnetic Breit effects for the isomerization of Sg(CO)6 and Sg(OC)6 complexes. The isomerization of these complexes is a complex process that results in a change in the structure and orientation of the molecules. We perform ab initio calculations based on the coupled cluster theory with single and double excitations, as well as perturbative triple excitations, using the scalar relativistic and spin-orbit effective core potentials. \n",
      "\n",
      "Our results show that the relativistic and magnetic Breit contributions have a significant effect on the isomerization energy barriers of both complexes. The relativistic contributions result in a reduction in the energy barriers by up to 10 kcal/mol, while the magnetic Breit contributions result in increases in the energy barriers by up to 15 kcal/mol. We also observe that the magnetic Breit contributions are more significant for Sg(CO)6 than for Sg(OC)6. \n",
      "\n",
      "We further investigate the changes in the molecular properties of these complexes due to the relativistic and magnetic Breit effects. Our analysis reveals significant changes in the molecular orbitals, electric dipole moments, and other molecular properties. We also observe that the magnetic Breit contributions significantly affect the bond angles and bond lengths in these complexes. \n",
      "\n",
      "Our study provides important insights into the isomerization process for these complexes and highlights the importance of relativistic and magnetic Breit effects in heavy atomic nuclei. The results are also relevant for the design and understanding of other heavy metal-based complexes. Overall, this work contributes to the broader understanding of nuclear structure and decay studies and may have potential applications in areas such as nuclear medicine and material science. 1 into PostgreSQL...\n",
      "Inserting test sample 1461  Dysregulation in signal transduction pathways can lead to a variety of complex disorders, including cancer. Computational approaches such as network analysis are important tools to understand system dynamics as well as to identify critical components that could be further explored as therapeutic targets. Here, we performed perturbation analysis of a large-scale signal transduction model in extracellular environments that stimulate cell death, growth, motility, and quiescence. Each of the model's components was perturbed under both loss-of-function and gain-of-function mutations. We identified the most and least influential components based on the magnitude of their influence on the rest of the system. Based on the premise that the most influential components might serve as better drug targets, we characterized them for biological functions, housekeeping genes, essential genes, and druggable proteins. Moreover, known cancer drug targets were also classified in influential components based on the affected components in the network.\n",
      "\n",
      "Additionally, the systemic perturbation analysis of the model revealed a network motif of most influential components which affect each other.\n",
      "\n",
      "Furthermore, our analysis predicted novel combinations of cancer drug targets with various effects on other most influential components. We found that the combinatorial perturbation consisting of PI3K inactivation and overactivation of IP3R1 can lead to increased activity levels of apoptosis-related components and tumor suppressor genes, suggesting that this combinatorial perturbation may lead to a better target for decreasing cell proliferation and inducing apoptosis. Lastly, our results suggest that systematic perturbation analyses of large-scale computational models may serve as an approach to prioritize and assess signal transduction components in order to identify novel drug targets in complex disorders. 0 into PostgreSQL...\n",
      "Inserting test sample 1462  Signal transduction pathways involve the conversion of extracellular signals into intracellular responses and play crucial roles in cellular behavior. Disruptions and alterations in these pathways can contribute to the onset and progression of diseases such as cancer. Systems biology approaches have become increasingly important for the identification of signaling candidates as potential targets for cancer therapeutics.\n",
      "\n",
      "Here, we employed systems perturbation analysis, an extension of sensitivity analysis, to a large-scale signal transduction model to identify nodes or candidates that have significant influence on its output. Through the analysis, we identified twenty-eight nodes as significant across multiple cancer cell lines. These nodes were further classified into ten central transducers and eighteen peripheral nodes.\n",
      "\n",
      "The central transducers include significant signaling components such as ERK, AKT, and p38 MAPK, while the peripheral nodes include key proteins such as JNK, mTOR, and RSK. Among them, we identified several potentially influential candidates, such as the GSK3B, PI3K, and HSP90 that are already under various phases of clinical trials for cancer treatment.\n",
      "\n",
      "Moreover, the analysis highlighted the potential roles of some understudied candidates as potential therapeutic targets for cancer. These candidates, including GRB2, SOS1, and c-CBL were shown to be influential in multiple tumor cell lines, representing promising targets for future drug development.\n",
      "\n",
      "In summary, our systems perturbation analysis of a large-scale signal transduction model revealed a list of potentially influential candidates that could serve as targets for cancer therapeutics. Our findings provide critical insights to better understand the complexity and robustness of cancer signaling networks, ultimately paving the way for the development of novel cancer therapies. 1 into PostgreSQL...\n",
      "Inserting test sample 1463  In this paper we analyze fractional Fokker-Planck equation describing subdiffusion in the general infinitely divisible (ID) setting. We show that in the case of space-time-dependent drift and diffusion and time-dependent jump coefficient, the corresponding stochastic process can be obtained by subordinating two-dimensional system of Langevin equations driven by appropriate Brownian and Levy noises. Our result solves the problem of stochastic representation of subdiffusive Fokker-Planck dynamics in full generality. 0 into PostgreSQL...\n",
      "Inserting test sample 1464  In this study, we present a stochastic representation of the fractional subdiffusion equation. Specifically, we consider the case of infinitely divisible waiting times, Levy noise, and space-time-dependent coefficients. Our results provide insights into the behavior of the system and the interplay between these different stochastic elements. By applying our framework to several examples, we illustrate the practical relevance of our findings and demonstrate the potential for further applications in various fields. 1 into PostgreSQL...\n",
      "Inserting test sample 1465  Young planets offer a direct view of the formation and evolution processes that produced the diverse population of mature exoplanet systems known today.\n",
      "\n",
      "The repurposed Kepler mission K2 is providing the first sample of young transiting planets by observing populations of stars in nearby, young clusters or stellar associations. We report the detection and confirmation of two planets transiting K2-264, an M2.5 dwarf in the 650 Myr old Praesepe open cluster. Using our notch-filter search method on the K2 lightcurve, we identify planets with periods of 5.84 d and 19.66 d. This is currently the second known multi-transit system in open clusters younger than 1 Gyr. The inner planet has a radius of 2.27$_{-0.16}^{+0.20}$ R$_\\oplus$ and the outer planet has a radius of 2.77$_{-0.18}^{+0.20}$ R$_\\oplus$. Both planets are likely mini-Neptunes.\n",
      "\n",
      "These planets are expected to produce radial velocity signals of 3.4 and 2.7 m/s respectively, which is smaller than the expected stellar variability in the optical ($\\simeq$30 m/s), making mass measurements unlikely in the optical, but possible with future near-infrared spectrographs. We use an injection-recovery test to place robust limits on additional planets in the system, and find that planets larger than 2 R$_\\oplus$ with periods of 1-20 d are unlikely. 0 into PostgreSQL...\n",
      "Inserting test sample 1466  Using data from the K2 Campaign 16, we report the discovery of a two-planet system in the Praesepe open cluster. The primary star, EPIC 249810660, is a mid-K dwarf with an age of approximately 600 million years, and the two planets have radii of 2.7 and 3.8 times that of Earth, and orbital periods of 3.6 and 7.4 days, respectively. Both planets have masses less than Neptune, and their densities suggest that they are composed predominantly of rock and/or ice. The two-planet system represents the first discovery of planets with well-defined radii and masses in Praesepe, and one of only a few exoplanet systems currently known in open clusters.\n",
      "\n",
      "The Praesepe open cluster provides an important sample for studying planet formation and evolution in open cluster environments. The presence of two planets in a relatively narrow range of masses and radii in this system adds to the growing number of well-characterized exoplanets in open clusters. Future studies of these planets, including radial velocity measurements and transit spectroscopy, will provide further insights into their atmospheric compositions and physical properties. The discovery of this two-planet system in Praesepe highlights the potential of using open clusters for exoplanet science, and underscores the importance of continued observations with the K2 mission and future facilities such as TESS and JWST. 1 into PostgreSQL...\n",
      "Inserting test sample 1467  We study the interactions of a possibly dense and/or quantum degenerate gas with driving light. Both the atoms and the electromagnetic fields are represented by quantum fields throughout the analysis. We introduce a field theory version of Markov and Born approximations for the interactions of light with matter, and devise a procedure whereby certain types of products of atom and light fields may be put to a desired, essentially normal, order. In the limit of low light intensity we find a hierarchy of equations of motion for correlation functions that contain one excited-atom field and one, two, three, etc., ground state atom fields. It is conjectured that the entire linear hierarchy may be solved by solving numerically the classical equations for the coupled system of electromagnetic fields and charged harmonic oscillators. We discuss the emergence of resonant dipole-dipole interactions and collective linewidths, and delineate the limits of validity of the column density approach in terms of non-cooperative atoms by presenting a mathematical example in which this approach is exact. 0 into PostgreSQL...\n",
      "Inserting test sample 1468  The cooperative response of atoms to a low light intensity holds an important place within the context of quantum field theory. The behavior of such a system is characterized by the emergence of collective dynamics, wherein the influence of each individual atom becomes entangled with that of its neighbors. The resulting coherence is described by a set of field equations, which involve the use of techniques like perturbation theory and renormalization. Recent work on this topic has focused on the role of density correlations and photon-mediated interactions in shaping the response of a dilute atomic gas. This has led to a deeper understanding of the fundamental mechanisms underlying light-matter interactions at the quantum level. A better comprehension of the quantum field theory of cooperative atom response under low light intensities can also have practical implications, for instance in the context of quantum communication and sensing. 1 into PostgreSQL...\n",
      "Inserting test sample 1469  The integration of electric vehicles (EVs) with the energy grid has become an important area of research due to the increasing EV penetration in today's transportation systems. Under appropriate management of EV charging and discharging, the grid can currently satisfy the energy requirements of a considerable number of EVs. Furthermore, EVs can help enhance the reliability and stability of the energy grid through ancillary services such as energy storage. This paper proposes the EV routing problem with time windows under time-variant electricity prices (EVRPTW-TP) which optimizes the routing of an EV fleet that are delivering products to customers, jointly with the scheduling of the charging and discharging of the EVs from/to the grid. The proposed model is a multiperiod vehicle routing problem where EVs can stop at charging stations to either recharge their batteries or inject stored energy to the grid. Given the energy costs that vary based on time-of-use, the charging and discharging schedules of the EVs are optimized to benefit from the capability of storing energy by shifting energy demands from peak hours to off-peak hours when the energy price is lower. The vehicles can recover the energy costs and potentially realize profits by injecting energy back to the grid at high price periods. EVRPTW-TP is formulated as an optimization problem. A Lagrangian relaxation approach and a hybrid variable neighborhood search/tabu search heuristic are proposed to obtain high quality lower bounds and feasible solutions, respectively. Numerical experiments on instances from the literature are provided. The proposed heuristic is also evaluated on a case study of an EV fleet providing grocery delivery at the region of Kitchener-Waterloo in Ontario, Canada. Insights on the impacts of energy pricing, service time slots, range reduction in winter as well as fleet size are presented. 0 into PostgreSQL...\n",
      "Inserting test sample 1470  The rise in electric vehicle (EV) utilization has created a need for effective optimization techniques for EV routing and charging/discharging. This paper proposes a solution for achieving efficient routing and charging by considering time-variant electricity prices. Electric vehicle routing and charging/discharging under time-variant electricity prices is a critical issue as the high costs of electricity can be a challenge for EV operation. This paper presents a model to solve this problem by considering the electricity price and charging demand for EVs in the network. The model is based on a mixed-integer linear programming (MILP) optimization approach, which maximizes the number of charged/discharged EVs while minimizing the electricity price.\n",
      "\n",
      "The proposed approach was evaluated using a simulation scenario based on a real-world small-scale power system. The simulation results showed that the proposed approach provides an efficient routing and charging solution under time-variant electricity prices. The results also showed that the proposed approach increases the number of fully charged EVs and lowers the total charging cost compared to other existing techniques.\n",
      "\n",
      "In conclusion, this paper presents a novel solution for achieving efficient routing and charging of electric vehicles by considering time-variant electricity prices. The proposed approach optimizes the routing and charging of EVs by minimizing the cost of electricity while maximizing the number of charged/discharged EVs. The results of simulation scenarios indicate that the proposed approach provides an efficient solution for electric vehicle routing and charging/discharging under time-variant electricity prices. This paper's findings have implications for policymakers and EV operators seeking to optimize EV routing and charging strategies. Additionally, this approach can be implemented in large-scale power systems and smart grids to support electric vehicle applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1471  The broadband X-ray spectra of AGNs contains information about the nuclear environment from Schwarzschild radii scales to distances of ~1 pc. In addition, the average shape of the X-ray spectrum is an important input into X-ray background synthesis models. Here, local (z \\approx 0) AGN luminosity functions (LFs) in five energy bands are used as a low-resolution, luminosity-dependent X-ray spectrometer in order to constrain the average AGN X-ray spectrum between 0.5 and 200 keV. The 15-55 keV LF measured by Swift-BAT is assumed to be the best determination of the local LF, and then a spectral model is varied to determine the best fit to the 0.5-2 keV, 2-10 keV, 3-20 keV and 14-195 keV LFs.\n",
      "\n",
      "The spectral model consists of a Gaussian distribution of power-laws with a mean photon-index <\\Gamma> and cutoff energy E_cut, as well as contributions from distant and disc reflection. The reflection strength is parameterised by varying the Fe abundance relative to solar, A_Fe, and requiring a specific Fe K equivalent width (EW). In this way, the presence of the X-ray Baldwin effect can be tested. The spectral model that best fits the four LFs has <\\Gamma> = 1.85 \\pm 0.15, E_cut=270^{+170}_{-80} keV, A_Fe=0.3^{+0.3}_{-0.15} (90% C.L.).\n",
      "\n",
      "The sub-solar A_Fe indicates the presence of strong reflection given the assumed Fe K EW. Indeed, parameterising the reflection strength with the R parameter gives R=1.7^{+1.7}_{-0.85}. There is moderate evidence for no X-ray Baldwin effect. A critical result of our procedure is that the shape of the local 2-10 keV LF measured by HEAO-1 and MAXI is incompatible with the LFs measured in the hard X-rays by Swift-BAT and RXTE. We therefore present a new determination of the local 2-10 keV LF that is consistent with all other energy bands.This new LF should be used to revise current measurements of the evolving AGN LF in the 2-10 keV band. 0 into PostgreSQL...\n",
      "Inserting test sample 1472  Active galactic nuclei (AGNs) are among the most energetic objects in the universe, capable of emitting radiation across the electromagnetic spectrum. In this study, we present the first determination of the average 0.5-200 keV spectrum of local AGNs. We analyze data from the Swift Burst Alert Telescope (BAT) and X-Ray Telescope (XRT), as well as from the Nuclear Spectroscopic Telescope Array (NuSTAR), which together provide a complete picture of the X-ray spectrum of AGNs out to high energies. Our analysis reveals a clear trend of increasing spectral hardness with increasing luminosity, which we interpret as evidence of a soft X-ray excess in low-luminosity AGNs.\n",
      "\n",
      "Motivated by our results, we also present a new determination of the 2-10 keV luminosity function of AGNs at z â‰ˆ 0. We combine data from the Chandra Deep Field-South and the COSMOS-Legacy surveys to derive a sample of nearly 2500 AGNs with well-constrained 2-10 keV luminosities. Using a maximum likelihood analysis, we fit a broken power-law model to the data, accounting for both the selection function and the Eddington bias.\n",
      "\n",
      "Our best-fit model reveals a break in the luminosity function around L* â‰ˆ 3 Ã— 10^42 erg s^-1, with a steep faint-end slope of Î± â‰ˆ 1.8 below the break and a shallower bright-end slope of Î² â‰ˆ 0.5 above it. We compare our results to previous determinations of the AGN luminosity function and find good agreement, indicating that our method correctly accounts for selection biases and other systematic effects.\n",
      "\n",
      "Overall, our study provides new insights into the X-ray properties of AGNs and their spatial distribution in the local universe. Our determination of the average 0.5-200 keV spectrum of AGNs can serve as a valuable reference for future observational and theoretical studies of these fascinating objects. 1 into PostgreSQL...\n",
      "Inserting test sample 1473  Interplay between orbital and spin degrees of freedom is theoretically studied for the phase transition to the spin-singlet state with lattice dimerization in pyroxene titanium oxides ATiSi2O6 (A=Na, Li). For the quasi one-dimensional spin-1/2 systems, we derive an effective spin-orbital-lattice coupled model in the strong correlation limit with explicitly taking account of the t_2g orbital degeneracy, and investigate the model by numerical simulation as well as the mean-field analysis. We find a nontrivial feedback effect between orbital and spin degrees of freedom; as temperature decreases, development of antiferromagnetic spin correlations changes the sign of orbital correlations from antiferro to ferro type, and finally the ferro-type orbital correlations induce the dimerization and the spin-singlet formation. As a result of this interplay, the system undergoes a finite-temperature transition to the spin-dimer and orbital-ferro ordered phase concomitant with the Jahn-Teller lattice distortion. The numerical results for the magnetic susceptibility show a deviation from the Curie-Weiss behavior, and well reproduce the experimental data. The results reveal that the Jahn-Teller energy scale is considerably small and the orbital and spin exchange interactions play a decisive role in the pyroxene titanium oxides. 0 into PostgreSQL...\n",
      "Inserting test sample 1474  Pyroxene titanium oxides are a fascinating class of materials that have been extensively investigated due to their unique structural and electronic properties. In this study, we investigate the interplay between orbital and spin degrees of freedom that lead to the formation of a magnetic spin-gap in the ATiSi2O6 (A=Na, Li) compounds. We use a combination of experimental techniques, including neutron diffraction and magnetic susceptibility measurements, along with first-principles calculations based on density functional theory and dynamical mean-field theory.\n",
      "\n",
      "Our results reveal that the formation of the spin-gap in these compounds is a consequence of a subtle interplay between the orbital and spin degrees of freedom. Specifically, we find that a cooperative Jahn-Teller effect, which arises due to the distortion of the TiO6 octahedra, plays a crucial role in promoting spin-gap formation. Our theoretical calculations reproduce the experimental trends and provide further insights into the underlying physics of the spin-gap formation.\n",
      "\n",
      "Our study sheds light on the mechanism of magnetic ordering in pyroxene titanium oxides and demonstrates the importance of the interplay between orbital and spin degrees of freedom in promoting novel magnetic behavior in complex materials. 1 into PostgreSQL...\n",
      "Inserting test sample 1475  We study the behaviour of weak gravitational fields in the 6D Cascading DGP model using a bulk-based approach. To deal with the ambiguity in the thin limit of branes of codimension higher than one, we consider a specific regularization of the internal structure of the branes where the 5D brane can be considered thin with respect to the 4D one. We consider the solutions corresponding to pure tension sources on the 4D brane, and study perturbations at first order around these background solutions. We adopt a 4D scalar-vector-tensor decomposition, and focus on the scalar sector of perturbations. We show that, in a suitable 4D limit, the trace part of the 4D metric perturbations obeys a decoupled equation which suggests that it is a ghost for background tensions smaller than a critical tension, while it is a healthy field otherwise. We give a geometrical interpretation of the existence of the critical tension and of the reason why the relevant field is a ghost or not depending on the background tension. We however find a value of the critical tension which is different from the one already found in the literature. Differently from the results in the literature, our analysis implies that, choosing the background tension suitably, we can construct ghost-free models for any value of the free parameters of the theory. We suggest that the difference lies in the procedure used to evaluate the pillbox integration across the codimension-2 brane. We confirm the validity of our analysis by performing numerically the integration in a particular case where the solution inside the thick cod-2 brane is known exactly. We stress that the singular structure of the perturbation fields in the nested branes set-ups is very subtle, and that great care has to be taken when deriving the codimension-2 junction conditions. 0 into PostgreSQL...\n",
      "Inserting test sample 1476  In recent years, the Cascading DGP model has gained popularity in the study of network dynamics due to its ability to incorporate the effects of both topology and dynamics. However, an issue that has been identified in this model is the presence of critical tension, which arises when the network is pushed beyond its optimal operating capacity. This critical tension is characterized by a cascade of failures that occur when individual components fail and trigger failures in connected components. To better understand this phenomenon, we examined the behavior of the Cascading DGP model under varying degrees of stress and identified the key factors that contribute to the occurrence of critical tension.\n",
      "\n",
      "Our analysis revealed that critical tension in the Cascading DGP model is highly dependent on the topology of the network and the strength of the connections between components. Specifically, when the network is highly interconnected and tightly coupled, the effects of critical tension are particularly pronounced. Moreover, we found that the timing of component failures also plays a significant role in the generation of critical tension, with simultaneous failures exacerbating the cascading effects.\n",
      "\n",
      "To address the issue of critical tension in the Cascading DGP model, we propose a novel approach that involves the use of distributed algorithms to dynamically adjust the connectivity of the network in response to changing conditions. Our simulation results show that this approach effectively mitigates the effects of critical tension, resulting in a more robust and resilient network.\n",
      "\n",
      "Overall, our study provides new insights into the behavior of the Cascading DGP model and highlights the importance of considering critical tension in the design and analysis of dynamic network systems. By understanding the mechanisms underlying critical tension, we can develop more effective strategies for improving the reliability and performance of these systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1477  We present deep JVLA observations at 1.4 GHz and 2.7 GHz (full polarization), as well as optical OmegaWINGS/WINGS and X-ray observations of two extended radio galaxies in the IIZW108 galaxy cluster at z = 0.04889. They show a bent tail morphology in agreement with a radio lobed galaxy falling into the cluster potential. Both galaxies are found to possess properties comparable with {narrow-angle} tail galaxies in the literature even though they are part of a low mass cluster. We find a spectral index steepening and an increase in fractional polarization through the galaxy jets and an ordered magnetic field component mostly aligned with the jet direction. This is likely caused by either shear due to the velocity difference of the intracluster medium and the jet fluid and/or magnetic draping of the intracluster medium across the galaxy jets. We find clear evidence that one source is showing two active galactic nuclei (AGN) outbursts from which we expect the AGN has never turned off completely. We show that pure standard electron cooling cannot explain the jet length. We demonstrate therefore that these galaxies can be used as a laboratory to study gentle re-acceleration of relativistic electrons in galaxy jets via transition from laminar to turbulent motion. 0 into PostgreSQL...\n",
      "Inserting test sample 1478  The galaxy cluster IIZW108 has recently been found to host two head-tail galaxies that exhibit peculiar characteristics. In this study, we investigate the physical mechanisms behind their unique appearances, which are likely caused by the cluster's turbulent environment. Using multi-wavelength observations and simulations, we explore the role of magnetic fields in the formation of the tail structures, including their compression and stretching. Moreover, we examine the possibility of particle re-acceleration by turbulence in the intracluster medium, which may contribute to the observed radio emission. Our results suggest that the two head-tail galaxies are in different stages of interaction with the surrounding medium, with one experiencing a strong ram-pressure stripping that has already disrupted its magnetic field, while the other is still undergoing compression by the intracluster gas. We also discuss the implications of these findings for our understanding of the transition to turbulence in galaxy clusters and the role of magnetic fields and plasma physics in shaping the morphology of galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 1479  The Hayabusa spacecraft rendezvoused with its target asteroid 25143 Itokawa in 2005 and brought an asteroidal sample back to the Earth in 2010. The onboard camera, AMICA, took more than 1400 images of Itokawa during the rendezvous phase. It was reported that the AMICA images were severely contaminated by light scatter inside the optics. The effect made it difficult to produce the color maps at longer wavelengths (>800 nm). In this paper, we demonstrate a method to subtract the scattered light by investigating the dim halos of Itokawa and the Moon taken by AMICA during the inflight operation. As the result, we found that the overall data reduction scheme including the scattered light correction enables to recognize ~3% regional differences in the relative reflectance spectra of Itokawa. We confirmed that the color variation in Itokawa was largely attributed to space weathering. 0 into PostgreSQL...\n",
      "Inserting test sample 1480  This research paper presents a scattered light correction method for HAYABUSA/AMICA data that facilitates quantitative analysis of spectral data obtained from the Itokawa asteroid surface. We developed a correction algorithm to account for the scattered sunlight that interferes with the spectral measurements. The comparisons of the corrected spectra with laboratory measurements of known mineral samples show good agreement, validating the correction method. Quantitative analysis of Itokawa spectra reveals the presence of various types of surface materials, including olivine and pyroxene. Our analysis also suggests that Itokawa's surface may have undergone thermal metamorphism, which could affect the interpretation of spectroscopic data. Overall, the presented data and correction method contribute to a better understanding of the mineralogy of Itokawa and enhance our ability to study the geological evolution of asteroids. 1 into PostgreSQL...\n",
      "Inserting test sample 1481  The supersymmetric SO(10) GUT based on the ${\\bf{210\\oplus 10\\oplus 120\\oplus 126\\oplus {\\bar {126}}}}$ Higgs system provides a minimal framework for the emergence of the R-parity exact MSSM at low energies and a viable supersymmetric seesaw explanation for the observed neutrino masses and mixing angles. We present formulae for MSSM decomposition of the superpotential invariants, tree level light charged fermion effective Yukawa couplings, Weinberg neutrino mass generation operator, and the $d=5,\\Delta B=\\Delta L \\neq 0$ effective superpotential in terms of GUT parameters. We use them to determine fits of the 18 available fermion mass-mixing data in terms of the superpotential parameters of the NMSGUT and SUGRY(NUHM) type soft supersymmetry breaking parameters ($\\{m_{\\tilde f},m_{1/2},A_0,M^2_{H,\\bar H}\\} $) specified at the MSSM one loop unification scale $M_X^0=10^{16.33} $ GeV. Our fits are compatible with electroweak symmetry breaking and Unification constraints and yield right-handed neutrino masses in the leptogenesis relevant range : $10^8-10^{13} $ GeV. Matching the SM data requires lowering the strange and down quark Yukawas in the MSSM via large $\\tan\\beta$ driven threshold corrections and characteristic soft Susy breaking spectra. The Susy spectra have light pure Bino LSP, heavy exotic Higgs(inos) and large $ \\mu,A_0,M_{H,\\bar{H}}$ parameters $\\sim 100$ TeV. Typically third generation sfermions are much \\emph{heavier} than the first two generations. The smuon is often the lightest charged sfermion thus offering a Bino-CDM co-annihilation channel. The parameter sets obtained are used to calculate B violation rates which are found to be generically much faster($\\sim 10^{-28}\\, yr^{-1}$) than the current experimental limits. Improvements which may allow acceptable B violation rates are identified. 0 into PostgreSQL...\n",
      "Inserting test sample 1482  This paper presents an analysis of the Minimal Supersymmetric Grand Unified Theory (SUSY GUT), which incorporates a new framework for interpreting experimental data on particle spectra and fermion fits. We first explore the key features of the model and discuss the role of the gauge symmetries in SUSY GUTs. Next, we employ a combination of theoretical calculations and experimental measurements to derive predictions for the masses and interactions of the fundamental particles in the theory, including quarks, leptons, and gauge bosons. We then apply a rigorous Renormalization Group (RG) analysis to the theory's parameters in order to elucidate the behavior of these parameters at different energy scales, ultimately shedding light on SUSY breaking and its implications for particle physics. Furthermore, we investigate the fermion masses and mixing angles in the theory, as determined through a variety of fits to experimental data. We also investigate the implications of the theory for neutrino physics, addressing the question of whether neutrinos in SUSY GUTs can be Majorana particles. Our results demonstrate the viability of the new SUSY GUT framework, pointing towards potentially fruitful avenues for future theoretical and experimental research. Specifically, we show that the new framework yields predictions that are consistent with available experimental data on particle spectra and fermion fits. Our investigation of the RG flow of the theory's parameters yields insights into SUSY breaking, while our study of neutrino physics provides a promising avenue for future investigation. 1 into PostgreSQL...\n",
      "Inserting test sample 1483  We investigate the superfluidity and the associated Nambu-Goldstone modes in a three-flavor atomic Fermi gas with SU(3) global symmetry. The s-wave pairing occurs in flavor anti-triplet channel due to the Pauli principle, and the superfluid state contains both gapped and gapless fermionic excitations.\n",
      "\n",
      "Corresponding to the spontaneous breaking of the SU(3) symmetry to a SU(2) symmetry with five broken generators, there are only three Nambu-Goldstone modes, one is with linear dispersion law and two are with quadratic dispersion law. The other two expected Nambu-Goldstone modes become massive with a mass gap of the order of the fermion energy gap in a wide coupling range. The abnormal number of Nambu-Goldstone modes, the quadratic dispersion law and the mass gap have significant effect on the low temperature thermodynamics of the matter. 0 into PostgreSQL...\n",
      "Inserting test sample 1484  This paper explores the superfluid behavior of a three-flavor Fermi gas with SU(3) symmetry. We investigate the properties of this system under the influence of strong interactions between atoms and determine the conditions necessary for superfluidity to manifest. Our analysis is based on a combination of analytical and numerical methods, including the mean-field approximation and Monte Carlo simulations. By investigating the spectrum and the pairing gaps of the system, we demonstrate that superfluidity arises via the spontaneous breaking of the symmetry. Additionally, we examine the role of the symmetry-breaking terms and the impact of external perturbations on the superfluid behavior of the described system. Our findings provide new insights into the nature of superfluidity in Fermi gases with more complex internal degrees of freedom and symmetries. 1 into PostgreSQL...\n",
      "Inserting test sample 1485  In 2019, outbreaks of vaccine-preventable diseases reached the highest number in the US since 1992. Medical misinformation, such as antivaccine content propagating through social media, is associated with increases in vaccine delay and refusal. Our overall goal is to develop an automatic detector for antivaccine messages to counteract the negative impact that antivaccine messages have on the public health. Very few extant detection systems have considered multimodality of social media posts (images, texts, and hashtags), and instead focus on textual components, despite the rapid growth of photo-sharing applications (e.g., Instagram). As a result, existing systems are not sufficient for detecting antivaccine messages with heavy visual components (e.g., images) posted on these newer platforms. To solve this problem, we propose a deep learning network that leverages both visual and textual information. A new semantic- and task-level attention mechanism was created to help our model to focus on the essential contents of a post that signal antivaccine messages. The proposed model, which consists of three branches, can generate comprehensive fused features for predictions. Moreover, an ensemble method is proposed to further improve the final prediction accuracy. To evaluate the proposed model's performance, a real-world social media dataset that consists of more than 30,000 samples was collected from Instagram between January 2016 and October 2019. Our 30 experiment results demonstrate that the final network achieves above 97% testing accuracy and outperforms other relevant models, demonstrating that it can detect a large amount of antivaccine messages posted daily. The implementation code is available at https://github.com/wzhings/antivaccine_detection. 0 into PostgreSQL...\n",
      "Inserting test sample 1486  In recent years, social media has become a powerful platform for sharing information, particularly in the area of healthcare. Although social media has the potential to significantly benefit public health, it also poses significant dangers, particularly when false or misleading information is disseminated. In response to this challenge, researchers have proposed various methods for detecting medical misinformation on social media, including those based on deep learning techniques.\n",
      "\n",
      "This paper presents a novel approach to detecting medical misinformation on social media using multimodal deep learning. Specifically, we propose a model that combines textual, visual, and temporal features to identify instances of misinformation. Our model is trained on a large dataset of labeled social media posts and achieves state-of-the-art performance.\n",
      "\n",
      "To test the effectiveness of our approach, we conduct a series of experiments on two popular social media platforms: Twitter and Reddit. Our results show that our approach is able to accurately detect instances of medical misinformation with high precision and recall. Moreover, our experiments demonstrate that our model is robust to different types of misinformation and can identify misinformation in real-time.\n",
      "\n",
      "Our work has important implications for public health and can help mitigate the risks associated with medical misinformation on social media. By providing a powerful and reliable tool for identifying misinformation, we hope to empower healthcare professionals and the broader public to make informed decisions about their health. 1 into PostgreSQL...\n",
      "Inserting test sample 1487  We report slowed propagation and storage and retrieval of thermal light in warm rubidium vapor using the effect of electromagnetically-induced transparency (EIT). We first demonstrate slowed-propagation of the probe thermal light beam through an EIT medium by measuring the second-order correlation function of the light field using the Hanbury-Brown$-$Twiss interferometer. We also report an experimental study on the effect of the EIT slow-light medium on the temporal coherence of thermal light. Finally, we demonstrate the storage and retrieval of thermal light beam in the EIT medium.\n",
      "\n",
      "The direct measurement of the photon number statistics of the retrieved light field shows that the photon number statistics is preserved during the storage and retrieval process. 0 into PostgreSQL...\n",
      "Inserting test sample 1488  This study investigates the possibility of storing and retrieving thermal light in warm atomic vapor. By using a process called electromagnetically induced transparency, we demonstrate that thermal light can be converted into a collective atomic excitation and stored in a buffer gas cell for up to several seconds. Moreover, we show that the stored atomic excitation can be efficiently retrieved by a control field. The results suggest that warm atomic vapor can serve as an effective and practical quantum memory for thermal light, which has important implications for quantum information processing and quantum communication. Our work provides a valuable reference for further exploration of the storage and retrieval of light in warm atomic media. 1 into PostgreSQL...\n",
      "Inserting test sample 1489  Vega and Fomalhaut, are similar in terms of mass, ages, and global debris disk properties; therefore, they are often referred as \"debris disk twins\". We present Spitzer 10-35 um spectroscopic data centered at both stars, and identify warm, unresolved excess emission in the close vicinity of Vega for the first time. The properties of the warm excess in Vega are further characterized with ancillary photometry in the mid infrared and resolved images in the far-infrared and submillimeter wavelengths. The Vega warm excess shares many similar properties with the one found around Fomalhaut. The emission shortward of ~30 um from both warm components is well described as a blackbody emission of ~170 K. Interestingly, two other systems, eps Eri and HR 8799, also show such an unresolved warm dust using the same approach. These warm components may be analogous to the solar system's zodiacal dust cloud, but of far greater. The dust temperature and tentative detections in the submillimeter suggest the warm excess arises from dust associated with a planetesimal ring located near the water-frost line and presumably created by processes occurring at similar locations in other debris systems as well. We also review the properties of the 2 um hot excess around Vega and Fomalhaut, showing that the dust responsible for the hot excess is not spatially associated with the dust we detected in the warm belt. We suggest it may arise from hot nano grains trapped in the magnetic field of the star. Finally, the separation between the warm and cold belt is rather large with an orbital ratio >~10 in all four systems. In light of the current upper limits on the masses of planetary objects and the large gap, we discuss the possible implications for their underlying planetary architecture, and suggest that multiple, low-mass planets likely reside between the two belts in Vega and Fomalhaut. 0 into PostgreSQL...\n",
      "Inserting test sample 1490  Recent studies have shown that the presence of asteroid belts in debris disks is a common occurrence in many star systems. In this paper, we present a detailed analysis of the asteroid belts found in two debris disk systems: VEGA and FOMALHAUT.\n",
      "\n",
      "Our study, based on data from various telescopes and surveys, reveals that both VEGA and FOMALHAUT have similar asteroid belts that are located at distances ranging between 80 and 150 astronomical units from their respective stars. However, the size distribution of the asteroids in these belts is quite different. While the VEGA asteroid belt contains primarily small, rocky planets, the FOMALHAUT asteroid belt is dominated by much larger bodies, some of which may be classified as minor planets.\n",
      "\n",
      "We also found that the VEGA and FOMALHAUT asteroid belts are particularly dense, with a high concentration of asteroids per volume. This suggests that the belts may have been created through a large-scale collision event between multiple protoplanets or dwarf planets in each system's early history.\n",
      "\n",
      "Our analysis further suggests that the presence and structures of asteroid belts in these two debris disks may have significant implications for the formation and evolution of planetary systems in general. The asteroid belts found in VEGA and FOMALHAUT may be indicative of the presence of other rocky planets in these star systems, and could provide valuable insights into the formation and evolution of such planets.\n",
      "\n",
      "In conclusion, our study provides a comprehensive analysis of the asteroid belts found in two debris disk systems: VEGA and FOMALHAUT. Despite their similarities in location, the differences in the size distribution of their asteroids suggest that the evolution of these systems may have been distinct. The findings of this research could offer insights into the origins of planetary systems and planet formation, and could drive future discoveries in this area of study. 1 into PostgreSQL...\n",
      "Inserting test sample 1491  The paper is devoted to the existence of positive solutions of nonlinear elliptic equations with $p$-Laplacian. We provide a general topological degree that detects solutions of the problem $$ \\{{array}{l} A(u)=F(u) u\\in M {array}.\n",
      "\n",
      "$$ where $A:X\\supset D(A)\\to X^*$ is a maximal monotone operator in a Banach space $X$ and $F:M\\to X^*$ is a continuous mapping defined on a closed convex cone $M\\subset X$. Next, we apply this general framework to a class of partial differential equations with $p$-Laplacian under Dirichlet boundary conditions. 0 into PostgreSQL...\n",
      "Inserting test sample 1492  In this study, we investigate the existence of positive stationary solutions for p-Laplacian problems with a nonpositive perturbation. Our research demonstrates that certain sets of conditions, including subcriticality and non-degeneracy of the problem, guarantee the existence of positive solutions. We also show that certain properties of the perturbation, such as its compact support and asymptotic behavior, can impact the existence and behavior of positive solutions. Our results have important implications for the study of differential equations and provide new insights into the interplay between nonlinear operators and perturbations in mathematical models. 1 into PostgreSQL...\n",
      "Inserting test sample 1493  We address the internal support against total free-fall collapse of the giant clumps that form by violent gravitational instability in high-z disc galaxies.\n",
      "\n",
      "Guidance is provided by an analytic model, where the proto-clumps are cut from a rotating disc and collapse to equilibrium while preserving angular momentum.\n",
      "\n",
      "This model predicts prograde clump rotation. This is confirmed in hydro-AMR zoom-in simulations of galaxies in a cosmological context. In most high-z clumps, the centrifugal force dominates the support, R=Vrot^2/Vcirc^2 > 0.5, where Vrot is the rotation velocity and Vcirc is the circular velocity. The clump spin indeed tends to be in the sense of the global disc angular momentum, but substantial tilts are frequent. Most clumps are in Jeans equilibrium, with the rest of the support provided by turbulence. Simulations of isolated gas-rich discs that resolve the clump substructure reveal that the cosmological simulations may overestimate R by ~30%, but the dominance of rotational support at high-z is not a resolution artifact. In turn, isolated gas-poor disc simulations produce at z=0 smaller gaseous non-rotating transient clouds, indicating that the difference in rotational support is associated with the fraction of cold baryons in the disc. In our current cosmological simulations, the clump rotation velocity is typically Vrot~100 km/s, but when beam smearing of \\geq 0.1 arcsec is imposed, the rotation signal is reduced to a small gradient of \\leq 30 km/s/kpc across the clump. The velocity dispersion in the simulated clumps is comparable to the disc dispersion so it is expected to leave only a marginal signal. Retrograde minor-merging galaxies could lead to massive clumps that do not show rotation.Testable predictions of the scenario as simulated are that the mean stellar age of the clumps, and the stellar fraction, are declining linearly with distance from the disc center. 0 into PostgreSQL...\n",
      "Inserting test sample 1494  This study explores the role of rotational support in the formation and dynamics of giant clumps in high-redshift disc galaxies. Using high-resolution cosmological hydrodynamical simulations, we investigate the morphological and kinematic properties of these clumps and the underlying galaxies they are embedded in. Our simulations include a detailed treatment of gas physics, star formation, and feedback from stellar winds and supernovae. \n",
      "\n",
      "We find that the clumps are formed through gravitational instability in the gas-rich discs, and their sizes and velocities are strongly influenced by the rotational support of the disc. The most massive clumps tend to be located in the outer regions of the disc where the rotational velocities are highest, while the smaller and slower clumps are formed in the inner regions where the rotational support is weaker. \n",
      "\n",
      "Using carefully selected observational and numerical diagnostics, we show that our simulated galaxies reproduce various observed properties of high-redshift clumpy discs, including their gas fractions, sizes, velocity dispersions, and specific star formation rates. Furthermore, we demonstrate that different measures of rotational support (such as the specific angular momentum and the rotational parameter) are strongly correlated with the masses and velocities of the clumps. \n",
      "\n",
      "We also investigate the impact of feedback on the clumps and their surroundings. Our results suggest that feedback from young massive stars can disrupt and disperse the gas in the clumps, leading to a slower star formation rate and a shorter clump lifetime. However, the global impact of feedback on the disc and the clump formation remains a complex and open question. \n",
      "\n",
      "Our study provides important insights into the physical origin and evolution of giant clumps in high-redshift disc galaxies. We conclude that the rotational support of the disc plays a crucial role in shaping the clumps and their properties, and that future observations and simulations can use this as a valuable diagnostic tool. 1 into PostgreSQL...\n",
      "Inserting test sample 1495  General equations for conservative yet dissipative (entropy producing) extended magnetohydrodynamics are derived from two-fluid theory. Keeping all terms generates unusual cross-effects, such as thermophoresis and a current viscosity that mixes with the usual velocity viscosity. While the Poisson bracket of the ideal version of this model have already been discovered, we determine its metriplectic counterpart that describes the dissipation. This is done using a new and general thermodynamic point of view for deriving dissipative brackets, a means of derivation that is natural for understanding and creating dissipative brackets. Finally the formalism is used to study dissipation in the Lagrangian variable picture where, in the context of extended magnetohydrodynamics, nonlocal dissipative brackets naturally emerge. 0 into PostgreSQL...\n",
      "Inserting test sample 1496  In this paper, we present a general metriplectic framework to study dissipative extended magnetohydrodynamics (MHD). Our approach combines the advantages of Hamiltonian and Lagrangian formulations to provide a powerful tool for investigating complex MHD systems with dissipation. The framework is demonstrated through several examples of MHD systems with different levels of complexity. We show that our approach is applicable to both ideal and non-ideal MHD systems, and it can capture important features such as magnetic helicity and cross-helicity. Our results provide a deeper understanding of the dynamics of MHD systems with dissipation and pave the way for future research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 1497  Obscured active galactic nuclei (AGNs) are thought to be very common in the Universe. Observations and surveys have shown that the number of sources increases for near galaxies and at the low-luminosity regime (the so-called LLAGNs). Furthermore, many AGNs show changes in their obscuration properties at X-rays that may suggest a configuration of clouds very close to the accretion disk. However, these variations could also be due to changes in the intrinsic continuum of the source. It is therefore important to study nearby AGN to better understand the locus and distribution of clouds in the neighbourhood of the nucleus. We aim to study the nuclear obscuration of LLAGN NGC835 and its extended emission using mid-infrared observations. We present mid-infrared 11.5 microns imaging of the LLAGN galaxy NGC835 obtained with the instrument CanariCam in the Gran Telescopio CANARIAS (GTC), archival Spitzer/IRS spectroscopy, and archival Chandra data observed in 2000, 2008, and 2013. The GTC/CanariCam 11.5 microns image reveals faint extended emission out to ~6 arcsec. We obtained a nuclear flux of F(11.5 microns) ~18 mJy, whereas the extended emission accounts for 90% of the total flux within the 6 arcsec. This means that the low angular resolution (~4 arcsec) IRS spectrum is dominated by this extended emission and not by the AGN, clearly seen in the Spitzer/IRS spectrum. Although the extended soft X-ray emission shows some resemblance with that of the mid-infrared, the knots seen at X-rays are mostly located in the inner side of this mid-infrared emission. The nuclear X-ray spectrum of the source has undergone a spectral change between 2000/2008 and 2013. We argue that this is most probably due to changes in the hydrogen column density from ~ 8x10E+23 cm-2 to ~ 3x10E+23 cm-2. NGC835 therefore is one of the few LLAGN, together with NGC1052, in which changes in the absorber can be claimed. 0 into PostgreSQL...\n",
      "Inserting test sample 1498  Understanding the mechanisms that drive the variability of low-luminosity active galactic nuclei (LLAGNs), such as NGC835, is crucial to our comprehension of the accretion processes that power these systems. In this work, we report on the long-term X-ray monitoring campaign of NGC835, focusing on its circumnuclear emission. We present the results of the X-ray spectral analysis, which indicate the presence of two distinct spectral components. The hard component dominates above â‰ˆ 2 keV, and its flux exhibits a significant variability, ranging from a factor of â‰ˆ 2 to 7. The soft component dominates below â‰ˆ 2 keV, and its flux remains significantly constant over the entire period of observation. The observed variability of the hard component can be explained by the intrinsic variation of the LLAGN, while the soft component is likely due to the circumnuclear emission. Furthermore, we investigate the temporal correlation between the X-ray emission and the [O III] emission line flux. We found no significant correlation, implying distinct physical origins for the X-ray and [O III] emission. Finally, we estimated the accretion rate onto the central black hole of NGC835, assuming a range of models for the Comptonization process. Our results suggest that the accretion rate is low, within the range of (0.2âˆ’5) Ã— 10âˆ’4 M âŠ™ yrâˆ’1. Overall, our X-ray monitoring campaign provides new insights into the long-term variability of LLAGN hosted in massive early-type galaxies, and constitutes fundamental steps towards a comprehensive understanding of the accretion processes that power these systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1499  A weak law of large numbers is established for a sequence of systems of N classical point particles with logarithmic pair potential in $\\bbR^n$, or $\\bbS^n$, $n\\in \\bbN$, which are distributed according to the configurational microcanonical measure $\\delta(E-H)$, or rather some regularization thereof, where H is the configurational Hamiltonian and E the configurational energy.\n",
      "\n",
      "When $N\\to\\infty$ with non-extensive energy scaling $E=N^2 \\vareps$, the particle positions become i.i.d. according to a self-consistent Boltzmann distribution, respectively a superposition of such distributions. The self-consistency condition in n dimensions is some nonlinear elliptic PDE of order n (pseudo-PDE if n is odd) with an exponential nonlinearity. When n=2, this PDE is known in statistical mechanics as Poisson-Boltzmann equation, with applications to point vortices, 2D Coulomb and magnetized plasmas and gravitational systems. It is then also known in conformal differential geometry, where it is the central equation in Nirenberg's problem of prescribed Gaussian curvature. For constant Gauss curvature it becomes Liouville's equation, which also appears in two-dimensional so-called quantum Liouville gravity. The PDE for n=4 is Paneitz' equation, and while it is not known in statistical mechanics, it originated from a study of the conformal invariance of Maxwell's electromagnetism and has made its appearance in some recent model of four-dimensional quantum gravity. In differential geometry, the Paneitz equation and its higher order n generalizations have applications in the conformal geometry of n-manifolds, but no physical applications yet for general n. Interestingly, though, all the Paneitz equations have an interpretation in terms of statistical mechanics. 0 into PostgreSQL...\n",
      "Inserting test sample 1500  This paper presents a statistical mechanics approach to some fundamental problems in conformal geometry. Conformal geometry is a branch of mathematics that studies the geometric properties of objects and spaces that are preserved under conformal transformations. These transformations include dilations, translations, rotations, and inversions, and play a critical role in a wide range of scientific fields.\n",
      "\n",
      "The goal of this paper is to apply statistical mechanics techniques to these geometrical problems and provide a new perspective on them. Statistical mechanics is the branch of physics that studies the behavior of large systems by analyzing the properties of their constituent particles. By applying these methods to problems in conformal geometry, we can gain insights into the underlying physical principles governing these systems, and potentially develop new tools for solving them.\n",
      "\n",
      "The paper begins by introducing the basic concepts of conformal geometry and statistical mechanics. We then demonstrate how these two fields can be combined to solve some fundamental problems, such as the classification of conformal classes of metrics on surfaces and the study of asymptotic behavior of several systems, including the regularity of harmonic maps between surfaces.\n",
      "\n",
      "The approach presented in this paper has the potential to advance our understanding of the geometrical and physical principles governing conformal transformations. Furthermore, it provides a powerful mathematical tool which can be applied to a broad range of fields, including theoretical physics, differential geometry, and materials science. Overall, this paper demonstrates the potential for interdisciplinary collaboration and innovation between statistical mechanics and conformal geometry. 1 into PostgreSQL...\n",
      "Inserting test sample 1501  The existence of neutrino mass and mixing is a strong pointer towards physics beyond the standard model. An overview of the possibility of having neutrino masses in supersymmetric theories is attempted here. Some of the recent works reviewed suggest Dirac masses, whereas others include Majorana masses as well.\n",
      "\n",
      "Side by side, it is shown how R-parity violating supersymmetry opens new avenues in the neutrino sector. Reference is also made to light sterile neutrinos, nearly degenerate neutrinos and neutrinos acquiring masses from hard supersymmetry breaking terms which are suppressed by the Planck scale. In several of the cases, it is pointed out how the models that give neutrino masses and mixing have independent motivations of their own, and can be tested in accelerator experiments. 0 into PostgreSQL...\n",
      "Inserting test sample 1502  Supersymmetry and neutrino mass are two fundamental concepts in modern particle physics. Neutrinos have been observed to have minute but non-zero masses, which cannot be explained within the Standard Model of particle physics. Supersymmetry, on the other hand, proposes a symmetry between particles with integer and half-integer spin, and could have important implications for the Higgs boson and dark matter. As a result, exploring the connections between supersymmetry and neutrino mass has become a topic of great interest in the field. This paper reviews the theoretical framework of supersymmetry and its role in explaining neutrino masses. We also discuss the experimental evidence supporting supersymmetry and its implications for future research. 1 into PostgreSQL...\n",
      "Inserting test sample 1503  We study the physics of 3d supersymmetric abelian gauge theories (with small supersymmetry breaking perturbations) at finite density. Using mirror symmetry, which provides a natural generalization of the duality between the XY model and the abelian Higgs model but now including fermionic fields, we see many dynamical phenomena conjectured to be of relevance in condensed matter systems.\n",
      "\n",
      "In particular, we find examples of the emergence of a Fermi surface at low energies from hybridization of fermions localized at magnetic defects at high energies, as well as fractionalization of charged fermions into spinon-holon pairs with the concomitant appearance of emergent gauge fields. We also find dual descriptions for Fermi surfaces coupled to critical bosons, which give rise to non-Fermi liquids. 0 into PostgreSQL...\n",
      "Inserting test sample 1504  The study of supersymmetric Quantum Electrodynamics (QED) reveals new emergent Fermi surfaces and novel phases of matter with exotic fractionalization properties. The peculiar behavior of these phases is explained in terms of duality transformations, where the dynamics of electric and magnetic charges is interchanged. In particular, the duality between electron and magnetic-monopole excitations plays a fundamental role in the formation of the emergent Fermi surfaces and the fractionalization of elementary excitations in 2+1D QED. This work presents a comprehensive analysis of these phases and fractionalization phenomena, and shows how the intricate interplay between topological effects, supersymmetry-breaking terms, and electromagnetic interactions can lead to nontrivial modifications of the underlying theory and the emergence of fundamentally new phases of matter. 1 into PostgreSQL...\n",
      "Inserting test sample 1505  Methane is considered being a good choice as a propellant for future reusable launch systems. However, the heat transfer prediction for supercritical methane flowing in cooling channels of a regeneratively cooled combustion chamber is challenging. Because accurate heat transfer predictions are essential to design reliable and efficient cooling systems, heat transfer modeling is a fundamental issue to address. Advanced computational fluid dynamics (CFD) calculations achieve sufficient accuracy, but the associated computational cost prevents an efficient integration in optimization loops. Surrogate models based on artificial neural networks (ANNs) offer a great speed advantage. It is shown that an ANN, trained on data extracted from samples of CFD simulations, is able to predict the maximum wall temperature along straight rocket engine cooling channels using methane with convincing precision. The combination of the ANN model with simple relations for pressure drop and enthalpy rise results in a complete reduced order model, which can be used for numerically efficient design space exploration and optimization. 0 into PostgreSQL...\n",
      "Inserting test sample 1506  This paper proposes a novel method for predicting heat transfer in methane regenerative cooling channels using artificial neural networks. Regenerative cooling is widely used in the aerospace industry to prevent overheating of components, hence accurate prediction of heat transfer is crucial for reliable and efficient system performance. However, existing numerical methods for predicting heat transfer in such channels are computationally expensive and time-consuming. To overcome these limitations, we develop a new approach based on artificial neural networks, which significantly reduces computational costs while maintaining high accuracy. Our results demonstrate that the proposed method outperforms existing numerical methods, with a mean absolute error of 0.005 compared to 0.14 for the traditional numerical approach. Overall, this work offers a valuable new tool for designing and optimizing regenerative cooling systems, with multiple applications in the aerospace industry and beyond. 1 into PostgreSQL...\n",
      "Inserting test sample 1507  This is the lecture notes on the interplay between optimal transport and Riemannian geometry. On a Riemannian manifold, the convexity of entropy along optimal transport in the space of probability measures characterizes lower bounds of the Ricci curvature. We then discuss geometric properties of general metric measure spaces satisfying this convexity condition. 0 into PostgreSQL...\n",
      "Inserting test sample 1508  This paper studies the relationship between Ricci curvature, entropy, and optimal transport. We explore the properties of these concepts and provide examples of their usage in various fields of study. Our findings demonstrate the significance of these ideas to a wide range of research disciplines, from mathematics to physics. 1 into PostgreSQL...\n",
      "Inserting test sample 1509  The solar wind speed plays a key role in the transport of CME out of the Sun and ultimately determines the arrival time of CME-driven shocks in the heliosphere. Here, we develop an empirical model of the solar wind parameters at the inner boundary (18 solar radii, Rs) used in our global, 3D MHD model (G3DMHD) or other equivalent ones. The model takes solar magnetic field maps at 2.5 Rs (which is based on the Potential Field Source Surface, PFSS model) and interpolates the solar wind plasma and field out to 18 Rs using the algorithm of Wang and Sheeley [1990a]. A formula V_{18Rs} = V1 + V2 fs^{\\alpha} is used to calculate the solar wind speed at 18 Rs, where V1 is in a range of 150-350 km/s, V2 is in the range of 250-500 km/s, and fs is an expansion factor, which was derived from the Wang and Sheeley (WS) algorithm at 2.5 Rs. To estimate the solar wind density and temperature at 18 Rs, we assume an incompressible solar wind and a constant total pressure. The three free parameters are obtained by adjusting simulation results to match in-situ observations (Wind) for more than 54 combination of V1, V2 and {\\alpha} during a quiet solar wind interval, CR2082. We found V18Rs = (150 +/- 50) + (500 +/- 100) fs^-0.4 km/s performs reasonably well in predicting solar wind parameters at 1 AU not just for CR 2082 but other quiet solar period. Comparing results from the present study with those from WSA [Arge et al. 2000; 2004] we conclude that i) Results of using V_{18Rs} with the full rotation data (FR) as input to drive G3DMHD model is better than the results of WSA using FR, or daily updated. ii) When using a modified daily updated 4-day-advanced solar wind speed predictions WSA performs slightly better than our G3DMHD. iii) When using V_{18Rs} as input, G3DMHD model performs much better than the WSA formula. We argue the necessity of the extra angular width ({\\theta}b) parameter used in WSA. 0 into PostgreSQL...\n",
      "Inserting test sample 1510  This research paper focuses on the modeling of inner boundary values at 18 solar radii during a period of solar quiet time, utilizing global three-dimensional time-dependent magnetohydrodynamic numerical simulation. This study attempts to understand the magnetic activity of the sun during its quieter periods, with a specific emphasis on the corona and solar wind. \n",
      "\n",
      "The methodology involved the creation of a numerical model to simulate the solar atmosphere at a distance of 18 solar radii from the center of the sun. The model was created using parameters obtained from observational data, and it included the inclusion of magnetic fields and plasma dynamics. Simulation results were used to create a magnetic field structure that governs the corona's behavior during a quiet period.\n",
      "\n",
      "Results of the study indicate a close agreement between the simulations and the observed results. This study offers important insights into the nature and behavior of the corona during solar quiet time and presents a detailed understanding of the magnetic field structure at a distance of 18 solar radii from the center of the sun. These results may provide important implications for further studies on the sun's inner magnetic activity, including space weather forecasting, which is essential for mitigating the disruptive effects of solar flares and coronal mass ejections on communication systems and satellites.\n",
      "\n",
      "In conclusion, this study presents a detailed numerical simulation of the behavior of the corona during a period of solar quiet time using global three-dimensional time-dependent magnetohydrodynamic numerical simulation. The results demonstrate a close agreement between simulation and observation and provide critical insights into the magnetic field structure at 18 solar radii from the center of the sun. These insights have important implications for space weather forecasting and related fields. 1 into PostgreSQL...\n",
      "Inserting test sample 1511  We investigate the impact of modifying the constraining relations of a Constraint Satisfaction Problem (CSP) instance, with a fixed template, on the set of solutions of the instance. More precisely we investigate sensitive instances: an instance of the CSP is called sensitive, if removing any tuple from any constraining relation invalidates some solution of the instance.\n",
      "\n",
      "Equivalently, one could require that every tuple from any one of its constraints extends to a solution of the instance.\n",
      "\n",
      "Clearly, any non-trivial template has instances which are not sensitive.\n",
      "\n",
      "Therefore we follow the direction proposed (in the context of strict width) by Feder and Vardi (SICOMP 1999) and require that only the instances produced by a local consistency checking algorithm are sensitive. In the language of the algebraic approach to the CSP we show that a finite idempotent algebra $\\mathbf{A}$ has a $k+2$ variable near unanimity term operation if and only if any instance that results from running the $(k, k+1)$-consistency algorithm on an instance over $\\mathbf{A}^2$ is sensitive.\n",
      "\n",
      "A version of our result, without idempotency but with the sensitivity condition holding in a variety of algebras, settles a question posed by G.\n",
      "\n",
      "Bergman about systems of projections of algebras that arise from some subalgebra of a finite product of algebras.\n",
      "\n",
      "Our results hold for infinite (albeit in the case of $\\mathbf{A}$ idempotent) algebras as well and exhibit a surprising similarity to the strict width $k$ condition proposed by Feder and Vardi. Both conditions can be characterized by the existence of a near unanimity operation, but the arities of the operations differ by 1. 0 into PostgreSQL...\n",
      "Inserting test sample 1512  The Constraint Satisfaction Problem (CSP) is a classical combinatorial optimization problem that is frequently encountered in various fields, ranging from computer science to artificial intelligence and operations research. A CSP is represented by a set of variables, domains, and constraints, and the task is to find a consistent set of variable assignments that satisfies all the given constraints. In this paper, we focus on sensitive instances of the CSP, where a small perturbation of the input parameters can lead to a radical change in the problem difficulty and solvability.\n",
      "\n",
      "We investigate the properties of sensitive CSP instances and provide a systematic framework for their analysis. We present a theoretical model that characterizes the sensitivity of CSP instances based on their structural properties, including the degree of constraint density and the distribution of variable domains. Moreover, we propose several practical tools and techniques for identifying and constructing sensitive CSP instances, such as random sampling, hybrid encoding, and local search algorithms.\n",
      "\n",
      "Our experimental results demonstrate the effectiveness of our approach in generating sensitive instances of the CSP for benchmarking and testing purposes. We show that sensitive CSP instances pose a particular challenge to state-of-the-art CSP solvers, as small changes in the problem instance can drastically affect their performance. Finally, we discuss the implications of our findings for the design and evaluation of CSP algorithms and highlight several directions for future research. Overall, our work contributes to a better understanding of the complexity and sensitivity of combinatorial optimization problems and provides practical insights for their realistic and efficient solution. 1 into PostgreSQL...\n",
      "Inserting test sample 1513  Aims: Stars twinkle because their light propagates through the atmosphere.\n",
      "\n",
      "The same phenomenon is expected at a longer time scale when the light of remote stars crosses an interstellar molecular cloud, but it has never been observed at optical wavelength. In a favorable case, the light of a background star can be subject to stochastic fluctuations on the order of a few percent at a characteristic time scale of a few minutes. Our ultimate aim is to discover or exclude these scintillation effects to estimate the contribution of molecular hydrogen to the Galactic baryonic hidden mass. This feasibility study is a pathfinder toward an observational strategy to search for scintillation, probing the sensitivity of future surveys and estimating the background level.\n",
      "\n",
      "Methods: We searched for scintillation induced by molecular gas in visible dark nebulae as well as by hypothetical halo clumpuscules of cool molecular hydrogen ($\\mathrm{H_2-He}$) during two nights. We took long series of 10s infrared exposures with the ESO-NTT telescope toward stellar populations located behind visible nebulae and toward the Small Magellanic Cloud (SMC). We therefore searched for stars exhibiting stochastic flux variations similar to what is expected from the scintillation effect. According to our simulations of the scintillation process, this search should allow one to detect (stochastic) transverse gradients of column density in cool Galactic molecular clouds of order of $\\sim 3\\times 10^{-5}\\,\\mathrm{g/cm^2/10\\,000\\,km}$. Results: We found one light-curve that is compatible with a strong scintillation effect through a turbulent structure characterized by a diffusion radius $R_{diff}<100\\, km$ in the B68 nebula. Complementary observations are needed to clarify the status of this candidate, and no firm conclusion can be established from this single observation. We can also infer limits on the existence of turbulent dense cores (of number density $n>10^9\\, cm^{-3}$) within the dark nebulae. Because no candidate is found toward the SMC, we are also able to establish upper limits on the contribution of gas clumpuscules to the Galactic halo mass. Conclusions: The limits set by this test do not seriously constrain the known models, but we show that the short time-scale monitoring for a few $10^6 star\\times hour$ in the visible band with a $>4$ meter telescope and a fast readout camera should allow one to quantify the contribution of turbulent molecular gas to the Galactic halo. The LSST (Large Synoptic Survey Telescope) is perfectly suited for this search. 0 into PostgreSQL...\n",
      "Inserting test sample 1514  This research paper examines the use of interstellar scintillation to detect hidden gas in the galaxy, with a focus on the results obtained from a test using the NTT-SOFI detector. Interstellar scintillation is a phenomenon where stars appear to twinkle due to turbulence in the interstellar medium, and this effect can be used to identify gas clouds that are otherwise invisible to traditional telescope surveys. The NTT-SOFI detector, which is equipped with a near-infrared camera, was used to observe a number of stars in the galactic bulge over a series of nights, and the resulting scintillation data was analyzed to search for signs of hidden gas.\n",
      "\n",
      "The study found that interstellar scintillation was indeed a useful technique for detecting galactic gas, and that the NTT-SOFI detector was capable of achieving the necessary sensitivity to detect even faint gas clouds. The researchers were able to identify several new gas clouds using this technique, and were also able to confirm the presence of known gas complexes. In addition, the scintillation data provided information about the physical properties of the gas, such as its temperature, density, and velocity, which is crucial for understanding its origin and evolution.\n",
      "\n",
      "These results have important implications for our understanding of the structure and dynamics of the Milky Way, as well as for the study of star formation and evolution. Hidden gas clouds are thought to play a critical role in the process of forming new stars, but they are difficult to detect using traditional methods, which rely on the detection of emissions from ionized gas or dust. Interstellar scintillation offers a new avenue for detecting such clouds, and this study demonstrates the potential of this technique for unlocking new insights into the workings of our galaxy.\n",
      "\n",
      "Overall, the results obtained from this study highlight the importance of interstellar scintillation as a tool for exploring the hidden regions of the Milky Way. The NTT-SOFI detector proved to be an effective instrument for this task, and its use in future studies is likely to yield further discoveries about the structure and evolution of our galaxy. The findings of this study also underscore the need for continued technological innovation in the field of observational astronomy, as we strive to deepen our understanding of the universe around us. 1 into PostgreSQL...\n",
      "Inserting test sample 1515  3D lung segmentation is essential since it processes the volumetric information of the lungs, removes the unnecessary areas of the scan, and segments the actual area of the lungs in a 3D volume. Recently, the deep learning model, such as U-Net outperforms other network architectures for biomedical image segmentation. In this paper, we propose a novel model, namely, Recurrent Residual 3D U-Net (R2U3D), for the 3D lung segmentation task. In particular, the proposed model integrates 3D convolution into the Recurrent Residual Neural Network based on U-Net. It helps learn spatial dependencies in 3D and increases the propagation of 3D volumetric information. The proposed R2U3D network is trained on the publicly available dataset LUNA16 and it achieves state-of-the-art performance on both LUNA16 (testing set) and VESSEL12 dataset. In addition, we show that training the R2U3D model with a smaller number of CT scans, i.e., 100 scans, without applying data augmentation achieves an outstanding result in terms of Soft Dice Similarity Coefficient (Soft-DSC) of 0.9920. 0 into PostgreSQL...\n",
      "Inserting test sample 1516  Medical image segmentation plays a critical role in diagnosing and treating various diseases. In this paper, we propose a novel deep learning architecture - the Recurrent Residual 3D U-Net (R2U3D) - for accurate and efficient lung segmentation. R2U3D combines the benefits of recurrent neural networks and residual connections with the 3D U-Net architecture. The recurrent component of R2U3D captures long-term contextual information, whilst the residual connections allow for easy gradient flow and feature reuse. The proposed architecture was trained on a large dataset of clinical CT scans, achieving state-of-the-art performance in lung segmentation with an average Dice coefficient of 0.977. Execution time on a standard GPU was also significantly reduced compared to other state-of-the-art methods. Moreover, we show that R2U3D can be generalized to segment other organs and tumors effectively. The proposed R2U3D architecture has great potential for clinical use, enabling accurate and efficient medical image segmentation for improved diagnosis and treatment planning. 1 into PostgreSQL...\n",
      "Inserting test sample 1517  Aims. We present a spectroscopic study of the properties of 64 Balmer break galaxies that show signs of star formation. The studied sample of star-forming galaxies spans a redshift range from 0.094 to 1.475 with stellar masses in the range 10$^{8}-$10$^{12}$ $M_{\\odot}$. The sample also includes eight broad emission line galaxies with redshifts between 1.5 $<z<$ 3.0.\n",
      "\n",
      "Methods. We derived star formation rates (SFRs) from emission line luminosities and investigated the dependence of the SFR and specific SFR (SSFR) on the stellar mass and color. Furthermore, we investigated the evolution of these relations with the redshift.\n",
      "\n",
      "Results. We found that the SFR correlates with the stellar mass, our data is consistent with previous results from other authors in that there is a break in the correlation, which reveals the presence of massive galaxies with lower SFR values (i.e., decreasing star formation). We also note an anticorrelation for the SSFR with the stellar mass. Again in this case, our data is also consistent with a break in the correlation, revealing the presence of massive star-forming galaxies with lower SSFR values, thereby increasing the anticorrelation. These results might suggest a characteristic mass ($M_{0}$) at which the red sequence could mostly be assembled. In addition, at a given stellar mass, high-redshift galaxies have on average higher SFR and SSFR values than local galaxies.\n",
      "\n",
      "Finally, we explored whether a similar trend could be observed with redshift in the SSFR$-(u-B)$ color diagram, and we hypothesize that a possible $(u-B)_{0}$ break color may define a characteristic color for the formation of the red sequence. 0 into PostgreSQL...\n",
      "Inserting test sample 1518  This research paper investigates the star formation activities in Balmer break galaxies at redshifts $z$ < 1.5. Our study utilizes a sample of 128 galaxies selected from the Cosmic Evolution Survey field and the Sloan Digital Sky Survey. We analyze the UV-to-optical photometry and spectroscopic data to derive physical properties such as star formation rates and stellar masses of the galaxies. \n",
      "\n",
      "We find a significant correlation between the Balmer break strength and the specific star formation rate in our sample. Our results suggest that the Balmer break method is a reliable indicator of the star formation activity in galaxies at $z$ < 1.5. We also find evidence for a mass-dependent evolution of the star formation activity, where low-mass galaxies have higher specific star formation rates at higher redshifts compared to high-mass galaxies.\n",
      "\n",
      "Additionally, we investigate the impact of environment on the star formation activity in Balmer break galaxies. We find that galaxies in denser environments have lower specific star formation rates compared to galaxies in less dense environments, indicating a quenching of star formation due to environmental effects. \n",
      "\n",
      "Our study provides a comprehensive analysis of the star formation activities in Balmer break galaxies at $z$ < 1.5 and sheds light on the environmental dependence of star formation in these galaxies. Our results have important implications for our understanding of galaxy evolution and the formation of structures in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1519  The forecasted accuracy of upcoming surveys of large-scale structure cannot be achieved without a proper quantification of the error induced by foreground removal (or other systematics like 0-point photometry offset). Because these errors are highly correlated on the sky, their influence is expected to be especially important at very large scales. In this work we quantify how the uncertainty in the visibility mask of a survey influences the measured power spectrum of a sample of tracers of the density field and its covariance matrix.\n",
      "\n",
      "We start from a very large set of 10,000 catalogs of dark matter halos in periodic cosmological boxes, produced with the PINOCCHIO approximate method. To make an analytic approach feasible, we assume luminosity-independent halo bias and an idealized geometry for the visibility mask. We find that the power spectrum of these biased tracers can be expressed as the sum of a cosmological term, a mask term and a term involving their convolution. The mask and convolution terms scale like $P\\propto l^2\\sigma_A^2$, where $\\sigma_A^2$ is the variance of the uncertainty on the visibility mask. With $l=30-100$ Mpc$/h$ and $\\sigma_A=5-20$\\%, the mask term can be significant at $k\\sim0.01-0.1\\ h/$Mpc, and the convolution term can amount to $\\sim 1-10$\\% of the total. For the power spectrum covariance, the coupling of the convolution term with the other two gives rise to several mixed terms, that we quantify by difference using the mock catalogs. These are found to be of the same order of the mask covariance, and to introduce non-diagonal terms at large scales. Then, the power spectrum covariance matrix cannot be expressed as the sum of a cosmological and of a mask term. Our results lie down the theoretical bases to quantify the impact that uncertainties in the mask calibration have on the derivation of cosmological constraints from large spectroscopic surveys.\n",
      "\n",
      "[Abridged] 0 into PostgreSQL...\n",
      "Inserting test sample 1520  This research paper investigates the impact of uncertainty in the visibility mask of surveys on the clustering of biased tracers. In particular, we focus on situations in which the completeness of data is not uniform across the survey area. This situation arises when the limited resources used in the survey lead to an incompleteness of data.\n",
      "\n",
      "We develop a model to study the effects of the visibility mask on the clustering of biased tracers, which are astrophysical objects that are non-uniformly distributed across the sky. Our model accounts for the impact of the incompleteness of the data on the clustering of these objects.\n",
      "\n",
      "Our results show that the bias introduced by incomplete data is not a simple one-to-one relation with the completeness of the survey. Instead, the bias is strongly dependent on the clustering properties of the tracer population, the properties of the mask, and the specifics of the survey data.\n",
      "\n",
      "We further explore the impact of the bias on the cosmological observables derived from the clustering of the tracers, such as the power spectrum and higher-order statistics. We find that the bias can lead to significant deviations from the true values of the observables.\n",
      "\n",
      "The results of this study have important implications for ongoing and future surveys of the universe. It is crucial to take into account the impact of incomplete data and the visibility mask on the clustering of tracers in order to ensure the accurate interpretation of survey data.\n",
      "\n",
      "In conclusion, our work sheds light on the complex interplay between visibility masks, incomplete data, and the clustering of biased tracers. Our model provides a powerful tool for the interpretation of survey data and serves as an important step towards a more precise understanding of the nature of our universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1521  Recently, the LHCb Collaboration have updated their analysis of the resonant $J/\\psi p$ mass spectrum in the decay $\\Lambda_b^0 \\to J/\\psi p K^-$. In the combined Run~1 and Run~2 LHCb data, three peaks are observed, with the former $P_c(4450)^+$ state split into two narrow states, $P_c(4440)^+$ and $P_c(4457)^+$, having the masses $M=(4440.3\\pm 1.3^{+4.1}_{-4.7})$ MeV and $M=(4457.3\\pm 0.6^{+4.1}_{-1.7})$ MeV, and decay widths $\\Gamma=(20.6\\pm 4.9^{+8.7}_{-10.1})$ MeV and $\\Gamma=(6.4\\pm 2.0^{+5.7}_{-1.9})$ MeV, respectively. In addition, a third narrow peak, $P_c(4312)^+$, having the mass $M=(4311.9\\pm 0.7^{+6.8}_{-0.6})$ MeV and decay width $\\Gamma=(9.8\\pm 2.7^{+3.7}_{-4.5})$ MeV is also observed. The LHCb analysis is not sensitive to broad $J/\\psi p$ contributions like the former $P_c(4380)^+$, implying that there could be more states present in the data. Also the spin-parity, $J^P$, assignments of the states are not yet determined. We interpret these resonances in the compact diquark model as hidden-charm diquark-diquark-antiquark baryons, having the following spin and angular momentum quantum numbers: $P_c(4312)^+ =\\{\\bar c [cu]_{s=1} [ud]_{s=0}; L_P=0, J^P=3/2^-\\}$, the $S$-wave state, and the other two as $P$-wave states, with $P_c(4440)^+ =\\{\\bar c [cu]_{s=1} [ud]_{s=0}; L_P=1, J^P=3/2^+\\}$ and $P_c(4457)^+ = \\{\\bar c [cu]_{s=1} [ud]_{s=0}; L_P=1, J^P=5/2^+ \\}$. The subscripts denote the spins of the diquarks and $L_P=0,1$ is the orbital angular momentum quantum number of the pentaquark. These assignments are in accord with the heavy-quark-symmetry selection rules for $\\Lambda_b$-baryon decays, in which the spin $S=0$ of the light diquark $[ud]_{s=0}$ is conserved. The masses of observed states can be accommodated in this framework and the two heaviest states have the positive parities as opposed to the molecular-like interpretations. 0 into PostgreSQL...\n",
      "Inserting test sample 1522  The interpretation of the narrow $J/\\psi p$ peaks observed in $\\Lambda_b \\to J/\\psi p K^-$ decay is a topic of current interest. In this study, we explore the possibility of explaining the peaks within the framework of the compact diquark model. This model postulates the existence of tightly bound, color-antisymmetric diquarks that can form hadrons. We show that the observed peaks can be explained if the $\\Lambda_b$ baryon contains a compact, spin-zero diquark that is coupled to a $J/\\psi$ meson and a proton. \n",
      "\n",
      "To investigate this hypothesis, we perform a detailed analysis of the angular distributions of the decay products. We show that the data can be accurately described by this diquark model, whereas the traditional approach of considering only three-body decay diagrams fails to reproduce the observed features. Our analysis also suggests that the diquark mass is around 2.5 GeV, which is consistent with the expectations of the diquark model.\n",
      "\n",
      "We further examine the implications of our findings. We show that the presence of a compact diquark in the $\\Lambda_b$ baryon can provide a natural explanation for the observed suppression of the $\\Lambda_b$ baryon production rate relative to the production rates of other baryons. We also discuss the consequences of our results for the broader context of hadron spectroscopy and the nature of hadronic matter.\n",
      "\n",
      "In summary, we present evidence that the narrow $J/\\psi p$ peaks observed in $\\Lambda_b \\to J/\\psi p K^-$ decay can be explained within the framework of the compact diquark model. Our findings suggest that the $\\Lambda_b$ baryon contains a spin-zero diquark, which is tightly bound with a mass of around 2.5 GeV. This study has important implications for our understanding of hadron spectroscopy and the nature of hadronic matter. 1 into PostgreSQL...\n",
      "Inserting test sample 1523  We report the first results from a survey for 1665, 1667, and 1720 MHz OH emission over a small region of the Outer Galaxy centered at $l \\approx 105.0\\deg , b \\approx +1.0\\deg$ . This sparse, high-sensitivity survey ($\\Delta Ta \\approx \\Delta Tmb \\approx 3.0 - 3.5$ mK rms in 0.55 km/s channels), was carried out as a pilot project with the Green Bank Telescope (GBT, FWHM $\\approx 7.6'$) on a 3 X 9 grid at $0.5\\deg$ spacing. The pointings chosen correspond with those of the existing $^{12}$CO(1-0) CfA survey of the Galaxy (FWHM $\\approx 8.4'$). With 2-hr integrations, 1667 MHz OH emission was detected with the GBT at $\\gtrsim 21$ of the 27 survey positions ($\\geq 78\\%$ ), confirming the ubiquity of molecular gas in the ISM as traced by this spectral line. With few exceptions, the main OH lines at 1665 and 1667 MHz appear in the ratio of 5:9 characteristic of LTE at our sensitivity levels. No OH absorption features are recorded in the area of the present survey, in agreement with the low levels of continuum background emission in this direction. At each pointing the OH emission appears in several components extending over a range of radial velocity and coinciding with well-known features of Galactic structure such as the Local Arm and the Perseus Arm. In contrast, little CO emission is seen in the survey area; less than half of the $\\gtrsim 50$ identified OH components show detectable CO at the CfA sensitivity levels, and these are generally faint. There are no CO profiles without OH emission. With few exceptions, peaks in the OH profiles coincide with peaks in the GBT HI spectra (obtained concurrently, FWHM $8.9'$), although the converse is not true. We conclude that main-line OH emission is a promising tracer for the \"dark molecular gas\" in the Galaxy discovered earlier in Far-IR and gamma-ray emission. Further work is needed to establish the quantitative details of this connection. 0 into PostgreSQL...\n",
      "Inserting test sample 1524  This paper presents the results of a pilot survey probing the structure of dark molecular gas in the Milky Way. By conducting observations towards $l \\approx 105^{\\deg}, b \\approx +1^{\\deg}$ at a wavelength of 18-cm, we were able to detect and map the OH emission from molecular clouds that are otherwise invisible at traditional optical wavelengths.\n",
      "\n",
      "The spatial distribution of the dark molecular gas traced by this OH emission shows clear evidence of filamentary structure, with individual clouds stretching across tens of parsecs. Our results indicate that these structures may be associated with the densest regions of interstellar gas, suggesting that they may be important sites for the formation of new stars.\n",
      "\n",
      "We have also analyzed the kinematics of the detected molecular gas, revealing that it is largely coherent with the known rotation of the Milky Way disk. However, we have also identified a number of clouds exhibiting significant non-circular motions, which may be indicative of interactions with other gas clouds or the Galactic Bar.\n",
      "\n",
      "In addition to mapping the distribution and kinematics of the dark molecular gas, we present detailed analysis of the physical properties of the detected clouds. We find that they are generally characterized by high densities and low temperatures, consistent with expected conditions in molecular clouds.\n",
      "\n",
      "Overall, our pilot survey provides important new insights into the structure of dark molecular gas in the Milky Way, and serves as an important proof-of-concept for future surveys targeting this elusive component of the interstellar medium. 1 into PostgreSQL...\n",
      "Inserting test sample 1525  Web service compositions are gaining attention to develop complex web systems by combination of existing services. Thus, there are many works that leverage the advantages of this approach. However, there are only few works that use web service compositions to manage distributed resources. In this paper, we then present a formal model that combines orchestrations written in BPEL with distributed resources, by using WSRF. 0 into PostgreSQL...\n",
      "Inserting test sample 1526  BPEL-RF is a formal framework that integrates distributed resources in Business Process Execution Language (BPEL) orchestrations. The proposed framework extends BPEL with a formal semantics for expressing distributed resource constraints, including resource availability and allocation. BPEL-RF enhances the reliability and efficiency of distributed resource integration in BPEL, leading to better performance and service delivery. This paper presents the BPEL-RF framework, its formal description, and a case study demonstrating its effectiveness in an e-commerce scenario. 1 into PostgreSQL...\n",
      "Inserting test sample 1527  The discrete wavelet packet transform (DWPT) and discrete wavelet transform (DWT) are used to extract and study the dynamics of coherent structures in a turbulent rotating fluid. Three-dimensional (3D) turbulence is generated by strong pumping through tubes at the bottom of a rotating tank (48.4 cm high, 39.4 cm diameter). This flow evolves toward two-dimensional (2D) turbulence with increasing height in the tank. Particle Image Velocimetry (PIV) measurements on the quasi-2D flow reveal many long-lived coherent vortices with a wide range of sizes. The vorticity fields exhibit vortex birth, merger, scattering, and destruction. We separate the flow into a low-entropy ``coherent'' and a high-entropy ``incoherent'' component by thresholding the coefficients of the DWPT and DWT of the vorticity fields. Similar thresholdings using the Fourier transform and JPEG compression together with the Okubo-Weiss criterion are also tested for comparison. We find that the DWPT and DWT yield similar results and are much more efficient at representing the total flow than a Fourier-based method. Only about 3% of the large-amplitude coefficients of the DWPT and DWT are necessary to represent the coherent component and preserve the vorticity probability density function, transport properties, and spatial and temporal correlations. The remaining small amplitude coefficients represent the incoherent component, which has near Gaussian vorticity PDF, contains no coherent structures, rapidly loses correlation in time, and does not contribute significantly to the transport properties of the flow. This suggests that one can describe and simulate such turbulent flow using a relatively small number of wavelet or wavelet packet modes. 0 into PostgreSQL...\n",
      "Inserting test sample 1528  In rotating turbulent flows, large-scale coherent structures play an important role in the mixing and transport of momentum, heat, and other scalar quantities. The identification and characterization of these structures is crucial for understanding and modeling such flows. In this study, we present a method for the extraction of coherent structures in a rotating turbulent flow experiment using proper orthogonal decomposition (POD) analysis.\n",
      "\n",
      "The experiments were conducted in a cylindrical tank filled with water and rotated at a constant angular velocity. Particle image velocimetry (PIV) was used to obtain velocity measurements in the tank. The POD analysis was performed on the instantaneous velocity fields to identify the coherent structures.\n",
      "\n",
      "Results indicate that the method successfully identifies the large-scale vortices and other coherent structures in the flow. The spatial and spectral characteristics of these structures are analyzed and discussed. The identified coherent structures are compared to the results from previous studies and numerical simulations, and the agreement is found to be good.\n",
      "\n",
      "The method presented in this study provides a valuable tool for the analysis and modeling of rotating turbulent flows. By identifying the coherent structures in the flow, we gain insight into the underlying physics of the flow and can improve our understanding of its dynamics. This has potential applications in a wide range of fields, including engineering, environmental science, and geophysics. 1 into PostgreSQL...\n",
      "Inserting test sample 1529  The protection of user privacy is an important concern in machine learning, as evidenced by the rolling out of the General Data Protection Regulation (GDPR) in the European Union (EU) in May 2018. The GDPR is designed to give users more control over their personal data, which motivates us to explore machine learning frameworks for data sharing that do not violate user privacy.\n",
      "\n",
      "To meet this goal, in this paper, we propose a novel lossless privacy-preserving tree-boosting system known as SecureBoost in the setting of federated learning. SecureBoost first conducts entity alignment under a privacy-preserving protocol and then constructs boosting trees across multiple parties with a carefully designed encryption strategy. This federated learning system allows the learning process to be jointly conducted over multiple parties with common user samples but different feature sets, which corresponds to a vertically partitioned data set. An advantage of SecureBoost is that it provides the same level of accuracy as the non-privacy-preserving approach while at the same time, reveals no information of each private data provider.\n",
      "\n",
      "We show that the SecureBoost framework is as accurate as other non-federated gradient tree-boosting algorithms that require centralized data and thus it is highly scalable and practical for industrial applications such as credit risk analysis. To this end, we discuss information leakage during the protocol execution and propose ways to provably reduce it. 0 into PostgreSQL...\n",
      "Inserting test sample 1530  Federated learning, as an emerging paradigm, allows multiple parties to collaboratively train a model, while keeping their data local and decentralized. However, privacy and security challenges arise from the distributed nature of the data, which call for robust approaches that ensure confidentiality throughout the process. In this paper, we propose SecureBoost, a lossless federated learning framework that preserves both data privacy and model accuracy. Specifically, our approach builds upon Boosting to optimize the learning objective while incorporating differential privacy to achieve strong privacy guarantees. By integrating secure multi-party computation techniques, our framework enables each party to only share encrypted model updates, thereby eliminating the need for a trusted third party. We theoretically analyze the privacy and accuracy properties of SecureBoost, and perform experiments on real-world datasets to demonstrate its efficacy. The results show that our approach not only achieves state-of-the-art performance in terms of model accuracy, but also offers robust privacy guarantees against various privacy attacks, including membership inference and model inversion attacks. The proposed framework can be readily applied to various federated learning scenarios, such as online advertising, personalized recommendation, and healthcare data analysis, where privacy is a critical concern. 1 into PostgreSQL...\n",
      "Inserting test sample 1531  Adversarial training has become one of the most effective methods for improving robustness of neural networks. However, it often suffers from poor generalization on both clean and perturbed data. In this paper, we propose a new algorithm, named Customized Adversarial Training (CAT), which adaptively customizes the perturbation level and the corresponding label for each training sample in adversarial training. We show that the proposed algorithm achieves better clean and robust accuracy than previous adversarial training methods through extensive experiments. 0 into PostgreSQL...\n",
      "Inserting test sample 1532  This paper introduces Customized Adversarial Training (CAT), a novel technique for improving the robustness of machine learning models. CAT leverages an adaptive weighting scheme to identify and emphasize the importance of difficult-to-classify data points during training. Our experiments show that CAT achieves state-of-the-art performance against adversarial attacks on benchmark datasets, while maintaining high accuracy on clean data. Our findings demonstrate the effectiveness of CAT in improving the robustness of machine learning models in real-world scenarios. 1 into PostgreSQL...\n",
      "Inserting test sample 1533  We used the APEX telescope to observe spectral lines occurring at about 170 GHz frequency towards 14 positions along the full extent of the filamentary Seahorse infrared dark cloud. Six spectral line transitions were detected ($\\geq3\\sigma$) altogether, namely, SO$(N_J=4_4-3_3)$, H$^{13}$CN$(J=2-1)$, H$^{13}$CO$^+(J=2-1)$, SiO$(J=4-3)$, HN$^{13}$C$(J=2-1)$, and C$_2$H$(N=2-1)$.\n",
      "\n",
      "While SO, H$^{13}$CO$^+$, and HN$^{13}$C were detected in every source, the detection rates for C$_2$H and H$^{13}$CN were 92.9% and 85.7%, respectively.\n",
      "\n",
      "Only one source (SMM 3) showed detectable SiO emission (7.1% detection rate).\n",
      "\n",
      "Three clumps (SMM 5, 6, and 7) showed the SO, H$^{13}$CN, H$^{13}$CO$^+$, HN$^{13}$C, and C$_2$H lines in absorption. We found three positive correlations among the derived molecular abundances, of which those between C$_2$H and HN$^{13}$C and HN$^{13}$C and H$^{13}$CO$^+$ are the most significant (correlation coefficient $r\\simeq0.9$). The statistically most significant evolutionary trends we uncovered are the drops in the C$_2$H abundance and in the $[{\\rm HN^{13}C}]/[{\\rm H^{13}CN}]$ ratio as the clump evolves from an IR dark stage to an IR bright stage and then to an HII region.\n",
      "\n",
      "The correlations we found between the different molecular abundances can be understood as arising from the gas-phase electron (ionisation degree) and atomic carbon abundances. The [C$_2$H] evolutionary indicator we found is in agreement with previous studies, and can be explained by the conversion of C$_2$H to other species (e.g. CO) when the clump temperature rises, especially after the ignition of a hot molecular core in the clump. The decrease of $[{\\rm HN^{13}C}]/[{\\rm H^{13}CN}]$ as the clump evolves is also likely to reflect the increase in the clump temperature, which leads to an enhanced formation of HCN and its $^{13}$C isotopologue. 0 into PostgreSQL...\n",
      "Inserting test sample 1534  The Seahorse infrared dark cloud (IRDC) has long been a subject of interest for astronomers and astrochemists due to its complex and unique properties. In this study, we present observations of the Seahorse IRDC using the Atacama Pathfinder EXperiment (APEX) telescope at a frequency of 170 GHz. Our aim was to investigate the chemical conditions and molecular composition of the IRDC.\n",
      "\n",
      "We found that the Seahorse IRDC is a prolate cloud with dimensions of approximately 25 Ã— 5 pc, and a mass of approximately 17,000 solar masses. The APEX observations revealed the presence of several molecular lines, including HCO+, HCN, N2H+, and others.\n",
      "\n",
      "Using non-LTE radiative transfer models, we derived the physical parameters of the IRDC, such as the gas temperature, density, and column density. We found that the gas in the IRDC is cold, with a kinetic temperature of about 10 K, and it is also very dense, with a number density of approximately 105 cm-3.\n",
      "\n",
      "Furthermore, we detected the presence of complex organic molecules (COMs) in the IRDC, including methanol (CH3OH), formic acid (HCOOH), and ethanol (CH3CH2OH). The abundance of these COMs is relatively high, and their presence suggests that the IRDC is a chemically rich environment.\n",
      "\n",
      "Finally, we discuss the implications of our results for the understanding of star formation in the Seahorse IRDC. The high density and cold temperatures we derived from our observations suggest that the IRDC is a promising site for future studies of the early stages of star formation.\n",
      "\n",
      "In conclusion, our APEX observations of the Seahorse IRDC have revealed a complex molecular environment with a rich and diverse chemistry. Our results provide valuable new insights into the physical and chemical conditions in this unique region, and have important implications for our understanding of star formation processes in high-mass molecular clouds. 1 into PostgreSQL...\n",
      "Inserting test sample 1535  For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of these cosmic rays. At the highest energies, they are supposed to point toward their sources with good accuracy. However, the observations are so rare that testing the distribution of such samples of directional data on the sphere is nontrivial. In this paper, we choose a nonparametric framework that makes weak hypotheses on the alternative distributions and allows in turn to detect various and possibly unexpected forms of anisotropy. We explore two particular procedures. Both are derived from fitting the empirical distribution with wavelet expansions of densities.\n",
      "\n",
      "We use the wavelet frame introduced by [SIAM J. Math. Anal. 38 (2006b) 574-594 (electronic)], the so-called needlets. The expansions are truncated at scale indices no larger than some ${J^{\\star}}$, and the $L^p$ distances between those estimates and the null density are computed. One family of tests (called Multiple) is based on the idea of testing the distance from the null for each choice of $J=1,\\ldots,{J^{\\star}}$, whereas the so-called PlugIn approach is based on the single full ${J^{\\star}}$ expansion, but with thresholded wavelet coefficients. We describe the practical implementation of these two procedures and compare them to other methods in the literature. As alternatives to isotropy, we consider both very simple toy models and more realistic nonisotropic models based on Physics-inspired simulations. The Monte Carlo study shows good performance of the Multiple test, even at moderate sample size, for a wide sample of alternative hypotheses and for different choices of the parameter ${J^{\\star}}$. On the 69 most energetic events published by the Pierre Auger Collaboration, the needlet-based procedures suggest statistical evidence for anisotropy. Using several values for the parameters of the methods, our procedures yield $p$-values below 1%, but with uncontrolled multiplicity issues. The flexibility of this method and the possibility to modify it to take into account a large variety of extensions of the problem make it an interesting option for future investigation of the origin of ultrahigh energy cosmic rays. 0 into PostgreSQL...\n",
      "Inserting test sample 1536  The quest to understand the origin of high energy cosmic rays has been a long-standing challenge for astrophysicists. One of the key properties of cosmic rays is their isotropy, or the uniformity of their distribution in space. In this paper, we explore the use of spherical needlets as a tool for testing the isotropy of high energy cosmic rays.\n",
      "\n",
      "Spherical needlets are mathematical functions that are particularly well-suited for analyzing data on the sphere, such as the distribution of cosmic rays. They provide a flexible and efficient means of decomposing a signal into different scales and orientations, while taking into account the curvature of the sphere.\n",
      "\n",
      "Using data from the Pierre Auger Observatory, we apply the spherical needlet transform to the arrival directions of high energy cosmic rays. We find that the distribution of cosmic rays is consistent with isotropy on large scales, but deviates from isotropy on smaller scales. This deviation is characterized by a clustering of cosmic rays in a few localized regions, known as hotspot regions.\n",
      "\n",
      "We perform a number of statistical tests to quantify the significance of these hotspot regions, including a comparison with randomized data and a calculation of the angular power spectrum. We find that the hotspot regions are highly significant and cannot be explained by statistical fluctuations or by known astrophysical sources.\n",
      "\n",
      "These results have important implications for our understanding of the origin of high energy cosmic rays. The existence of hotspot regions suggests the presence of unknown sources or acceleration mechanisms that produce cosmic rays preferentially in certain directions. Furthermore, the scale-dependence of the isotropy violation provides valuable clues about the properties of these sources and the conditions in the interstellar medium.\n",
      "\n",
      "Overall, our study demonstrates the potential of spherical needlets for analyzing cosmic ray data and sheds new light on one of the most intriguing puzzles in astrophysics. 1 into PostgreSQL...\n",
      "Inserting test sample 1537  The Vela supernova remnant displays several ejecta, which are fragment-like features protruding beyond the front of its primary blast shock wave. They appear to be \"shrapnel\", bowshock-shaped relics of the supernova explosion. One of these pieces of shrapnel (A), located in the northeastern edge of the remnant, is peculiar because its X-ray spectrum exhibits a high Si abundance, in contrast to the other observed ejecta fragments, which show enhanced O, Ne, and Mg abundances. We present the analysis of another fragment located opposite to shrapnel A with respect to the center of the shell, in the southwestern boundary of the remnant, named shrapnel G. We thoroughly analyzed a dedicated XMM-Newton observation of shrapnel G by producing background-subtracted and exposure-corrected maps in different energy ranges, which we complemented with a spatially resolved spectral analysis of the X-ray emission. The fragment presents a bowshock-like shape with its anti-apex pointing to the center of the remnant. Its X-ray spectrum is best fit by a thermal plasma out of equilibrium of ionization with low O and Fe, roughly solar Ne and Mg, and a significantly high Si abundance, which is required to fit a very clear Si line at ~1.85 keV.\n",
      "\n",
      "Its chemical composition and spectral properties are compatible with those of shrapnel A, which is located on the opposite side of the remnant. As a consequence of the nucleosynthesis, pieces of Si-rich shrapnel are expected to originate in deeper layers of the progenitor star compared to ejecta with lower-Z elements. A high velocity and density contrast with respect to the surrounding ejecta are necessary to make shrapnel A and G overtake the forward shock. The line connecting shrapnel A and G crosses almost exactly the expansion center of the remnant, strongly suggesting a Si-rich jet-counterjet structure, reminiscent of that observed in the young remnant Cas A. 0 into PostgreSQL...\n",
      "Inserting test sample 1538  Our research provides evidence of a Si-rich bilateral jet of ejecta in the Vela supernova remnant (SNR) as observed with XMM-Newton. The Vela SNR is a well-studied object due to its proximity and its young age estimated to be around 11,000 years. However, the detailed morphology of the remnant remains an open question. Our study aims to address this question with new observations and analysis. \n",
      "\n",
      "We performed an X-ray spectroscopic analysis of the Vela SNR with the XMM-Newton observatory. Our observations cover a large area of the remnant and reveal a bilateral jet-like structure. The north and south jet regions are significantly Si-rich compared to the surrounding regions, suggesting a different composition of ejecta. Moreover, we find that the X-ray emission in the jet regions is dominated by Si, whereas the rest of the remnant shows a mixture of Fe, Si, and S. \n",
      "\n",
      "We also compare our observations with previous studies in different wavelengths, including radio and optical, to better understand the morphology of the remnant. The Si-rich regions of the jets appear to be aligned with the direction of the Vela pulsar and its proper motion, providing further clues about the explosion mechanism. We suggest that the jets are associated with the equatorial ring-like structure observed at radio wavelengths, which may be a result of a bipolar outflow from the progenitor star. \n",
      "\n",
      "In summary, our observations reveal the presence of a Si-rich bilateral jet-like structure in the Vela SNR, which provides important clues about the composition and morphology of the remnant. Our results suggest that the Vela SNR resulted from a bipolar explosion, and provide support to the idea that the asymmetry of supernovae can originate from the properties of their progenitor stars. Our findings contribute to a better understanding of the evolution of massive stars and their explosions. 1 into PostgreSQL...\n",
      "Inserting test sample 1539  The Seyfert-1 galaxy NGC 3516 has undergone major spectral changes in recent years. In 2017 we obtained Chandra, NuSTAR, and Swift observations during its new low-flux state. Using these observations we model the spectral energy distribution (SED) and the intrinsic X-ray absorption, and compare the results with those from historical observations taken in 2006. We thereby investigate the effects of the changing-look phenomenon on the accretion-powered radiation and the ionized outflows. Compared to its normal high-flux state in 2006, the intrinsic bolometric luminosity of NGC 3516 was lower by a factor of 4 to 8 during 2017. Our SED modeling shows a significant decline in the luminosity of all the continuum components from the accretion disk and the X-ray source. As a consequence, the reprocessed X-ray emission lines have also become fainter. The Swift monitoring of NGC 3516 shows remarkable X-ray spectral variability on short (weeks) and long (years) timescales. We investigate whether this variability is driven by obscuration or the intrinsic continuum. We find that the new low-flux spectrum of NGC 3516, and its variability, do not require any new or variable obscuration, and instead can be explained by changes in the ionizing SED that result in lowering of the ionization of the warm-absorber outflows. This in turn induces enhanced X-ray absorption by the warm-absorber outflows, mimicking the presence of new obscuring gas. Using the response of the ionized regions to the SED changes, we place constraints on their density and location. 0 into PostgreSQL...\n",
      "Inserting test sample 1540  This research paper investigates the possible causes of a \"changing-look\" event in NGC 3516. We examine whether the observed changes in the continuum are due to intrinsic variability or obscuration variability. We analyzed multi-epoch optical spectra obtained from various observatories. We also examined historical data obtained from archives. The spectra cover a span of more than two decades, which allowed us to study the temporal evolution of the source. We found that the continuum flux decreased significantly, while the Balmer emission lines remained unchanged during the event. We tested the hypothesis of obscuration variability by modeling the spectral energy distribution (SED) with a combination of a power-law continuum and a dusty torus. The SED modeling supports the hypothesis of a dust-driven changing-look event. The dust is likely located in the vicinity of the broad-line region and is responsible for the changes in continuum flux. We also investigated the possibility of intrinsic variability by examining the flux variations of narrow emission lines. The results suggest that a variable ionizing continuum is unlikely to be the main cause of the changing-look event in NGC 3516. Our results show that the observed changes in the continuum flux are likely due to obscuration variability caused by dust. This research provides new insights into the mysterious phenomenon of changing-look events in active galactic nuclei. 1 into PostgreSQL...\n",
      "Inserting test sample 1541  Current magnetic traps can be made so anisotropic that the atomic gas trapped inside behaves like quasi one or two dimensional system. Unlike the homogeneous case, quantum phase fluctuations do not destroy macroscopic off-diagonal order of trapped Bose gases in $d\\leq 2$. In the dilute limit, quantum fluctuations increase, remain constant, and decrease with size for $% 3, 2, 1d$ respectively. These behaviors are due to the combination of a finite gap and the universal spectrum of the collective mode. 0 into PostgreSQL...\n",
      "Inserting test sample 1542  In this study, we investigate the properties of quasi-one and two-dimensional dilute Bose gases confined in magnetic traps. We show the existence of off-diagonal long-range order and anomalous quantum fluctuations in the gases. By analyzing the quantum states of the gases, we reveal the connection between the anomalous fluctuations and the breakdown of the superfluidity. Our findings suggest that the interplay between the magnetic confinement and quantum fluctuations play a crucial role in characterizing the behavior of dilute Bose gases in magnetic traps. 1 into PostgreSQL...\n",
      "Inserting test sample 1543  We consider a version of the extended Clifford Group which is defined in terms of a finite Galois field in odd prime power dimension. We show that Neuhauser's result, that with the appropriate choice of phases the standard (or metaplectic) representation of the discrete symplectic group is faithful also holds for the anti-unitary operators of the extended group. We also improve on Neuhauser's result by giving explicit formulae for the (anti-)unitary corresponding to an arbitrary (anti-)symplectic matrix. We then go on to find the eigenvalues and the order of an arbitrary (anti-)symplectic matrix. The fact that in prime power dimension the matrix elements belong to a field means that this can be done using the same techniques which are used to find the eigenvalues of a matrix defined over the reals-including the use of an extension field (the analogue of the complex numbers) when the eigenvalues are not in the base field. We then give an application of these results to SIC-POVMs (symmetric informationally complete positive operator valued measures). We show that in prime dimension our results can be used to find a natural basis for the eigenspace of the Zauner unitary in which SIC-fiducials are expected to lie. Finally, we apply our results to the MUB cycling problem.\n",
      "\n",
      "We show that in odd prime power dimension d, although there is no Clifford unitary, there is a Clifford anti-unitary which cycles through the full set of Wootters-Fields MUBs if d=3 (mod 4). Also, irrespective of whether d=1 or 3 (mod 4), the Wootters-Fields MUBs split into two groups of (d+1)/2 bases in such a way that there is a single Clifford unitary which cycles through each group separately. 0 into PostgreSQL...\n",
      "Inserting test sample 1544  The extended Clifford group, a generalization of the well-known Clifford group, has recently become the focus of studies analyzing its properties and their potential applications in quantum information theory. In this paper, we provide a comprehensive investigation of the extended Clifford group and its relevance to Symmetric Informationally Complete Positive Operator-Valued Measures (SIC-POVMs) and Mutually Unbiased Bases (MUBs). We first present a detailed exposition of the extended Clifford group and its algebraic structure, giving insight into the properties that make it a key tool in quantum information. We then explore the relationship between the extended Clifford group and SIC-POVMs, providing a unified framework for constructing and analyzing these measurements. We show that the extended Clifford group provides a powerful toolset for analyzing the properties of SIC-POVMs, leading to new insights and conjectures about these fundamental quantum structures. Finally, we examine the role of the extended Clifford group in the theory of MUBs, investigating the connection between the group's properties and the structure of MUBs. Our results demonstrate the essential role that the extended Clifford group plays in the study of quantum information, offering new insights on SIC-POVMs and MUBs. Our work is expected to have broad applications in quantum information theory, including in the design of quantum algorithms and the development of quantum error correction codes. 1 into PostgreSQL...\n",
      "Inserting test sample 1545  The field of succinct data structures has flourished over the last 16 years.\n",
      "\n",
      "Starting from the compressed suffix array (CSA) by Grossi and Vitter (STOC 2000) and the FM-index by Ferragina and Manzini (FOCS 2000), a number of generalizations and applications of string indexes based on the Burrows-Wheeler transform (BWT) have been developed, all taking an amount of space that is close to the input size in bits. In many large-scale applications, the construction of the index and its usage need to be considered as one unit of computation. Efficient string indexing and analysis in small space lies also at the core of a number of primitives in the data-intensive field of high-throughput DNA sequencing. We report the following advances in string indexing and analysis. We show that the BWT of a string $T\\in \\{1,\\ldots,\\sigma\\}^n$ can be built in deterministic $O(n)$ time using just $O(n\\log{\\sigma})$ bits of space, where $\\sigma \\leq n$. Within the same time and space budget, we can build an index based on the BWT that allows one to enumerate all the internal nodes of the suffix tree of $T$. Many fundamental string analysis problems can be mapped to such enumeration, and can thus be solved in deterministic $O(n)$ time and in $O(n\\log{\\sigma})$ bits of space from the input string. We also show how to build many of the existing indexes based on the BWT, such as the CSA, the compressed suffix tree (CST), and the bidirectional BWT index, in randomized $O(n)$ time and in $O(n\\log{\\sigma})$ bits of space. The previously fastest construction algorithms for BWT, CSA and CST, which used $O(n\\log{\\sigma})$ bits of space, took $O(n\\log{\\log{\\sigma}})$ time for the first two structures, and $O(n\\log^{\\epsilon}n)$ time for the third, where $\\epsilon$ is any positive constant. Contrary to the state of the art, our bidirectional BWT index supports every operation in constant time per element in its output. 0 into PostgreSQL...\n",
      "Inserting test sample 1546  Linear-time string indexing and analysis in small space is a fundamental problem in computer science with numerous applications ranging from text search to intrusion detection and data mining. In this paper, we present a novel algorithm that addresses this problem in an efficient, linear-time manner while utilizing a small amount of memory. \n",
      "\n",
      "The algorithm is based on the principle of factorization, which decomposes a given string into a set of smaller substrings and stores them in a compact data structure. Our approach achieves a space complexity of O(n log n) for a string of length n while preserving the ability to perform text search, pattern matching, and other string analysis operations in a time complexity of O(m) per query, where m is the length of the query string. \n",
      "\n",
      "We demonstrate the effectiveness of our algorithm through extensive experiments on a variety of datasets and applications. Our results show that our approach outperforms existing methods in terms of both time and space efficiency, especially for large datasets and queries. Additionally, our algorithm is highly scalable and can be easily parallelized to handle even larger datasets in a distributed environment.\n",
      "\n",
      "To further validate the effectiveness of our approach, we apply it to several real-world applications, including intrusion detection and data mining. Our algorithm provides significant improvements in both accuracy and speed compared to existing methods, making it a powerful tool for a wide range of applications.\n",
      "\n",
      "In conclusion, this paper presents a novel algorithm for linear-time string indexing and analysis in small space. Our approach offers significant performance benefits over existing methods while maintaining high accuracy and scalability. We believe that our algorithm has the potential to become a foundational technique in the field of string analysis and encourage further research into its applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1547  To cover a set of targets with known locations within an area with limited or prohibited ground access using a wireless sensor network, one approach is to deploy the sensors remotely, from an aircraft. In this approach, the lack of precise sensor placement is compensated by redundant de-ployment of sensor nodes. This redundancy can also be used for extending the lifetime of the network, if a proper scheduling mechanism is available for scheduling the active and sleep times of sensor nodes in such a way that each node is in active mode only if it is required to. In this pa-per, we propose an efficient scheduling method based on learning automata and we called it LAML, in which each node is equipped with a learning automaton, which helps the node to select its proper state (active or sleep), at any given time. To study the performance of the proposed method, computer simulations are conducted. Results of these simulations show that the pro-posed scheduling method can better prolong the lifetime of the network in comparison to similar existing method. 0 into PostgreSQL...\n",
      "Inserting test sample 1548  In this paper, we present a coverage monitoring algorithm based on learning automata for wireless sensor networks. The proposed algorithm utilizes the distributed nature of sensor networks and aims to maximize the network's lifetime by minimizing the energy consumed during coverage monitoring. The algorithm uses a reinforcement learning algorithm to dynamically adjust the sensing range of each node based on the observed coverage of its neighbors. By using learning automata, the algorithm is able to adapt to different network topologies and environmental changes. We evaluate the performance of the algorithm through simulations and compare it to existing coverage monitoring algorithms. The simulation results show that our approach outperforms existing algorithms in terms of network lifetime and coverage quality. Moreover, the algorithm requires only limited communication among nodes, making it scalable and suitable for large-scale sensor networks. Our results indicate that the proposed approach can significantly improve the coverage monitoring efficiency and energy efficiency of wireless sensor networks, which could lead to the development of more efficient and reliable monitoring systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1549  Let $X\\subseteq \\mathbb{P}^m$ be a totally real, non-degenerate, projective variety and let $\\Gamma\\subseteq X(\\mathbb{R})$ be a generic set of points. Let $P$ be the cone of nonnegative quadratic forms on $X$ and let $\\Sigma$ be the cone of sums of squares of linear forms. We examine the dimensions of the faces $P(\\Gamma)$ and $\\Sigma(\\Gamma)$ consisting of forms in $P$ and $\\Sigma$, which vanish on $\\Gamma$. As the cardinality of the set $\\Gamma$ varies in $1,\\dots,\\rm{codim}(X)$, the difference between the dimensions of $P(\\Gamma)$ and $\\Sigma(\\Gamma)$ defines a numerical invariant of $X$, which we call the gap vector of X. In this article we begin a systematic study of its fundamental properties. Our main result is a formula relating the components of the gap vector of $X$ and the quadratic deficiencies of $X$ and its generic projections. The quadratic deficiency is a fundamental numerical invariant of projective varieties introduced by F. L. Zak. The relationship between quadratic deficiency and gap vectors allows us to effectively compute the gap vectors of concrete varieties as well as to prove several general properties.\n",
      "\n",
      "We prove that gap vectors are weakly increasing, obtain upper bounds for their rate of growth and prove that these upper bounds are eventually achieved for all varieties. Moreover, we give a characterization of the varieties with the simplest gap vectors: We prove that the gap vector vanishes identically precisely for varieties of minimal degree, and characterize the varieties whose gap vector equals $(0,\\dots, 0,1)$. In particular, our results give a new proof of the theorem of Blekherman, Smith and Velasco saying that there are nonnegative quadratic forms which are not sums of squares on every variety, which is not of minimal degree. Finally, we determine the gap vector of all Veronese embeddings of $\\mathbb{P}^2$, generalizing work of the first three authors. 0 into PostgreSQL...\n",
      "Inserting test sample 1550  This paper investigates the gap vector of real projective varieties, an important invariant in algebraic geometry. The gap vector measures the dimension and position of gaps in the sequence of cohomology groups of a projective variety. Our main result is a description of the gap vector for a class of real projective varieties that arise as real slices of complex projective varieties. To obtain this result, we develop a new technique that relates the cohomology groups of real and complex projective varieties through the theory of real algebraic geometry.\n",
      "\n",
      "We begin by introducing the necessary background on real algebraic geometry and its relation to complex algebraic geometry. We then provide a detailed analysis of the gap vector for real hypersurfaces in complex projective space. We show that the gap vector is determined by the topology of the complex hypersurface and the position of its real locus. We further extend this analysis to real complete intersections in complex projective space.\n",
      "\n",
      "Our next focus is on the description of the gap vector for real slices of complex projective varieties. We first show that the gap vector is invariant under small deformations of the real slice. We then prove that the gap vector is determined by the topology of the complex variety and the position of its real locus, together with the topology of the real slice and its intersection with the complex variety.\n",
      "\n",
      "Finally, we apply our results to study the topology of real algebraic curves in complex projective surfaces. We show that the gap vector of such curves is in general an upper bound for the number of real components of the curve. Our results suggest a new approach to the study of real algebraic curves in complex projective surfaces, and open up new avenues for further research in algebraic geometry. 1 into PostgreSQL...\n",
      "Inserting test sample 1551  We give a new expression for the number of factorizations of a full cycle into an ordered product of permutations of specified cycle types. This is done through purely algebraic means, extending work of Biane. We deduce from our result a formula of Poulalhon and Schaeffer that was previously derived through an intricate combinatorial argument. 0 into PostgreSQL...\n",
      "Inserting test sample 1552  In this paper, we investigate the number of factorizations of a full cycle in the symmetric group. We prove that the number of factorizations is equal to the number of partitions of the cycle's length. Furthermore, we show how our results can be used to determine the number of conjugacy classes in the symmetric group. 1 into PostgreSQL...\n",
      "Inserting test sample 1553  We study the implicit regularization of mini-batch stochastic gradient descent, when applied to the fundamental problem of least squares regression.\n",
      "\n",
      "We leverage a continuous-time stochastic differential equation having the same moments as stochastic gradient descent, which we call stochastic gradient flow.\n",
      "\n",
      "We give a bound on the excess risk of stochastic gradient flow at time $t$, over ridge regression with tuning parameter $\\lambda = 1/t$. The bound may be computed from explicit constants (e.g., the mini-batch size, step size, number of iterations), revealing precisely how these quantities drive the excess risk.\n",
      "\n",
      "Numerical examples show the bound can be small, indicating a tight relationship between the two estimators. We give a similar result relating the coefficients of stochastic gradient flow and ridge. These results hold under no conditions on the data matrix $X$, and across the entire optimization path (not just at convergence). 0 into PostgreSQL...\n",
      "Inserting test sample 1554  In this study, we explore the implicit regularization properties of stochastic gradient flow for least squares problems. We show that as the number of iterations increases, the solution obtained through stochastic gradient flow converges to a low-norm solution. This approach involves the use of a constant step-size and a mini-batch sampling scheme. We analyze the regularizing effects of the algorithm and establish a connection between the dynamics of the iterates and the regularization properties. Our results demonstrate that stochastic gradient flow can be an effective tool for implicit regularization, even in the absence of explicit regularization terms. Furthermore, we provide numerical experiments which highlight the benefits of this approach over other standard regularization techniques. Overall, our study sheds light on the implicit regularization properties of stochastic gradient flow and provides insights into its behavior in the context of least squares problems. 1 into PostgreSQL...\n",
      "Inserting test sample 1555  In this paper we provide two ways of constructing complex coordinates on the moduli space of pairs of a Riemann surface and a stable holomorphic vector bundle centred around any such pair. We compute the transformation between the coordinates to second order at the center of the coordinates. We conclude that they agree to second order, but not to third order at the center. 0 into PostgreSQL...\n",
      "Inserting test sample 1556  In this paper, we explore the Universal Moduli Space of Holomorphic Vector Bundles and derive new coordinate systems to better understand this complex space. Our analysis employs the latest mathematical tools and techniques to reveal the underlying structure of this space. We demonstrate the effectiveness of our approach through numerical simulations and provide a roadmap for future research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 1557  Due to the exponential high gravitational red shift near the event horizon of a black hole, it might appear that the Hawking radiation would be highly sensitive to some unknown high energy physics. To study effects of any unknown physics at the Planck scale on the Hawking radiation, the dispersive field theory models have been proposed, which are variations of Unruh's sonic black hole analogy. In this paper, we use the Hamilton-Jacobi method to investigate the dispersive field theory models. The preferred frame is the free-fall frame of the black hole. The dispersion relation adopted agrees with the relativistic one at low energy but is modified near the Planck mass $m_{p}$. The corrections to the Hawking temperature are calculated for massive and charged particles to $\\mathcal{O}\\left( m_{p}^{-2}\\right) $ and neutral and massless particles with $\\lambda=0$ to all orders. The Hawking temperature of radiation agrees with the standard one at the leading order. After the spectrum of radiation near the horizon is obtained, we use the brick wall model to compute the thermal entropy of a massless scalar field near the horizon of a 4D spherically symmetric black hole and a 2D one. Finally, the luminosity of a Schwarzschild black hole is calculated by using the geometric optics approximation. 0 into PostgreSQL...\n",
      "Inserting test sample 1558  This paper investigates the radiation properties of black holes from the perspective of non-commutative geometry in the context of tunneling. We modify the dispersion relation by shifting the energy-momentum relation, which affects the probability of the emitted particles in the free-fall frame. The key assumption is that particles behave like waves in the presence of gravity, and their trajectories are governed by the Hamilton-Jacobi equation. We show that the modified dispersion relation leads to a nontrivial correction of the emission spectrum, which imposes a periodicity constraint on the black hole mass. We further explore the consequences of this relation and discuss their implications for quantum gravity. We also introduce a new method to compute the emission rate of black holes in the free-fall frame, which has important implications for testing the theory in future experiments. Our results demonstrate the potential of non-commutative geometry to generate new insights into the physics of black holes and pave the way for a deeper understanding of the nature of gravity at the quantum level. 1 into PostgreSQL...\n",
      "Inserting test sample 1559  Nearby sources may contribute to cosmic-ray electron (CRE) structures at high energies. Recently, the first DAMPE results on the CRE flux hinted at a narrow excess at energy ~1.4 TeV. We show that in general a spectral structure with a narrow width appears in two scenarios: I) \"Spectrum broadening\" for the continuous sources with a delta-function-like injection spectrum. In this scenario, a finite width can develop after propagation through the Galaxy, which can reveal the distance of the source. Well-motivated sources include mini-spikes and subhalos formed by dark matter (DM) particles $\\chi_{s}$ which annihilate directly into e+e- pairs. II) \"Phase-space shrinking\" for burst-like sources with a power-law-like injection spectrum. The spectrum after propagation can shrink at a cooling-related cutoff energy and form a sharp spectral peak. The peak can be more prominent due to the energy-dependent diffusion. In this scenario, the width of the excess constrains both the power index and the distance of the source. Possible such sources are pulsar wind nebulae (PWNe) and supernova remnants (SNRs). We analysis the DAMPE excess and find that the continuous DM sources should be fairly close within ~0.3 kpc, and the annihilation cross sections are close to the thermal value. For the burst-like source, the narrow width of the excess suggests that the injection spectrum must be hard with power index significantly less than two, the distance is within ~(3-4) kpc, and the age of the source is ~0.16 Myr. In both scenarios, large anisotropies in the CRE flux are predicted. We identify possible candidates of mini-spike (PWN) sources in the current Fermi-LAT 3FGL (ATNF) catalog. The diffuse gamma-rays from these sources can be well below the Galactic diffuse gamma-ray backgrounds and less constrained by the Ferm-LAT data, if they are located at the low Galactic latitude regions. 0 into PostgreSQL...\n",
      "Inserting test sample 1560  The origin of cosmic-ray electron structures and the DAMPE excess, which is an unusually intense high-energy cosmic-ray electron flux observed by the Dark Matter Particle Explorer (DAMPE), has long been a mystery in astrophysics. Recent research has suggested that these structures may be the result of various astrophysical processes, including cosmic-ray propagation and acceleration in parallel shock waves and turbulence. However, the exact mechanisms behind these processes are still not well understood.\n",
      "\n",
      "In this paper, we present a comprehensive analysis of the DAMPE excess and provide new insights into the origins of sharp cosmic-ray electron structures. Using data collected by the DAMPE detector, we have performed a detailed analysis of the spectrum and angular distribution of the excess and have found that it exhibits several distinct spectral features that cannot be explained by standard astrophysical models.\n",
      "\n",
      "We propose a new model that involves the interaction between cosmic-ray electrons and magnetic turbulence. Our model explains the observed features of the excess and predicts the existence of additional features that could be observed in future experiments. We also discuss the possible implications of our findings for the study of dark matter and the origin of cosmic rays.\n",
      "\n",
      "Our results highlight the importance of high-precision measurements and a careful analysis of cosmic-ray data. They demonstrate the potential of the DAMPE detector for shedding light on some of the most challenging questions in astrophysics, including the origin of cosmic rays and the nature of dark matter. We hope that this work will inspire further research in this field and help us to better understand the mysteries of the cosmos. 1 into PostgreSQL...\n",
      "Inserting test sample 1561  These notes are based on a lecture given at the 2016 Euclid Summer School in Narbonne. I will first give a quick overview of the concept of nuisance parameters in the context of large galaxy surveys. The second part will examine the case study of intrinsic alignments, a potential important contamination of weak lensing observables. 0 into PostgreSQL...\n",
      "Inserting test sample 1562  Galaxy surveys can be hindered by nuisance parameters which affect the accuracy of measurements. In this paper, we propose a method for identifying and removing these parameters, resulting in more precise measurements of galaxy clustering. Our approach is validated using mock data, paving the way for improved analysis of large-scale galaxy surveys. 1 into PostgreSQL...\n",
      "Inserting test sample 1563  Two transiting planet candidates with super-Earth radii around the nearby K7--M0 dwarf star TOI-1238 were announced by TESS. We aim to validate their planetary nature using precise radial velocities (RV) taken with the CARMENES spectrograph. We obtained 55 CARMENES RV data that span 11 months. For a better characterization of the parent star's activity, we also collected contemporaneous optical photometric observations and retrieved archival photometry from the literature. We performed a combined TESS+CARMENES photometric and spectroscopic analysis by including Gaussian processes and Keplerian orbits to account for the stellar activity and planetary signals simultaneously. We estimate that TOI-1238 has a rotation period of 40 $\\pm$ 5 d based on photometric and spectroscopic data. The combined analysis confirms the discovery of two transiting planets, TOI-1238 b and c, with orbital periods of $0.764597^{+0.000013}_{-0.000011}$ d and $3.294736^{+0.000034}_{-0.000036}$ d, masses of 3.76$^{+1.15}_{-1.07}$ M$_{\\oplus}$ and 8.32$^{+1.90}_{-1.88}$ M$_{\\oplus}$, and radii of $1.21^{+0.11}_{-0.10}$ R$_{\\oplus}$ and $2.11^{+0.14}_{-0.14}$ R$_{\\oplus}$. They orbit their parent star at semimajor axes of 0.0137$\\pm$0.0004 au and 0.036$\\pm$0.001 au, respectively. The two planets are placed on opposite sides of the radius valley for M dwarfs and lie between the star and the inner border of TOI-1238's habitable zone. The inner super-Earth TOI-1238 b is one of the densest ultra-short-period planets ever discovered ($\\rho=11.7^{+4.2}_{-3.4}$ g $\\rm cm^{-3}$). The CARMENES data also reveal the presence of an outer, non-transiting, more massive companion with an orbital period and radial velocity amplitude of $\\geq$600 d and $\\geq$70 m s$^{-1}$, which implies a likely mass of $M \\geq 2 \\sqrt{1-e^2}$ M$_{\\rm Jup}$ and a separation $\\geq$1.1 au from its parent star. 0 into PostgreSQL...\n",
      "Inserting test sample 1564  The discovery of a multi-planetary system orbiting the early-M dwarf TOI-1238 would have significant implications for our understanding of planetary formation and architecture. In this study, we present our analysis of transit photometry data obtained from the Transiting Exoplanet Survey Satellite (TESS), which led to the identification of three planets, TOI-1238 b, c, and d, all of which have radii less than 2.5 Earth radii and are located within the habitable zone of the star.\n",
      "\n",
      "We used the TESS data to construct a transit light curve for each planet, and then used these curves to calculate the planets' physical characteristics. Our analysis suggests that these planets are likely to be terrestrial in nature, with densities consistent with rocky compositions. The planets have orbital periods of 1.4, 3.5, and 4.9 days, respectively, and the innermost planet, TOI-1238 b, is the smallest and closest to the star.\n",
      "\n",
      "The system's architecture is of particular interest. The planets' orbits are tightly packed, with orbital spacings of only 1.7 and 1.4 times their individual Hill radii, suggesting that they may be in a near-resonant configuration. This configuration may be the result of convergent migration, which occurs when planets migrate inward toward their star while undergoing convergent orbital evolution due to dissipation of angular momentum. Alternatively, the near-resonant configuration may be primordial in origin, and thus shed light on the formation mechanisms of small, tightly packed planetary systems.\n",
      "\n",
      "Overall, the discovery of the multi-planetary system orbiting TOI-1238 has significant implications for our understanding of planetary formation and architecture. Further observations, including radial velocity measurements and atmospheric observations, will be required to fully characterize the system and determine its composition. Nevertheless, the proximity of this system to Earth, combined with its intriguing architecture, makes it a prime target for future exoplanet discoveries and follow-up studies. 1 into PostgreSQL...\n",
      "Inserting test sample 1565  A twisted generalization of Lie-Yamaguti algebras, called Hom-Lie-Yamaguti algebras, is defined. Hom-Lie-Yamaguti algebras generalize Hom-Lie triple systems (and susequently ternary Hom-Nambu algebras) and Hom-Lie algebras in the same way as Lie-Yamaguti algebras generalize Lie triple systems and Lie algebras. It is shown that the category of Hom-Lie-Yamaguti algebras is closed under twisting by self-morphisms. Constructions of Hom-Lie-Yamaguti algebras from classical Lie-Yamaguti algebras and Malcev algebras are given. It is observed that, when the ternary operation of a Hom-Lie-Yamaguti algebra expresses through its binary one in a specific way, then such a Hom-Lie-Yamaguti algebra is a Hom-Malcev algebra. 0 into PostgreSQL...\n",
      "Inserting test sample 1566  In this paper, we introduce a generalization of Lie-Yamaguti algebras, which we term \"twisted Lie-Yamaguti algebras.\" Our proposed framework extends previous work by allowing for a broad class of automorphisms that satisfy certain unintuitive relations. We demonstrate that these generalized Lie-Yamaguti algebras have a number of important properties, including a well-defined differential form reminiscent of the usual Lie algebra. This leads to a variety of potential applications, from representation theory to topology. We also study specific examples of twisted Lie-Yamaguti algebras, including those derived from certain quantum groups and Lie superalgebras. Our results contribute to the ongoing development of Lie theory and have important implications for the representation of algebraic structures in physical systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1567  Simulation of unsteady creeping flows in complex geometries has traditionally required the use of a time-stepping procedure, which is typically costly and unscalable. To reduce the cost and allow for computations at much larger scales, we propose an alternative approach that is formulated based on the unsteady Stokes equation expressed in the time-spectral domain. This transformation results in a boundary value problem with an imaginary source term proportional to the computed mode that is discretized and solved in a complex-valued finite element solver using Bubnov-Galerkin formulation. This transformed spatio-spectral formulation presents several advantages over the traditional spatio-temporal techniques. Firstly, for cases with boundary conditions varying smoothly in time, it provides a significant saving in computational cost as it can resolve time-variation of the solution using a few modes rather than thousands of time steps. Secondly, in contrast to the traditional time integration scheme with a finite order of accuracy, this method exhibits a super convergence behavior versus the number of computed modes. Thirdly, in contrast to the stabilized finite element methods for fluid, no stabilization term is employed in our formulation, producing a solution that is consistent and more accurate. Fourthly, the proposed approach is embarrassingly parallelizable owing to the independence of the solution modes, thus enabling scalable calculations at a much larger number of processors. The comparison of the proposed technique against a standard stabilized finite element solver is performed using two- and three-dimensional canonical and complex geometries. The results show that the proposed method can produce more accurate results at 1% to 11% of the cost of the standard technique for the studied cases. 0 into PostgreSQL...\n",
      "Inserting test sample 1568  In this paper, we propose a scalable spectral Stokes solver for simulation of time-periodic flows in complex geometries. The solver combines the spectral method with the Immersed Boundary method for handling complex geometries, and the time-dependent problem is solved using a time-splitting approach with the Crank-Nicolson method. Our solver is tested on various benchmark problems and demonstrates high accuracy and efficiency, especially for time-periodic flow simulations in complex geometries.\n",
      "\n",
      "The scalability of the solver is achieved by utilizing parallel computing techniques, including domain decomposition and message passing interface (MPI). Scaling results indicate that our solver can achieve high parallel efficiency and scalability up to thousands of processors.\n",
      "\n",
      "To demonstrate the capabilities of our solver, we present several applications, including simulation of the Von KÃ¡rmÃ¡n vortex street formation behind a circular cylinder, flow around a flapping wing, and flow over a three-dimensional bluff body. The numerical results show good agreement with experimental data and demonstrate the ability of our solver in simulating complex time-periodic flows in realistic configurations.\n",
      "\n",
      "In conclusion, the proposed scalable spectral Stokes solver is an efficient and accurate tool for simulating time-periodic flows in complex geometries. The solver can handle a wide range of applications and show good scalability for large-scale problems. It has great potential for future applications in various fields, such as aerospace engineering, fluid dynamics, and environmental sciences. 1 into PostgreSQL...\n",
      "Inserting test sample 1569  (Abridged). The type-I X-ray bursting low mass X-ray binary KS 1731-260 was recently detected for the first time in quiescence by Wijnands et al., following an approximately 13 yr outburst which ended in Feb 2001. Unlike all other known transient neutron stars, the duration of this recent outburst is as long as the thermal diffusion time of the crust. The large amount of heat deposited by reactions in the crust will have heated the crust to temperatures much higher than the equilibrium core temperature. As a result, the thermal luminosity currently observed from the neutron star is dominated not by the core, but by the crust. Moreover, the level and the time evolution of quiescent luminosity is determined mostly by the amount of heat deposited in the crust during the most recent outburst. Using estimates of the outburst mass accretion rate, our calculations of the quiescent flux immediately following the end of the outburst agree with the observed quiescent flux to within a factor of a few. We present simulations of the evolution of the quiescent lightcurve for different scenarios of the crust microphysics, and demonstrate that monitoring observations (with currently flying instruments) spanning from 1--30 yr can measure the crust cooling timescale and the total amount of heat stored in the crust. These quantities have not been directly measured for any neutron star. 0 into PostgreSQL...\n",
      "Inserting test sample 1570  The neutron star in KS 1731-260 has long been the focus of extensive research, and understanding its behaviors and emissions has been a subject of study for decades. In this paper, we investigate the correlation between crustal properties and the quiescent spectrum of the neutron star. By analyzing data obtained from both Chandra and XMM-Newton observatories, we have developed a comprehensive understanding of the crustal emission processes. We propose a physical model that accurately explains the sources of radiation, both thermal and non-thermal, encompassing all X-ray wavelengths. The model suggests that the surface temperature is strongly linked to the accretion rate, and by fitting the data with our model, we find that the temperature of the neutron star surface varies from 0.09-0.2 keV. Moreover, we provide evidence supporting the hypothesis that accretion heating is an important factor determining the quiescent spectrum of the neutron star. Our findings have significant implications for our understanding of neutron stars and their emission mechanisms and can also provide valuable insights into the formation and evolution of compact objects and their surrounding environments. 1 into PostgreSQL...\n",
      "Inserting test sample 1571  Aims. We aim to explore the photosphere of the very cool late-type star VX Sgr and in particular the existence and characterization of molecular layers above the continuum forming photosphere. Methods. We obtained interferometric observations with the VLTI/AMBER interferometer using the fringe tracker FINITO in the spectral domain 1.45-2.50 micron with a spectral resolution of about 35 and baselines ranging from 15 to 88 meters.We perform independent image reconstruction for different wavelength bins and fit the interferometric data with a geometrical toy model.We also compare the data to 1D dynamical models of Miras atmosphere and to 3D hydrodynamical simulations of red supergiant (RSG) and asymptotic giant branch (AGB) stars. Results. Reconstructed images and visibilities show a strong wavelength dependence. The H-band images display two bright spots whose positions are confirmed by the geometrical toy model. The inhomogeneities are qualitatively predicted by 3D simulations. At about 2,00 micron and in the region 2,35 - 2,50 micron, the photosphere appears extended and the radius is larger than in the H band. In this spectral region, the geometrical toy model locates a third bright spot outside the photosphere that can be a feature of the molecular layers. The wavelength dependence of the visibility can be qualitatively explained by 1D dynamical models of Mira atmospheres. The best-fitting photospheric models show a good match with the observed visibilities and give a photospheric diameter of theta = 8,82+-0,50 mas. The H2O molecule seems to be the dominant absorber in the molecular layers. Conclusions. We show that the atmosphere of VX Sgr rather resembles Mira/AGB star model atmospheres than RSG model atmospheres. In particular, we see molecular (water) layers that are typical for Mira stars. 0 into PostgreSQL...\n",
      "Inserting test sample 1572  The surface structure of VX Sagittarii's outer atmosphere has been the subject of intense study for years. In this paper, we present the results of our observation of VX Sgr using the VLTI/AMBER spectro-interferometry techniques. The observations were conducted for a period of several consecutive nights, allowing us to gather high-resolution images in the near-infrared region.\n",
      "\n",
      "Our results revealed the presence of an inhomogeneous atmosphere around VX Sgr, with a number of asymmetrical features. These asymmetries manifest themselves as small- and large-scale structures, such as elongated patches and spiral-like patterns. These structures vary in shape and size, with the largest ones reaching a scale of several AU. We also found evidence for rapid changes in the shape and position of these features over time. Our observations confirm the previously reported presence of hot and dense spots on the surface of VX Sgr.\n",
      "\n",
      "To better understand the nature and origin of these features, we employed 2D radiative transfer models to simulate the structure and dynamics of the observed atmosphere. Our models suggest that the observed asymmetries are most likely caused by large-scale convective cells that transport hot gas from the deeper layers of the atmosphere to the outer regions. These cells provoke a cooling effect when they reach the upper atmosphere which induces a low-density gas envelope and causes the growth of small density fluctuations that are responsible for the observed features.\n",
      "\n",
      "In conclusion, our VLTI/AMBER observations allowed us to obtain high-resolution images of VX Sgr's inhomogeneous outer atmosphere, revealing the presence of asymmetrical structures at different scales. We used radiative transfer modeling to attribute the observed features to large-scale convective cells, which could play a significant role in the transport of mass and energy in the atmosphere of evolved stars. 1 into PostgreSQL...\n",
      "Inserting test sample 1573  Strongly interacting Fermi gasses at low density possess universal thermodynamic properties which have recently seen very precise $PVT$ measurements by a group at MIT. This group determined local thermodynamic properties of a system of ultra cold $^6\\mbox{Li}$ atoms tuned to Feshbach resonance. In this paper, I analyze the MIT data with a thermodynamic theory of unitary thermodynamics based on ideas from critical phenomena. This theory was introduced in the first paper of this sequence, and characterizes the scaled thermodynamics by the entropy per particle $z= S/N k_B$, and energy per particle $Y(z)$, in units of the Fermi energy. $Y(z)$ is in two segments, separated by a second-order phase transition at $z=z_c$: a \"normal\" segment for $z>z_c$, and a \"superfluid\" segment for $z<z_c$. For small $z$, the theory obeys a series $Y(z)=y_0+y_1 z^{\\alpha }+y_2 z^{2 \\alpha}+\\cdots,$ where $\\alpha$ is a constant exponent, and $y_i$ ($i\\ge 0$) are constant series coefficients. For large $z$, the theory obeys a perturbation of the ideal gas $Y(z)= \\tilde{y}_0\\,\\mbox{exp}[2\\gamma z/3]+ \\tilde{y}_1\\,\\mbox{exp}[(2\\gamma/3-1)z]+ \\tilde{y}_2\\,\\mbox{exp}[(2\\gamma/3-2)z]+\\cdots$ where $\\gamma$ is a constant exponent, and $\\tilde{y}_i$ ($i\\ge 0$) are constant series coefficients. This limiting form for large $z$ differs from the series used in the first paper, and was necessary to fit the MIT data. I fit the MIT data by adjusting four free independent theory parameters: $(\\alpha,\\gamma,\\tilde{y}_0,\\tilde{y}_1)$.\n",
      "\n",
      "This fit process was augmented by trap integration and comparison with earlier thermal data taken at Duke University. The overall match to both the data sets was good, and had $\\alpha=1.21(3)$, $\\gamma=1.21(3)$, $z_c=0.69(2)$, scaled critical temperature $T_c/T_F=0.161(3)$, where $T_F$ is the Fermi temperature, and Bertsch parameter $\\xi_B=0.368(5)$. 0 into PostgreSQL...\n",
      "Inserting test sample 1574  This research paper aims at further developing the concept of unitary thermodynamics from thermodynamic geometry, focusing on fitting this new approach to a local density approximation. Taking into account the principles of quantum mechanics and combining them with the usual description of thermodynamics, we propose a unified framework which not only solves some inconsistencies in the current literature, but also provides a better understanding of the underlying physics. \n",
      "\n",
      "Using a geometric representation of thermodynamic systems, we study the thermodynamic properties of a variety of interacting quantum systems, obtaining useful insights into their behavior. In particular, we derive the equation of state for a generic system, which is shown to accurately reproduce the thermodynamic properties in the limit of large particle numbers. We also analyze the properties of various thermodynamic potentials and show how they relate to more familiar quantities such as the entropy and the energy.\n",
      "\n",
      "To validate our approach, we apply it to a local density approximation and compare the results with those obtained from more traditional approaches. Our method shows a better agreement with experimental and numerical results, indicating that our geometric approach is a more natural and powerful way to describe the thermodynamics of quantum systems. We provide several examples showing how our approach can be applied to a wide range of physical situations, such as condensed matter physics, quantum information theory, and high-energy physics.\n",
      "\n",
      "Finally, we discuss some of the implications of our work and point out some of the questions that remain open for future research. Our ultimate goal is to provide a more comprehensive understanding of quantum thermodynamics and its relation to the underlying quantum mechanics, paving the way for new insights and breakthroughs in this exciting field. 1 into PostgreSQL...\n",
      "Inserting test sample 1575  Neodymium is a remarkable active component in numerous magnetic alloys that are used in various applications. However, the application of bare neodymium thin film is limited due to the lack of information about its electrical and magnetic properties. We report synergistic study of Nd thin film using experimental and theoretical techniques of polarized neutron reflectometry, magnetoresistance measurement and density functional theory. Unlike bulk Nd, thin film specimen is a very poor electrical conductor. Also, as grown thin film on silicon substrate does not exhibit any magnetism in zero field.\n",
      "\n",
      "However, moderate inplane field application of $H$ = 1.2 T tends to induce weak magnetism in the system at low temperature of $T$ $<$ 18 K, which coincides with an unusual cross-over behavior in magnetoresistance. The study provides important insight in the physical characteristics of Nd thin film that are atypical for a magnetic system. 0 into PostgreSQL...\n",
      "Inserting test sample 1576  The study of nonconventional magnetic phenomena in neodymium (Nd) thin film has gained significant interest due to its potential for technological applications. In this research paper, we explore the magnetic properties of Nd thin film under various conditions, including temperature and magnetic field. Our findings show that Nd thin film displays distinct magnetic behavior, such as unconventional magnetization loops and field-induced irreversibility. Furthermore, we investigate the origins of these nonconventional magnetic properties using advanced experimental techniques, including x-ray diffraction and magnetometry measurements. Our results suggest that the nonconventional magnetic behavior in Nd thin film could be attributed to the presence of domain walls and magnetic anisotropy. The insights gained from this study could pave the way for the development of new materials with tailored magnetic properties for future electronic and spintronic applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1577  Recent observations have discovered a population of extended Lya sources, dubbed Lyman-alpha blobs (LABs), at high redshift z ~ 3 - 6.6. These LABs typically have a luminosity of L ~ 10^42-10^44 erg/s, and a size of tens of kiloparsecs, with some giant ones reaching up to D ~ 100 kpc. However, the origin of these LABs is not well understood. In this paper, we investigate a merger model for the formation of LABs by studying Lya emission from interacting galaxies at high redshifts by means of a combination of hydrodynamics simulations with three dimensional radiative transfer calculations. Our galaxy simulations focus on a set of binary major mergers of galaxies with a mass range of 3-7 *10^12 Msun in the redshift range of z ~ 3 -7, and we use the newly improved ART^2 code to perform the radiative transfer calculations which couple multi-wavelength continuum, ionization of hydrogen, and Lya line emission. We find that intense star formation and enhanced cooling induced by gravitational interaction produce strong Lya emission from these merging galaxies. The Lya emission appears to be extended due to the extended distribution of sources and gas. During the close encounter of galaxy progenitors when the star formation rate peaks at ~ 10^3 Msun/yr, our model produces Lya blobs with luminosity of L ~ 10^42-10^44 erg/s, and size of D ~ 10-20 kpc at z>6 and D ~ 20-50 kpc at z ~ 3, in broad agreement with observations in the same redshift range. Our results suggest that merging galaxies may produce some typical LABs as observed, but the giant ones may be produced by mergers more massive than those in our model, or a combination of mergers and cold accretion from filaments on a large scale. 0 into PostgreSQL...\n",
      "Inserting test sample 1578  This study investigates extended Lyman-alpha (LyÎ±) emission in galaxies at high redshifts resulting from galactic interactions. Observations were made using the Multi-Unit Spectroscopic Explorer (MUSE) detector on the ESO Very Large Telescope (VLT) and were focused on six interacting galaxy pairs at redshifts z ~ 3. These galaxies exhibited significant LyÎ± emission that extended beyond their individual stellar components. We use a combination of spectroscopic and photometric data to analyze the physical properties of the extended emission in these interacting systems. Our results show that the extended LyÎ± emission profiles are more complex than previously assumed, highlighting the importance of interactions and outflows in driving the LyÎ± signal. The observed extended LyÎ± emission from the galaxy pairs is consistent with models of gas being stripped from the galaxies and ionized by the surrounding medium. The MUSE observations provide a map of the extended LyÎ± emission that is invaluable for future studies on the interaction between galaxies and the intergalactic medium (IGM). We discuss the implications of our findings for understanding the star formation history of galaxies, the role of feedback in the formation of galaxies, and the distribution of hydrogen in the early Universe. This work opens new avenues for exploring the properties of high-redshift galaxies and provides a foundation for future studies of the LyÎ± emission from interacting systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1579  A non-diagonal vielbein ansatz is applied to the $N$-dimension field equations of $f(T)$ gravity. An analytical vacuum solution is derived for the quadratic polynomial $f(T)=T+\\epsilon T^2$ in the presence of a cosmological constant $\\Lambda$. Since the induced metric has off diagonal components, that cannot be removed by a mere of a coordinate transformation, the solution has a rotating parameter. The curvature and torsion scalars invariants are calculated to study the singularities and horizons of the solution. In contrast to the general relativity (GR), the Cauchy horizon is differ from the horizon which shows the effect of the higher order torsion. The general expression of the energy-momentum vector of $f(T)$ gravity is used to calculate the energy of the system. Finally, we have shown that this kind of solution satisfies the first law of thermodynamics in the framework of $f(T)$ gravitational theories. 0 into PostgreSQL...\n",
      "Inserting test sample 1580  This paper investigates rotating black hole solutions in $N$-dimensional $f(T)$ gravity, a modified theory of gravity based on the teleparallel equivalent of general relativity. We start by reviewing the basics of teleparallel gravity and explain its extension, $f(T)$ gravity. We then derive the field equations for a rotating black hole in $N$ dimensions using the Newman-Janis algorithm and evaluate them using the $f(T)$ gravity framework. We obtain analytic solutions for the metric, including the metric function and the determinant of the spatial metric, which can describe the geometry of a rotating black hole in any number of dimensions. As an example, we discuss the particular case of a charged rotating black hole in 5D $f(T)$ gravity. Our results illustrate the versatility and potential of $f(T)$ gravity, providing new insights into the properties of black holes in different dimensions and the behavior of gravity in extreme conditions. 1 into PostgreSQL...\n",
      "Inserting test sample 1581  Penrose transform tells us that there is an isomorphism of the kernel of an invariant differential operator studied in the paper [TS] and sheaf cohomology of some vector bundle on twistor space. The point of this paper is to write down this isomorphism explicitly. Explicit form of the isomorphism will be crucial for further investigation on the properties of the complex. 0 into PostgreSQL...\n",
      "Inserting test sample 1582  The Penrose transform is a significant concept in complex analysis, Fourier analysis, and differential geometry. Monogenic functions refer to a special class of holomorphic functions in several complex variables. In this paper, we investigate the relationship between the Penrose transform and monogenic functions by exploring the boundary behavior of monogenic functions and their integral representation through the Penrose transform. Our results establish new connections and provide insights into the interplay between these two fundamental mathematical concepts. 1 into PostgreSQL...\n",
      "Inserting test sample 1583  We study a Monte Carlo algorithm for simulation of probability distributions based on stochastic step functions, and compare to the traditional Metropolis/Hastings method. Unlike the latter, the step function algorithm can produce an uncorrelated Markov chain. We apply this method to the simulation of Levy processes, for which simulation of uncorrelated jumps are essential.\n",
      "\n",
      "We perform numerical tests consisting of simulation from probability distributions, as well as simulation of Levy process paths. The Levy processes include a jump-diffusion with a Gaussian Levy measure, as well as jump-diffusion approximations of the infinite activity NIG and CGMY processes.\n",
      "\n",
      "To increase efficiency of the step function method, and to decrease correlations in the Metropolis/Hastings method, we introduce adaptive hybrid algorithms which employ uncorrelated draws from an adaptive discrete distribution defined on a space of subdivisions of the Levy measure space.\n",
      "\n",
      "The nonzero correlations in Metropolis/Hastings simulations result in heavy tails for the Levy process distribution at any fixed time. This problem is eliminated in the step function approach. In each case of the Gaussian, NIG and CGMY processes, we compare the distribution at t=1 with exact results and note the superiority of the step function approach. 0 into PostgreSQL...\n",
      "Inserting test sample 1584  This paper presents a novel method for simulating Levy processes by using stochastic step functions. Levy processes have wide applications in finance, physics, and neuroscience, among others, due to their ability to model various real-world phenomena with a high degree of accuracy. Traditional simulation methods for Levy processes involve complex mathematical models, which can be computationally expensive. In contrast, our proposed method uses stochastic step functions to approximate the Levy process, thus reducing the computational burden. We prove the convergence of the method and provide numerical examples demonstrating its effectiveness. Additionally, we show that the method can be used to simulate both stationary and non-stationary Levy processes, making it a versatile tool for a range of applications. Our proposed method has the potential to significantly advance the field of simulation for Levy processes, providing a more efficient and accurate way to model complex systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1585  In this paper, we propose a procedure for constructing an infinite number of families of solutions of given linear differential equations with partial derivatives with constant coefficients. We use monogenic functions that are defined on some sequences of commutative associative algebras over the field of complex numbers. To achieve this goal, we first study the solutions of the so-called characteristic equation on a given sequence of algebras. Further, we investigate monogenic functions on the sequence of algebras and study their relation with solutions of partial deferential equations. The proposed method is used to construct solutions of some equations of mathematical physics. In particular, for the three-dimensional Laplace equation and the wave equation, for the equation of transverse oscillations of the elastic rod and the conjugate equation, a generalized biharmonic equation and the two-dimensional Helmholtz equation. We note that this method yields all analytic solutions of the two-dimensional Laplace equation and the two-dimensional biharmonic equation (Goursat formula). 0 into PostgreSQL...\n",
      "Inserting test sample 1586  This research presents a novel hypercomplex method for solving linear partial differential equations with constant coefficients. Hypercomplex numbers extend the traditional complex numbers, allowing for a wider variety of calculations and representations. The proposed method relies on the decomposition of a PDE into its eigenvalues and eigenvectors, which are then operated on using hypercomplex numbers. This approach provides a more flexible solution than the classical separation of variables method, allowing for the treatment of more complex PDEs, including those with variable coefficients. We demonstrate the efficiency and accuracy of the hypercomplex method through two case studies: the wave equation and the heat equation. Results show that the proposed method outperforms existing approaches in terms of computational time and accuracy, particularly for PDEs with multiple dimensions. This research contributes to the advancement of hypercomplex methods in the field of PDEs, providing an alternative and efficient way to solve problems with constant coefficients. 1 into PostgreSQL...\n",
      "Inserting test sample 1587  (Abridged) The abundances of alpha-elements are a powerful diagnostic of the star formation history and chemical evolution of a galaxy. Sulphur, being moderately volatile, can be reliably measured in the interstellar medium (ISM) of damped Ly-alpha galaxies and extragalactic HII regions. Measurements in stars of different metallicity in our Galaxy can then be readily compared to the abundances in external galaxies. Such a comparison is not possible for Si or Ca that suffer depletion onto dust in the ISM. Furthermore, studying sulphur is interesting because it probes nucleosynthetic conditions that are very different from those of O or Mg. The measurements in star clusters are a reliable tracers of the Galactic evolution of sulphur. We find <A(S)>NLTE=6.11+/-0.04 for M 4, <A(S)>NLTE=7.17+/-0.02 for NGC 2477, and <A(S)>NLTE=7.13+/-0.06 for NGC 5822. For the only star studied in Trumpler 5 we find A(S)NLTE=6.43+/-0.03 and A(S)LTE=6.94+/-0.05. Our measurements show that, by and large, the S abundances in Galactic clusters trace reliably those in field stars. The only possible exception is Trumpler 5, for which the NLTE sulphur abundance implies an [S/Fe] ratio lower by roughly 0.4 dex than observed in field stars of comparable metallicity, even though its LTE sulphur abundance is in line with abundances of field stars. Moreover the LTE sulphur abundance is consistent only with the abundance of another alpha-element, Mg, in the same star, while the low NLTE value is consistent with Si and Ca. The S abundances in our sample of stars in clusters imply that the clusters are chemically homogeneous for S within 0.05 dex. 0 into PostgreSQL...\n",
      "Inserting test sample 1588  This paper presents tracer element evidence on the Galactic chemical evolution of sulphur from star clusters. Our study is based on a sample of 25 open star clusters located in the Galactic disk that cover a wide range of ages and metallicities, as well as different Galactocentric distances. For each cluster, we derived ages and metallicities from isochrone fitting and high-resolution spectroscopy, respectively. The sulphur content, together with that of other key elements such as iron, Î± elements, and nitrogen, among others, was obtained from equivalent width measurements of stellar absorption lines in medium-resolution optical spectra. In particular, we focus on the abundance ratio [S/Fe] as a function of [Fe/H], and how it evolves with time and at different positions in the Galaxy. We find that [S/Fe] behaves similarly to other Î± elements, with a tendency to flatten out and become almost constant for stars with [Fe/H] > âˆ’0.5, whereas at lower metallicities it declines with decreasing [Fe/H]. This plateau in [S/Fe] is supported by chemical evolution models and owes to a complex interplay between the nucleosynthesis channels of sulphur (mainly massive stars and Type Ia supernovae) and the specific net yields, which are dependent on the initial stellar mass function and metallicity. Finally, we also investigated the radial abundance gradient of sulphur in the Galactic disk and placed our results in the context of other elements' gradients, providing new clues on the assembly history of the Milky Way. 1 into PostgreSQL...\n",
      "Inserting test sample 1589  Galaxy mergers are expected to have a significant role in the mass assembly of galaxies in the early Universe, but there are very few observational constraints on the merger history of galaxies at $z>2$. We present the first study of galaxy major mergers (mass ratios $>$ 1:4) in mass-selected samples out to $z\\approx6$. Using all five fields of the HST/CANDELS survey and a probabilistic pair count methodology that incorporates the full photometric redshift posteriors and corrections for stellar mass completeness, we measure galaxy pair-counts for projected separations between 5 and 30 kpc in stellar mass selected samples at $9.7 < \\log_{10}(\\rm{M}_{*}/\\rm{M}_{\\odot}) < 10.3$ and $\\log_{10}(\\rm{M}_{*}/\\rm{M}_{\\odot}) > 10.3$. We find that the major merger pair fraction rises with redshift to $z\\approx6$ proportional to $(1+z)^{m}$, with $m = 0.8\\pm0.2$ ($m = 1.8\\pm0.2$) for $\\log_{10}(\\rm{M}_{*} / \\rm{M}_{\\odot}) > 10.3$ ($9.7 < \\log_{10}(\\rm{M}_{*}/\\rm{M}_{\\odot}) < 10.3$).\n",
      "\n",
      "Investigating the pair fraction as a function of mass ratio between 1:20 and 1:1, we find no evidence for a strong evolution in the relative numbers of minor to major mergers out to $z<3$. Using evolving merger timescales we find that the merger rate per galaxy ($\\mathcal{R}$) rises rapidly from $0.07\\pm 0.01$ Gyr$^{-1}$ at $z < 1$ to $7.6\\pm 2.7$ Gyr$^{-1}$ at $z = 6$ for galaxies at $\\log_{10}(\\rm{M}_{*}/\\rm{M}_{\\odot}) > 10.3$. The corresponding co-moving major merger rate density remains roughly constant during this time, with rates of $\\Gamma \\approx 10^{-4}$ Gyr$^{-1}$ Mpc$^{-3}$. Based on the observed merger rates per galaxy, we infer specific mass accretion rates from major mergers that are comparable to the specific star-formation rates for the same mass galaxies at $z>3$ - observational evidence that mergers are as important a mechanism for building up mass at high redshift as in-situ star-formation. 0 into PostgreSQL...\n",
      "Inserting test sample 1590  This study presents probabilistic galaxy pair counts in the CANDELS fields to constrain the merger history of galaxies since $z\\approx6$. Our methodology employed a sample of galaxies with $M_* > 10^{9.5} M_\\odot$ and redshift range $0.5<z<3$ in three fields: GOODS-South, GOODS-North and UDS. We further selected pairs with projected separations $5 < r_p < 20\\, \\text{kpc}$ and relative velocities $\\Delta v < 500\\, \\text{km/s}$. The resulting sample consists of $(247\\pm13)\\times 10^3$ pairs, with GOODS-South contributing the most massive and lower-redshift pairs. To determine the true merger fraction and the merger timescale, we build Monte Carlo simulations including data on galaxy stellar masses, redshift distributions, and the observed pair fraction. Our results suggest that the merger fraction increases since $z\\approx3$ and, assuming an average stellar mass ratio of 1:4, the merger timescale is shorter at higher redshifts. We show that our probability distribution method is well-suited to constrain the accuracy of galaxy merger rates, and we compare our findings with previous studies based on other observational approaches. Our study reveals that the evolution of the galaxy merger rate is driven by both the mass and luminosity of the galaxy pairs. The high merger rate at high-redshift suggests that the initial stages of galaxy formation were dominated by hierarchical merging. Moreover, massive galaxies have a higher probability of undergoing mergers and the minor merger events significantly contribute to growth in stellar mass. Our analysis has important implications for understanding galaxy formation and evolution and can be used to refine cosmological simulations and models. 1 into PostgreSQL...\n",
      "Inserting test sample 1591  Experimental and numerical studies of the velocity field of dark solitons in Bose-Einstein condensates are presented. The formation process after phase imprinting as well as the propagation of the emerging soliton are investigated using spatially resolved Bragg-spectroscopy of soliton states in Bose-Einstein condensates of Rubidium87. A comparison of experimental data to results from numerical simulations of the Gross-Pitaevskii equation clearly identifies the flux underlying a dark soliton propagating in a Bose-Einstein condensate. The results allow further optimization of the phase imprinting method for creating collective exitations of Bose-Einstein condensates. 0 into PostgreSQL...\n",
      "Inserting test sample 1592  This study focuses on the spectroscopic analysis of dark soliton states formed within Bose-Einstein condensates. By manipulating the inter-atomic interactions and confinement potential, we were able to create stable dark soliton states in a quasi-1D atomic cloud. Using absorption imaging and phase-contrast microscopy, we observed the non-uniform density profile and dynamics of these states, as well as the presence of higher-order soliton structures. Our results provide valuable insights into the fundamental properties of dark soliton states and their potential applications in atomic physics and beyond. 1 into PostgreSQL...\n",
      "Inserting test sample 1593  The Lightning Network (LN) is a prominent payment channel network aimed at addressing Bitcoin's scalability issues. Due to the privacy of channel balances, senders cannot reliably choose sufficiently liquid payment paths and resort to a trial-and-error approach, trying multiple paths until one succeeds.\n",
      "\n",
      "This leaks private information and decreases payment reliability, which harms the user experience. This work focuses on the reliability and privacy of LN payments. We create a probabilistic model of the payment process in the LN, accounting for the uncertainty of the channel balances. This enables us to express payment success probabilities for a given payment amount and a path.\n",
      "\n",
      "Applying negative Bernoulli trials for single- and multi-part payments allows us to compute the expected number of payment attempts for a given amount, sender, and receiver. As a consequence, we analytically derive the optimal number of parts into which one should split a payment to minimize the expected number of attempts. This methodology allows us to define service level objectives and quantify how much private information leaks to the sender as a side effect of payment attempts. We propose an optimized path selection algorithm that does not require a protocol upgrade. Namely, we suggest that nodes prioritize paths that are most likely to succeed while making payment attempts. A simulation based on the real-world LN topology shows that this method reduces the average number of payment attempts by 20% compared to a baseline algorithm similar to the ones used in practice. This improvement will increase to 48% if the LN protocol is upgraded to implement the channel rebalancing proposal described in BOLT14. 0 into PostgreSQL...\n",
      "Inserting test sample 1594  The Lightning Network (LN) is a second-layer payment network built on top of the Bitcoin blockchain. It is designed to enable fast and cheap cryptocurrency transactions by creating a network of peer-to-peer payment channels. However, these channels pose significant security and privacy risks, particularly when channel balances are uncertain.\n",
      "\n",
      "This paper presents a comprehensive analysis of the security and privacy risks associated with LN payments when channel balances are unknown. We identify several attack vectors that could compromise the confidentiality and integrity of payments and propose countermeasures to mitigate these risks.\n",
      "\n",
      "Our analysis reveals that adversaries can gain valuable information by monitoring the LN network and inferring channel balances based on payment activity. This information leakage can be exploited to mount attacks such as channel exhaustion, denial-of-service, and payment interception. Additionally, adversaries can leverage the irrational behavior of some users to manipulate channel balances and extract monetary gains.\n",
      "\n",
      "To address these issues, we propose a set of countermeasures that include path diversification, payment randomization, and fee diversity, among others. These measures aim at increasing the privacy and security of LN payments, even in the presence of uncertain channel balances. We evaluate the effectiveness of our proposed techniques by simulating various attack scenarios and measuring their impact on the LN network.\n",
      "\n",
      "Our results indicate that the proposed countermeasures effectively reduce the likelihood and impact of attacks on LN payments, thus improving the security and privacy of the network. We conclude by highlighting the importance of robust security and privacy mechanisms in payment networks such as LN. 1 into PostgreSQL...\n",
      "Inserting test sample 1595  The Talbot like effect of symmetric Pearcey beams (SPBs) is presented numerically and experimentally in the free space. Owing to the Talbot like effect, the SPBs have the property of periodic and multiple autofocusing.\n",
      "\n",
      "Meanwhile, the focal positions and focal times of SPBs are controlled by the beam shift factor and the distribution factors. What is more, the beam shift factor can also affect the Talbot-like effect and the Talbot period. Therefore, several tiny optical bottles can be generated under the appropriate parameter setting. It is believed that the results can diversify the application of the Talbot effect. 0 into PostgreSQL...\n",
      "Inserting test sample 1596  This paper proposes a novel approach for autofocusing self-imaging through the Symmetric Pearcey Talbot-like Effect. A new optical setup is introduced which enable us to generate a focused beam without any additional focusing optics. The experimental results demonstrate the feasibility of the proposed method. Moreover, it is shown that the self-imaging effect can be achieved using different types of input beams. The proposed technique can be implemented in various fields of optics, including microscopy and laser engineering. This work opens up new avenues for the creation of high-resolution imaging devices without the need for additional focusing components. 1 into PostgreSQL...\n",
      "Inserting test sample 1597  Numerical semigroup rings are investigated from the relative viewpoint. It is known that algebraic properties such as singularities of a numerical semigroup ring are properties of a flat numerical semigroup algebra. In this paper, we show that arithmetic and set-theoretic properties of a numerical semigroup ring are properties of an equi-gcd numerical semigroup algebra. 0 into PostgreSQL...\n",
      "Inserting test sample 1598  In this paper, we investigate the coefficient rings of numerical semigroup algebras. We prove that the coefficient ring of a numerical semigroup algebra is Noetherian, and we provide an explicit description of its prime ideals. Moreover, we show that the Hilbert series of the coefficient ring can be computed using the semigroup data. 1 into PostgreSQL...\n",
      "Inserting test sample 1599  We explore the momentum and velocity dependent elastic scattering between the dark matter (DM) particles and the nuclei in detectors and the Sun. In terms of the non-relativistic effective theory, we phenomenologically discuss ten kinds of momentum and velocity dependent DM-nucleus interactions and recalculate the corresponding upper limits on the spin-independent DM-nucleon scattering cross section from the current direct detection experiments. The DM solar capture rate is calculated for each interaction. Our numerical results show that the momentum and velocity dependent cases can give larger solar capture rate than the usual contact interaction case for almost the whole parameter space. On the other hand, we deduce the Super-Kamiokande's constraints on the solar capture rate for eight typical DM annihilation channels. In contrast to the usual contact interaction, the Super-Kamiokande and IceCube experiments can give more stringent limits on the DM-nucleon elastic scattering cross section than the current direct detection experiments for several momentum and velocity dependent DM-nucleus interactions. In addition, we investigate the mediator mass's effect on the DM elastic scattering cross section and solar capture rate. 0 into PostgreSQL...\n",
      "Inserting test sample 1600  This paper explores the direct detection and solar capture of dark matter in the context of momentum and velocity dependent elastic scattering. Dark matter detection is a critical component in studying its physical properties and interactions with ordinary matter. The analysis is based on a study of solar capture of dark matter, which considers the time-dependent behavior of a weakly interacting massive particle (WIMP) signal against the background noise of neutrinos. The work proposed here offers new insight into the precise characterizations of the WIMPs- the mass, cross-section, and velocity distribution that satiate the relic density required to explain dark matter in the universe, and the detection capabilities of research instruments. The main results indicate that an astro-particle experiment with wide energy threshold capabilities would improve the sensitivity of dark matter searches, thereby contributing to the current search for dark matter and the nature of its interaction with standard model particles. 1 into PostgreSQL...\n",
      "Inserting test sample 1601  The DeepDoseNet 3D dose prediction model based on ResNet and Dilated DenseNet is proposed. The 340 head-and-neck datasets from the 2020 AAPM OpenKBP challenge were utilized, with 200 for training, 40 for validation, and 100 for testing. Structures include 56Gy, 63Gy, 70Gy PTVs, and brainstem, spinal cord, right parotid, left parotid, larynx, esophagus, and mandible OARs. Mean squared error (MSE) loss, mean absolute error (MAE) loss, and MAE plus dose-volume histogram (DVH) based loss functions were investigated. Each model's performance was compared using a 3D dose score, $\\bar{S_{D}}$, (mean absolute difference between ground truth and predicted 3D dose distributions) and a DVH score, $\\bar{S_{DVH}}$ (mean absolute difference between ground truth and predicted dose-volume metrics).Furthermore, DVH metrics Mean[Gy] and D0.1cc [Gy] for OARs and D99%, D95%, D1% for PTVs were computed. DeepDoseNet with the MAE plus DVH-based loss function had the best dose score performance of the OpenKBP entries. MAE+DVH model had the lowest prediction error (P<0.0001, Wilcoxon test) on validation and test datasets (validation: $\\bar{S_{D}}$=2.3Gy, $\\bar{S_{DVH}}$=1.9Gy; test: $\\bar{S_{D}}$=2.0Gy, $\\bar{S_{DVH}}$=1.6Gy) followed by the MAE model (validation: $\\bar{S_{D}}$=3.6Gy, $\\bar{S_{DVH}}$=2.4Gy; test: $\\bar{S_{D}}$=3.5Gy, $\\bar{S_{DVH}}$=2.3Gy). The MSE model had the highest prediction error (validation: $\\bar{S_{D}}$=3.7Gy, $\\bar{S_{DVH}}$=3.2Gy; test: $\\bar{S_{D}}$=3.6Gy, $\\bar{S_{DVH}}$=3.0Gy). No significant difference was found among models in terms of Mean [Gy], but the MAE+DVH model significantly outperformed the MAE and MSE models in terms of D0.1cc[Gy], particularly for mandible and parotids on both validation (P<0.01) and test (P<0.0001) datasets.\n",
      "\n",
      "MAE+DVH outperformed (P<0.0001) in terms of D99%, D95%, D1% for targets.\n",
      "\n",
      "MAE+DVH reduced $\\bar{S_{D}}$ by ~60% and $\\bar{S_{DVH}}$ by ~70%. 0 into PostgreSQL...\n",
      "Inserting test sample 1602  Deep learning technology has emerged as a successful tool for predicting outcomes in various domains, including healthcare. Our research introduces the DeepDoseNet model, which has been specifically created for 3D dose prediction in radiation therapy. This is an important and challenging task in medical physics, which has the potential to benefit from the use of deep learning.\n",
      "\n",
      "The DeepDoseNet architecture consists of a 3D convolutional neural network (CNN), which enables the model to automatically extract informative features from the input data. The model has been trained on a large dataset of simulation studies that mimic the actual treatment delivery process, and its performance has been evaluated on several datasets from clinical trials.\n",
      "\n",
      "Our experiments show that DeepDoseNet has performed very well in 3D dose prediction, achieving promising results in terms of accuracy, precision, and recall. The model has outperformed existing methods such as linear regression and decision tree-based models. Furthermore, we have analyzed the model's behavior and discovered that it has learned to identify important features related to the dose distribution, such as the shape and intensity of the tumor.\n",
      "\n",
      "Overall, DeepDoseNet is a significant contribution to the field of medical physics, as it introduces a highly effective model for 3D dose prediction in radiation therapy. The model's ability to learn important features from the input data provides it with a unique advantage over the existing methods. This could lead to better treatment planning, which is crucial for improving patient outcomes in radiation therapy. In the future, we plan to investigate the potential of DeepDoseNet on real patient data and compare it to the currently-used models. 1 into PostgreSQL...\n",
      "Inserting test sample 1603  The evolution of low- and intermediate-mass stars on the asymptotic giant branch (AGB) is mainly controlled by the rate at which these stars lose mass in a stellar wind. Understanding the driving mechanism and strength of the stellar winds of AGB stars and the processes enriching their surfaces with products of nucleosynthesis are paramount to constraining AGB evolution and predicting the chemical evolution of galaxies. In a previous paper we have constrained the structure of the outflowing envelope of W Hya using spectral lines of the $^{12}$CO molecule. Here we broaden this study by modelling an extensive set of H$_{2}$O and $^{28}$SiO lines observed by the three instruments on board Herschel using a state-of-the-art molecular excitation and radiative transfer code. The oxygen isotopic ratios and the $^{28}$SiO abundance profile can be connected to the initial stellar mass and to crucial aspects of dust formation at the base of the stellar wind, respectively. The modelling of H$_{2}$O and $^{28}$SiO confirms the properties of the envelope model of W Hya derived from $^{12}$CO lines. We find an H$_2$O ortho-to-para ratio of 2.5\\,$^{+2.5}_{-1.0}$, consistent with what is expected for an AGB wind. The O$^{16}$/O$^{17}$ ratio indicates that W Hya has an initial mass of about 1.5 M$_\\odot$. Although the ortho- and para-H$_{2}$O lines observed by HIFI appear to trace gas of slightly different physical properties, a turbulence velocity of $0.7\\pm0.1$ km s$^{-1}$ fits the HIFI lines of both spin isomers and those of $^{28}$SiO well. The ortho- and para-H$_2^{16}$O and $^{28}$SiO abundances relative to H$_{2}$ are $(6^{+3}_{-2}) \\times 10^{-4}$, $(3^{+2}_{-1}) \\times 10^{-4}$, and $(3.3\\pm 0.8)\\times 10^{-5}$, respectively. Assuming a solar silicon-to-carbon ratio, the $^{28}$SiO line emission model is consistent with about one-third of the silicon atoms being locked up in dust particles. 0 into PostgreSQL...\n",
      "Inserting test sample 1604  This research paper presents an analysis of the molecular envelope around the star W Hya, based on observations made by the Herschel Space Observatory. W Hya is a solitary asymptotic giant branch star, located about 375 light years away from the Earth. AGB stars are known to be mass-losing objects, where the mass loss occurs by a slow, wind-like outflow of gas and dust from the star's envelope.\n",
      "\n",
      "The Herschel observations presented here, which include data on spectral line emissions from molecules like H2O, CO, and SiO, provide a new perspective on the structure and dynamics of W Hya's molecular envelope. Our results suggest that the overall shape of the envelope is consistent with the predictions of theoretical models, indicating that mass loss from W Hya is dominated by a slow, wide-angle wind.\n",
      "\n",
      "However, we also note the presence of several unusual features in the Herschel data. For instance, we observed strong H2O emission from a narrow shell located at a distance of approximately 10 arcseconds from the star. This shell appears to be a result of a recent mass loss event, and its origin is currently under investigation.\n",
      "\n",
      "Our observations also suggest the presence of complex spatial and kinematic structures in the molecular envelope around W Hya. For instance, we observed asymmetries in the distribution of molecular emission, which may be related to ongoing instabilities in the stellar wind or the presence of a binary companion. Additionally, we observed a pronounced velocity gradient in the CO emission, which may be suggestive of rotation or infall motions in the envelope.\n",
      "\n",
      "Overall, our results provide new insights into the structure and dynamics of W Hya's molecular envelope, and highlight the power of Herschel observations for studying the mass loss process in AGB stars. Further observations with new telescopes, such as the Atacama Large Millimeter/submillimeter Array (ALMA), will be crucial for unraveling the mysteries of this complex system. 1 into PostgreSQL...\n",
      "Inserting test sample 1605  Stellar-mass binary black holes (BBHs) may merge in the vicinity of a supermassive black hole (SMBH). It is suggested that the gravitational-wave (GW) emitted by a BBH has a high probability to be lensed by the SMBH if the BBH's orbit around the SMBH (i.e., the outer orbit) has a period of less than a year and is less than the duration of observation of the BBH by a space-borne GW observatory. For such a BBH + SMBH triple system, the de Sitter precession of the BBH's orbital plane is also significant. In this work, we thus study GW waveforms emitted by the BBH and then modulated by the SMBH due to effects including Doppler shift, de Sitter precession, and gravitational lensing. We show specifically that for an outer orbital period of 0.1 yr and an SMBH mass of $10^7 M_\\odot$, there is a 3\\%-10\\% chance for the standard, strong lensing signatures to be detectable by space-borne GW detectors such as LISA and/or TianGO. For more massive lenses ($\\gtrsim 10^8 M_\\odot$) and more compact outer orbits with periods <0.1 yr, retro-lensing of the SMBH might also have a 1%-level chance of detection. Furthermore, by combining the lensing effects and the dynamics of the outer orbit, we find the mass of the central SMBH can be accurately determined with a fraction error of $\\sim 10^{-4}$. This is much better than the case of static lensing because the degeneracy between the lens' mass and the source's angular position is lifted by the outer orbital motion.\n",
      "\n",
      "Including lensing effects also allows the de Sitter precession to be detectable at a precession period 3 times longer than the case without lensing. Lastly, we demonstrate that one can check the consistency between the SMBH's mass determined from the orbital dynamics and the one inferred from gravitational lensing, which serves as a test on theories behind both phenomena. The statistical error on the deviation can be constrained to a 1% level. 0 into PostgreSQL...\n",
      "Inserting test sample 1606  One of the most fascinating phenomena in the universe is gravitational lensing, which is caused by the bending of light due to the gravitational pull of massive objects. In recent years, the study of hierarchical triples in galactic nuclei has emerged as a promising way to detect this effect and glean insights into the composition and behavior of massive objects in the universe. While ground-based gravitational wave observatories have proved useful in measuring the effects of lensing, it is now widely acknowledged that space-borne observatories offer even greater potential for detecting this phenomenon.\n",
      "\n",
      "This paper explores the use of space-borne gravitational-wave observatories to detect gravitational lensing in hierarchical triples in galactic nuclei. The research builds on previous work in the field, but introduces some new and exciting developments. For example, by focusing on hierarchical triples, the study offers a more nuanced understanding of the behavior of massive objects and the interactions between them. Additionally, it explores the use of space-borne observatories, which offer the potential for greater precision and sensitivity in detecting gravitational lensing.\n",
      "\n",
      "The study employs a range of advanced techniques and tools, including simulations of gravitational waves and triple systems, as well as statistical analysis and machine learning algorithms. These methods are used to test the sensitivity of space-borne observatories to gravitational lensing in hierarchical triples and to identify the most promising signals for future exploration. The results of the study suggest that space-borne observatories could significantly enhance our understanding of the origin, distribution, and behavior of massive objects in galactic nuclei, as well as the fundamental nature of gravitational waves.\n",
      "\n",
      "Overall, this study presents a compelling case for the use of space-borne gravitational-wave observatories to detect gravitational lensing in hierarchical triples in galactic nuclei. The research offers important new insights into the composition and behavior of massive objects, and provides a foundation for future exploration and discovery in this exciting area of study. 1 into PostgreSQL...\n",
      "Inserting test sample 1607  Ensemble learning is a statistical paradigm built on the premise that many weak learners can perform exceptionally well when deployed collectively. The BART method of Chipman et al. (2010) is a prominent example of Bayesian ensemble learning, where each learner is a tree. Due to its impressive performance, BART has received a lot of attention from practitioners. Despite its wide popularity, however, theoretical studies of BART have begun emerging only very recently. Laying the foundations for the theoretical analysis of Bayesian forests, Rockova and van der Pas (2017) showed optimal posterior concentration under conditionally uniform tree priors. These priors deviate from the actual priors implemented in BART. Here, we study the exact BART prior and propose a simple modification so that it also enjoys optimality properties.\n",
      "\n",
      "To this end, we dive into branching process theory. We obtain tail bounds for the distribution of total progeny under heterogeneous Galton-Watson (GW) processes exploiting their connection to random walks. We conclude with a result stating the optimal rate of posterior convergence for BART. 0 into PostgreSQL...\n",
      "Inserting test sample 1608  Bayesian Additive Regression Trees (BART) is a highly sophisticated statistical method employed in regression. In recent years, it has become increasingly popular in the machine learning field, owing to its versatility and ability to handle big data sets. This paper presents a detailed theoretical framework for BART, touching on its structural components, implementation, and limitations. The paper builds on existing literature on BART to provide a deeper understanding of the method. In addition, through a series of simulations, we highlight the performance of BART in different scenarios. The simulations involve data sets with varying sizes, distributions, and levels of noise. Our results reveal that BART is particularly robust in settings where the regression function is nonlinear and when the sample size is relatively small. The paper concludes that BART provides a flexible and robust framework for handling complex regression problems with high-dimensional predictor variables. 1 into PostgreSQL...\n",
      "Inserting test sample 1609  A novel exact dynamical real space renormalization group for a Langevin equation derivable from a Euclidean Gaussian action is presented. It is demonstrated rigorously that an algebraic temporal law holds for the Green function on arbitrary structures of infinite extent. In the case of fractals it is shown on specific examples that two different fixed points are found at variance with periodic structures. Connection with growth dynamics of interfaces is also discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 1610  We apply the real space renormalization group method to investigate Langevin dynamics when the system lacks translational invariance. Our findings indicate that the effective dynamics can be described by a fixed point stochastic process, which can be used to obtain the scaling behavior of the observables. Our approach provides a useful framework to analyze the dynamics of non-equilibrium systems in the presence of disorder. 1 into PostgreSQL...\n",
      "Inserting test sample 1611  In the ring of holomorphic functions at the origin of C^2, we consider the equation uf'_x+vf'_y=wf where f and w are given. We introduce intersection multiplicities relative to w and f'_y along the branches of f, and we study the solutions (u,v) using these valuations. As an application, we construct an explicit functional equation satisfied by f. 0 into PostgreSQL...\n",
      "Inserting test sample 1612  In this paper, we study the magic matrix associated with a germ of a plane curve and its division by the Jacobian ideal. Using algebraic geometry techniques, we investigate the properties of this matrix and its relation to the geometry of the curve. Our results provide insights into the structure of singular plane curves and their intersections. This work highlights the importance of algebraic geometric methods in understanding curve singularities. 1 into PostgreSQL...\n",
      "Inserting test sample 1613  We present SQLova, the first Natural-language-to-SQL (NL2SQL) model to achieve human performance in WikiSQL dataset. We revisit and discuss diverse popular methods in NL2SQL literature, take a full advantage of BERT {Devlin et al., 2018) through an effective table contextualization method, and coherently combine them, outperforming the previous state of the art by 8.2% and 2.5% in logical form and execution accuracy, respectively. We particularly note that BERT with a seq2seq decoder leads to a poor performance in the task, indicating the importance of a careful design when using such large pretrained models. We also provide a comprehensive analysis on the dataset and our model, which can be helpful for designing future NL2SQL datsets and models. We especially show that our model's performance is near the upper bound in WikiSQL, where we observe that a large portion of the evaluation errors are due to wrong annotations, and our model is already exceeding human performance by 1.3% in execution accuracy. 0 into PostgreSQL...\n",
      "Inserting test sample 1614  In this paper, we present a comprehensive exploration of the WikiSQL dataset, a large-scale, human-annotated collection of tables and corresponding natural language queries. Specifically, we focus on the task of table-aware word contextualization, which involves mapping a given word in a query to its appropriate column in the corresponding table. To accomplish this, we propose a novel approach that leverages a combination of deep learning models, including a table encoder and a word context encoder. We conducted extensive experiments on the WikiSQL dataset and achieved state-of-the-art results, significantly outperforming previous approaches. Additionally, we conducted a thorough analysis of our approach, providing insight into how different components of our model contribute to performance. Our experiments demonstrate that our approach is highly effective at resolving table-aware word contextualization, and we believe it will have significant implications for the development of more robust and accurate natural language interfaces for databases. 1 into PostgreSQL...\n",
      "Inserting test sample 1615  We systematically study the properties of pure nucleonic and hyperonic magnetic stars using a density-dependent relativistic mean field (DD-RMF) equations of state. We explore several parameter sets and hyperon coupling schemes within the DD-RMF formalism. We focus on sets that are in better agreement with nuclear and other astrophysical data, while generating heavy neutron stars. Magnetic field effects are included in the matter equation of state and in general relativity solutions, which in addition fulfill Maxwell's equations. We find that pure nucleonic matter, even without magnetic field effects, generates neutron stars that satisfy the potential GW190814 mass constraint; however, this is not the case for hyperonic matter, which instead only satisfies the more conservative 2.1 M$_{\\odot}$ constraint. In the presence of strong but still somehow realistic internal magnetic fields $\\approx10^{17}$ G, the stellar charged particle population re-leptonizes and de-hyperonizes. As a consequence, magnetic fields stiffen hyperonic equations of state and generate more massive neutron stars, which can satisfy the possible GW190814 mass constraint but present a large deformation with respect to spherical symmetry. 0 into PostgreSQL...\n",
      "Inserting test sample 1616  Neutron stars are fascinating celestial objects resulting from the gravitational collapse of massive stars. Heavy magnetic neutron stars, in particular, constitute a unique subgroup of these astrophysical entities, distinguished by their very strong magnetic fields ranging from 10^12 to 10^16 Gauss. The magnetic field influences the dynamics of the star and its surroundings, playing a pivotal role in astrophysical phenomena such as gamma-ray bursts and pulsar timing anomalies. Thanks to advances in observational techniques and theoretical models, our understanding of heavy magnetic neutron stars has greatly improved. This review paper aims to present the current state of knowledge on the properties and physics of heavy magnetic neutron stars by discussing topics such as magnetar formation, their structure, dynamics, and potential future research directions. It outlines the importance of these objects in astrophysical contexts and their implications for the study of neutron star populations, compact object mergers, and the evolution of magnetic fields in the Universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1617  We prove a structure theorem for non-isomorphic endomorphisms of weak Q-Fano threefolds, or more generally for threefolds with big anti-canonical divisor.\n",
      "\n",
      "Also provided is a criterion for a fibred rationally connected threefold to be rational. As a consequence, we show (without using the classification) that every smooth Fano threefold having a non-isomorphic surjective endomorphism is rational. 0 into PostgreSQL...\n",
      "Inserting test sample 1618  We investigate the rationality of rationally connected threefolds possessing non-isomorphic endomorphisms. Specifically, we study the failure of the LÃ¼roth problem for such varieties, and provide examples to support our findings. Our results show the significance of studying the interplay between algebraic geometry and arithmetic dynamics in providing insight into the rationality of higher-dimensional varieties. 1 into PostgreSQL...\n",
      "Inserting test sample 1619  We discuss the properties of the X-ray sources with faint optical counterparts in the very young open cluster NGC 6231. From their positions in the H-R diagram, we find that the bulk of these objects probably consists of low-mass pre-main sequence stars with masses in the range 0.3 to 3.0 M$_{\\odot}$. The age distribution of these objects indicates that low-mass star formation in NGC 6231 started more than 10 Myr ago and culminated in a starburst-like event about 1 to 4 Myr ago when the bulk of the low-mass PMS stars as well as the massive cluster members formed. We find no evidence for a spatial age gradient that could point towards a sequential star formation process. Only a few X-ray sources have counterparts with a reddening exceeding the average value of the cluster or with infrared colours indicating the presence of a moderate near-IR excess. The X-ray spectra of the brightest PMS sources are best fitted by rather hard thermal plasma models and a significant fraction of these sources display flares in their light curve. The X-ray brightest flaring sources have decay times between 2 and 16 ks. The X-ray selected PMS stars in NGC 6231 have $\\log{L_{\\rm X}/L_{\\rm bol}}$ values that increase strongly with decreasing bolometric luminosity and can reach a saturation level ($\\log{L_{\\rm X}/L_{\\rm bol}} \\sim -2.4$) for non-flaring sources and even more extreme values during flares. 0 into PostgreSQL...\n",
      "Inserting test sample 1620  The young open cluster NGC 6231 has been observed with the XMM-Newton telescope, revealing a number of optically faint X-ray sources. In this study, we present the analysis of the X-ray emission from the cluster and discuss the properties of the identified sources. We detected a total of 237 X-ray sources within the observed region, of which 153 have optical counterparts in the 2MASS catalog. Based on the X-ray luminosity and hardness ratios, we classified the sources into four categories: 27 foreground stars, 69 cluster members, 43 probable cluster members, and 14 extragalactic sources. We also identified three X-ray sources that show significant variability in their light curves and discuss their possible nature. Our results indicate that the X-ray emission in the cluster is dominated by young, low-mass stars, which are expected to be highly active and magnetically active. The presence of extragalactic sources suggests that the cluster is not completely isolated and may be part of a larger structure. Our study provides valuable insights into the formation and evolution of young star clusters, as well as the properties of X-ray emitting sources in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1621  Consider a sequence {X(i,0) : i = 1, ..., n} of i.i.d. random variables.\n",
      "\n",
      "Associate to each X(i,0) an independent mean-one Poisson clock. Every time a clock rings replace that X-variable by an independent copy. In this way, we obtain i.i.d. stationary processes {X(i,t) : t >= 0} (i=1,2, ...) whose invariant distribution is the law of X(1,0). Benjamini, Haggstrom, Peres, and Steif (2003) introduced the dynamical walk S(n,t) = X(1,t) + ... + X(n,t), and proved among other things that the LIL holds for {S(n,t) : n =1,2, ...} simultaneously for all t. In other words, the LIL is dynamically stable.\n",
      "\n",
      "Subsequently, we showed that in the case that the X(i,0)'s are standard normal, the classical integral test is not dynamically stable. Presently, we study the set of times t when {S(n,t) : n=1,2, ...} exceeds a given envelope infinitely often. Our analysis is made possible thanks to a connection to the Kolmogorov epsilon-entropy. When used in conjunction with the invariance principle of this paper, this connection has other interesting by-products some of which we relate. We prove also that viewed as an infinite-dimensional process, the rescaled dynamical random walk converges weakly in D(D([0,1])) to the Ornstein-Uhlenbeck process in C([0,1]). For this we assume only that the increments have mean zero and variance one. In addition, we extend a result of Benjamini, Haggstrom, Peres and Steif (2003) by proving that if the X(i,0)'s are lattice, mean-zero variance-one, and possess 2 + epsilon finite absolute moments for some positive epsilon, then the recurrence of the origin is dynamically stable. To prove this we derive a gambler's ruin estimate that is valid for all lattice random walks that have mean zero and finite variance. We believe the latter may be of independent interest. 0 into PostgreSQL...\n",
      "Inserting test sample 1622  In this paper, we investigate the invariance of dynamical random walks in exceptional times. Our analysis focuses on understanding how these processes behave in distinctly different environments when subjected to external forces that destabilize traditional walking patterns. We demonstrate the existence of new types of invariant walks in these exceptional situations and describe the conditions that lead to their emergence.\n",
      "\n",
      "Our research is built upon a comprehensive study of various dynamical random walks, including classic, anomalous, and fractional types, among others. Through analytical derivations, numerical simulations, and experimental measurements, we show that the behavior of these walks in exceptional times can be dramatically different from what is typically observed during normal diffusion. Specifically, we observe the formation of new regimes, such as \"superdiffusion\" and \"subdiffusion,\" and discover new statistical properties that are unique to these regimes.\n",
      "\n",
      "We further explore the practical implications of these findings by applying our analysis to real-world problems, such as predicting diffusion rates in crowded environments and enhancing the accuracy of computer vision systems that rely on random walks. Our results suggest that by accounting for exceptional situations, we can significantly improve the reliability of these applications and avoid costly errors that may result from assuming invariant behavior under all circumstances.\n",
      "\n",
      "Overall, our study sheds new light on the behavior of dynamical random walks in exceptional times and provides a more comprehensive understanding of the role of external forces in shaping these processes. We hope that our findings will inspire further research in this area and ultimately lead to the development of new models and techniques for analyzing complex systems in a broad range of disciplines. 1 into PostgreSQL...\n",
      "Inserting test sample 1623  We investigate strong-coupling properties of a two-dimensional ultracold Fermi gas in the normal state. Including pairing fluctuations within the framework of a $T$-matrix approximation, we calculate the distribution function $n({\\boldsymbol Q})$ of Cooper pairs in terms of the center of mass momentum ${\\boldsymbol Q}$. In the strong-coupling regime, $n({\\boldsymbol Q}=0)$ is shown to exhibit a remarkable increase with decreasing the temperature in the low temperature region, which agrees well with the recent experiment on a two-dimensional $^6$Li Fermi gas [M. G. Ries, {\\it et. al.}, Phys. Rev. Lett.\n",
      "\n",
      "{\\bf 114}, 230401 (2015)]. Our result indicates that the observed remarkable increase of the number of Cooper pairs with zero center of mass momentum can be explained without assuming the Berezinskii-Kosterlitz-Thouless (BKT) transition, when one properly includes pairing fluctuations that are enhanced by the low-dimensionality of the system. Since the BKT transition is a crucial topic in two-dimensional Fermi systems, our results would be useful for the study toward the realization of this quasi-long-range order in an ultracold Fermi gas. 0 into PostgreSQL...\n",
      "Inserting test sample 1624  In this paper, we investigate the momentum distribution of Cooper-pairs and strong-coupling effects in a two-dimensional (2D) Fermi gas near the Berezinskii-Kosterlitz-Thouless (BKT) transition. We employ a theoretical framework based on the time-dependent Ginzburg-Landau (TDGL) theory coupled with the Bogoliubov-de Gennes approach to account for the non-equilibrium dynamics of the system. Our simulations reveal that the momentum distribution of the Cooper-pairs exhibits a characteristic signature of the BKT transition, which manifests as a power-law tail in the high-momentum regime. Moreover, we find that the strength of the coupling between the Cooper-pairs and the Fermi sea has a significant impact on the system's properties, such as the coherence length and the spectral function. Our results provide insight into the behavior of 2D Fermi gases near the BKT transition and may offer guidance for the design and optimization of future experiments. 1 into PostgreSQL...\n",
      "Inserting test sample 1625  We analyze analytically the asymptotic regions of the quasinormal mode frequency spectra with infinitely large overtone numbers for $D$-dimensional Schwarzschild black holes in anti de Sitter spacetimes. In this limit, we confirm the analytic results obtained previously in the literature using different methods. In addition, we show that in certain spacetime dimensions these techniques imply the existence of other regions of the asymptotic quasinormal mode frequency spectrum which have not previously appeared in the literature. For large black holes, some of these modes have a damping rate of $1.2T_H$, where $T_H$ is the Hawking temperature. This is less than the damping rate of the lowest overtone quasinormal mode calculated by other authors. It is not completely clear whether these modes actually exist or are an artifact of an unknown flaw in the analytic techniques being used. We discuss the possibility of the existence of these modes and explore some of the consequences. We also examine the possible connection between the asymptotic quasinormal modes of Schwarzschild-anti de Sitter black holes and the quantum level spacing of their horizon area spectrum. 0 into PostgreSQL...\n",
      "Inserting test sample 1626  In this paper, we present a comprehensive analysis of the asymptotic quasinormal modes (QNMs) of Schwarzschild-Anti De Sitter (SAdS) black holes. Utilizing analytical techniques alongside numerical computations, we explore the crucial features of the QNMs that originate from the nontrivial properties of the SAdS background. Our approach is based on the study of the scalar perturbations of these black holes, and we investigate the role of both the asymptotic and near-horizon limits in the determination of the QNMs. Our analysis reveals the existence of a discrete set of complex frequencies that describe the characteristic decay modes of perturbations that are imposed on SAdS black holes. Furthermore, we discuss the significance of the imaginary part of the QNMs to the instability of these black holes when their size becomes smaller than a critical value. The results of this study may provide valuable insights into the behavior of higher-dimensional black holes and their relation to the AdS/CFT correspondence in theoretical physics. 1 into PostgreSQL...\n",
      "Inserting test sample 1627  We report a quantum phase transition between orbital-selective Mott states, with different localized orbitals, in a Hund's metals model. Using the density matrix renormalization group, the phase diagram is constructed varying the electronic density and Hubbard $U$, at robust Hund's coupling. We demonstrate that this transition is preempted by charge fluctuations and the emergence of free spinless fermions, as opposed to the magnetically-driven Mott transition.\n",
      "\n",
      "The Luttinger correlation exponent is shown to have a universal value in the strong-coupling phase, whereas it is interaction dependent at intermediate couplings. At weak coupling we find a second transition from a normal metal to the intermediate-coupling phase. 0 into PostgreSQL...\n",
      "Inserting test sample 1628  The phenomenon of orbital-selective Mott states has been observed in various iron-based materials, where electron interactions lead to strong correlations in certain orbital degrees of freedom. In this work, we investigate the occurrence of a quantum phase transition between such states in Hund's metals, which are characterized by the presence of strong Hund's coupling. Using dynamical mean-field theory calculations, we show that the Hund's coupling plays a crucial role in determining the nature of the transition, moving it from being continuous to first-order, and modifying the critical exponents. Our findings deepen our understanding of the interplay between electronic correlations and Hund's coupling in this class of materials. 1 into PostgreSQL...\n",
      "Inserting test sample 1629  We study the prospects for three-dimensional mapping of the dark matter to high redshift through the shearing of faint galaxies images at multiple distances by gravitational lensing. Such maps could provide invaluable information on the nature of the dark energy and dark matter. While in principle well-posed, mapping by direct inversion introduces exceedingly large, but usefully correlated noise into the reconstruction. By carefully propagating the noise covariance, we show that lensing contains substantial information, both direct and statistical, on the large-scale radial evolution of the density field. This information can be efficiently distilled into low-order signal-to-noise eigenmodes which may be used to compress the data by over an order of magnitude. Such compression will be useful for the statistical analysis of future large data sets. The reconstructed map also contains useful information on the localization of individual massive dark matter halos, and hence the dark energy from halo number counts, but its extraction depends strongly on prior assumptions. We outline a procedure for maximum entropy and point-source regularization of the maps that can identify alternate reconstructions. 0 into PostgreSQL...\n",
      "Inserting test sample 1630  The universe is filled with an invisible substance called dark matter, which scientists believe makes up roughly 85% of all matter in the universe. Despite being elusive, dark matter has many observable gravitational effects on visible matter, such as galaxies and stars. Mapping the distribution of dark matter in 3D is crucial for testing current theories of the universe's formation and evolution. In this study, we present a new algorithm that uses weak gravitational lensing measurements to create a 3D map of the large-scale distribution of dark matter. Our algorithm can produce maps with unprecedented resolution and precision, allowing us to study the small-scale features of this invisible matter for the first time. Furthermore, the method can be applied to black hole formation and galactic evolution studies, helping to unravel the mysteries of the universe's dark side. The results presented in this paper are an important step towards a deeper understanding of the universe and its complex structures. 1 into PostgreSQL...\n",
      "Inserting test sample 1631  Hyperparameter tuning is an omnipresent problem in machine learning as it is an integral aspect of obtaining the state-of-the-art performance for any model.\n",
      "\n",
      "Most often, hyperparameters are optimized just by training a model on a grid of possible hyperparameter values and taking the one that performs best on a validation sample (grid search). More recently, methods have been introduced that build a so-called surrogate model that predicts the validation loss for a specific hyperparameter setting, model and dataset and then sequentially select the next hyperparameter to test, based on a heuristic function of the expected value and the uncertainty of the surrogate model called acquisition function (sequential model-based Bayesian optimization, SMBO).\n",
      "\n",
      "In this paper we model the hyperparameter optimization problem as a sequential decision problem, which hyperparameter to test next, and address it with reinforcement learning. This way our model does not have to rely on a heuristic acquisition function like SMBO, but can learn which hyperparameters to test next based on the subsequent reduction in validation loss they will eventually lead to, either because they yield good models themselves or because they allow the hyperparameter selection policy to build a better surrogate model that is able to choose better hyperparameters later on. Experiments on a large battery of 50 data sets demonstrate that our method outperforms the state-of-the-art approaches for hyperparameter learning. 0 into PostgreSQL...\n",
      "Inserting test sample 1632  This article addresses a current challenge faced by researchers and practitioners in the field of machine learning: tuning the hyperparameters. It introduces a novel approach to hyperparameter optimization using reinforcement learning, called Hyp-RL. The approach relies on a meta-learning framework that learns an efficient search policy for the hyperparameter space. Hyp-RL is based on a deep Q-network architecture that learns to output optimized hyperparameter configurations. The approach is shown to be effective on a variety of task domains, including image classification and speech recognition, where it outperforms baseline methods such as grid and random search. Furthermore, this paper discusses an analysis of sample efficiency, comparing Hyp-RL with state-of-the-art methods. The results demonstrate that Hyp-RL achieves higher sample efficiency, meaning it requires less computational resources to identify optimal hyperparameters. Hyp-RL represents an innovative and efficient approach to hyperparameter optimization by combining recent advances in reinforcement learning and meta-learning. An important benefit of this approach is that it is applicable to various types of models and tasks. To facilitate future research, the code and experimental results of Hyp-RL are made available to the community. 1 into PostgreSQL...\n",
      "Inserting test sample 1633  Using recent results on BPS quiver theory, we develop a group theoretical method to describe the quiver mutations encoding the quantum mechanical duality relating the spectra of distinct quivers. We illustrate the method by computing the BPS spectrum of the infinite weak chamber of some examples of N=2 supersymmetric gauge models without and with quark hypermultiplets. 0 into PostgreSQL...\n",
      "Inserting test sample 1634  We investigate weak coupling chambers in N=2 BPS quiver theory. We focus on the cases where the gauge algebra is a product of U(1) factors, and examine their dependence on electric and magnetic charges. Our results reveal a rich structure of quantum vacua, and provide a platform for exploring interesting geometrical properties of the underlying gauge theories. 1 into PostgreSQL...\n",
      "Inserting test sample 1635  The liquid-liquid phase transition in high-pressure Hydrogen is a problem of longstanding and controversy. The recent Nature paper by Cheng et al. [vol.\n",
      "\n",
      "585, p. 217] makes a set of strong claims to the effect that all the previous density functional theory molecular dynamics (MD-DFT) and quantum Monte Carlo calculations of that transition are incorrect because of finite size effects and, in the MD-DFT case, short run times. The basis of those claims is their use of large systems and long durations for classical MD driven by a machine-learnt potential (MLP) which they developed. The straightforward test of their claims is to do MD-DFT on systems as large or larger than Cheng et al.\n",
      "\n",
      "used and for significantly longer durations than in the previous MD-DFT simulations. We have done so and find that neither diagnosis of theirs (size effects, duration limits) is correct. Instead, we find that the MLP does not drive MD in fidelity with the underlying DFT electronic structure that it is supposed to replicate. The result is that the MLP-driven MD results are artifactual, not systematically connected to the theoretical underpinning on which the MLP was trained. 0 into PostgreSQL...\n",
      "Inserting test sample 1636  In this study, we investigate the nature of the liquid-liquid transition (LLT) in dense hydrogen using ab initio molecular dynamics simulations. Our results reveal that the transition is subcritical in nature, meaning it lacks a distinct discontinuity in the thermodynamic properties. Instead, the transition occurs gradually as the system evolves towards a state with different characteristics in terms of its electronic and vibrational properties, as well as its density. We find that this subcritical behavior can be attributed to the presence of quantum fluctuations, which lead to the formation of local structures that gradually grow in size and eventually span the entire system. Our results also suggest that the LLT in hydrogen may be associated with several other phenomena observed in this system, including the anomalous melting behavior and the appearance of superconductivity at high pressures. Overall, our findings shed light on the nature of the LLT in dense hydrogen and highlight the importance of considering quantum fluctuations in the description of complex many-body systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1637  In situations where it is difficult to enroll patients in randomized controlled trials, external data can improve efficiency and feasibility. In such cases, adaptive trial designs could be used to decrease enrollment in the control arm of the trial by updating the randomization ratio at the interim analysis. Updating the randomization ratio requires an estimate of the amount of information effectively borrowed from external data, which is typically done with a linear approximation. However, this linear approximation is not always a reliable estimate, which could potentially lead to sub-optimal randomization ratio updates. In this note, we highlight this issue through simulations for exponential time-to-event outcomes, because in this simple setting there is an exact solution available for comparison. We also propose a potential generalization that could complement the linear approximation in more complex settings, discuss challenges for this generalization, and recommend best practices for computing and interpreting estimates of the effective number of events borrowed. 0 into PostgreSQL...\n",
      "Inserting test sample 1638  Hybrid controlled trials are becoming increasingly popular in clinical research. They combine the strengths of randomized controlled trials with the benefits of observational studies. This study focuses on one particular aspect of these trials - the amount of external data used to inform the trial design. Specifically, we examine the use of external data in hybrid controlled trials with time-to-event outcomes. We develop a novel approach to measure the amount of information borrowed from external data sources, and investigate its impact on the overall trial results. Our results show that the amount of external information used can have a significant impact on trial outcomes, and that careful consideration should be given to the selection and use of external data sources in the design of hybrid controlled trials. This study provides guidance for researchers and clinicians wishing to use external data sources in their trials, and highlights the importance of transparency in reporting the use of external data. 1 into PostgreSQL...\n",
      "Inserting test sample 1639  A system level view of cellular processes for human and several organisms can be cap- tured by analyzing molecular interaction networks. A molecular interaction network formed of differentially expressed genes and their interactions helps to understand key players behind disease development. So, if the functions of these genes are blocked by altering their interactions, it would have a great impact in controlling the disease. Due to this promising consequence, the problem of inferring disease causing genes and their pathways has attained a crucial position in computational biology research. However, considering the huge size of interaction networks, executing computations can be costly. Review of literatures shows that the methods proposed for finding the set of disease causing genes could be assessed in terms of their accuracy which a perfect algorithm would find. Along with accuracy, the time complexity of the method is also important, as high time complexities would limit the number of pathways that could be found within a pragmatic time interval. 0 into PostgreSQL...\n",
      "Inserting test sample 1640  The identification of disease-causing genes is a crucial problem in modern healthcare that can affect millions of people. In this paper, we present a mathematical approach to infer disease-causing genes and their associated pathways. By analyzing large-scale genomics and interactome data, we identify gene-disease associations and develop a novel pathway enrichment analysis method to discover enriched pathways. Furthermore, we propose a new network propagation algorithm to prioritize candidate disease genes and rank their importance. Our results demonstrate the effectiveness of our approach in identifying genetic factors that contribute to diseases, such as cancer and neurodegenerative diseases. In addition, we compare our approach with existing methods and show its superior performance. Our mathematical perspective provides a valuable tool for identifying disease-causing genes and pathways, which can help to develop new drugs and therapies for patients affected by these diseases. 1 into PostgreSQL...\n",
      "Inserting test sample 1641  In the present paper we study description of Kadison-Schwarz type quantum quadratic operators acting from $\\bm_2(\\mathbb{C})$ into $\\bm_2(\\mathbb{C})\\o\\bm_2(\\mathbb{C})$. Note that such kind of operator is a generalization of quantum convolution. By means of such a description we provide an example of q.q.o. which is not a Kadision-Schwartz operator.\n",
      "\n",
      "Moreover, we study dynamics of an associated nonlinear (i.e. quadratic) operators acting on the state space of $\\bm_2(\\mathbb{C})$. 0 into PostgreSQL...\n",
      "Inserting test sample 1642  This paper examines the properties of Kadison-Schwarz type quantum quadratic operators on the matrix algebra $\\bm_2(\\mathbb{C})$. Specifically, we investigate their spectral properties and provide examples of these operators. We also analyze their relationship with other types of quantum quadratic operators. Our findings suggest that Kadison-Schwarz type quantum quadratic operators possess unique spectral signatures that distinguish them from other types of operators. Furthermore, our results have potential applications in quantum mechanics and other related fields. 1 into PostgreSQL...\n",
      "Inserting test sample 1643  We investigate the influence of low-dimensionality and disorder in phonon transport in ultra-narrow armchair graphene nanoribbons (GNRs) using non-equilibrium Greens function (NEGF) simulation techniques. We specifically focus on how different parts of the phonon spectrum are influenced by geometrical confinement and line edge roughness. With the introduction of line edge roughness, the phonon transmission is reduced, but non-uniformly throughout the spectrum. We identify four distinct behaviors within the phonon spectrum in the presence of disorder: i) the low-energy, low-wavevector acoustic branches have very long mean-free-paths and are affected the least by edge disorder, even in the case of ultra-narrow W=1nm wide GNRs; ii) energy regions that consist of a dense population of relatively flat phonon modes (including the optical branches) are also not significantly affected, except in the case of the ultranarrow W=1nm GNRs, in which case the transmission is reduced because of band mismatch along the phonon transport path; iii) quasi-acoustic bands that lie within the intermediate region of the spectrum are strongly affected by disorder as this part of the spectrum is depleted of propagating phonon modes upon both confinement and disorder especially as the channel length increases; iv) the strongest reduction in phonon transmission is observed in energy regions that are composed of a small density of phonon modes, in which case roughness can introduce transport gaps that greatly increase with channel length. We show that in GNRs of widths as small as W=3nm, under moderate roughness, both the low-energy acoustic modes and dense regions of optical modes can retain semi-ballistic transport properties, even for channel lengths up to L=1 um. Modes in the sparse regions of the spectrum fall into the localization regime even for channel lengths as short as 10s of nanometers. 0 into PostgreSQL...\n",
      "Inserting test sample 1644  This paper investigates the influence of phonon transport in ultra-narrow graphene nanoribbons (GNRs) characterized by disorder-induced perturbations. Our analysis reveals that ultra-narrow GNRs exhibit remarkably different phonon transport behaviors compared to their wider counterparts. Specifically, we demonstrate that, due to the confinement of phonons in the narrow dimension, scattering mechanism in the disorder limit GNRs results in enhanced inelastic phonon-phonon coupling. Our simulation also illustrates that phonon wave-packet diffusion in such systems follows multiple diffusive regimes, characterized by phonon frequency and ribbon width, and that the transition between these regimes can be described by a power-law index. \n",
      "\n",
      "The low dimensionality of disordered ultra-narrow GNRs strongly affects their phonon transport properties. By examining the dynamic structure factor, as a fundamental quantity that describes the scattered intensity of a phonon wave-packet from a disordered medium, we quantitatively show that disorder broadens the phonon lifetime distribution, and thus it reduces the coherence length of the phonon wave-packet. Based on these findings, we also demonstrate that our proposed phonon transport model can accurately predict the thermal conductivity of ultra-narrow GNRs in the presence of disorder. \n",
      "\n",
      "Our research not only provides insights into the fundamental physics of phonon transport in low-dimensional disordered materials, but also advances the understanding of non-equilibrium heat transfer mechanisms in graphene-based nanostructures. Our results are critical for designing efficient thermoelectric materials and providing new perspectives on the role of disorder in low-dimensional heat transport applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1645  Deep learning architectures with a huge number of parameters are often compressed using pruning techniques to ensure computational efficiency of inference during deployment. Despite multitude of empirical advances, there is a lack of theoretical understanding of the effectiveness of different pruning methods. We inspect different pruning techniques under the statistical mechanics formulation of a teacher-student framework and derive their generalization error (GE) bounds. It has been shown that Determinantal Point Process (DPP) based node pruning method is notably superior to competing approaches when tested on real datasets. Using GE bounds in the aforementioned setup we provide theoretical guarantees for their empirical observations.\n",
      "\n",
      "Another consistent finding in literature is that sparse neural networks (edge pruned) generalize better than dense neural networks (node pruned) for a fixed number of parameters. We use our theoretical setup to prove this finding and show that even the baseline random edge pruning method performs better than the DPP node pruning method. We also validate this empirically on real datasets. 0 into PostgreSQL...\n",
      "Inserting test sample 1646  Neural network pruning has been widely used as a technique for reducing the computational complexity of deep learning models. However, the theoretical understanding of pruning is still limited. In this paper, we present a statistical mechanical analysis of neural network pruning. We investigate the impact of pruning on network weights and activations. Our analysis reveals that the tendency of weights to become smaller during pruning can be explained by the principle of minimum description length. We also introduce a new metric for measuring the degree of pruning in a network based on the mean activation of neurons. We validate our analysis through experiments on several benchmark datasets, where we demonstrate that our proposed metric correlates well with the generalization performance of pruned networks. Our findings have implications for further expanding the theoretical understanding of neural network pruning and its practical applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1647  If M is a hyperbolic 3-manifold with a quasigeodesic flow then we show that \\pi_1(M) acts in a natural way on a closed disc by homeomorphisms.\n",
      "\n",
      "Consequently, such a flow either has a closed orbit or the action on the boundary circle is M\\\"obius-like but not conjugate into PSL(2, R). We conjecture that the latter possibility cannot occur. 0 into PostgreSQL...\n",
      "Inserting test sample 1648  Our paper studies quasigeodesic flows and M\\\"obius-like groups in the context of geometric network structures. We investigate how these flows can be used to model the trajectories of particles moving through a network. Furthermore, we examine the properties of M\\\"obius-like groups, which are important for understanding the dynamics of quasigeodesic flows. Our findings contribute to the development of new network analysis techniques and enhance the understanding of geometric objects relevant to physics and topology. 1 into PostgreSQL...\n",
      "Inserting test sample 1649  We study the problem of aligning multiple sequences with the goal of finding an alignment that either maximizes the number of aligned symbols (the longest common subsequence (LCS)), or minimizes the number of unaligned symbols (the alignment distance (AD)). Multiple sequence alignment is a well-studied problem in bioinformatics and is used to identify regions of similarity among DNA, RNA, or protein sequences to detect functional, structural, or evolutionary relationships among them. It is known that exact computation of LCS or AD of $m$ sequences each of length $n$ requires $\\Theta(n^m)$ time unless the Strong Exponential Time Hypothesis is false. In this paper, we provide several results to approximate LCS and AD of multiple sequences.\n",
      "\n",
      "If the LCS of $m$ sequences each of length $n$ is $\\lambda n$ for some $\\lambda \\in [0,1]$, then in $\\tilde{O}_m(n^{\\lfloor\\frac{m}{2}\\rfloor+1})$ time, we can return a common subsequence of length at least $\\frac{\\lambda^2 n}{2+\\epsilon}$ for any arbitrary constant $\\epsilon >0$.\n",
      "\n",
      "It is possible to approximate the AD within a factor of two in time $\\tilde{O}_m(n^{\\lceil\\frac{m}{2}\\rceil})$. However, going below-2 approximation requires breaking the triangle inequality barrier which is a major challenge in this area. No such algorithm with a running time of $O(n^{\\alpha m})$ for any $\\alpha < 1$ is known. If the AD is $\\theta n$, then we design an algorithm that approximates the AD within an approximation factor of $\\left(2-\\frac{3\\theta}{16}+\\epsilon\\right)$ in $\\tilde{O}_m(n^{\\lfloor\\frac{m}{2}\\rfloor+2})$ time. Thus, if $\\theta$ is a constant, we get a below-two approximation in $\\tilde{O}_m(n^{\\lfloor\\frac{m}{2}\\rfloor+2})$ time. Moreover, we show if just one out of $m$ sequences is $(p,B)$-pseudorandom then, we get a below-2 approximation in $\\tilde{O}_m(nB^{m-1}+n^{\\lfloor \\frac{m}{2}\\rfloor+3})$ time irrespective of $\\theta$. 0 into PostgreSQL...\n",
      "Inserting test sample 1650  This paper explores the problem of approximating the longest common subsequence (LCS) and alignment distance for multiple sequences. LCS is a fundamental problem in bioinformatics, and it has applications in various fields, including data compression, cryptography, and text editing. In the context of multiple sequences, the problem becomes more challenging due to the increased complexity of identifying the common subsequences among them.\n",
      "\n",
      "We propose a novel algorithm that approximates LCS and alignment distance over multiple sequences with high accuracy and efficiency. The algorithm is based on a dynamic programming approach that iteratively computes the score of all pairs of subsequences across the input sequences. We show that our algorithm achieves state-of-the-art performance on benchmark datasets and outperforms existing methods in terms of accuracy and computational time.\n",
      "\n",
      "Furthermore, we analyze the properties of our algorithm and demonstrate its robustness to different settings, such as varying sequence lengths, uneven sequence distribution, and noise. We provide experimental evidence that our algorithm is highly scalable and can handle large datasets with thousands of sequences efficiently.\n",
      "\n",
      "As an application of our algorithm, we present a case study on predicting the functional similarity of proteins based on their sequence alignment. We show that our algorithm outperforms existing methods in this context, and it has potential for accelerating the discovery of new protein functions.\n",
      "\n",
      "In conclusion, we present a new algorithm for approximating LCS and alignment distance over multiple sequences that is highly accurate, efficient, and robust. Our work contributes to the field of bioinformatics and has potential for applications in various domains, such as protein functional prediction and genomics. 1 into PostgreSQL...\n",
      "Inserting test sample 1651  Vocal fold (VF) motion is fundamental to voice production and diagnosis in speech and health sciences. The motion is a consequence of air flow interacting with elastic vocal fold structures. Motivated by existing lumped mass models and known flow properties, we propose to model the continuous shape of vocal fold in motion by the two dimensional compressible Navier-Stokes equations coupled with an elastic damped driven wave equation on the fold cover. In this paper, instead of pursuing a direct two dimensional numerical simulation, we derive reduced quasi-one-dimensional model equations by averaging two dimensional solutions along the flow cross sections. We then analyze the oscillation modes of the linearized system about a flat fold, and found that the fold motion goes through a Hopf bifurcation into temporal oscillation if the flow energy is sufficient to overcome the damping in the fold consistent with the early models. We also analyze the further reduced system under the quasi-steady approximation and compare the resulting vocal fold equation in the small vibration regime with that of the Titze model. Our model shares several qualitative features with the Titze model yet differs in the specific form of energy input from the air flow to the fold. Numerical issues and results of the quasi-one-dimensional model system will be presented in part II (view resulting web VF animation at http://www.ma.utexas.edu/users/jxin). 0 into PostgreSQL...\n",
      "Inserting test sample 1652  In this paper, we present a model for simulating the motion of vocal folds based on a continuum fluid dynamic approach. The model is derived from first principles and takes into account the fundamental physics of fluid dynamics as well as the biological and mechanical properties of the vocal fold tissue. We analyze the model using both numerical simulations and analytical methods to gain insight into the behavior of the vocal folds under different conditions.\n",
      "\n",
      "Our model provides a detailed and accurate representation of the complex motion of vocal folds, which is essential for understanding the mechanisms of speech production. Through our simulations, we demonstrate the importance of viscosity, inertia, and elasticity in governing the dynamics of vocal folds. We also investigate the effects of different parameters such as pressure, airflow, and tissue properties on the motion of the vocal folds.\n",
      "\n",
      "Our results show that the continuum fluid dynamic model is capable of predicting the motion of the vocal folds with a high degree of accuracy, and provides a powerful tool for analyzing and understanding the mechanisms of speech production. This model can be used to develop new approaches for diagnosing and treating speech disorders, as well as to guide the design of new vocal fold implant materials and surgical techniques. Overall, our work represents a significant step forward in our understanding of the physical and biological processes that underlie human speech production. 1 into PostgreSQL...\n",
      "Inserting test sample 1653  In this paper, a new rigorous numerical method to compute fundamental matrix solutions of non-autonomous linear differential equations with periodic coefficients is introduced. Decomposing the fundamental matrix solutions $\\Phi(t)$ by their Floquet normal forms, that is as product of real periodic and exponential matrices $\\Phi(t)=Q(t)e^{Rt}$, one solves simultaneously for $R$ and for the Fourier coefficients of $Q$ via a fixed point argument in a suitable Banach space of rapidly decaying coefficients. As an application, the method is used to compute rigorously stable and unstable bundles of periodic orbits of vector fields. Examples are given in the context of the Lorenz equations and the $\\zeta^3$-model. 0 into PostgreSQL...\n",
      "Inserting test sample 1654  In this paper, we present a rigorous numerical method for computing stable and unstable bundles of periodic orbits in Floquet theory. Our approach utilizes the concept of spectral stability and allows for the efficient computation of bundles with high accuracy. We demonstrate the effectiveness of our method by computing bundles for various nonlinear systems, including the Van der Pol oscillator and the Lorenz system. Our results show that our method is able to accurately capture the intricate structure of the unstable manifolds of these systems, providing new insights into their dynamical behavior. This work contributes to the development of robust numerical tools for the study of nonlinear systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1655  We prove several interesting equalities for the integrals of higher order derivatives on the homogeneous groups. As consequences, we obtain the sharp Hardy--Rellich type inequalities for higher order derivatives including both the subcritical and critical inequalities on the homogeneous groups. We also prove several uncertainty principles on the homogeneous groups. Our results seem to be new even in the case of Euclidean space $\\mathbb R^n$ and give a simple proof of several classical Hardy--Rellich type inequalities in $\\mathbb R^n$. 0 into PostgreSQL...\n",
      "Inserting test sample 1656  In this paper, we establish sharp higher order Hardy-Rellich type inequalities on homogeneous groups. We prove the general form of these inequalities in terms of the homogeneous dimensions and the order of the operators involved. We also obtain explicit results for specific cases of homogeneous groups, such as the Heisenberg and Engel groups. Our work provides a valuable extension of previous results and sheds new light on the geometry and analysis of homogeneous spaces. 1 into PostgreSQL...\n",
      "Inserting test sample 1657  We show that optomechanical systems can test the Schr\\\"{o}dinger-Newton equation of gravitational quantum mechanics due to Yang et al. This equation is motivated by semiclassical gravity, a widely used theory of interacting gravitational and quantum fields. From the many-body Schr\\\"{o}dinger-Newton equation follows an approximate equation for the center-of-mass dynamics of macroscopic objects. This predicts a distinctive double-peaked signature in the output optical quadrature power spectral density of certain optomechanical systems. Since the Schr\\\"{o}dinger-Newton equation lacks free parameters, these will allow its experimental confirmation or refutation. 0 into PostgreSQL...\n",
      "Inserting test sample 1658  We present optomechanical tests for the validity of the Schr\\\"odinger-Newton equation in the context of gravitational quantum mechanics. These tests involve a specially designed optomechanical system, which allows for the reduction of the effective mass of the mechanical oscillator to the level of a single elementary particle. By comparing the results of our measurements to the theoretical predictions of the Schr\\\"odinger-Newton equation, we obtain new bounds on the validity of this equation at the quantum level. Our experimental results provide evidence for the validity of the Schr\\\"odinger-Newton equation in the quantum regime. 1 into PostgreSQL...\n",
      "Inserting test sample 1659  Matrix valued data has become increasingly prevalent in many applications.\n",
      "\n",
      "Most of the existing clustering methods for this type of data are tailored to the mean model and do not account for the dependence structure of the features, which can be very informative, especially in high-dimensional settings. To extract the information from the dependence structure for clustering, we propose a new latent variable model for the features arranged in matrix form, with some unknown membership matrices representing the clusters for the rows and columns. Under this model, we further propose a class of hierarchical clustering algorithms using the difference of a weighted covariance matrix as the dissimilarity measure. Theoretically, we show that under mild conditions, our algorithm attains clustering consistency in the high-dimensional setting.\n",
      "\n",
      "While this consistency result holds for our algorithm with a broad class of weighted covariance matrices, the conditions for this result depend on the choice of the weight. To investigate how the weight affects the theoretical performance of our algorithm, we establish the minimax lower bound for clustering under our latent variable model. Given these results, we identify the optimal weight in the sense that using this weight guarantees our algorithm to be minimax rate-optimal in terms of the magnitude of some cluster separation metric. The practical implementation of our algorithm with the optimal weight is also discussed. Finally, we conduct simulation studies to evaluate the finite sample performance of our algorithm and apply the method to a genomic dataset. 0 into PostgreSQL...\n",
      "Inserting test sample 1660  High-dimensional matrix-valued data presents complex problems in many scientific fields where the feature set is vast and continuous. In contrast to standard clustering methods, variable clustering has been introduced as a powerful tool for data dimensionality reduction. The idea behind variable clustering is to group relevant columns of a matrix into a small number of parameter clusters to simplify further analysis. However, selecting the optimal number of clusters and variables for clustering is non-trivial and dependent on many factors, including data properties and application requirements.\n",
      "\n",
      "In this research paper, we introduce an optimal variable clustering algorithm for high-dimensional matrix-valued data that aims to overcome the limitations of previous methods. Our method constructs an optimization problem to determine the optimal parameter clusters and variables, ensuring that the selected variables are structurally meaningful and provide the most relevant information for subsequent analysis.\n",
      "\n",
      "We apply our algorithm to several real-world datasets, including gene expression data, and demonstrate its effectiveness in reducing data dimensionality while preserving structural properties of the original data. Our experimental results show that our algorithm outperforms traditional variable clustering methods in terms of variable selection and predictive performance.\n",
      "\n",
      "Overall, our method provides a versatile and easily applicable solution for handling high-dimensional matrix-valued data, which is a fundamental challenge in many scientific domains. 1 into PostgreSQL...\n",
      "Inserting test sample 1661  Quantum chromodynamics predicts that the interaction between its fundamental constituents, quarks and gluons, can lead to different states of strongly interacting matter, dependent on its temperature and baryon density. We first survey the possible states of matter in QCD and discuss the transition from a color-confining hadronic phase to a plasma of deconfined colored quarks and gluons. Next, we summarize the results from non-perturbative studies of QCD at finite temperature and baryon density, and address the origin of deconfinement in the different regimes. Finally, we consider possible probes to test the basic features of bulk matter in QCD. 0 into PostgreSQL...\n",
      "Inserting test sample 1662  The study of Quantum Chromodynamics (QCD) has shown that matter exhibits various states at different temperatures and densities. The transition between these states is known as the phase transition, which has important implications for the behavior of the early universe and the phenomenon of neutron stars. In this paper, we explore the different phases of QCD matter, including quark-gluon plasma, hadronic gas, and nuclear matter. By utilizing lattice QCD simulations, we are able to gain insights into the properties of these phases, including their equation of state and transport coefficients. Our findings contribute to a better understanding of the nature of matter under extreme conditions. 1 into PostgreSQL...\n",
      "Inserting test sample 1663  Chen Ang, Bhalla and Cross [Phys. Rev. B 64, 184104 (2001)] have studied the low-frequency (20 Hz -- 100 kHz) dielectric dispersion of the KTaO3 crystal under a dc electric field. Performing fits of the electric field dependence they came to conclusion that an appreciable contribution to the dielectric permittivity originates from polar clusters. In this Comment we show that the dielectric permittivity at low frequencies (100 Hz -- 1 MHz) equals to that in the THz region, close below the polar phonon response. This excludes the possibility of any appreciable dielectric dispersion due to polar clusters. In addition, we demonstrate that correct treatment using Landau-Ginzburg-Devonshire theory allows to fit the electric field dependence of the dielectric constant without assuming any polarization mechanism besides the polar phonon modes. 0 into PostgreSQL...\n",
      "Inserting test sample 1664  This work presents a comment on the dielectric behavior of paraelectric KTaO3, CaTiO3, and (Ln_{1/2}Na_{1/2})TiO3 under a dc electric field. The paper under discussion highlights the importance of such investigations towards understanding the intrinsic properties and response of these materials under external stimuli. The authors of the discussed research paper report on significant observations of the dielectric behavior of these oxides, which are important for many technological applications. This comment article elaborates on the significance and implications of these observations, providing additional insights into the underlying physics. It concludes by highlighting the importance of continued research on the dielectric properties of these and related materials, towards advancing their applications in various fields. 1 into PostgreSQL...\n",
      "Inserting test sample 1665  We study knots in $S^3$ with infinitely many $SU(2)$-cyclic surgeries, which are Dehn surgeries such that every representation of the resulting fundamental group into $SU(2)$ has cyclic image. We show that for every such nontrivial knot $K$, its set of $SU(2)$-cyclic slopes is bounded and has a unique limit point, which is both a rational number and a boundary slope for $K$. We also show that such knots are prime and have infinitely many instanton L-space surgeries. Our methods include the application of holonomy perturbation techniques to instanton knot homology, using a strengthening of recent work by the second author. 0 into PostgreSQL...\n",
      "Inserting test sample 1666  This paper explores the relationship between SU(2)-cyclic surgeries and the pillowcase, a closed surface with cone-type singularities. The focus is on how these surgeries can be used to create new hyperbolic 3-manifolds, particularly through the use of pillowcase surfaces. By studying these surgeries and their effects on the geometry of the manifold, researchers hope to gain a better understanding of the topological properties of these objects. The paper includes numerous examples of these surgeries and their resulting 3-manifolds, as well as an analysis of the ways in which they fit into the larger context of hyperbolic geometry. 1 into PostgreSQL...\n",
      "Inserting test sample 1667  Motivated by the need to study the molecular mechanism underlying Type 1 Diabetes (T1D) with the gene expression data collected from both the patients and healthy controls at multiple time points, we propose an innovative method for jointly estimating multiple dependent Gaussian graphical models. Compared to the existing methods, the proposed method has a few significant advantages.\n",
      "\n",
      "First, it includes a meta-analysis procedure to explicitly integrate information across distinct conditions. In contrast, the existing methods often integrate information through prior distributions or penalty function, which is usually less efficient. Second, instead of working on original data, the Bayesian step of the proposed method works on edge-wise scores, through which the proposed method avoids to invert high-dimensional covariance matrices and thus can perform very fast. The edge-wise score forms an equivalent measure of the partial correlation coefficient and thus provides a good summary for the graph structure information contained in the data under each condition. Third, the proposed method can provide an overall uncertainty measure for the edges detected in multiple graphical models, while the existing methods only produce a point estimate or are feasible for very small size problems. We prove consistency of the proposed method under mild conditions and illustrate its performance using simulated and real data examples. The numerical results indicate the superiority of the proposed method over the existing ones in both estimation accuracy and computational efficiency. Extension of the proposed method to joint estimation of multiple mixed graphical models is straightforward. 0 into PostgreSQL...\n",
      "Inserting test sample 1668  This paper presents a novel approach for fast Bayesian integrative learning of multiple gene regulatory networks (GRNs) for Type 1 Diabetes (T1D). We propose a recursive feature elimination-based method that accounts for the heterogeneous nature of biological data and leverages prior biological knowledge to integrate multiple data types. We apply our method to transcriptomic and epigenomic data of T1D patients and controls. Our results suggest that our approach uncovers biologically meaningful GRNs that are enriched in T1D-associated pathways. Our model outperforms state-of-the-art integrative approaches and identifies unique regulatory relationships that are specific to the T1D condition. Our method highlights the importance of incorporating prior biological knowledge in the modeling of complex biological systems. Moreover, our approach is computationally efficient, enabling the analysis of large-scale datasets and facilitating future studies aiming to investigate T1D mechanisms. Overall, our findings provide new insights into the mechanistic underpinnings of T1D and demonstrate the potential of our approach to unravel the complexity of other complex diseases. Future studies might use our method to provide personalized treatments for T1D patients and enable prompt diagnosis for a more effective disease management. 1 into PostgreSQL...\n",
      "Inserting test sample 1669  Magnetised exoplanets are expected to emit at radio frequencies analogously to the radio auroral emission of Earth and Jupiter. We predict the radio emission from V830 Tau b, the youngest (2 Myr) detected exoplanet to date. We model the host star wind using 3DMHD simulations that take into account its surface magnetism. With this, we constrain the local conditions around V830 Tau b that we use to then compute its radio emission. We estimate average radio flux densities of 6 to 24mJy, depending on the assumed radius of the planet (one or two Rjupiter). These radio fluxes are present peaks that are up to twice the average values. We show here that these fluxes are weakly dependent (a factor of 1.8) on the assumed polar planetary magnetic field (10 to 100G), opposed to the maximum frequency of the emission, which ranges from 18 to 240MHz. We also estimate the thermal radio emission from the stellar wind. By comparing our results with VLA and VLBA observations of the system, we constrain the stellar mass-loss rate to be <3e-9 Msun/yr, with likely values between ~1e-12 and 1e-10 Msun/yr. The frequency-dependent extension of the radio-emitting wind is around ~ 3 to 30 Rstar for frequencies in the range of 275 to 50MHz, implying that V830 Tau b, at an orbital distance of 6.1 Rstar, could be embedded in the regions of the host star's wind that are optically thick to radio wavelengths, but not deeply so. Planetary emission can only propagate in the stellar wind plasma if the frequency of the cyclotron emission exceeds the stellar wind plasma frequency. For that, we find that for planetary radio emission to propagate through the host star wind, planetary magnetic field strengths larger than ~1.3 to 13 G are required. The V830 Tau system is a very interesting system for conducting radio observations from both the perspective of radio emission from the planet as well as from the host star's wind. 0 into PostgreSQL...\n",
      "Inserting test sample 1670  The recent discovery of exoplanets has extended our understanding of planetary formation and evolution. V830 Tau is a newly discovered hot Jupiter that is a viable candidate to help us better understand the radio emission phenomena occurring in exoplanets. Despite the growing research on such radio emissions, a limited amount of attention has been given to the radio emission from hot Jupiters and their host stars, and it is unclear how they may vary across different types of exoplanets. Therefore, in this study we seek to predict the radio emission from V830 Tau as well as from its host star, to enhance our understanding of radio emission in exoplanets. \n",
      "\n",
      "To achieve this goal, we propose a theoretical framework based on previous knowledge of radio emission mechanisms from stellar, planetary, and satellite sources. We drew upon this knowledge to generate a detailed model of the radio emission from V830 Tau and its host star. By incorporating a variety of parameters, such as the magnetic field and composition, we were able to predict the expected radio emission levels. Our results show that, while the radio emission from V830 Tau and its host star is quite low, it is detectable using current technology. \n",
      "\n",
      "The relevance of this study is that it can aid in the understanding of the link between exoplanetary systems and their radio emission. This information is critical for the detection, characterization, and even exploration of these exoplanetary systems using radio telescopes. We believe that our proposed framework can serve as a guide for future research in predicting radio emissions in hot Jupiters and their host stars, which will deepen our understanding of the magnetic and radio properties of exoplanetary systems. In conclusion, our work serves as an essential step in the continued advancement of the field of exoplanetary science, broadening our knowledge of exoplanet formation and evolution, and facilitating the search for life beyond our solar system. 1 into PostgreSQL...\n",
      "Inserting test sample 1671  This article is devoted to a description of the dynamics of the phase flow of monotone contact Hamiltonian systems. Particular attention is paid to locating the maximal attractor (or repeller), which could be seen as the union of compact invariant sets, and investigating its topological and dynamical properties. This is based on an analysis from the viewpoint of gradient-like systems. 0 into PostgreSQL...\n",
      "Inserting test sample 1672  In this paper, we study the global behavior of contact Hamiltonian systems with a focus on monotone systems. We investigate the existence of periodic solutions and show that such solutions exist either in the interior of the phase space or at the boundary. Additionally, we develop a Poincare-Birkhoff fixed point theorem which guarantees the existence of closed orbits with specific properties. 1 into PostgreSQL...\n",
      "Inserting test sample 1673  Recently, the one-loop free energy of higher spin (HS) theories in Euclidean AdS_{d+1} was calculated and matched with the order N^0 term in the free energy of the large N \"vectorial\" scalar CFT on the S^d boundary. Here we extend this matching to the boundary theory defined on S^1 x S^{d-1}, where the length of S^1 may be interpreted as the inverse temperature. It has been shown that the large N limit of the partition function on S^1 x S^2 in the U(N) singlet sector of the CFT of N free complex scalars matches the one-loop thermal partition function of the Vasiliev theory in AdS_4, while in the O(N) singlet sector of the CFT of N real scalars it matches the minimal theory containing even spins only. We extend this matching to all dimensions d. We also calculate partition functions for the singlet sectors of free fermion CFT's in various dimensions and match them with appropriately defined higher spin theories, which for d>3 contain massless gauge fields with mixed symmetry. In the zero-temperature case R x S^{d-1} we calculate the Casimir energy in the scalar or fermionic CFT and match it with the one-loop correction in the global AdS_{d+1}. For any odd-dimensional CFT the Casimir energy must vanish on general grounds, and we show that the HS duals obey this. In the U(N) symmetric case, we exhibit the vanishing of the regularized 1-loop Casimir energy of the dual HS theory in AdS_{d+1}. In the minimal HS theory the vacuum energy vanishes for odd d while for even d it is equal to the Casimir energy of a single conformal scalar in R x S^{d-1} which is again consistent with AdS/CFT, provided the minimal HS coupling constant is ~ 1/(N-1). We demonstrate analogous results for singlet sectors of theories of N Dirac or Majorana fermions. We also discuss extensions to CFT's containing N_f flavors in the fundamental representation of U(N) or O(N). 0 into PostgreSQL...\n",
      "Inserting test sample 1674  This paper investigates the interplay of partition functions and Casimir energies in higher spin AdS/CFT dualities. We consider a d-dimensional conformal field theory (CFT) and its semiclassical gravity dual in (d+1)-dimensional anti-de Sitter spacetime (AdS). We specifically focus on the higher spin sector of the CFT, which contains an infinite number of conserved currents with higher and higher spin angular momentum. \n",
      "\n",
      "The partition function, a fundamental quantity in statistical physics, characterizes the thermodynamic behavior of the system. We study the large spin limit of the partition function in the CFT, which computes correlation functions of higher spin operators at large separations. Our analysis reveals a surprising connection between the partition function of the higher spin CFT and the Casimir energy of a scalar field propagating in AdS space. \n",
      "\n",
      "The Casimir energy is the difference in vacuum energy between the confined and unconfined regions of space, which causes an attractive or repulsive force between the boundaries. We calculate the Casimir energy for a massive scalar field with Dirichlet boundary conditions in AdS space and relate it to the partition function of the dual CFT. Intriguingly, we find that the the Casimir energy depends on the dimension of the CFT and the spin of the higher spin operators. \n",
      "\n",
      "We further explore the implications of our results for holographic renormalization. We argue that the partition function captures non-local information about the CFT which cannot be obtained by local functionals. This, in turn, leads to a non-local contribution to the holographic renormalization counterterm, which we calculate explicitly in AdS/CFT. \n",
      "\n",
      "Our findings have important implications for the AdS/CFT correspondence and highlight the intricate connection between partition functions and Casimir energies in higher spin theories. We conclude with a discussion of open questions and future directions for research in this area, including the extension of our results to more general holographic theories and non-integer spin CFTs. 1 into PostgreSQL...\n",
      "Inserting test sample 1675  We analyze the stellar absorption features in high signal-to-noise ratio near-infrared (NIR) spectra of the nuclear region of 12 nearby galaxies, mostly spirals. The features detected in some or all of the galaxies in this sample are the TiO (0.843 $\\mu$m\\ and 0.886 $\\mu$m), VO (1.048 $\\mu$m), CN (1.1 $\\mu$m\\ and 1.4 $\\mu$m), H$\\rm _2$O (1.4 $\\mu$m\\ and 1.9 $\\mu$m) and CO (1.6 $\\mu$m\\ and 2.3 $\\mu$m) bands. The C$\\rm _2$ (1.17 $\\mu$m\\ and 1.76 $\\mu$m) bands are generally weak or absent, although C$\\rm _2$ (1.76 $\\mu$m) may be weakly present in the mean galaxy spectrum. A deep feature near 0.93 $\\mu$m, likely caused by CN, TiO and/or ZrO, is also detected in all objects. Fitting a combination of stellar spectra to the mean spectrum shows that the absorption features are produced by evolved stars: cool giants and supergiant stars in the early- or thermally-pulsing asymptotic giant branch (E-AGB or TP-AGB) phases.\n",
      "\n",
      "The high luminosity of TP-AGB stars, and the appearance of VO and ZrO features in the data, suggest that TP-AGB stars dominate these spectral features.\n",
      "\n",
      "However, a contribution from other evolved stars is also likely. Comparison with evolutionary population synthesis models shows that models based on empirical libraries that predict relatively strong NIR features provide a more accurate description of the data. However, none of the models tested accurately reproduces all of the features observed in the spectra. To do so, the models will need to not only improve the treatment of TP-AGB stars, but also include good quality spectra of red giant and E-AGB stars. The uninterrupted wavelength coverage, high S/N, and quantity of features we present here will provide a benchmark for the next generation of models aiming to explain and predict the NIR properties of galaxies. 0 into PostgreSQL...\n",
      "Inserting test sample 1676  This paper investigates the question: do the stellar spectral features of nearby galaxies in the near-infrared act as tracers of thermally-pulsing asymptotic giant branch (TP-AGB) stars? TP-AGB stars are identified by their unique circumstellar envelopes, which emit thermal radiation at near-infrared wavelengths. However, TP-AGB stars are often difficult to study because they exist in complex stellar populations and their circumstellar envelopes can be contaminated by other sources of near-infrared radiation. \n",
      "\n",
      "Our analysis focuses on nearby galaxies in order to minimize the impact of galaxy evolution and spatial resolution effects on our results. We use data from the FourStar Galaxy Evolution Survey and the Spitzer Space Telescope to examine the near-infrared spectral features of TP-AGB stars in the context of the overall stellar population of each galaxy. \n",
      "\n",
      "We find that the near-infrared spectral features of TP-AGB stars can be used as tracers of their presence in nearby galaxies. Specifically, we identify a correlation between the strength of the near-infrared spectral features and the relative numbers of TP-AGB stars. We demonstrate that this correlation is robust to corrections for dust extinction and other sources of contamination. \n",
      "\n",
      "Our results have important implications for understanding the role of TP-AGB stars in galaxy evolution, as well as for interpreting the near-infrared spectra of distant galaxies. Through the use of our novel method for identifying TP-AGB stars based on near-infrared spectral features, we are able to gain insights into the underlying stellar population of nearby galaxies with greater accuracy and precision than previously possible. \n",
      "\n",
      "In summary, our study demonstrates that the near-infrared spectral features of TP-AGB stars can act as tracers of their presence in nearby galaxies. Our results have significant implications for future studies of galaxy evolution and the interpretation of near-infrared spectra in distant galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 1677  The advent of new experimental genomic technologies and the massive increase of DNA sequence information is helping researchers better understand how our genes work. Recently, experiments on mRNA abundance (gene expression) have revealed that gene expression shows a stationary organization described by a power-law distribution (scale-free organization) (i.e., gene expression $k$ decays as $k^{-\\gamma}$), which is highly conserved in all the major five kingdoms of life, from Bacteria to Human. An underlying gene expression dynamics \"rich-travel-more\" was suggested to recover that evolutional conservation of transcriptional organization. Here we propose a constructive approach to gene expression dynamics with larger scope. Our gene expression construction restores the stationary state, predicts the power-law exponent for different organisms with natural explanation for small correction at high and low expression levels, describes the intermediate state dynamics (time finite) and elucidates the gene expression stability. This approach requires only one assumption: Markov property. 0 into PostgreSQL...\n",
      "Inserting test sample 1678  This paper describes a constructive approach to gene expression dynamics, based on the experimentally observed variability of a gene's response to variations of input signal. We propose a mathematical framework where gene models are constructed from a small set of \"building blocks\" that capture the essential features of gene regulation. These building blocks are combined in a modular fashion to create models that accurately predict the behavior of real genes under different stimuli. The approach is validated using a set of benchmark genes whose behavior is well-characterized experimentally. Our results show that the constructed models predict the behavior of these genes with high accuracy, and that the approach is robust to variations in the input signal and the type of gene considered. This constructive approach has the potential to provide insights into the design principles of gene regulation and to aid in the design of synthetic gene circuits with predictable behavior. 1 into PostgreSQL...\n",
      "Inserting test sample 1679  Dynamic nature of the cloud environment has made distributed resource management process a challenge for cloud service providers. The importance of maintaining the quality of service in accordance with customer expectations as well as the highly dynamic nature of cloud-hosted applications add new levels of complexity to the process. Advances to the big data learning approaches have shifted conventional static capacity planning solutions to complex performance-aware resource management methods. It is shown that the process of decision making for resource adjustment is closely related to the behaviour of the system including the utilization of resources and application components.\n",
      "\n",
      "Therefore, a continuous monitoring of system attributes and performance metrics provide the raw data for the analysis of problems affecting the performance of the application. Data analytic methods such as statistical and machine learning approaches offer the required concepts, models and tools to dig into the data, find general rules, patterns and characteristics that define the functionality of the system. Obtained knowledge form the data analysis process helps to find out about the changes in the workloads, faulty components or problems that can cause system performance to degrade. A timely reaction to performance degradations can avoid violations of the service level agreements by performing proper corrective actions including auto-scaling or other resource adjustment solutions. In this paper, we investigate the main requirements and limitations in cloud resource management including a study of the approaches in workload and anomaly analysis in the context of the performance management in the cloud.\n",
      "\n",
      "A taxonomy of the works on this problem is presented which identifies the main approaches in existing researches from data analysis side to resource adjustment techniques. 0 into PostgreSQL...\n",
      "Inserting test sample 1680  Cloud computing has emerged as a popular paradigm for delivering computing resources and services. Many companies are increasingly relying on cloud-based systems to host their applications and services. However, managing cloud resources efficiently is a challenging task, particularly when it comes to resource allocation and performance optimization. This article presents a taxonomy for performance-aware management of cloud resources and discusses future directions for research in the area.\n",
      "\n",
      "The proposed taxonomy for performance-aware cloud resource management is based on three key dimensions: application characteristics, resource management policies, and performance metrics. Within each dimension, various subcategories are defined that can be used to classify different approaches for managing cloud resources. By using this taxonomy, researchers and practitioners can easily compare different approaches and identify gaps in existing research.\n",
      "\n",
      "In addition, this article identifies several promising research directions for performance-aware cloud resource management. These include workload prediction, resource scheduling, performance modeling, and energy-efficient resource management. The challenges and opportunities associated with each direction are discussed, and recommendations for future research are provided.\n",
      "\n",
      "Overall, the taxonomy and future directions presented in this article provide a valuable framework for researchers and practitioners working in the area of cloud resource management. By using this framework, they can develop more effective approaches for managing cloud resources, which in turn can lead to improved performance and cost savings. 1 into PostgreSQL...\n",
      "Inserting test sample 1681  Hydride molecules lie at the base of interstellar chemistry, but the synthesis of sulfuretted hydrides is poorly understood. Motivated by new observations of the Orion Bar PDR - 1'' resolution ALMA images of SH+; IRAM 30m detections of H2S, H2S34, and H2S33; H3S+ (upper limits); and SOFIA observations of SH - we perform a systematic study of the chemistry of S-bearing hydrides. We determine their column densities using coupled excitation, radiative transfer as well as chemical formation and destruction models. We revise some of the key gas-phase reactions that lead to their chemical synthesis. This includes ab initio quantum calculations of the vibrational-state-dependent reactions SH+ + H2 <-> H2S+ + H and S + H2 <-> SH + H. We find that reactions of UV-pumped H2 (v>1) with S+ explain the presence of SH+ in a high thermal-pressure gas component, P_th~10^8 cm^-3 K, close to the H2 dissociation front. However, subsequent hydrogen abstraction reactions of SH+, H2S+, and S with vibrationally excited H2, fail to ultimately explain the observed H2S column density (~2.5x10^14 cm^-2, with an ortho-to-para ratio of 2.9+/-0.3). To overcome these bottlenecks, we build PDR models that include a simple network of grain surface reactions leading to the formation of solid H2S (s-H2S). The higher adsorption binding energies of S and SH suggested by recent studies imply that S atoms adsorb on grains (and form s-H2S) at warmer dust temperatures and closer to the UV-illuminated edges of molecular clouds.\n",
      "\n",
      "Photodesorption and, to a lesser extent, chemical desorption, produce roughly the same H2S column density (a few 10^14 cm-^2) and abundance peak (a few 10^-8) nearly independently of n_H and G_0. This agrees with the observed H2S column density in the Orion Bar as well as at the edges of dark clouds without invoking substantial depletion of elemental sulfur abundances. 0 into PostgreSQL...\n",
      "Inserting test sample 1682  Interstellar sulfur chemistry plays a crucial role in the formation and evolution of stars and planetary systems. However, it is hindered by several bottlenecks that limit our understanding of the underlying physical and chemical processes. In this paper, we focus on sulfur-bearing hydrides in UV-illuminated gas and grains, which are key intermediates in the interstellar sulfur cycle. Through a combination of laboratory experiments, observational data, and theoretical modeling, we investigate the formation, destruction, and transformation of these molecules in various astrophysical environments.\n",
      "\n",
      "Our results reveal that the sulfur chemistry is highly sensitive to the local physical conditions, such as temperature, density, and radiation field, as well as the availability of other reactive species. In particular, the UV irradiation of gas-phase sulfur-bearing molecules leads to the production of a complex mixture of species, including radicals, cations, anions, and neutral molecules. The subsequent reactions of these species with each other and with the surrounding material may give rise to unexpected products, such as novel sulfur-nitrogen compounds or organic molecules.\n",
      "\n",
      "We find that the interaction of sulfur-bearing hydrides with dust grains is also a critical factor in the interstellar sulfur cycle. The adsorption, desorption, and diffusion of these molecules on grain surfaces affect their abundance, chemistry, and reactivity, leading to diverse chemical pathways and spatial distributions. Moreover, the formation of complex organic molecules in grain mantles may be influenced by the presence of sulfur-bearing species, such as H2S and CS.\n",
      "\n",
      "Our study sheds light on the complex nature of interstellar sulfur chemistry and highlights the challenges and opportunities for future observational and laboratory investigations. We conclude by discussing the implications of our results for the understanding of the origin and evolution of life in the Universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1683  We present eight new T4.5-T7.5 dwarfs identified in the UKIRT Infrared Deep Sky Survey (UKIDSS) Large Area Survey (LAS) Data Release 1 (DR1). In addition we have recovered the T4.5 dwarf SDSS J020742.91+000056.2 and the T8.5 dwarf ULAS J003402.77-005206.7. Photometric candidates were picked up in two-colour diagrams over 190 square degrees (DR1) and selected in at least two filters.\n",
      "\n",
      "All candidates exhibit near-infrared spectra with strong methane and water absorption bands characteristic of T dwarfs and the derived spectral types follow the unified scheme of Burgasser et al. (2006). We have found 6 new T4.5-T5.5 dwarfs, one T7 dwarf, one T7.5 dwarf, and recovered a T4.5 dwarf and a T8.5 dwarf. We provide distance estimates which lie in the 15-85 pc range; the T7.5 and T8.5 dwarfs are probably within 25 pc of the Sun. We conclude with a discussion of the number of T dwarfs expected after completion of the LAS, comparing these initial results to theoretical simulations. 0 into PostgreSQL...\n",
      "Inserting test sample 1684  We report the discovery of eight new T4.5-T7.5 dwarf candidates identified in the UKIRT Infrared Deep Sky Survey Large Area Survey (LAS) Data Release 1. The candidates were selected by utilising a combination of Optical/IR colour cuts based on the known colours of T dwarfs in the near-infrared and optical regime. UKIDSS LAS provides an excellent source of photometric information to study the coolest and lowest mass stars in the Galactic neighbourhood. The discovery of these eight new T dwarfs in such a small area of the sky is significant since they have much redder colours than any of the previously known T dwarfs at the same spectral types and indicate that UKIDSS will provide an excellent dataset for building up a larger sample of T dwarfs. Further observations are needed to confirm the nature of these objects but the present study opens up prospects for the discovery of many more T dwarfs with the UKIDSS LAS. 1 into PostgreSQL...\n",
      "Inserting test sample 1685  The standard unitarity-cut method is applied to several massive two-dimensional models, including the world-sheet AdS$_5\\times S^5$ superstring, to compute $2\\to 2$ scattering S-matrices at one loop from tree level amplitudes. Evidence is found for the cut-constructibility of supersymmetric integrable models, while for models without supersymmetry (but integrable) the missing rational terms can be interpreted as a shift in the coupling. 0 into PostgreSQL...\n",
      "Inserting test sample 1686  In two-dimensional scattering problems, unitarity-based techniques have proven to be a powerful tool to perform intelligent resummations of perturbative amplitudes. This paper provides an overview of the unitarity method and its applications in two-dimensional scattering. We show how to extract precise information about the scattering amplitudes from cuts of Feynman diagrams, enabling the computation of cross sections and total rates. These techniques provide a fundamental building block for the computation of scattering amplitudes in higher dimensions. 1 into PostgreSQL...\n",
      "Inserting test sample 1687  The amplitude of the primordial magnetic field (PMF) is constrained from observational limits on primordial nuclear abundances. Within this constraint, it is possible that nuclear motion is regulated by Coulomb scattering with electrons and positrons ($e^\\pm$'s), while $e^\\pm$'s are affected by a PMF rather than collisions. For example, at a temperature of $10^9$ K, thermal nuclei typically experience $\\sim 10^{21}$ scatterings per second that are dominated by very small angle scattering leading to minuscule changes in the nuclear kinetic energy of order $\\mathcal{O}$(1) eV. In this paper the upper limit on the effects of a possible discretization of the $e^\\pm$ momenta by the PMF on the nuclear momentum distribution is estimated under the extreme assumptions that the momentum of the $e^\\pm$ is relaxed before and after Coulomb scattering to Landau levels, and that during Coulomb scattering the PMF is neglected. This assumption explicitly breaks the time reversal invariance of Coulomb scattering, and the Maxwell-Boltzmann distribution is not a trivial steady state solution of the Boltzmann equation under these assumptions. We numerically evaluate the collision terms in the Boltzmann equation, and show that the introduction of a special direction in the $e^\\pm$ distribution by the PMF generates no directional dependence of the collisional destruction term of nuclei. Large anisotropies in the nuclear distribution function are then constrained from big bang nucleosynthesis. Ultimately, we conclude that a PMF does not significantly affect the isotropy or BBN. 0 into PostgreSQL...\n",
      "Inserting test sample 1688  In this research paper, we investigate the distribution function of nuclei resulting from $e^\\pm$ scattering in the presence of a strong primordial magnetic field. Our aim is to understand the effects of magnetic fields on the evolution of cosmic structures as they are believed to play a crucial role in the formation of galaxies and clusters of galaxies. To achieve this objective, we perform numerical simulations of the scattering process using a Monte Carlo method. Our simulations consider different strengths of the magnetic field, from weak to extremely strong, and we analyze the resulting changes in the distribution function of the scattered nuclei. We find that the presence of a primordial magnetic field significantly affects the distribution of the scattered nuclei, leading to an anisotropic distribution in the scattered particles and reducing the total number of scattered nuclei. Our results suggest that primordial magnetic fields could have significant effects on the evolution of cosmic structures and could provide insights into the formation of magnetic fields in the early universe. These findings may have broader implications for understanding the universe's evolution, and they could inspire further investigations in this field. 1 into PostgreSQL...\n",
      "Inserting test sample 1689  A bipartite state is said to be steerable if and only if it does not have a single system description, i.e., the bipartite state cannot be explained by a local hidden state model. Several steering inequalities have been derived using different local uncertainty relations to verify the ability to control the state of one subsystem by the other party. Here, we derive complementarity relations between coherences measured on mutually unbiased bases using various coherence measures such as the $l_1$-norm, relative entropy and skew information. Using these relations, we derive conditions under which non-local advantage of quantum coherence can be achieved and the state is steerable. We show that not all steerable states can achieve such advantage. 0 into PostgreSQL...\n",
      "Inserting test sample 1690  The ubiquitous phenomenon of quantum coherence, especially in the context of non-locality, has drawn immense attention in the research community. This study explores the concept of non-local advantage arising from quantum coherence, wherein the quantum system is shown to outperform its classical counterpart in the presence of certain non-local correlations. Through analytical and computational modeling, we demonstrate that the non-locality of quantum coherence translates into a significant advantage in various quantum information processing tasks, such as quantum communication and computation. Our findings not only deepen our fundamental understanding of the quantum world but also have practical implications for developing quantum technologies that offer unprecedented performance and security. This study paves the way for the design of novel quantum systems and protocols optimized for non-local advantage. 1 into PostgreSQL...\n",
      "Inserting test sample 1691  We show that correlation functions have to satisfy contraint relations, owing to the non-negativity of the power spectrum of the underlying random process.\n",
      "\n",
      "Specifically, for any statistically homogeneous and (for more than one spatial dimension) isotropic random field with correlation function $\\xi(x)$, we derive inequalities for the correlation coefficients $r_n\\equiv \\xi(n x)/\\xi(0)$ (for integer $n$) of the form $r_{n{\\rm l}}\\le r_n\\le r_{n{\\rm u}}$, where the lower and upper bounds on $r_n$ depend on the $r_j$, with $j<n$. Explicit expressions for the bounds are obtained for arbitrary $n$. These constraint equations very significantly limit the set of possible correlation functions. For one particular example of a fiducial cosmic shear survey, we show that the Gaussian likelihood ellipsoid has a significant spill-over into the forbidden region of correlation functions, rendering the resulting best-fitting model parameters and their error region questionable, and indicating the need for a better description of the likelihood function.\n",
      "\n",
      "We conduct some simple numerical experiments which explicitly demonstrate the failure of a Gaussian description for the likelihood of $\\xi$. Instead, the shape of the likelihood function of the correlation coefficients appears to follow approximately that of the shape of the bounds on the $r_n$, even if the Gaussian ellipsoid lies well within the allowed region.\n",
      "\n",
      "For more than one spatial dimension of the random field, the explicit expressions of the bounds on the $r_n$ are not optimal. We outline a geometrical method how tighter bounds may be obtained in principle. We illustrate this method for a few simple cases; a more general treatment awaits future work. 0 into PostgreSQL...\n",
      "Inserting test sample 1692  The concept of constrained correlation functions has gained considerable interest in the field of statistical physics in recent years. These functions are used to describe the behavior of complex systems subject to external constraints, such as those induced by boundary conditions or interactions with other systems. They are particularly useful for understanding the properties of condensed matter systems, as well as for modeling diverse phenomena in fields as diverse as biology and economics.\n",
      "\n",
      "In this paper, we analyze the behavior of constrained correlation functions in two and three-dimensional systems, using a variety of analytical and numerical techniques. Our results show that these functions provide a powerful tool for understanding the behavior of systems in complex environments, and allow us to accurately predict a wide range of physical and chemical properties.\n",
      "\n",
      "We demonstrate the versatility of this method by applying it to systems as diverse as magnetic materials, glasses, and thermoelectric materials, as well as biological and environmental systems. Our results show that the use of constrained correlation functions can provide valuable insights into the behavior of these systems, and can be used to design novel materials and technologies with enhanced properties.\n",
      "\n",
      "Overall, our work highlights the importance of constrained correlation functions in the study of complex systems, and demonstrates their potential for advancing our understanding of a wide range of physical, chemical, and biological phenomena. We hope that our findings will inspire further research in this area, and provide a foundation for the development of new models and methods in statistical physics and related fields. 1 into PostgreSQL...\n",
      "Inserting test sample 1693  Motivated by recent experiments, we study the relaxation dynamics and thermalization in the one-dimensional Bose-Hubbard model induced by a global interaction quench. Specifically, we start from an initial state that has exactly one boson per site and is the ground state of a system with infinitely strong repulsive interactions at unit filling. Using exact diagonalization and the density matrix renormalization group method, we compute the time dependence of such observables as the multiple occupancy and the momentum distribution function. Typically, the relaxation to stationary values occurs over just a few tunneling times. The stationary values are identical to the so-called diagonal ensemble on the system sizes accessible to our numerical methods and we further observe that the micro-canonical ensemble describes the steady state of many observables reasonably well for small and intermediate interaction strength.\n",
      "\n",
      "The expectation values of observables in the canonical ensemble agree quantitatively with the time averages obtained from the quench at small interaction strengths, and qualitatively provide a good description of steady-state values even in parameter regimes where the micro-canonical ensemble is not applicable due to finite-size effects. We discuss our numerical results in the framework of the eigenstate thermalization hypothesis. Moreover, we also observe that the diagonal and the canonical ensemble are practically identical for our initial conditions already on the level of their respective energy distributions for small interaction strengths. Finally, we discuss implications of our results for the interpretation of a recent sudden expansion experiment [Phys. Rev. Lett. 110, 205301 (2013)], in which the same interaction quench was realized. 0 into PostgreSQL...\n",
      "Inserting test sample 1694  In this work, we study the dynamics of relaxation and thermalization of the one-dimensional Bose-Hubbard model following a sudden quench of the atomic interaction from the non-interacting or atomic limit. The model is an archetypal example of many-body quantum systems that exhibit a rich interplay between superfluid and Mott insulating phases. Specifically, we focus on the case when the initial state is a Mott insulating one and the interaction strength is suddenly switched on, leading to a quench in the Hamiltonian. Our goal is to explore how the system evolves after the quench and how it reaches thermal equilibrium. To this end, we use a combination of numerical simulations and analytical techniques, based on both exact diagonalization and a truncated Wigner approximation. Our results show that the dynamics of the system strongly depend on the strength of the quench and on the final interaction strength, with several interesting phenomena arising at specific values of these parameters. In particular, we find that the system exhibits a universal scaling in the long-time limit, which is independent of the initial state. Our study sheds light on the behavior of strongly correlated quantum systems after a sudden quench and provides important insights into the physics of the Bose-Hubbard model. 1 into PostgreSQL...\n",
      "Inserting test sample 1695  In this paper, we outline the theory of epidemic percolation networks and their use in the analysis of stochastic SIR epidemic models on undirected contact networks. We then show how the same theory can be used to analyze stochastic SIR models with random and proportionate mixing. The epidemic percolation networks for these models are purely directed because undirected edges disappear in the limit of a large population. In a series of simulations, we show that epidemic percolation networks accurately predict the mean outbreak size and probability and final size of an epidemic for a variety of epidemic models in homogeneous and heterogeneous populations. Finally, we show that epidemic percolation networks can be used to re-derive classical results from several different areas of infectious disease epidemiology. In an appendix, we show that an epidemic percolation network can be defined for any time-homogeneous stochastic SIR model in a closed population and prove that the distribution of outbreak sizes given the infection of any given node in the SIR model is identical to the distribution of its out-component sizes in the corresponding probability space of epidemic percolation networks. We conclude that the theory of percolation on semi-directed networks provides a very general framework for the analysis of stochastic SIR models in closed populations. 0 into PostgreSQL...\n",
      "Inserting test sample 1696  Stochastic SIR epidemic models are widely used to understand the spread of infectious diseases. However, many of these models use assumptions that do not accurately reflect real-world situations. In this paper, we present a new approach for analyzing such models that takes into account both random and proportionate mixing. We use a network-based approach that allows us to examine the role of different types of mixing in the spread of disease.\n",
      "\n",
      "Our results show that proportionate mixing can significantly alter the dynamics of an epidemic. In particular, it can lead to a longer duration of the epidemic and to a higher infection rate. Moreover, the network structure can affect the impact of proportionate mixing. Specifically, a well-connected network could increase the spread of disease, while a network with many disconnected sub-networks could reduce it.\n",
      "\n",
      "Finally, we propose a general framework for analyzing stochastic SIR epidemic models that incorporates both random and proportionate mixing. Our approach is based on the use of dynamical network models. We demonstrate the effectiveness of our framework by applying it to several different types of epidemics. Our results suggest that our framework has broad applicability and can improve the accuracy of SIR epidemic models. 1 into PostgreSQL...\n",
      "Inserting test sample 1697  A two-parameter extension of the density-scaled double hybrid approach of Sharkas et al. [J. Chem. Phys. 134, 064113 (2011)] is presented. It is based on the explicit treatment of a fraction of multideterminantal exact exchange. The connection with conventional double hybrids is made when neglecting density scaling in the correlation functional as well as second-order corrections to the density. In this context, the fraction ac of second-order M{\\o}ller-Plesset (MP2) correlation energy is not necessarily equal to the square of the fraction ax of Hartree-Fock exchange. More specifically, it is shown that ac \\leq ax2, a condition that conventional semi-empirical double hybrids actually fulfill. In addition, a new procedure for calculating the orbitals, which has a better justification than the one routinely used, is proposed. Referred to as {\\lambda}1 variant, the corresponding double hybrid approximation has been tested on a small set consisting of H2, N2, Be2, Mg2, and Ar2. Three conventional double hybrids (B2-PLYP, B2GP-PLYP, and PBE0-DH) have been considered. Potential curves obtained with {\\lambda}1- and regular double hybrids can, in some cases, differ significantly. In particular, for the weakly bound dimers, the {\\lambda}1 variants bind systematically more than the regular ones, which is an improvement in many but not all cases. Including density scaling in the correlation functionals may of course change the results significantly. Moreover, optimized effective potentials based on a partially-interacting system could also be used to generate proper orbitals.\n",
      "\n",
      "Work is currently in progress in those directions. 0 into PostgreSQL...\n",
      "Inserting test sample 1698  Double-hybrid density-functional theory has become increasingly popular as it allows for the improvement of computational accuracy without the high cost associated with traditional methods. Recently, efforts have been made to formulate two-parameter double-hybrid density-functionals to provide even greater accuracy in predictions of molecular properties. \n",
      "\n",
      "In this paper, we present a rigorous formulation of two-parameter double-hybrid density-functionals based on ab initio calculations and statistical analysis. Our approach involves the selection of suitable model systems and the optimization of functional parameters using a database of experimental and high-level theoretical data.\n",
      "\n",
      "Using this methodology, we demonstrate the improved accuracy and transferability of our two-parameter double-hybrid density-functional through comparisons with experimental measurements and other density functional methods. We also investigate the performance of these functionals in a range of chemical environments, including main-group elements and transition metals.\n",
      "\n",
      "Furthermore, we explore the potential of our two-parameter double-hybrid density-functional in predicting reaction energies and reaction barriers. Our results demonstrate that the use of two-parameter double-hybrid density-functional can significantly improve the accuracy of reaction barrier predictions.\n",
      "\n",
      "In summary, our rigorous formulation of two-parameter double-hybrid density-functionals provides a valuable tool for the accurate and efficient prediction of molecular properties. The improved accuracy and transferability of this method makes it a promising avenue for future research in the field of computational chemistry. 1 into PostgreSQL...\n",
      "Inserting test sample 1699  Let $\\Sigma$ be a surface with a symplectic form, let $\\phi$ be a symplectomorphism of $\\Sigma$, and let $Y$ be the mapping torus of $\\phi$. We show that the dimensions of moduli spaces of embedded pseudoholomorphic curves in $\\R\\times Y$, with cylindrical ends asymptotic to periodic orbits of $\\phi$ or multiple covers thereof, are bounded from above by an additive relative index. We deduce some compactness results for these moduli spaces.\n",
      "\n",
      "This paper establishes some of the foundations for a program with Michael Thaddeus, to understand the Seiberg-Witten Floer homology of $Y$ in terms of such pseudoholomorphic curves. Analogues of our results should also hold in three dimensional contact homology. 0 into PostgreSQL...\n",
      "Inserting test sample 1700  We establish a new inequality for the index of embedded pseudoholomorphic curves in symplectizations of contact manifolds. This lower bound is given in terms of a count of intersections of the curve with a certain collection of hypersurfaces. The main result has applications to the study of closed characteristics on Reeb flows. We prove the inequality using a homotopy argument that involves perturbing curves and constructing geometric objects associated to each intersection point. Finally, we give examples showing that the inequality is sharp in some cases, which suggests that it may be a useful tool in future investigations of symplectic and contact geometry. 1 into PostgreSQL...\n",
      "Inserting test sample 1701  The cosmic infrared background (CIB) contains emissions accumulated over the entire history of the Universe, including from objects inaccessible to individual telescopic studies. The near-IR (~1-10 mic) part of the CIB, and its fluctuations, reflects emissions from nucleosynthetic sources and gravitationally accreting black holes (BHs). If known galaxies are removed to sufficient depths the source-subtracted CIB fluctuations at near-IR can reveal sources present in the first-stars-era and possibly new stellar populations at more recent times. This review discusses the recent progress in this newly emerging field which identified, with new data and methodology, significant source-subtracted CIB fluctuations substantially in excess of what can be produced by remaining known galaxies. The CIB fluctuations further appear coherent with unresolved cosmic X-ray background (CXB) indicating a very high fraction of BHs among the new sources producing the CIB fluctuations. These observations have led to intensive theoretical efforts to explain the measurements and their properties. While current experimental configurations have limitations in decisively probing these theories, their potentially remarkable implications will be tested in the upcoming CIB measurements with the ESA's Euclid dark energy mission. We describe the goals and methodologies of LIBRAE (Looking at Infrared Background Radiation with Euclid), a NASA-selected project for CIB science with Euclid, which has the potential for transforming the field into a new area of precision cosmology. 0 into PostgreSQL...\n",
      "Inserting test sample 1702  The cosmic near-infrared background radiation has long been a subject of study for astronomers. This radiation, which is believed to be a remnant of the earliest stages of the universe's formation, provides important clues about the evolution of the cosmos. Recently, researchers have been investigating anisotropies in this radiation, or variations in its intensity from different directions. These anisotropies offer new insights into the distribution of matter in space and the nature of the processes that created the early universe.\n",
      "\n",
      "In this paper, we report on a study of cosmic near-infrared background radiation anisotropies using data collected by the Euclid mission. We explain how we analyzed this data to create a map of the anisotropies and discuss the implications of our findings. Our analysis shows that there are significant variations in the intensity of the cosmic near-infrared background radiation across different regions of the sky. We also find that these anisotropies are consistent with the predictions of current models of cosmic evolution, which suggest that the universe formed from the \"bottom-up,\" with small structures merging to form larger ones over time.\n",
      "\n",
      "Overall, our study provides new insights into the physical processes that shaped the universe in its infancy. It demonstrates the power of near-infrared observations to reveal the hidden structures of the cosmos and lays the groundwork for future research into this important field. 1 into PostgreSQL...\n",
      "Inserting test sample 1703  We have systematically investigated the mass spectrum and rearrangement decay properties of the exotic tetraquark states with four different flavors using a color-magnetic interaction model. Their masses are estimated by assuming that the $X(4140)$ is a $cs\\bar{c}\\bar{s}$ tetraquark state and their decay widths are obtained by assuming that the Hamiltonian for decay is a constant.\n",
      "\n",
      "According to the adopted method, we find that the most stable states are probably the isoscalar $bs\\bar{u}\\bar{d}$ and $cs\\bar{u}\\bar{d}$ with $J^P=0^+$ and $1^+$. The width for most unstable tetraquarks is about tens of MeVs, but that for unstable $cu\\bar{s}\\bar{d}$ and $cs\\bar{u}\\bar{d}$ can be around 100 MeV. For the $X(5568)$, our method cannot give consistent mass and width if it is a $bu\\bar{s}\\bar{d}$ tetraquark state. For the $I(J^P)=0(0^+),0(1^+)$ double-heavy $T_{bc}=bc\\bar{u}\\bar{d}$ states, their widths can be several MeVs. 0 into PostgreSQL...\n",
      "Inserting test sample 1704  Quarks, the building blocks of protons and neutrons, can also form exotic configurations known as tetraquarks. In this study, we investigate tetraquark states composed of four different flavors of quarks. Using lattice quantum chromodynamics simulations, we determine the spectrum, or range of possible masses, and study the properties of these tetraquark states. Our results show that the tetraquark states form a spectrum with a similar pattern to that of the meson and baryon states. Furthermore, we find evidence for a phenomenon known as rearrangement decay, where the tetraquark states transform into two mesons or a meson and a baryon. This decay mechanism provides insights into the nature of tetraquark states and highlights their potential as probes of the strong interaction. Our study contributes to the ongoing exploration of the rich and complex world of quark configurations, paving the way for future experimental investigations. 1 into PostgreSQL...\n",
      "Inserting test sample 1705  A parameter estimation problem for a class of semilinear stochastic evolution equations is considered. Conditions for consistency and asymptotic normality are given in terms of growth and continuity properties of the nonlinear part.\n",
      "\n",
      "Emphasis is put on the case of stochastic reaction-diffusion systems.\n",
      "\n",
      "Robustness results for statistical inference under model uncertainty are provided. 0 into PostgreSQL...\n",
      "Inserting test sample 1706  This paper presents a novel approach for drift estimation in stochastic reaction-diffusion systems. By exploiting the properties of the underlying Markov processes, we derive an estimator which can accurately estimate the drift parameters even when the observation noise is large. The effectiveness of the proposed method is demonstrated through numerical simulations. 1 into PostgreSQL...\n",
      "Inserting test sample 1707  A definition of entropy via the Kolmogorov algorithmic complexity is discussed. As examples, we show how the meanfield theory for the Ising model, and the entropy of a perfect gas can be recovered. The connection with computations are pointed out, by paraphrasing the laws of thermodynamics for computers. Also discussed is an approach that may be adopted to develop statistical mechanics using the algorithmic point of view. 0 into PostgreSQL...\n",
      "Inserting test sample 1708  This paper analyzes the relationship between entropy and perpetual computers. Entropy is a measure of the amount of disorder or randomness in a system, while perpetual computers are machines which operate indefinitely without the need for an external energy source. By examining the thermodynamic properties of such computers, we explore the theoretical limits on their efficiency and durability. Our findings have implications for the design and engineering of future computing systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1709  Wide accessibility of imaging and profile sensors in modern industrial systems created an abundance of high-dimensional sensing variables. This led to a a growing interest in the research of high-dimensional process monitoring.\n",
      "\n",
      "However, most of the approaches in the literature assume the in-control population to lie on a linear manifold with a given basis (i.e., spline, wavelet, kernel, etc) or an unknown basis (i.e., principal component analysis and its variants), which cannot be used to efficiently model profiles with a nonlinear manifold which is common in many real-life cases. We propose deep probabilistic autoencoders as a viable unsupervised learning approach to model such manifolds. To do so, we formulate nonlinear and probabilistic extensions of the monitoring statistics from classical approaches as the expected reconstruction error (ERE) and the KL-divergence (KLD) based monitoring statistics. Through extensive simulation study, we provide insights on why latent-space based statistics are unreliable and why residual-space based ones typically perform much better for deep learning based approaches. Finally, we demonstrate the superiority of deep probabilistic models via both simulation study and a real-life case study involving images of defects from a hot steel rolling process. 0 into PostgreSQL...\n",
      "Inserting test sample 1710  In this research, we propose a novel approach to profile monitoring based on deep probabilistic autoencoders, which can handle high-dimensional and nonlinear data. The proposed method uses a deep neural network to capture the underlying nonlinear relationship between the input variables, and a probabilistic autoencoder to model the distribution of the data in the latent space. The proposed method has several advantages over existing methods. It does not require prior knowledge of the data distribution and can effectively handle missing data. The performance of the proposed method is evaluated using simulation and real data experiments. The results show that the proposed method has a high detection rate and a low false alarm rate, outperforming existing methods in terms of detection accuracy and computational efficiency. The proposed method has a wide range of applications in industries such as manufacturing, healthcare, and finance, where detecting anomalies in high-dimensional and nonlinear data is critical for ensuring quality and safety. 1 into PostgreSQL...\n",
      "Inserting test sample 1711  We have compared the kinematics and metallicity of the main sequence binary and single {\\it uvby} F stars from the {\\it HIPPARCOS} catalog to see if the populations of these stars originate from the same statistical ensemble. The velocity dispersions of the known unresolved binary F stars have been found to be dramatically smaller than those of the single F stars. This suggests that the population of these binaries is, in fact, younger than that of the single stars, which is further supported by the difference in metal abundance: the binaries turn out to be, on average, more metal rich than the single stars. So, we conclude that the population of these binaries is indeed {\\it younger} than that of the single F stars. Comparison of the single F stars with the C binaries (binary candidates identified in Suchkov & McMaster 1999) has shown, on the other hand, that the latter stars are, on average, {\\it older} than the single F stars. We suggest that the age difference between the single F stars, known unresolved binaries, and C binaries is associated with the fact that stellar evolution in a binary systems depends on the binary components mass ratio and separation, with these parameters being statistically very different for the known binaries and C binaries (e.g., mostly substellar secondaries in C binaries versus stellar secondaries in known binaries).\n",
      "\n",
      "In general we conclude that the populations of known binaries, C binaries, and single F stars do not belong to the same statistical ensemble. The implications of the discovered age difference between these populations along with the corresponding differences in kinematics and metallicity should be important not only for understanding the evolution of stars but also for the history of star formation and the evolution of the local galactic disk. 0 into PostgreSQL...\n",
      "Inserting test sample 1712  The HIPPARCOS dataset has been analyzed to examine age differences between populations of binary and single F~type stars. F~type stars are well recognized for their widespread occurrence as both single and binary stars. Previous studies have suggested that binary F~type stars may possess a shorter lifespan than single stars. This hypothesis, however, has never been tested directly on a large sample of F~type stars, until now. \n",
      "\n",
      "Our team has used the HIPPARCOS dataset of F~type stars to examine the age difference between binary and single stars. We have identified a total of 345 candidate F~type binary systems which we carefully parsed through with a rigorous statistical analysis. After a careful observation, we have found that the stars in binaries are, on average, younger than their single star counterparts. We have concluded that this difference is likely due to the evolution of the stars over time. It's possible that the stars in binary systems evolve more slowly or that they consume larger amounts of their hydrogen fuel, causing them to age more rapidly. \n",
      "\n",
      "Further study revealed that the age difference between single and binary stars was more pronounced for less massive stars. This could indicate that less massive stars have a greater dependence on companionship, with binary merger playing a greater role in their evolution. We thus conclude that the age difference between binary and single stars is a common feature of F~type stars and correlates with their mass.\n",
      "\n",
      "In conclusion, our study has provided new insights into the age differences between populations of binary and single F~type stars. The results suggest that binary stars may possess a shorter lifespan than single stars, and that this effect is more pronounced for less massive stars. This discovery represents a significant step forward in our understanding of stellar evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 1713  To study the crucial range of Galactocentric distances between 12 and 16 kpc, where little information is available, we have obtained VI CCD imaging of Berkeley 20 and BVI CCD imaging of Berkeley 66 and Tombaugh 2, three distant, old open clusters. Using the synthetic colour magnitude diagram (CMD) technique with three types of evolutionary tracks of different metallicities, we have determined age, distance, reddening and indicative metallicity of these systems. The CMD of Be 20 is best reproduced by stellar models with a metallicity about half of solar (Z=0.008 or 0.01), in perfect agreement with high resolution spectroscopic estimates. Its age is between 5 and 6 Gyr from stellar models with overshooting and between 4.3 and 4.5 Gyr from models without it. The distance modulus from the best fitting models is always (m-M)0=14.7 (corresponding to a Galactocentric radius of about 16 kpc), and the reddening E(B-V) ranges between 0.13 and 0.16. A slightly lower metallicity (Z ~ 0.006) appears to be more appropriate for Be 66. This cluster is younger, (age of 3 Gyr), and closer, (m-M)0=13.3 (i.e., at 12 kpc from the Galactic centre), than Be 20, and suffers from high extinction, 1.2 < E(B-V) < 1.3, variable at the 2-3 per cent level. Finally, the results for To 2 indicate that it is an intermediate age cluster, with an age of about 1.4 Gyr or 1.6-1.8 Gyr for models without and with overshooting, respectively. The metallicity is about half of solar (Z=0.006 to 0.01), in agreement with spectroscopic determinations. The distance modulus is (m-M)0=14.5, implying a distance of about 14 kpc from the Galactic centre; the reddening E(B-V) is 0.31-0.4, depending on the model and metallicity, with a preferred value around 0.34. 0 into PostgreSQL...\n",
      "Inserting test sample 1714  Old open clusters located in the Milky Way galaxy serve as excellent tracers for the Galactic metallicity gradient. In this study, we focused on three such clusters, namely Berkeley 20, Berkeley 66, and Tombaugh 2, to better understand the spatial distribution of metallicity in the Milky Way. By analyzing the chemical abundance patterns of these open clusters, we aimed to investigate the variation of metallicity over different galactocentric distances.\n",
      "\n",
      "Our analysis was based on high-resolution spectroscopic observations of member stars in these clusters. We used spectral line synthesis techniques to derive detailed abundances of various elements such as iron, magnesium, calcium, nickel, and silicon. Our results show that all three clusters have similar Fe/H values, suggesting that they might have formed at similar epochs in the Galactic history. The abundance ratios of alpha elements to iron, like [Mg/Fe], were lower than those in the Sun, indicating early chemical enrichment of the interstellar medium.\n",
      "\n",
      "We also computed the Galactocentric distances of these clusters using Gaia DR2 parallax measurements and found that Berkeley 20 and Berkeley 66 are located closer to the Galactic center than Tombaugh 2. This suggests that the metallicity gradient in the inner regions of the Milky Way is steeper than in the outer regions. Our study further revealed that the age of Berkeley 20 is slightly younger than Berkeley 66 and Tombaugh 2, possibly due to the ongoing star formation activity in the Galactic disk. \n",
      "\n",
      "In conclusion, this study highlights the importance of old open clusters as tracers of the Galactic metallicity gradient, demonstrating that Berkeley 20, Berkeley 66, and Tombaugh 2 contribute to our understanding of the spatial distribution of metallicity in the Milky Way. It also emphasizes the need for spectroscopic studies of more open clusters to further unravel the complex nature of metallicity gradients in galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 1715  Nanostructured TiO2 thin films were prepared by pulsed laser deposition (PLD) on indium doped tin oxide (ITO) substrates. Results from X-ray photoelectron spectroscopy (XPS) show that Ti 2p core level peaks shift toward the lower binding energy with decrease in the buffer gas pressure (O2:Ar = 1:1). This suggests that oxygen vacancies are created under insufficient oxygen conditions. Anatase to rutile ratio is also found to be system pressure dependent. Under deposition pressure of 750 mTorr only anatase phase was observed even at 1073 K substrate temperature which is much higher that the bulk anatase to rutile phase transformation temperature. The deposited TiO2 thin films were fabricated as photoanodes for photoelectrochemical (PEC) studies. PEC measurements on TiO2 photoanodes show that the flatband potential (Vfb) increases by 0.088 eV on absolute vacuum energy scale (AVS) with decrease in the deposition pressure, from 750 to 250 mTorr at 873K. The highest incident photon to current conversion efficiency [IPCE(lambda)] of 2.5 to 6 % was obtained from the thin films prepared at substrate temperature of 873K.\n",
      "\n",
      "Combining the results from XPS and PEC studies, we conclude that the deposition pressure affects the concentration of the oxygen vacancies which changes the electronic structure of the TiO2. With reference to photoelectrochemical catalytic performance, our results suggest that it is possible to adjust the Fermi energy level and structure of TiO2 thin films by controlling the buffer gas pressure and temperature to align the energy of the flatband potential (Vfb) with respect to specific redox species in the electrolyte. 0 into PostgreSQL...\n",
      "Inserting test sample 1716  This paper discusses the advanced photocatalytic activity of titanium dioxide (TiO2) thin films fabricated by pulsed laser deposition (PLD) technique. The TiO2 thin films were deposited on silicon (Si) substrates, and their microstructure, morphology, and composition were analyzed using various characterization techniques. The results show that the PLD method has produced a well-crystallized anatase-rich phase with excellent photocatalytic activity. The photocatalytic activity of the TiO2 thin films was further evaluated by the degradation of methylene blue (MB) dye under UV light irradiation. The results demonstrate a highly efficient photocatalytic performance with a degradation rate as high as 99.6% after only 90 minutes of irradiation. The photocatalytic activity of the TiO2 thin films was also evaluated based on the analysis of active species during the photocatalytic degradation process. The results indicate that photogenerated holes and hydroxyl radicals, rather than superoxide radicals, play a dominant role in the photocatalytic degradation of MB. Finally, the underlying mechanisms for the high photocatalytic activity of the TiO2 thin films were discussed based on the theoretical notions of surface plasmon resonance and quantum confinement effects. This study provides valuable insights into the production of advanced photocatalytic materials for environmental pollution control and other practical applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1717  This paper presents a BeppoSAX observation of NGC 7582 made during 1998 November and an optical spectrum taken in 1998 October. The new X-ray data reveal a previously unknown hard X-ray component in NGC 7582, peaking close to 20 keV. Rapid variability is observed with correlated changes in the 5-10 and 13-60 keV bands indicating that a single continuum component, produced by the active nucleus, provides the dominant flux across both bands. Changes in the nuclear X-ray flux appear unrelated to the gradual decline in optical flux noted since the high-state in 1998 July. The X-ray continuum is attenuated by a thick absorber of N_H ~ 1.6 x 10E24 cm^-2 covering ~60% of the nucleus, plus a screen with N_H ~ 1.4 x 10E23 cm^-2 covering the entire nucleus. Comparison of the BeppoSAX and ASCA spectra shows an increase in the full screen by N_H ~7 x 10E22 cm^-2 since 1994, confirming the absorption variability found by Xue et al. The increase in soft X-ray flux between 1994 and 1998 is consistent with the appearance of holes in the full screen producing some clear lines-of-sight to the broad-line-region. 0 into PostgreSQL...\n",
      "Inserting test sample 1718  The paper presents results of an analysis of the BeppoSAX observation of NGC 7582, with a particular focus on the constraints on the X-ray absorber. The study of the absorbing material in active galactic nuclei (AGNs) is important for understanding the physical processes involved in the formation of such structures, as well as their evolution. The spectrum of NGC 7582 was obtained through a series of measurements taken over a period of time, with the aim of characterizing the X-ray absorbing material and its properties.\n",
      "\n",
      "Our findings suggest that the X-ray absorber in NGC 7582 is highly ionized and that it is located close to the inner edge of the accretion disk. The data indicates that the absorber has a complex structure with multiple cloudlets of varying density, and that it has a covering factor of almost 90 percent. Furthermore, the analysis shows that the absorber has a column density of about 10^23 cm^-2, which is consistent with previous estimates. The results presented in this paper will help to further our understanding of the physical processes taking place in AGNs, and improve our ability to model such systems more accurately. 1 into PostgreSQL...\n",
      "Inserting test sample 1719  Let $G$ be a finite group and exp$(G)$ = lcm$\\{$ord$(g)$$\\mid$$g \\in G \\}$. A finite unordered sequence of terms from $G$, where repetition is allowed, is a product-one sequence if its terms can be ordered such that their product equals the identity element of $G$. We denote by $\\mathsf s (G)$ (or $\\mathsf E (G)$ respectively) the smallest integer $\\ell$ such that every sequence of length at least $\\ell$ has a product-one subsequence of length $\\exp (G)$ (or $|G|$ respectively). In this paper, we provide the exact values of $\\mathsf s (G)$ and $\\mathsf E (G)$ for Dihedral and Dicyclic groups and we provide explicit characterizations of all sequences of length $\\mathsf s (G) - 1$ (or $\\mathsf E (G) - 1$ respectively) having no product-one subsequence of length $\\exp (G)$ (or $|G|$ respectively). 0 into PostgreSQL...\n",
      "Inserting test sample 1720  Erd\\H{o}s-Ginzburg-Ziv inverse theorems have become a fundamental concept in modern number theory and combinatorics. In this paper, we investigate the inverse theorems specifically for dihedral and dicyclic groups. We present new results concerning the minimal sum-free sets in such groups and analyze their structural properties. We also establish the relationship between the Erd\\H{o}s-Ginzburg-Ziv constant and the properties of these groups. Our analysis leads to the development of new techniques and methodology for studying sum-free sets in generalized cyclic groups. Additionally, we provide concrete examples and computational evidence to support our findings. Our results have important implications in cryptography, coding theory, and other areas of mathematics. We anticipate that this work will stimulate further research and inspire new applications of the Erd\\H{o}s-Ginzburg-Ziv theorem. 1 into PostgreSQL...\n",
      "Inserting test sample 1721  We use LAMOST DR4 M giants combined with Gaia DR2 proper motions and ALLWISE photometry to obtain an extremely pure sample of Sagittarius (Sgr) stream stars. Using TiO5 and CaH spectral indices as an indicator, we selected out a large sample of M giant stars from M dwarf stars in LAMOST DR4 spectra.\n",
      "\n",
      "Considering the position, distance, proper motion and the angular momentum distribution, we obtained 164 pure Sgr stream stars. We find that the trailing arm has higher energy than the leading arm in same angular momentum. The trailing arm we detected extends to a heliocentric distance of $\\sim 130$ kpc at $\\tilde\\Lambda_{\\odot}\\sim 170^{\\circ}$, which is consistent with the feature found in RR Lyrae in \\citet{sgr2017}. Both of these detections of Sgr, in M giants and in RR Lyrae, imply that the Sgr stream may contain multiple stellar populations with a broad metallicity range. 0 into PostgreSQL...\n",
      "Inserting test sample 1722  The Sagittarius stream, a remnant of an accreted satellite galaxy, has long been a focal point of interest in the field of Galactic archeology. In this paper, we present a comprehensive study of the Sagittarius stream using luminous, evolved M giants from the fourth data release of LAMOST and astrometric data from the second data release of Gaia. By cross-matching the two datasets, we are able to trace the stream over a vast stretch of the Galactic disk for the first time. Through distance estimates and kinematic analysis, we confirm the stream's existence in the region monitored by LAMOST and suggest some possible extensions. Additionally, we conduct a detailed comparison of our results with both theoretical models and previous observations. Our findings provide a significant contribution towards improving our understanding of the complex structure and history of the Milky Way. 1 into PostgreSQL...\n",
      "Inserting test sample 1723  The rules governing cell division and differentiation are central to understanding the mechanisms of development, aging and cancer. By utilising inducible genetic labelling, recent studies have shown that the clonal population in transgenic mouse epidermis can be tracked in vivo. Drawing on these results, we explain how clonal fate data may be used to infer the rules of cell division and differentiation underlying the maintenance of adult murine tail-skin. We show that the rates of cell division and differentiation may be evaluated by considering the long-time and short-time clone fate data, and that the data is consistent with cells dividing independently rather than synchronously. Motivated by these findings, we consider a mechanism for cancer onset based closely on the model for normal adult skin. By analysing the expected changes to clonal fate in cancer emerging from a simple two-stage mutation, we propose that clonal fate data may provide a novel method for studying the earliest stages of the disease. 0 into PostgreSQL...\n",
      "Inserting test sample 1724  Epidermis, the outermost layer of skin, is a self-renewing tissue that serves as a barrier against the environment. The maintenance of this tissue through the regulated division of cells is a key aspect of epidermal biology. The kinetics of cell division in the epidermis must be tightly controlled to ensure proper tissue architecture and homeostasis. This review summarizes current knowledge of the cellular and molecular events underlying the proliferation of epidermal cells and their differentiation into specialized skin cell types. We discuss the mechanisms that govern the entry and exit of cells from the cell cycle, the signaling pathways that regulate the balance between proliferation and differentiation, and the importance of spatial and temporal regulation of cell division in the establishment and maintenance of the epidermal architecture. A comprehensive understanding of epidermal cell division kinetics is essential for the development of therapies targeting skin diseases and for advances in tissue engineering. 1 into PostgreSQL...\n",
      "Inserting test sample 1725  Using the pseudomode method, we evaluate exactly time-dependent entanglement for two independent qubits, each coupled to a non-Markovian structured environment. Our results suggest a possible way to control entanglement sudden death by modifying the qubit-pseudomode detuning and the spectrum of the reservoirs. Particularly, in environments structured by a model of a density-of-states gap which has two poles, entanglement trapping and prevention of entanglement sudden death occur in the weak-coupling regime. 0 into PostgreSQL...\n",
      "Inserting test sample 1726  This paper investigates the entanglement sudden death (ESD) phenomenon in the context of band gaps. We demonstrate that ESD can occur between two qubits embedded in bandgap materials, as the leakage of the qubitsâ€™ excitation to the band continues. To prevent ESD, control resources should increase with the width of the band gap. Our theoretical predictions are based on a Markovian master equation approach, demonstrating that ESD depends on the spectral density of the reservoir. 1 into PostgreSQL...\n",
      "Inserting test sample 1727  We study displaced signatures of sneutrino pairs potentially emerging at the Large Hadron Collider (LHC) in a Next-to-Minimal Supersymmetric Standard Model supplemented with right-handed neutrinos triggering a Type-I seesaw mechanism.\n",
      "\n",
      "We show how such signatures can be established through a heavy Higgs portal in the presence of both leptonic and hadronic decays of the sneutrinos. We finally illustrate how the Yukawa parameters of the latter can be extracted from fitting kinematical quantities pertaining to the corresponding two displaced vertices, thereby characterising the dynamics of the underlying mechanism of neutrino mass generation. We show our numerical results for the case of both the current and High-Luminosity LHC. 0 into PostgreSQL...\n",
      "Inserting test sample 1728  In this paper, we present a novel method for extracting the neutrino Yukawa parameters using the displaced vertices of sneutrinos. We consider a supersymmetric extension of the standard model, where the neutrino mass is generated by the Yukawa interactions with the sneutrinos. By analyzing the decay patterns of sneutrinos, we are able to determine the values of the neutrino Yukawa parameters. Our approach offers a promising avenue for probing the nature of neutrino masses, which remain one of the most intriguing mysteries of particle physics. We demonstrate the effectiveness of our technique through a detailed simulation study and discuss the implications of our results for future experimental searches. 1 into PostgreSQL...\n",
      "Inserting test sample 1729  We present a novel method for computing slow manifolds and their fast fibre bundles in geometric singular perturbation problems. This coordinate-independent method is inspired by the parametrisation method introduced by Cabr\\'e, Fontich and de la Llave. By iteratively solving a so-called conjugacy equation, our method simultaneously computes parametrisations of slow manifolds and fast fibre bundles, as well as the dynamics on these objects, to arbitrarily high degrees of accuracy. We show the power of this top-down method for the study of systems with multiple (i.e., three or more) timescales. In particular, we highlight the emergence of hidden timescales and show how our method can uncover these surprising multiple timescale structures. We also apply our parametrisation method to several reaction network problems. 0 into PostgreSQL...\n",
      "Inserting test sample 1730  Geometric singular perturbation theory has been an important tool in study of dynamical systems. One issue that arises with singularly perturbed systems is the presence of multiple timescales, which often makes the analysis more challenging. In this paper, we investigate the use of parametrization method to effectively address this problem. We establish the efficacy of the method through application to several examples. We also present a thorough analysis of the behavior of the system in different limits. We show that the parametrization method can be a powerful approach to deal with singularly perturbed systems with multiple timescales. Our work can contribute to a better understanding and characterization of the dynamics in various real-world problems. 1 into PostgreSQL...\n",
      "Inserting test sample 1731  In this work, a new class of slowly rotating black hole solutions in dilaton gravity has been obtained where dilaton field is coupled with nonlinear Maxwell invariant. The background space-time is a stationary axisymmetric geometry.\n",
      "\n",
      "Here, it has been shown that the dilaton potential can be written in the form of generalized three Liouville-type potentials. In the presence of this three Liouville-type dilaton potential, the asymptotic behavior of the obtained solutions are neither flat nor (A)dS. One bizarre property of the electric field is that the electric field goes to zero when $r\\rightarrow 0$ and diverges at $r\\rightarrow\\infty$. We show the validity of the first law of thermodynamics in thermodynamic investigations. The local and global thermodynamical stability are investigated through the use of heat capacity and Gibbs free-energy. Also, the bounded, phase transition and the Hawking-Page phase transition points as well as the ranges of black hole stability have been shown in the corresponding diagrams. From these diagrams, we can say that the presence of the dilaton field makes the solutions to be locally stable near origin and vanishes the global stability of our solutions. In final thermodynamics analysis, we obtain the Smarr formula for our solution. We will show that the presence of dilaton field brings a new term in the Smarr formula.\n",
      "\n",
      "Also, we find that the dilaton field makes the black hole(AdS) mass to decrease for every fix values of S(entropy). 0 into PostgreSQL...\n",
      "Inserting test sample 1732  This paper explores a new black hole solution in dilaton gravity inspired by power-law electrodynamics. The authors begin by presenting the action of the theory and derive the field equations. They then analyze the behavior of the dilaton field in the near-horizon region and find that it diverges as the event horizon is approached. This leads to the introduction of a new boundary condition at the horizon, which is shown to be consistent with the full set of field equations. With this new boundary condition, the authors are able to solve the field equations analytically and find a new black hole solution.\n",
      "\n",
      "The solution is characterized by several interesting features, including a non-zero dilaton field that changes asymptotically, and a power-law behavior of the Maxwell field near the horizon. The authors further investigate the thermodynamic properties of the black hole, and show that it satisfies the first law of thermodynamics. They also compute the entropy of the black hole using Wald's formula and demonstrate that it has the correct functional form.\n",
      "\n",
      "Overall, this paper contributes to our understanding of dilaton gravity and power-law electrodynamics, and opens up new avenues for research in this area. The new black hole solution presented here provides a novel example of the interplay between gravitational and electromagnetic fields, and its implications for the nature of space-time warrant further investigation. 1 into PostgreSQL...\n",
      "Inserting test sample 1733  We present cross-correlation analyses of simultaneous X-ray and near-infrared (near-IR) observations of the microquasar GRS 1915+105 during relativistic jet-producing epochs (X-ray class $\\alpha$ and $\\beta$). While previous studies have linked the large-amplitude IR flares and X-ray behaviors to jet formation in these states, our new analyses are sensitive to much lower-amplitude IR variability, providing more sensitive probes of the jet formation process. The X-ray to IR cross-correlation function (CCF) shows significant correlations which vary in form between the different X-ray states. During low/hard dips in both classes, we find no significant X-ray/IR correlation. During high-variability epochs, we find consistently significant correlations in both $\\alpha$ and $\\beta$ classes, but with strong differences in the CCF structure.\n",
      "\n",
      "The high-variability $\\alpha$ CCF shows strong anti-correlation between X-ray/IR, with the X-ray preceding the IR by $\\sim$ 13 $\\pm$ 2s. The high-variability $\\beta$ state shows time-variable CCF structure, which is statistically significant but without a clearly consistent lag. Our simulated IR light curves, designed to match the observed CCFs, show variably-flickering IR emission during the class $\\beta$ high-variability epoch, while the class $\\alpha$ can be fit by IR flickering with frequencies in the range 0.1 to 0.3 Hz, strengthening $\\sim$10 s after every X-ray subflare. We interpret these features in the context of the X-ray-emitting accretion disk and IR emission from relativistic jet formation in GRS 1915+105, concluding that the CCF analysis places the origin in a synchrotron-emitting relativistic compact jet at a distance from the compact object of $\\sim$0.02AU. 0 into PostgreSQL...\n",
      "Inserting test sample 1734  The Galactic microquasar GRS1915+105 is an established anomaly within the universe of X-ray binaries. Its highly variable emission in every electromagnetic band, including variability patterns on timescales as short as milliseconds, have made it an object of intense scrutiny by scientists hoping to understand the underlying mechanisms governing these phenomena in Compact Object formations. This research paper presents a comprehensive study on the behavior of relativistic jets that are formed by GRS 1915+105. Our study investigates the time-lagged relationships between X-ray and IR light curves emitted by the object, to understand the correlation between the fluctuation of matter inside the quasar and how these fluctuations trigger the formation of jets.\n",
      "\n",
      "In order to map the variability of GRS 1915+105, we combined extensive infrared observations from the United Kingdom Infrared Telescope with simultaneous hard X-ray data by the Rossi X-Ray Timing Explorer. By applying rigorous time-domain techniques, we establish a previously unseen fine structure in these multi-wavelength light curves and demonstrate that a simple linear cross-correlation analysis is insufficient to explain them fully. Our results require that we apply more complex models that take into account unique relativistic and non-linear effects as well as the variability within the accretion disc. Lastly, our study provides previously unknown details into the physical conditions necessary for jet launching, and informs our understanding on the formation of relativistic jets in other black hole binary systems.\n",
      "\n",
      "In summary, our research presents an extensive time-domain analysis of the GRS 1915+105 quasar and its strong X-ray and IR flares. Through rigorous analysis, we discovered a time-lagged relationship that explains how fluctuations in the matter fueling the quasar can trigger the formation of relativistic jets. 1 into PostgreSQL...\n",
      "Inserting test sample 1735  Measuring electron transport (ETp) across proteins in the solid-state offers a way to study electron transfer (ET) mechanism(s) that minimizes solvation effects on the process. Solid state ETp is sensitive to any static (conformational) or dynamic (vibrational) changes in the protein. Our macroscopic measurement technique extends the use of ETp meas-urements down to low temperatures and the concomitant lower current densities, because the larger area still yields measurable currents. Thus, we reported previously a surprising lack of temperature-dependence for ETp via the blue copper protein azurin (Az), from 80K till denaturation, while ETp via apo-(Cu-free) Az was found to be temperature de-pendent \\geq 200K. H/D substitution (deuteration) can provide a potentially powerful means to unravel factors that affect the ETp mechanism at a molecular level. Therefore, we measured and report here the kinetic deuterium isotope effect (KIE) on ETp through holo-Az as a function of temperature (30-340K). We find that deuteration has a striking effect in that it changes ETp from temperature independent to temperature dependent above 180K. This change is expressed in KIE values between 1.8 at 340K and 9.1 at \\leq 180K. These values are particularly remarkable in light of the previously reported inverse KIE on the ET in Az in solution. The high values that we obtain for the KIE on the ETp process across the protein monolayer are consistent with a transport mechanism that involves through-(H-containing)-bonds of the {\\beta}-sheet structure of Az, likely those of am-ide groups. 0 into PostgreSQL...\n",
      "Inserting test sample 1736  This study investigates the effects of deuteration on electron transport through blue copper proteins using azurin as a model system. Deuteration, which involves the substitution of hydrogen with deuterium, has long been used in the study of protein dynamics due to its effects on protein flexibility and hydrogen bonding. Solid-state electron transport measurements were performed on samples of native and deuterated azurin using a bespoke scanning tunneling microscope apparatus, facilitating temperature-dependent measurements to determine how the protein's conductivity is affected by deuteration-induced changes in protein structure. Our results show that deuteration leads to significant alterations in electron transport through azurin in the solid state, likely due to changes in resonance energies and molecular orbitals resulting from the presence of deuterium. These findings highlight the importance of considering the effects of subtle structural changes, such as those induced by deuterium incorporation, on the electronic properties of biomolecules. Moreover, solid-state measurements open up the possibility for studying electron transport in other proteins and biological systems, making use of bespoke experimental setups and techniques. Overall, this work provides insight into the principles governing charge transport through proteins and highlights the importance of considering dynamic effects in protein-metal interactions. 1 into PostgreSQL...\n",
      "Inserting test sample 1737  We have developed the Smoothed Bandpass Calibration (SBC) method and the best suitable scan pattern to optimize radio spectroscopic observations. Adequate spectral smoothing is applied to the spectrum toward OFF-source blank sky adjacent to a target source direction for the purpose of bandpass correction.\n",
      "\n",
      "Because the smoothing process reduces noise, the integration time for OFF-source scans can be reduced keeping the signal-to-noise ratio. Since the smoothing is not applied to ON-source scans, the spectral resolution for line features is kept. An optimal smoothing window is determined by bandpass flatness evaluated by Spectral Allan Variance (SAV). An efficient scan pattern is designed to the OFF-source scans within the bandpass stability timescale estimated by Time-based Allan Variance (TAV). We have tested the SBC using the digital spectrometer, VESPA, on the VERA Iriki station. For the targeted noise level of 5e-4 as a ratio to the system noise, the optimal smoothing window was 32 - 60 ch in the whole bandwidth of 1024 ch, and the optimal scan pattern was designed as a sequence of 70-s ON + 10-s OFF scan pairs. The noise level with the SBC was reduced by a factor of 1.74 compared with the conventional method.\n",
      "\n",
      "The total telescope time to achieve the goal with the SBC was 400 s, which was 1/3 of 1200 s required by the conventional way. Improvement in telescope time efficiency with the SBC was calculated as 3x, 2x and 1.3x for single-beam, dual-beam, and on-the-fly (OTF) scans, respectively. The SBC works to optimize scan patterns for observations from now, and also works to improve signal-to-noise ratios of archival data if ON- and OFF-source spectra are individually recorded, though the efficiency depends on the spectral stability of the receiving system. 0 into PostgreSQL...\n",
      "Inserting test sample 1738  In radio spectroscopy, the accuracy of calibration is critical for obtaining high-quality data. One method for calibration is known as the bandpass calibration. This technique involves the characterization of the instrument's frequency response by comparing it to a known standard. However, the spectral features of many sources are often broad and may not be fully characterized by a bandpass calibration.\n",
      "\n",
      "To address this limitation, we propose a new calibration technique, called smoothed bandpass calibration (SBC). The SBC method applies a smoothing function to the data, which effectively generates a bandpass with a broader width than what is obtained with traditional bandpass calibration. With this method, the frequency response of the instrument is characterized more accurately, especially in cases where the spectral features are broad.\n",
      "\n",
      "We demonstrate the effectiveness of the SBC method through a series of experiments using simulated data. Our results show that the SBC method yields a significant improvement in calibration accuracy compared to traditional bandpass calibration techniques, especially when the spectral lines are broad. Furthermore, we show that the SBC method requires only a small increase in observation time, making it a practical solution for radio spectroscopy observations.\n",
      "\n",
      "In summary, our study presents a new approach to calibration in radio spectroscopy that builds on the traditional bandpass calibration technique. The smoothed bandpass calibration method improves calibration accuracy and is particularly effective in cases where the spectral features of the source are broad. Our approach provides valuable insights for improving the calibration process in radio spectroscopy and has important implications for future research in this field. 1 into PostgreSQL...\n",
      "Inserting test sample 1739  We consider orbit configuration spaces $C_n^G(S)$, where $S$ is a surface obtained out of a closed orientable surface $\\bar{S}$ by removing a finite number of points (eventually none) and $G$ is a finite group acting freely continuously on $S$. We prove that the fibration $\\pi_{n,k} : C_{n}^G(S) \\to C_k^G(S)$ obtained by projecting on the first $k$ coordinates is a rational fibration. As a consequence, the space $C_{n}^G(S)$ has a Sullivan model $A_{n,k}=\\Lambda V_{C_k^G(S)}\\otimes \\Lambda V_{C_{n-k}^G(S_{G,k})}$ fitting in a cdga sequence: $\\Lambda V_{C_k^G(S)}\\to A_{n,k} \\to \\Lambda V_{C_{n-k}^G(S_{G,k})},$ where $\\Lambda V_X$ denotes the minimal model of $X$, and $C_{n-k}^G(S_{G,k})$ is the fiber of $\\pi_{n,k}$. We show that this model is minimal except for some cases when $S\\simeq S^2$ and compute in all the cases the higher $\\psi$-homotopy groups (related to the generators of the minimal model) of $C_n^G(S)$. We deduce from the computation that $C_n^G(S)$ having finite Betti numbers is a rational $K(\\pi,1)$, i.e its minimal model and $1$-minimal model are the same (or equivalently the $\\psi$-homotopy space vanishes in degree grater then $2$), if and only if $S$ is not homeomorphic to $S^2$. In particular, for $S$ not homeomorphic to $S^2$, the minimal model (isomorphic to the $1$-minimal model) is entirely determined by the Malcev Lie algebra of $\\pi_1 C_n^G(S)$. When $A_{n,k}$ is minimal, we get an exact sequence of Malcev Lie algebras $0\\to L_{C_{n-k}^G(S_{G,k})}\\to L_{C_{n}^G(S)}\\to L_{C_k^G(S)}\\to 0$, where $L_X$ is the Malcev Lie algebra of $\\pi_1X$. For $S \\varsubsetneq \\bar{S}=S^2$ and $G$ acting by orientation preserving homeomorphism, we prove that the cohomology ring of $C_n^G(S)$ is Koszul, and that for some of these spaces the minimal model can be obtained out of a Cartan-Chevally-Eilenberg construction applied to graded Lie algebra computed in an earlier work. 0 into PostgreSQL...\n",
      "Inserting test sample 1740  This paper studies the models of orbit configuration spaces of surfaces by examining their geometric properties and analyzing their topological structures. Specifically, we focus on the relationship between the symplectic geometry of such spaces and the representation theory of their symplectic fundamental groups. By doing so, we hope to provide a deeper insight into the geometric properties and topological behavior of such spaces. We also present important results on the deformation theory of orbit configuration spaces and their associated moduli spaces, which enable us to understand the space of solutions to important equations governing these spaces. In particular, we explore the role of certain geometric structures like KÃ¤hler and symplectic structures in the topology and geometry of these spaces. To achieve our objectives, we use a wide range of mathematical tools and techniques, including algebraic geometry, representation theory, and differential geometry.\n",
      "\n",
      "Our work is motivated by recent developments in diverse areas of mathematics, including mathematical physics, geometric analysis, and low-dimensional topology. Our approach builds on a growing body of literature on the topology and geometry of orbit configuration spaces, and aims at contributing to a deeper understanding of their algebraic and geometric properties. Our main findings include the explicit computation of the symplectic cohomology of certain orbit configuration spaces, the identification of their fundamental groups, and the explicit construction of their KÃ¤hler structures.\n",
      "\n",
      "Overall, our research provides a comprehensive study of the models of orbit configuration spaces of surfaces. Our results have implications for a wide range of applications, including mathematical physics and topology, and we hope that they will inspire further research in this exciting and rapidly evolving area of mathematics. 1 into PostgreSQL...\n",
      "Inserting test sample 1741  We study a mobile facility (MF) fleet sizing, routing, and scheduling problem with random demand. Specifically, we seek to find the number of MFs to use in a given service region over a specified planning horizon and the route and schedule for each selected MF. The demand for MF service at each demand node in each period is random. The distributions of the demands are unknown. To address uncertainty and distributional ambiguity, we propose and analyze two distributionally robust MF routing and scheduling (DMFRS) models. These models seek to find MF fleet sizing, routing, and scheduling decisions that minimize the fixed cost of establishing the MF fleet and maximum expected transportation and unmet demand costs over all possible demand distributions residing within an ambiguity set. In the first model, we use a moment-based ambiguity set. In the second model, we use an ambiguity set that incorporates all distributions within a 1-Wasserstein distance from a reference distribution. To solve DMFRS models, we propose a decomposition-based algorithm and derive lower bound and two families of symmetry breaking inequalities to strengthen the master problem and speed up convergence. We extend DMFRS models using a mean conditional value-at-risk criterion to model the decision maker's risk-averse attitude.\n",
      "\n",
      "Finally, we present extensive computational experiments comparing the operational and computational performance of the proposed distributionally robust models and a stochastic programming model and drive insights into DMFRS. 0 into PostgreSQL...\n",
      "Inserting test sample 1742  In this paper, we consider a stochastic mobile facility routing and scheduling problem under uncertain supply and demand. To address the challenge of optimizing solutions to such dynamic and complex problems without precise probability information, we propose a distributionally robust optimization approach. Specifically, we use moment-based ambiguity sets to capture our uncertainty around probability distributions of the random parameters involved in this problem. We then develop a mathematical programming model that incorporates these ambiguity sets into the objective function to minimize a long-term expected cost. We provide a worst-case analysis of our model by considering the largest possible deviation from our uncertain probability distributions, and show that our approach provides a robust optimization strategy. Furthermore, we propose an enhanced version of the model that includes additional constraints on the expected number of visited customer locations and proves robust to uncertainty around supply and demand distributions. Experimental results on real-world instances demonstrate the effectiveness of our approach in providing high-quality solutions under uncertainty. Overall, this paper contributes to the literature by addressing the crucial issue of stochasticity in mobile facility routing and scheduling problems and providing effective optimization approaches for decision-makers and practitioners. 1 into PostgreSQL...\n",
      "Inserting test sample 1743  Noisy Intermediate-Scale Quantum (NISQ) computers are entering an era in which they can perform computational tasks beyond the capabilities of the most powerful classical computers, thereby achieving \"Quantum Supremacy\", a major milestone in quantum computing. NISQ Supremacy requires comparison with a state-of-the-art classical simulator. We report HPC simulations of hard random quantum circuits (RQC), which have been recently used as a benchmark for the first experimental demonstration of Quantum Supremacy, sustaining an average performance of 281 Pflop/s (true single precision) on Summit, currently the fastest supercomputer in the World. These simulations were carried out using qFlex, a tensor-network-based classical high-performance simulator of RQCs. Our results show an advantage of many orders of magnitude in energy consumption of NISQ devices over classical supercomputers. In addition, we propose a standard benchmark for NISQ computers based on qFlex. 0 into PostgreSQL...\n",
      "Inserting test sample 1744  In an effort to explore the uncharted territories of quantum computing, this study presents a simulation consisting of 10 billion quantum circuits. With a computational capacity of 281 Pflop/s, the simulation enables the observation of quantum supremacy, the point at which quantum computers outperform classical computers in certain tasks. The output of the simulation exhibited characteristics indicative of quantum supremacy, reaffirming the potential of quantum computing to revolutionize scientific research. Additionally, the results provide a foundation for further research into the boundaries and capabilities of quantum computing. The presented simulation highlights the importance of continued investment and innovation in quantum computing, as it stands to offer unprecedented advancements in fields such as cryptography, materials science, and drug discovery. 1 into PostgreSQL...\n",
      "Inserting test sample 1745  In the hypothesis that the 5.4m binary RXJ0806.3+1527 consists of a low mass helium white dwarf (donor) transferring mass towards its more massive white dwarf companion (primary), we consider as possible donors white dwarfs which are the result of common envelope evolution occurring when the helium core mass of the progenitor giant was still very small (~ 0.2Msun), so that they are surrounded by a quite massive hydrogen envelope (~1/100Msun or larger), and live for a very long time supported by proton--proton burning. Mass transfer from such low mass white dwarfs very probably starts during the hydrogen burning stage, and the donor structure will remain dominated by the burning shell until it loses all the hydrogen envelope and begins transferring helium.\n",
      "\n",
      "We model mass transfer from these low mass white dwarfs, and show that the radius of the donor decreases while they shed the hydrogen envelope. This radius behavior, which is due to the fact that the white dwarf is not fully degenerate, has two important consequences on the evolution of the binary: 1) the orbital period decreases, with a timescale consistent with the period decrease of the binary RXJ0806.3+1527; 2) the mass transfer rate is a factor of about 10 smaller than from a fully degenerate white dwarf, easing the problem connected with the small X-ray luminosity of this object. The possibility that such evolution describes the system RXJ0806.3+1527 is also consistent with the possible presence of hydrogen in the optical spectrum of the star, whose confirmation would become a test of the model. 0 into PostgreSQL...\n",
      "Inserting test sample 1746  This research paper presents a thorough analysis of the closest double degenerate system RXJ0806.3+1527 and its decreasing period. Double degenerate (DD) systems, where two white dwarfs are in orbit around each other, are crucial to understanding the evolution of close binary stars. RXJ0806.3+1527 is particularly interesting as it is the closest DD system known, making it an ideal target for study. \n",
      "\n",
      "Our observations were carried out using the Gran Telescopio Canarias (GTC) and the Keck Observatory, with additional data from the Gemini Observatory. We obtained a total of 12 new spectra of the system, which allowed us to determine its spectroscopic orbital period. We also measured the radial velocity amplitude and the projected rotational velocity of both components.\n",
      "\n",
      "Our results show that the period of RXJ0806.3+1527 is decreasing, which is consistent with it being driven by gravitational wave radiation. This finding supports the prediction of general relativity and provides further evidence for the existence of gravitational waves.\n",
      "\n",
      "Finally, we used our observations to construct a three-dimensional model of the system. Our model shows that the two white dwarfs are of similar mass and that they are in a nearly edge-on orientation. This configuration explains the weaker eclipses observed in the light curve.\n",
      "\n",
      "In conclusion, our analysis of RXJ0806.3+1527 provides a unique insight into the dynamics of DD systems and offers important constraints for future theoretical studies. Our work demonstrates the power of spectroscopic observations and highlights the continued value of studying close binary stars. 1 into PostgreSQL...\n",
      "Inserting test sample 1747  A search for exclusive or semi-exclusive photon pair production, pp to p(*) + photon pair + p(*) (where p(*) stands for a diffractively-dissociated proton), and the observation of exclusive and semi-exclusive electron pair production, pp to p(*) + ee + p(*), in proton-proton collisions at sqrt(s) = 7 TeV, are presented. The analysis is based on a data sample corresponding to an integrated luminosity of 36 inverse picobarns recorded by the CMS experiment at the LHC at low instantaneous luminosities. Candidate photon pair or electron pair events are selected by requiring the presence of two photons or a positron and an electron, each with transverse energy ET > 5.5 GeV and pseudorapidity abs(eta) < 2.5, and no other particles in the region abs(eta) < 5.2. No exclusive or semi-exclusive diphoton candidates are found in the data. An upper limit on the cross section for the reaction pp to p(*) + photon pair + p(*), within the above kinematic selections, is set at 1.18 pb at 95% confidence level. Seventeen exclusive or semi-exclusive dielectron candidates are observed, with an estimated background of 0.85 +/- 0.28 (stat.) events, in agreement with the QED-based prediction of 16.3 +/- 1.3 (syst.) events. 0 into PostgreSQL...\n",
      "Inserting test sample 1748  This paper reports the results of a study on photon and electron pair production in proton-proton collisions at a center-of-mass energy of 7 TeV. The search focuses on identifying exclusive or semi-exclusive events, which are characterized by the absence or presence of additional particles aside from the pair. The exclusive process involves the exchange of a quasi-real photon between the colliding protons, while the semi-exclusive process involves the emission of additional particles in association with the pair. The data were collected with the CMS detector at the CERN LHC and correspond to an integrated luminosity of 36 inverse picobarns. The study comprises a detailed analysis of the photon and electron kinematics, as well as the investigation of potential backgrounds and systematic uncertainties. The results show clear evidence of exclusive and semi-exclusive electron pair production, as well as indications of exclusive and semi-exclusive photon pair production. These findings provide valuable insights into the nature of photon and electron interactions in high-energy proton-proton collisions, and contribute towards a better understanding of the underlying physics processes. 1 into PostgreSQL...\n",
      "Inserting test sample 1749  We conducted a comprehensive study on the shell structure of the Cygnus Loop using 41 observation data obtained by the Suzaku and the XMM-Newton satellites.\n",
      "\n",
      "To investigate the detailed plasma structure of the Cygnus Loop, we divided our fields of view into 1042 box regions. From the spectral analysis, the spectra obtained from the limb of the Loop are well fitted by the single-component non-equilibrium ionization plasma model. On the other hand, the spectra obtained from the inner regions are well fitted by the two-component model. As a result, we confirmed that the low-temperature and the high-temperature components originated from the surrounding interstellar matter (ISM) and the ejecta of the Loop, respectively.\n",
      "\n",
      "From the best-fit results, we showed a flux distribution of the ISM component. The distribution clearly shows the limb-brightening structure, and we found out some low-flux regions. Among them, the south blowout region has the lowest flux. We also found other large low-flux regions at slightly west and the northeast from the center. We estimated the former thin shell region to be 1.3 degrees in diameter and concluded that there exists a blowout along the line of sight in addition to the south blowout. We also calculated the emission measure distribution of the ISM component and showed that the Cygnus Loop is far from the result obtained by a simple Sedov evolution model. From the results, we support that the Cygnus Loop originated from a cavity explosion.\n",
      "\n",
      "The emission measure distribution also suggests that the cavity-wall density is higher in the northeast than that in the southwest. These results suggest that the thickness of the cavity wall surrounding the Cygnus Loop is not uniform. 0 into PostgreSQL...\n",
      "Inserting test sample 1750  The Cygnus Loop is a supernova remnant located in the constellation Cygnus, and it is visible from Earth with the naked eye. In recent years, the shell structure of the Cygnus Loop has become a popular subject of study, particularly the line-of-sight component of its shell structure. This paper presents new observations and analyses of the line-of-sight shell structure of the Cygnus Loop, obtained through the use of high-resolution X-ray imaging and spectroscopy.\n",
      "\n",
      "The data reveals a complex network of filaments throughout the shell structure of the Cygnus Loop, with different regions of the shell exhibiting a range of morphologies and characteristics. We find that the X-ray emission from the Cygnus Loop is dominated by a few large filaments, which have widths ranging from a few arc seconds to several arc minutes. These filaments show significant variation in their brightness and emission properties, with some being brighter and more clearly defined than others.\n",
      "\n",
      "The line-of-sight structure of the Cygnus Loop also shows evidence of hydrodynamic instabilities, such as Kelvin-Helmholtz instabilities and Rayleigh-Taylor instabilities. These instabilities are believed to play a significant role in shaping the overall morphology of the shell structure, and we discuss the implications of our data for understanding these processes.\n",
      "\n",
      "Finally, we compare our findings to previous studies of the Cygnus Loop and discuss the broader implications of our results for supernova remnant research more generally. Our findings support the importance of detailed observations of the line-of-sight shell structure of supernova remnants, and highlight the potential of advanced imaging and spectroscopic techniques for studying these systems in greater detail. 1 into PostgreSQL...\n",
      "Inserting test sample 1751  The fireball of -10 mag was observed over Poland on February 20, 2004 at 18:54 UT. Except many visual observations the event was caught by two photographic stations: one in the Czech Republic and one in Poland. A description, ground track map, atmospheric trajectory and orbital data for the fireball are presented. 0 into PostgreSQL...\n",
      "Inserting test sample 1752  The EN200204 Laskarzew fireball is a rare and significant occurrence, which warrants closer study. This research paper aims to trace the trajectory and orbit of the fireball through analyses of eyewitness accounts, photographic and video evidence, and measurements of its path through the atmosphere. The findings shed light on the origins of the fireball and its potential implications. 1 into PostgreSQL...\n",
      "Inserting test sample 1753  Video captioning combines video understanding and language generation.\n",
      "\n",
      "Different from image captioning that describes a static image with details of almost every object, video captioning usually considers a sequence of frames and biases towards focused objects, e.g., the objects that stay in focus regardless of the changing background. Therefore, detecting and properly accommodating focused objects is critical in video captioning. To enforce the description of focused objects and achieve controllable video captioning, we propose an Object-Oriented Non-Autoregressive approach (O2NA), which performs caption generation in three steps: 1) identify the focused objects and predict their locations in the target caption; 2) generate the related attribute words and relation words of these focused objects to form a draft caption; and 3) combine video information to refine the draft caption to a fluent final caption. Since the focused objects are generated and located ahead of other words, it is difficult to apply the word-by-word autoregressive generation process; instead, we adopt a non-autoregressive approach. The experiments on two benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness of O2NA, which achieves results competitive with the state-of-the-arts but with both higher diversity and higher inference speed. 0 into PostgreSQL...\n",
      "Inserting test sample 1754  In video captioning, non-autoregressive models have been increasingly used recently due to their ability to generate captions efficiently. Nevertheless, such models tend to suffer from a lack of control over the generated captions. This paper introduces an Object-Oriented Non-Autoregressive approach for Controllable Video Captioning, or O2NA for short, which addresses this issue. The proposed method utilizes visual object features and incorporates key information in the generation process, allowing for more fine-grained control over the generated captions. With this model, object information is used as a guide in the decoding process to ensure that each generated word is relevant to the specific objects in the frame. In order to optimize the model's performance, a multi-task training objective is used to maximize the captioning accuracy while minimizing the control loss. Experimental results on benchmark datasets show that O2NA outperforms state-of-the-art models in terms of controllability while maintaining overall captioning performance. The proposed approach has the potential to improve the quality and efficiency of video captioning, broadening its applications in fields such as video retrieval and video description. 1 into PostgreSQL...\n",
      "Inserting test sample 1755  The quartet of galaxies NGC 7769, 7770, 7771 and 7771A is a system of interacting galaxies. Close interaction between galaxies caused characteristic morphological features: tidal arms and bars, as well as an induced star formation. In this study, we performed the Fabry-Perot scanning interferometry of the system in Ha line and studied the velocity fields of the galaxies. We found that the rotation curve of NGC 7769 is weakly distorted. The rotation curve of NGC 7771 is strongly distorted with the tidal arms caused by direct flyby of NGC 7769 and flyby of a smaller neighbor NGC 7770. The rotation curve of NGC 7770 is significantly skewed because of the interaction with the much massive NGC 7771. The rotation curves and morphological disturbances suggest that the NGC 7769 and NGC 7771 have passed the first pericenter stage, however, probably the second encounter has not happened yet. Profiles of surface brightness of NGC 7769 have a characteristic break, and profiles of color indices have a minimum at a radius of intensive star formation induced by the interaction with NGC 7771. 0 into PostgreSQL...\n",
      "Inserting test sample 1756  This research paper aims to study the HÎ± velocity fields and galaxy interaction amongst a quartet of galaxies: NGC 7769, NGC 7770, NGC 7771, and NGC 7771A. We present the results of our observational and analysis work, which were conducted using the Optical System for Imaging and low-intermediate-Resolution Integrated Spectroscopy (OSIRIS) at the Gran Telescopio Canarias (GTC). Our analysis of the HÎ± velocity fields reveals a complex kinematic structure within the region, including asymmetric and multiple velocity peaks. Based on our findings, we suggest that the galaxies are interacting, and we propose a model that can explain the observed kinematics. The model includes tidal tails, a bridge, and a possible merger between NGC 7769 and NGC 7771A. Our results also reveal a potential counter-rotating disk in NGC 7771, which suggests past interactions with other galaxies. These findings provide important insights into the mechanisms of galaxy interaction and their impact on the kinematics of galaxies in close proximity. 1 into PostgreSQL...\n",
      "Inserting test sample 1757  Let $\\mathrm{XY_{L,T}}$ denote the class of countably infinite $L$-structures that satisfy the axioms $T$ and in which all homomorphisms of type X (these could be homomorphisms, monomorphisms, or isomorphisms) between finite substructures of $M$ are restrictions of an endomorphism of $M$ of type Y (for example, an automorphism or a surjective endomorphism). Lockett and Truss introduced 18 such morphism-extension classes for relational structures. For a given pair $L,T$, however, two or more morphism-extension properties may define the same class of structures.\n",
      "\n",
      "In this paper, we establish all equalities and inequalities between morphism-extension classes of countable (undirected, loopless) graphs. 0 into PostgreSQL...\n",
      "Inserting test sample 1758  In this paper, we introduce the poset of morphism-extension classes of countable graphs - a partial order that captures the inherent complexity of graph morphisms. We show that this poset is a lattice under certain conditions and provide explicit examples of classes of graphs for which it is a lattice. Furthermore, using a combination of model-theoretic and combinatorial techniques, we investigate the properties of this poset, including its distributivity and semimodularity, yielding a range of novel results and open problems. Our findings extend the scope of graph theory and provide a deeper understanding of the structure of countable graphs. 1 into PostgreSQL...\n",
      "Inserting test sample 1759  We develop a model for the evolution of economic entities within a geographical type of framework. On a square symmetry lattice made of three (economic) regions, firms, described by a scalar fitness, are allowed to move, adapt, merge or create spin-offs under predetermined rules, in a space and time dependent economic environment. We only consider here one timely variation of the ''external economic field condition''. For the firm fitness evolution we take into account a constraint such that the disappearance of a firm modifies the fitness of nearest neighboring ones, as in Bak-Sneppen population fitness evolution model. The concentration of firms, the averaged fitness, the regional distribution of firms, and fitness for different time moments, the number of collapsed, merged and new firms as a function of time have been recorded and are discussed. Also the asymptotic values of the number of firms present in the three regions together with their average fitness, as well as the number of respective births and collapses in the three regions are examined. It appears that a sort of $critical$ selection pressure exists. A power law dependence, signature of self-critical organization is seen in the birth and collapse asymptotic values for a high selection pressure only. A lack of self-organization is also seen at region borders. 0 into PostgreSQL...\n",
      "Inserting test sample 1760  This paper proposes a model for the evolution of macroeconomic variables in stable, regionally-dependent economic fields. The model builds on previous work in the field of economic geography, which has shown that location-specific factors play a significant role in shaping economic outcomes.\n",
      "\n",
      "Our model takes a bottom-up approach, starting with individual firms and their interactions with the local environment. We then simulate the interactions between neighboring firms, which allows us to capture emergent properties of the local economy, such as agglomeration effects and supply chain dynamics.\n",
      "\n",
      "We apply our model to a case study of a stable economic region, finding that it captures the observed patterns of economic evolution over time. Our results highlight the importance of including regionally-specific factors in macroeconomic models, and suggest new avenues for policy interventions.\n",
      "\n",
      "Overall, our model provides a valuable tool for understanding the evolution of macroeconomic variables in real-world economies. It offers a way to explore the mechanisms behind local economic dynamics, and can inform policy decisions aimed at promoting economic growth and stability. 1 into PostgreSQL...\n",
      "Inserting test sample 1761  Power dissipation and energy consumption have become one of the most important problems in the design of processors today. This is especially true in power-constrained environments, such as embedded and mobile computing. While lowering the operational voltage can reduce power consumption, there are limits imposed at design time, beyond which hardware components experience faulty operation. Moreover, the decrease in feature size has led to higher susceptibility to process variations, leading to reliability issues and lowering yield. However, not all computations and all data in a workload need to maintain 100% fidelity. In this paper, we explore the idea of employing functional or storage units that let go the conservative guardbands imposed on the design to guarantee reliable execution. Rather, these units exhibit Elastic Fidelity, by judiciously lowering the voltage to trade-off reliable execution for power consumption based on the error guarantees required by the executing code. By estimating the accuracy required by each computational segment of a workload, and steering each computation to different functional and storage units, Elastic Fidelity Computing obtains power and energy savings while reaching the reliability targets required by each computational segment. Our preliminary results indicate that even with conservative estimates, Elastic Fidelity can reduce the power and energy consumption of a processor by 11-13% when executing applications involving human perception that are typically included in modern mobile platforms, such as audio, image, and video decoding. 0 into PostgreSQL...\n",
      "Inserting test sample 1762  Modern devices are becoming more and more energy-efficient, but there is still a pervasive need for further optimization. One approach that has gained traction is the elastic fidelity scheme, which trades off computational accuracy for energy savings. This framework affords control over accuracy requirements, which can significantly reduce energy consumption while maintaining acceptable performance levels. However, determining the optimal balance between accuracy and energy savings remains challenging, particularly for complex and heterogeneous systems. In this paper, we present a comprehensive investigation of the elastic fidelity approach and its effectiveness in various scenarios. We propose a method of adaptive accuracy control through a fine-grained adjustment of hardware parameters, such as voltage and frequency, without sacrificing performance. Through experimental evaluation, we illustrate the efficacy of our approach, demonstrating energy savings of up to 50% while maintaining the desired accuracy level in various benchmarks and real-world applications. Furthermore, we present a theoretical analysis of our technique's effectiveness in different settings, and we provide insights into the potential for further exploration and development of this approach. Our results indicate that the elastic fidelity strategy is a promising approach to energy reduction in modern computer systems, with broad applicability across various levels of the system stack. 1 into PostgreSQL...\n",
      "Inserting test sample 1763  In this paper we study current accumulations in 3D \"tilted\" nulls formed by a folding of the spine and fan. A non-zero component of current parallel to the fan is required such that the null's fan plane and spine are not perpendicular.\n",
      "\n",
      "Our aims are to provide valid magnetohydrostatic equilibria and to describe the current accumulations in various cases involving finite plasma pressure.To create our equilibrium current structures we use a full, non-resistive, magnetohydrodynamic (MHD) code so that no reconnection is allowed. A series of experiments are performed in which a perturbed 3D tilted null relaxes towards an equilibrium via real, viscous damping forces. Changes to the initial plasma pressure and to magnetic parameters are investigated systematically.An initially tilted fan is associated with a non-zero Lorentz force that drives the fan and spine to collapse towards each other, in a similar manner to the collapse of a 2D X-point. In the final equilibrium state for an initially radial null with only the current perpendicular to the spine, the current concentrates along the tilt axis of the fan and in a layer about the null point with a sharp peak at the null itself. The continued growth of this peak indicates that the system is in an asymptotic regime involving an infinite time singularity at the null. When the initial tilt disturbance (current perpendicular to the spine) is combined with a spiral-type disturbance (current parallel to the spine), the final current density concentrates in three regions: one on the fan along its tilt axis and two around the spine, above and below the fan. The increased area of current accumulation leads to a weakening of the singularity formed at the null. The 3D spine-fan collapse with generic current studied here provides the ideal setup for non-steady reconnection studies. 0 into PostgreSQL...\n",
      "Inserting test sample 1764  We present a study of the dynamical relaxation of coronal magnetic fields in 3D tilted nulls using the magnetohydrodynamics (MHD) model. The coronal magnetic field is one of the most important components in understanding and modeling the behavior of the Sun's corona. We investigate the relaxation of tangled magnetic field configurations induced by a process known as nanoflare heating. The MHD model is used to simulate the time evolution of the magnetic field in a region surrounding a 3D tilted null. The null region is inherently unstable and prone to reconnection, allowing for the magnetic energy to be released and transformed into heat and kinetic energy. We evaluate the effect of the tilt angle on the formation of current channels and their role in initiating the reconnection process. Our simulations demonstrate that increasing the tilt angle of the null causes the current channels to become narrower and more intense, leading to increased heating of the corona. We compare our 3D MHD simulations to observations from the Atmospheric Imaging Assembly (AIA) on board the Solar Dynamics Observatory (SDO), and we find good agreement between the modeled and observed magnetic field structures. Our study provides new insights into the relaxation of coronal magnetic fields in 3D tilted nulls, and highlights the importance of understanding the role of nanoflares in the heating and dynamical evolution of the solar corona. 1 into PostgreSQL...\n",
      "Inserting test sample 1765  Chiral effective lagrangians may differ in their prediction of meson-nucleon scattering amplitudes off-meson-mass-shell, but must yield identical S-matrix elements. We argue that the effective meson mass in nuclear matter obtained from chiral effective lagrangians is also unique. Off-mass-shell amplitudes obtained using the PCAC choice of pion field must therefore not be viewed as fundamental constraints on the dynamics, the determination of the effective meson mass in nuclear matter or the possible existence of meson condensates in the ground state of nuclear matter. This hypothesis is borne out by a calculation of the effective mass in two commonly employed formulations of chiral perturbation theory which yield different meson-nucleon scattering amplitudes off-meson-mass-shell. 0 into PostgreSQL...\n",
      "Inserting test sample 1766  In this paper, we investigate the interactions of S-wave mesons with nucleons in nuclear matter using chiral effective lagrangians. By studying the equations of motion for meson fields and nucleon operators, we derive the meson self-energy and effective mass in nuclear matter. Our results suggest that the meson mass decreases significantly in nuclear matter due to meson-nucleon interactions. We also find that the strength of the meson-nucleon interaction is dependent on the chiral symmetry breaking scale. Our findings have implications for understanding the properties of nuclear matter and the behavior of mesons in dense environments. Overall, our study highlights the importance of chiral effective lagrangians in investigating meson-nucleon interactions in nuclear matter. 1 into PostgreSQL...\n",
      "Inserting test sample 1767  The Hubbard model is a prototype for strongly correlated many-particle systems, including electrons in condensed matter and molecules, as well as for fermions or bosons in optical lattices. While the equilibrium properties of these systems have been studied in detail, the nonequilibrium dynamics following a strong non-perturbative excitation only recently came into the focus of experiments and theory. It is of particular interest how the dynamics depend on the coupling strength and on the particle number and whether there exist universal features in the time evolution. Here, we present results for the dynamics of finite Hubbard clusters based on a selfconsistent nonequilibrium Green functions (NEGF) approach invoking the generalized Kadanoff--Baym ansatz (GKBA). We discuss the conserving properties of the GKBA with Hartree--Fock propagators in detail and present a generalized form of the energy conservation criterion of Baym and Kadanoff for NEGF. Furthermore, we demonstrate that the HF-GKBA cures some artifacts of prior two-time NEGF simulations. Besides, this approach substantially speeds up the numerical calculations and thus presents the capability to study comparatively large systems and to extend the analysis to long times allowing for an accurate computation of the excitation spectrum via time propagation. Our data obtained within the second Born approximation compares favorably with exact diagonalization results (available for up to 13 particles) and are expected to have predictive capability for substantially larger systems in the weak coupling limit. 0 into PostgreSQL...\n",
      "Inserting test sample 1768  The study of Hubbard nanoclusters has proved to be a promising area of research in condensed matter physics, offering a rich landscape of complex electronic and structural phenomena. Here we present a comprehensive investigation on the behavior of these systems when subjected to nonequilibrium conditions. Our approach combines simulations using ab initio density functional theory and kinetic Monte Carlo algorithms to investigate the kinetics of a small set of Hubbard nanoclusters with different sizes and geometries, as well as their structural properties under non-equilibrium conditions. In particular, we explore the effects of external stimuli, such as temperature and pressure, as well as the interaction with surrounding environments, on the behavior of Hubbard nanoclusters. Our results show that far from equilibrium, the systems exhibit significant deviations from their equilibrium counterparts, with the emergence of a range of novel phenomena, such as charge fluctuations and structural reconfigurations. Overall, our study demonstrates the potential of Hubbard nanoclusters as new platforms for the development of advanced materials with tailored properties, and paves the way for future investigations in this exciting and rapidly evolving field. 1 into PostgreSQL...\n",
      "Inserting test sample 1769  We consider a minimalistic dynamic model of the idiotypic network of B-lymphocytes. A network node represents a population of B-lymphocytes of the same specificity (idiotype), which is encoded by a bitstring. The links of the network connect nodes with complementary and nearly complementary bitstrings, allowing for a few mismatches. A node is occupied if a lymphocyte clone of the corresponding idiotype exists, otherwise it is empty. There is a continuous influx of new B-lymphocytes of random idiotype from the bone marrow.\n",
      "\n",
      "B-lymphocytes are stimulated by cross-linking their receptors with complementary structures. If there are too many complementary structures, steric hindrance prevents cross-linking. Stimulated cells proliferate and secrete antibodies of the same idiotype as their receptors, unstimulated lymphocytes die.\n",
      "\n",
      "Depending on few parameters, the autonomous system evolves randomly towards patterns of highly organized architecture, where the nodes can be classified into groups according to their statistical properties. We observe and describe analytically the building principles of these patterns, which allow to calculate number and size of the node groups and the number of links between them. The architecture of all patterns observed so far in simulations can be explained this way. A tool for real-time pattern identification is proposed. 0 into PostgreSQL...\n",
      "Inserting test sample 1770  Randomly evolving idiotypic networks (REINs) have recently come to the forefront of research in immunology as numerous studies have highlighted the crucial role they play in regulating the immune system. REINs are a class of adaptive networks that represent the diverse repertoire of B cells and their corresponding antibodies. In this paper, we investigate the structural properties and architecture of REINs using computational models and tools. Specifically, we analyze the effects of random evolution on network growth, the distribution of node degree, and connectivity patterns. Our results reveal that REINs exhibit significant plasticity and robustness, with highly variable network structures that are sensitive to mutation rates and selection pressures. We also explore the self-organization and emergent properties of REINs, which underpin their ability to generate diverse and effective immune responses. Our study sheds new light on the complex interplay between evolutionary dynamics and network topology in the immune system, and has implications for designing novel therapies for immune-related diseases. 1 into PostgreSQL...\n",
      "Inserting test sample 1771  Building a full understanding of the complex magnetism in the rare-earth pyrochlore iridates, A2Ir2O7, is an ongoing issue in condensed matter physics.\n",
      "\n",
      "The possibility of two interpenetrating and interacting frustrated magnetic pyrochlore sublattices, connected with the strong spin-orbit coupling of the 5d Ir4+ ion, lends itself to a wide array of potential electronic and magnetic states. In this paper, we present longitudinal field and zero field muSR measurements of Er2Ir2O7. The muSR response of Er2Ir2O7 is dominated by the strong fluctuating fields of the Er3+ (J = 15/2) moments, showing a peak in fitted relaxation rate, {\\lambda}_2, at T = 15 K . The muSR spectra show just weak signatures of the Ir transition, that were easily suppressed in applied longitudinal field, however full decoupling of the muons from local magnetic fields was not achieved even in applied fields up to 3 kG. 0 into PostgreSQL...\n",
      "Inserting test sample 1772  We investigated the spin dynamics of the pyrochlore iridate Er2Ir2O7 using muSR spectroscopy. Our measurements reveal a highly dynamic spin-ice-like state in zero applied field. At low temperatures, the spin dynamics exhibit well-defined precession arising from a weakly anisotropic exchange, and the out-of-equilibrium state relaxes slowly. At higher temperatures, the precession amplitude decreases, becoming incoherent with a faster relaxation rate. The characteristic temperature of the spin dynamics is found to be around 1 K, which is consistent with the energy scale set by the exchange interaction. A magnetic field is found to modify the spin dynamics remarkably, and can evolve the precession into a coherent mode, leading to a non-equilibrium steady-state. Our findings provide important insights into the spin dynamics of Er2Ir2O7 and related materials. 1 into PostgreSQL...\n",
      "Inserting test sample 1773  For vehicular networks, safety distances are important, but existing spatial models fail to characterize this parameter, especially for inter-lane communications. This work proposes a Matern hard-core processes based framework to appraise the performance of signal fractions (SF), where the hard-core distance is used to depict safety distances. By considering both semicircle and omnidirectional antennas, we derive high-accurate closed-form probability density functions of communication distances to acquire the complementary cumulative distribution function of SF. The derived expressions theoretically demonstrate that the nearest vehicle within the safety distance follows a uniform distribution and there is an upper limit for SF in terms of the transmit power. 0 into PostgreSQL...\n",
      "Inserting test sample 1774  This research paper proposes a comprehensive approach towards analyzing signal fractions and developing safety distance models for vehicle-to-vehicle (V2V) communications with a focus on inter-lane scenarios. Our methodology considers a range of parameters, including propagation loss, vehicle positioning, and channel impairments to accurately model the V2V communication link and derive safety distance metrics. Our results suggest that the proposed approach can effectively estimate the signal fractions and provide safe separation distances for V2V communication links, even in challenging inter-lane scenarios. This work has the potential to inform the design and development of V2V communication systems, contributing towards safer and more efficient transportation networks. 1 into PostgreSQL...\n",
      "Inserting test sample 1775  Bragg coherent diffraction imaging (BCDI), the well-established technique for imaging internal strain of nanoparticles, was used to image the internal compositional distribution of binary alloys in thermal equilibrium. The images experimentally obtained for Pd-Rh alloy nanoparticles are presented and discussed. The direct correspondence between the lattice strain and the compositional deviation is discussed in the derivation of the BCDI displacement field aided by illustrations. 0 into PostgreSQL...\n",
      "Inserting test sample 1776  In this study, we employ Bragg Coherent Diffraction Imaging for studying the internal compositions of Pd-Rh nanoparticles. The binary alloy nanoparticles were synthesized using co-reduction of Pd and Rh precursors, and their morphologies were characterized using High-Resolution Transmission Electron Microscopy. The experimentally obtained Bragg peak positions and intensities were used to retrieve the three-dimensional electron density maps of the nanoparticles, revealing the internal structures and compositions. 1 into PostgreSQL...\n",
      "Inserting test sample 1777  We develop conditions under which individual choices and Walrasian equilibrium prices and allocations can be exactly inferred from finite market data. First, we consider market data that consist of individual demands as prices and incomes change. Second, we show that finitely many observations of individual endowments and associated Walrasian equilibrium prices, and only prices, suffice to identify individual demands and, as a consequence, equilibrium comparative statics. 0 into PostgreSQL...\n",
      "Inserting test sample 1778  This paper introduces a novel approach for exact inference from finite market data. We present a method for inferring the underlying structure and parameters of a market from observed data. Unlike existing techniques, our method does not rely on simplifications or approximations, and is capable of handling complex, finite market scenarios. We demonstrate the effectiveness of our approach on a variety of simulation studies and real-world datasets. 1 into PostgreSQL...\n",
      "Inserting test sample 1779  We present a family of networks, expanded deterministic Apollonian networks, which are a generalization of the Apollonian networks and are simultaneously scale-free, small-world, and highly clustered. We introduce a labeling of their vertices that allows to determine a shortest path routing between any two vertices of the network based only on the labels. 0 into PostgreSQL...\n",
      "Inserting test sample 1780  In this paper, we investigate the problem of vertex labeling and routing in expanded Apollonian networks. We propose a novel algorithm that assigns unique labels to each vertex and enables an efficient routing of messages. Our approach is based on the hierarchical structure of the network and achieves high accuracy and robustness. 1 into PostgreSQL...\n",
      "Inserting test sample 1781  We demonstrate via a muon spin rotation experiment that the electronic ground state of the iridium spinel compound, CuIr$_2$S$_4$, is not the presumed spin-singlet state but a novel paramagnetic state, showing a quasistatic spin glass-like magnetism below ~100 K. Considering the earlier indication that IrS$_6$ octahedra exhibit dimerization associated with the metal-to-insulator transition below 230 K, the present result suggests that a strong spin-orbit interaction may be playing an important role in determining the ground state that accompanies magnetic frustration. 0 into PostgreSQL...\n",
      "Inserting test sample 1782  We investigate the magnetic frustration phenomenon in the CuIr$_2$S$_4$ compound through experimental and theoretical studies. We present evidence of spin glass behavior and deviation from the ideal Heisenberg spin system. Using specific heat data analysis, we discovered a highly frustrated magnetic ground state arising from competing interactions. Our results provide insight into the spin behaviors of CuIr$_2$S$_4$ and other materials with frustrated magnetism. 1 into PostgreSQL...\n",
      "Inserting test sample 1783  We derive the evolution of the infrared (IR) luminosity function (LF) over the last 4/5ths of cosmic time, using deep 24um and 70um imaging of the GOODS North and South fields. We use an extraction technique based on prior source positions at shorter wavelengths to build the 24 and 70um source catalogs. The majority (93%) of the sources have a spectroscopic (39%) or a photometric redshift (54%) and, in our redshift range of interest (i.e., 1.3<z<2.3) ~20% of the sources have a spectroscopic redshifts. To extend our study to lower 70um luminosities we perform a stacking analysis and we characterize the observed L_24/(1+z) vs L_70/(1+z) correlation. Using spectral energy distribution templates which best fit this correlation, we derive the IR luminosity of sources from their 24 and 70 um fluxes. We then compute the IR LF at z=1.55+/-0.25 and z=2.05+/-0.25. The redshift evolution of the IR LF from z=1.3 to z=2.3 is consistent with a luminosity evolution proportional to (1+z)^1.0+/-0.9 combined with a density evolution proportional to (1+z)^-1.1+/-1.5. At z~2, luminous IR galaxies (LIRGs: 10^11Lsun< LIR <10^12Lsun) are still the main contributors to the total comoving IR luminosity density (IR LD) of the Universe. At z~2, LIRGs and ultra-luminous IR galaxies (ULIRGs: 10^12Lsun< LIR) account for ~49% and ~17% respectively of the total IR LD of the Universe. Combined with previous results for galaxies at z<1.3 and assuming a constant conversion between the IR luminosity and star-formation rate (SFR) of a galaxy, we study the evolution of the SFR density of the Universe from z=0 to z=2.3. We find that the SFR density of the Universe strongly increased with redshift from z=0 to z=1.3, but is nearly constant at higher redshift out to z=2.3. As part of the online material accompanying this article, we present source catalogs at 24um and 70um for both the GOODS-North and -South fields. 0 into PostgreSQL...\n",
      "Inserting test sample 1784  This research paper investigates the dusty infrared luminosity function (LF) from redshift z=0 to z=2.3, utilizing observational data from the Spitzer telescope. The aim is to analyze how the evolution of the LF relates to the evolution of galaxies over cosmic time. By examining the relationship between LF and redshift, the study provides insights into the physical processes driving galaxy evolution.\n",
      "\n",
      "The team analyzed the LF using a sample of galaxies with 24 Î¼m flux measurements, derived from data in the Spitzer Legacy Survey. They applied a maximum likelihood method to fit the Schechter function to the LF, which describes the shape of the distribution of infrared luminosities. The results reveal that the shape of the LF evolves significantly with redshift, with the characteristic luminosity (L*) increasing by an order of magnitude from z=0 to z=2.3. Furthermore, the faint-end slope of the LF steepens at high redshifts, indicating an increase in the number of low-luminosity, heavily obscured galaxies.\n",
      "\n",
      "The study also provides insights into the relationship between the LF and the stellar mass of galaxies. The research team found that the characteristic luminosity of the LF increases with stellar mass, providing evidence for a link between star formation and dust obscuration. Additionally, they found that the faint-end slope of the LF depends on stellar mass, with lower mass galaxies exhibiting steeper slopes.\n",
      "\n",
      "Finally, the study explores the implications of the results for galaxy evolution. The evolution of the LF observed in this study is consistent with previous findings, which suggest that galaxies were more actively forming stars at earlier times. The increasing number of low-luminosity, heavily obscured galaxies at high redshifts supports the conjecture that these galaxies may be responsible for a significant fraction of cosmic star formation. \n",
      "\n",
      "In conclusion, this research paper provides a detailed analysis of the evolution of the dusty infrared luminosity function from z=0 to z=2.3 using observational data from the Spitzer telescope. The findings shed light on the physical processes driving galaxy evolution and show that the relationship between the LF and redshift can provide insights into the evolution of galaxies over cosmic time. 1 into PostgreSQL...\n",
      "Inserting test sample 1785  The COVID-19 pandemic has been damaging to the lives of people all around the world. Accompanied by the pandemic is an infodemic, an abundant and uncontrolled spreading of potentially harmful misinformation. The infodemic may severely change the pandemic's course by interfering with public health interventions such as wearing masks, social distancing, and vaccination. In particular, the impact of the infodemic on vaccination is critical because it holds the key to reverting to pre-pandemic normalcy. This paper presents findings from a global survey on the extent of worldwide exposure to the COVID-19 infodemic, assesses different populations' susceptibility to false claims, and analyzes its association with vaccine acceptance. Based on responses gathered from over 18,400 individuals from 40 countries, we find a strong association between perceived believability of misinformation and vaccination hesitancy. Additionally, our study shows that only half of the online users exposed to rumors might have seen the fact-checked information.\n",
      "\n",
      "Moreover, depending on the country, between 6% and 37% of individuals considered these rumors believable. Our survey also shows that poorer regions are more susceptible to encountering and believing COVID-19 misinformation. We discuss implications of our findings on public campaigns that proactively spread accurate information to countries that are more susceptible to the infodemic. We also highlight fact-checking platforms' role in better identifying and prioritizing claims that are perceived to be believable and have wide exposure. Our findings give insights into better handling of risk communication during the initial phase of a future pandemic. 0 into PostgreSQL...\n",
      "Inserting test sample 1786  The COVID-19 pandemic has led to a global infodemic of misinformation and rumors, which have undermined public health efforts and vaccine acceptance worldwide. This study aimed to investigate the relationship between misinformation, believability, and vaccine acceptance across 40 countries during the initial phase of the pandemic. Based on a large-scale survey of over 100,000 individuals, our findings highlight the crucial role of misinformation in shaping people's attitudes and behaviors towards vaccination. We found that people who believed in conspiracy theories or held false beliefs about COVID-19 were less likely to accept vaccines. Moreover, the credibility of the information source played a significant role in mediating the relationship between misinformation and vaccine acceptance. Specifically, people who perceived scientific experts as more trustworthy were more likely to accept vaccines despite exposure to misinformation. Our findings provide important insights for public health efforts to combat misinformation and increase vaccine acceptance. We suggest that interventions should focus on addressing the root causes of misinformation, including lack of trust in scientific institutions and poor science communication. In addition, we highlight the need for tailored communication strategies that take into account the cultural and social context of different populations. Overall, this study sheds light on the complex interplay between misinformation, trust, and vaccine acceptance, and emphasizes the urgency of addressing the COVID-19 infodemic. 1 into PostgreSQL...\n",
      "Inserting test sample 1787  We explore the dual version of Gottschalk's conjecture recently introduced by Capobianco, Kari, and Taati, and the notion of dual surjunctivity in general.\n",
      "\n",
      "We show that dual surjunctive groups satisfy Kaplansky's direct finiteness conjecture for all fields of positive characteristic. By quantifying the notions of injectivity and post-surjectivity for cellular automata, we show that the image of the full topological shift under an injective cellular automaton is a subshift of finite type in a quantitative way. Moreover we show that dual surjunctive groups are closed under ultraproducts, under elementary equivalence, and under certain semidirect products (using the ideas of Arzhantseva and Gal for the latter); they form a closed subset in the space of marked groups, fully residually dual surjunctive groups are dual surjunctive, etc. We also consider dual surjunctive systems for more general dynamical systems, namely for certain expansive algebraic actions, employing results of Chung and Li. 0 into PostgreSQL...\n",
      "Inserting test sample 1788  This paper presents a study on the concept of dual surjunctivity and its applications. Dual surjunctivity is a property of mathematical optimization problems in which the primal and dual problems both have unique solutions. The paper provides a comprehensive analysis of this concept, including its implications and conditions for guaranteeing its existence. This property has been shown to have significant impacts on the design of optimization algorithms, particularly in the area of convex optimization. Furthermore, this study discusses various applications of dual surjunctivity, including its importance in decision-making, economics, and engineering systems. Our analysis provides a complete understanding of this concept and its use in solving real-world problems. Finally, we present some future research directions on dual surjunctivity, which will likely provide opportunities for further investigation and improvement in the field. 1 into PostgreSQL...\n",
      "Inserting test sample 1789  We study the quantum dynamics of localized impurity states created by a point interaction for an electron moving in two dimensions under the influence of a perpendicular magnetic field and an in-plane weak electric field. All impurity states are unstable in presence of the electric field. Their lifetimes are computed and shown to grow in a Gaussian way as the electric field tends to zero. 0 into PostgreSQL...\n",
      "Inserting test sample 1790  We investigate the lifetimes of impurity states in crossed magnetic and electric fields. Specifically, we calculate the decay rates of impurity states using a generalized time-dependent SchrÃ¶dinger equation and multiple scaling analyses. Our results reveal that the lifetimes of impurity states strongly depend on the strength and direction of the magnetic and electric fields. These findings are significant as they provide insights into controlling the dynamics of impurity states for potential technological applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1791  We perform MHD modeling of a single bright coronal loop to include the interaction with a non-uniform magnetic field. The field is stressed by random footpoint rotation in the central region and its energy is dissipated into heating by growing currents through anomalous magnetic diffusivity that switches on in the corona above a current density threshold. We model an entire single magnetic flux tube, in the solar atmosphere extending from the high-beta chromosphere to the low-beta corona through the steep transition region. The magnetic field expands from the chromosphere to the corona. The maximum resolution is ~30 km. We obtain an overall evolution typical of loop models and realistic loop emission in the EUV and X-ray bands. The plasma confined in the flux tube is heated to active region temperatures (~3 MK) after ~2/3 hr.\n",
      "\n",
      "Upflows from the chromosphere up to ~100 km/s fill the core of the flux tube to densities above 10^9 cm^-3. More heating is released in the low corona than the high corona and is finely structured both in space and time. 0 into PostgreSQL...\n",
      "Inserting test sample 1792  In this study, we present a three-dimensional (3D) magnetohydrodynamic (MHD) modeling of twisted coronal loops in the solar atmosphere. These twisted structures, often observed in the solar corona, are believed to be the outcome of the plasma motions and magnetic fields that threads the solar surface. The employed 3D MHD model takes into account the interactions between the plasma, magnetic fields, and wave motions in the corona. Our simulations suggest that the twisted coronal loops undergo oscillations of various modes, with periods in the range of a few minutes. These oscillations are characterized by a mixture of transverse and longitudinal motions of the plasma, which lead to significant variations in the magnetic field. The results of this study advance our understanding of the dynamics and energetics of twisted coronal loops and provide important insights into their role in the heating of the solar corona. This work may have implications for space weather forecasting and the prediction of solar storms that can impact Earth's technological infrastructure. 1 into PostgreSQL...\n",
      "Inserting test sample 1793  Cooperative video caching and transcoding in mobile edge computing (MEC) networks is a new paradigm for future wireless networks, e.g., 5G and 5G beyond, to reduce scarce and expensive backhaul resource usage by prefetching video files within radio access networks (RANs). Integration of this technique with other advent technologies, such as wireless network virtualization and multicarrier non-orthogonal multiple access (MC-NOMA), provides more flexible video delivery opportunities, which leads to enhancements both for the network's revenue and for the end-users' service experience. In this regard, we propose a two-phase RAF for a parallel cooperative joint multi-bitrate video caching and transcoding in heterogeneous virtualized MEC networks. In the cache placement phase, we propose novel proactive delivery-aware cache placement strategies (DACPSs) by jointly allocating physical and radio resources based on network stochastic information to exploit flexible delivery opportunities.\n",
      "\n",
      "Then, for the delivery phase, we propose a delivery policy based on the user requests and network channel conditions. The optimization problems corresponding to both phases aim to maximize the total revenue of network slices, i.e., virtual networks. Both problems are non-convex and suffer from high-computational complexities. For each phase, we show how the problem can be solved efficiently. We also propose a low-complexity RAF in which the complexity of the delivery algorithm is significantly reduced. A Delivery-aware cache refreshment strategy (DACRS) in the delivery phase is also proposed to tackle the dynamically changes of network stochastic information. Extensive numerical assessments demonstrate a performance improvement of up to 30% for our proposed DACPSs and DACRS over traditional approaches. 0 into PostgreSQL...\n",
      "Inserting test sample 1794  This paper proposes a cooperative multi-bitrate video caching and transcoding mechanism in the context of multicarrier Non-Orthogonal Multiple Access (NOMA)-assisted heterogeneous virtualized Mobile Edge Computing (MEC) networks. The proposed mechanism aims to optimize video delivery, taking into account the available resources of the network and the requirements of the heterogenous devices. \n",
      "\n",
      "The proposed architecture comprises three main components: a Multi-Bitrate Video Caching (MBVC) module, a cooperative NOMA-based transcoding approach, and a virtualized MEC architecture based on the group multicast and unicast technologies. \n",
      "\n",
      "The MBVC module enhances video caching performance by pre-fetching content and distributing it according to device profiles and the popularity of the requested content. Meanwhile, the NOMA-based transcoding approach optimizes bitrate adaptation by providing a cooperative transcoding function that enables user devices to transcode video content based on their processing capacities. \n",
      "\n",
      "The proposed architecture was evaluated in terms of Quality of Service (QoS) and video playback performance. The results show that the proposed transcoding mechanism can significantly improve video rendering efficiency and minimize data traffic overhead. Furthermore, the proposed architecture can provide users with a high-quality viewing experience while reducing latency and network congestion. \n",
      "\n",
      "In summary, this paper contributes to the field by introducing a cooperative caching and transcoding approach for video delivery in heterogeneous virtualized MEC networks. The results demonstrate that the proposed mechanism can overcome the challenges of video delivery in the context of complex MEC networks. 1 into PostgreSQL...\n",
      "Inserting test sample 1795  This white paper addresses selected new (to X-ray astronomy) physics and data analysis issues that will impact ASTRO-H SWG observations as a result of its high-spectral-resolution X-ray microcalorimeter, the focussing hard X-ray optics and corresponding detectors, and the low background soft gamma-ray detector. We concentrate on issues of atomic and nuclear physics, including basic bound-bound and bound-free transitions as well as scattering and radiative transfer. The major topic categories include the physics of charge exchange, solar system X-ray sources, advanced spectral model, radiative transfer, and hard X-ray emission lines and sources. 0 into PostgreSQL...\n",
      "Inserting test sample 1796  The ASTRO-H White Paper on New Spectral Features seeks to provide a comprehensive overview of novel spectral signatures discovered through advanced X-ray observations. Drawing upon an impressive array of instruments and techniques, this research paper examines a range of astronomical targets including black holes, supernovae, and cataclysmic variables, among others. By detailing the nuances of these new spectral features, the paper offers new insight into the composition, temperature, and variability of celestial objects. Ultimately, this work represents a significant contribution to the field of X-ray astronomy and provides a foundation for future investigations into the mysteries of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1797  Quantum computing using two-dimensional NMR has recently been described using scalar coupling evolution technique [J. Chem. Phys.,109,10603 (1998)]. In the present paper, we describe two-dimensional NMR quantum computing with the help of selective pulses. A number of logic gates are implemented using two and three qubits with one extra observer spin. Some many-in-one gates (or Portmanteau gates) are implemented. Toffoli gate (or AND/NAND gate) and OR/NOR gates are implemented on three qubits. Deutsch-Jozsa quantum algorithm for one and two qubits, using one extra work qubit, has also been implemented using selective pulses after creating a coherent superposition state, in the two-dimensional methodology. 0 into PostgreSQL...\n",
      "Inserting test sample 1798  Two-dimensional nuclear magnetic resonance (2D NMR) has emerged as an attractive technique for quantum computing. Spin and transition-based selective radiofrequency pulses can selectively manipulate nuclear spins in a spin system, thus achieving a high degree of coherence. In this work, we present a quadrupole-modulated term-based method using three transition-selective radiofrequency pulses for the excitation of an ensemble of nuclear spins in a two-dimensional array. We demonstrate that our method is sensitive to changes in the excitation fluorescence decay rate and can be used to detect the presence of an absorber in a sample. Our findings may provide new insights on the use of 2D NMR for quantum computing applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1799  Submanifolds of coordinate finite-type were introduced in HV1. A submanifold of a Euclidean space is called a coordinate finite-type submanifold if its coordinate functions are eigenfunctions of {\\Delta}. In the present study we consider coordinate finite-type surfaces in E^4. We give necessary and sufficient conditions for generalized rotation surfaces in E^4 to become coordinate finite-type. We also give some special examples. 0 into PostgreSQL...\n",
      "Inserting test sample 1800  In this research paper, we investigate the properties of coordinate finite type rotational surfaces in Euclidean spaces. We provide a detailed analysis of the curvature and torsion of these surfaces and show how they can be used to model a variety of physical phenomena. Our results demonstrate the usefulness of these surfaces in fields such as engineering, physics, and computer science. Furthermore, we highlight some open problems for future research in the area of rotational surfaces. 1 into PostgreSQL...\n",
      "Inserting test sample 1801  In the dynamic minimum set cover problem, a challenge is to minimize the update time while guaranteeing close to the optimal $\\min(O(\\log n), f)$ approximation factor. (Throughout, $m$, $n$, $f$, and $C$ are parameters denoting the maximum number of sets, number of elements, frequency, and the cost range.) In the high-frequency range, when $f=\\Omega(\\log n)$, this was achieved by a deterministic $O(\\log n)$-approximation algorithm with $O(f \\log n)$ amortized update time [Gupta et al. STOC'17]. In the low-frequency range, the line of work by Gupta et al. [STOC'17], Abboud et al. [STOC'19], and Bhattacharya et al. [ICALP'15, IPCO'17, FOCS'19] led to a deterministic $(1+\\epsilon)f$-approximation algorithm with $O(f \\log (Cn)/\\epsilon^2)$ amortized update time. In this paper we improve the latter update time and provide the first bounds that subsume (and sometimes improve) the state-of-the-art dynamic vertex cover algorithms. We obtain: 1. $(1+\\epsilon)f$-approximation ratio in $O(f\\log^2 (Cn)/\\epsilon^3)$ worst-case update time: No non-trivial worst-case update time was previously known for dynamic set cover. Our bound subsumes and improves by a logarithmic factor the $O(\\log^3 n/\\text{poly}(\\epsilon))$ worst-case update time for unweighted dynamic vertex cover (i.e., when $f=2$ and $C=1$) by Bhattacharya et al. [SODA'17].\n",
      "\n",
      "2. $(1+\\epsilon)f$-approximation ratio in $O\\left((f^2/\\epsilon^3)+(f/\\epsilon^2) \\log C\\right)$ amortized update time: This result improves the previous $O(f \\log (Cn)/\\epsilon^2)$ update time bound for most values of $f$ in the low-frequency range, i.e. whenever $f=o(\\log n)$.\n",
      "\n",
      "It is the first that is independent of $m$ and $n$. It subsumes the constant amortized update time of Bhattacharya and Kulkarni [SODA'19] for unweighted dynamic vertex cover (i.e., when $f = 2$ and $C = 1$). 0 into PostgreSQL...\n",
      "Inserting test sample 1802  Dynamic set cover is a fundamental problem in computer science with numerous practical applications. In this paper, we present novel techniques for improving the amortized and worst-case update time of dynamic set cover algorithms. Our approach leverages the properties of balanced binary trees, achieving a succinct representation of the set cover instance in a compact manner.\n",
      "\n",
      "To achieve this, we introduce a new data structure called Binary Space Partitioned Tree (BSPT) that allows us to perform updates efficiently. Our algorithm uses both persistent data structure and dynamic programming to ensure a worst-case update time of O(log n). Moreover, our algorithm uses only O(n log n) space, making it more scalable than many other methods.\n",
      "\n",
      "We also present a variant of our algorithm that provides an amortized update time of O(log n). This is achieved by precomputing the minimum set covers required for each node in the tree, effectively reducing the update time to a constant-time operation.\n",
      "\n",
      "Finally, we demonstrate the efficacy of our approach through extensive experimental evaluations. Our results show that the BSPT algorithm outperforms state-of-the-art algorithms in terms of both runtime and memory usage.\n",
      "\n",
      "In conclusion, our work provides a significant step towards practical dynamic set cover algorithms with efficient update times. The BSPT algorithm has a worst-case update time of O(log n) and an amortized update time of O(log n), while requiring only O(n log n) space. We believe our approach opens new avenues for the development of dynamic set cover algorithms that scale to large instances and real-world scenarios. 1 into PostgreSQL...\n",
      "Inserting test sample 1803  Let $\\{\\mathbb{P}_t\\}_{t>0}$ be the classical Poisson semigroup on $\\mathbb{R}^d$ and $G^{\\mathbb{P}}$ the associated Littlewood-Paley $g$-function operator: $$G^{\\mathbb{P}}(f)=\\Big(\\int_0^\\infty t|\\frac{\\partial}{\\partial t} \\mathbb{P}_t(f)|^2dt\\Big)^{\\frac12}.$$ The classical Littlewood-Paley $g$-function inequality asserts that for any $1<p<\\infty$ there exist two positive constants $\\mathsf{L}^{\\mathbb{P}}_{t, p}$ and $\\mathsf{L}^{\\mathbb{P}}_{c, p}$ such that $$ \\big(\\mathsf{L}^{\\mathbb{P}}_{t, p}\\big)^{-1}\\big\\|f\\big\\|_{p}\\le \\big\\|G^{\\mathbb{P}}(f)\\big\\|_{p} \\le \\mathsf{L}^{\\mathbb{P}}_{c,p}\\big\\|f\\big\\|_{p}\\,,\\quad f\\in L_p(\\mathbb{R}^d).\n",
      "\n",
      "$$ We determine the optimal orders of magnitude on $p$ of these constants as $p\\to1$ and $p\\to\\infty$. We also consider similar problems for more general test functions in place of the Poisson kernel.\n",
      "\n",
      "The corresponding problem on the Littlewood-Paley dyadic square function inequality is investigated too. Let $\\Delta$ be the partition of $\\mathbb{R}^d$ into dyadic rectangles and $S_R$ the partial sum operator associated to $R$.\n",
      "\n",
      "The dyadic Littlewood-Paley square function of $f$ is $$S^\\Delta(f)=\\Big(\\sum_{R\\in\\Delta} |S_R(f)|^2\\Big)^{\\frac12}.$$ For $1<p<\\infty$ there exist two positive constants $\\mathsf{L}^{\\Delta}_{c,p, d}$ and $ \\mathsf{L}^{\\Delta}_{t,p, d}$ such that $$ \\big(\\mathsf{L}^{\\Delta}_{t,p, d}\\big)^{-1}\\big\\|f\\big\\|_{p}\\le \\big\\|S^\\Delta(f)\\big\\|_{p}\\le \\mathsf{L}^{\\Delta}_{c,p, d}\\big\\|f\\big\\|_{p},\\quad f\\in L_p(\\mathbb{R}^d).\n",
      "\n",
      "$$ We show that $$\\mathsf{L}^{\\Delta}_{t,p, d}\\approx_d (\\mathsf{L}^{\\Delta}_{t,p, 1})^d\\;\\text{ and }\\; \\mathsf{L}^{\\Delta}_{c,p, d}\\approx_d (\\mathsf{L}^{\\Delta}_{c,p, 1})^d.$$ All the previous results can be equally formulated for the $d$-torus $\\mathbb{T}^d$. We prove a de Leeuw type transference principle in the vector-valued setting. 0 into PostgreSQL...\n",
      "Inserting test sample 1804  This paper investigates the optimal orders of the best constants in the Littlewood-Paley inequalities. Littlewood-Paley theory allows for the analysis of functions in a locally harmonic context by decomposing them into their frequency components. These inequalities provide upper bounds on the weighted norm of functions and their derivatives, and are particularly useful in the study of partial differential equations. \n",
      "\n",
      "In this work, we aim to determine the optimal orders of the best constants in such inequalities on the real line, as well as on more general spaces, including Euclidean spaces and certain non-homogeneous spaces. We utilize techniques such as the extrapolation method and the real interpolation method in our analysis. \n",
      "\n",
      "Our main results include the determination of the optimal orders of certain best constants in the Littlewood-Paley inequalities on the real line, in both the classical and weighted settings. These results extend to certain broader settings, such as the Gaussian setting and the setting of mixed-norm spaces. Moreover, we establish sharp estimates for the best constants in certain non-homogeneous Littlewood-Paley inequalities on the Euclidean spaces.\n",
      "\n",
      "Overall, this paper provides a comprehensive study of the optimal orders of the best constants in the Littlewood-Paley inequalities, and extends their analysis to various settings. These results can be of great interest to researchers and practitioners in the fields of harmonic analysis, partial differential equations, and related areas. 1 into PostgreSQL...\n",
      "Inserting test sample 1805  Roughly 25% of the optical extragalactic sky is obscured by the dust and stars of our Milky Way. Dynamically important structures might still lie hidden in this zone. Various surveys are presently being employed to uncover the galaxy distribution in the Zone of Avoidance (ZOA) but all suffer from (different) limitations and selection effects.\n",
      "\n",
      "We illustrate the promise of using a multi-wavelength approach for extragalactic large-scale studies behind the ZOA, i.e. a combination of three surveys -- optical, systematic blind HI and near-infrared (NIR), which will allow the mapping of the peculiar velocity field in the ZOA through the NIR Tully-Fisher relation. In particular, we present here the results of cross-identifying HI-detected galaxies with the DENIS NIR survey, and the use of NIR colours to determine foreground extinctions. 0 into PostgreSQL...\n",
      "Inserting test sample 1806  We present observations of 21-cm neutral hydrogen (HI) emission from 25 galaxies in the Zone of Avoidance (ZOA), performed using the Deep Extragalactic Imaging Survey with the VLA at 21 cm (DENIS). Out of the 25 observed galaxies, 18 were detected for the first time in HI, while the remaining 7 were detected previously with low resolution. The observed galaxies have redshifts between 2,500 km/s and 25,000 km/s. Our detection of 18 new galaxies in the ZOA has increased the number of known galaxies in the region by over 50%. Further observations of galaxies in the ZOA will help to improve our understanding of galaxy distribution and evolution in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1807  Methanol masers at 6.7 GHz are well known tracers of high-mass star-forming regions. However, their origin is still not clearly understood. We aimed to determine the morphology and velocity structure for a large sample of the maser emission with generally lower peak flux densities than those in previous surveys. Using the European VLBI Network we imaged the remaining sources (17) from a sample of sources that were selected from the unbiased survey using the Torun 32 m dish. Together they form a database of a total of 63 source images with high sensitivity, milliarcsecond angular resolution and very good spectral resolution for detailed studies. We studied in detail the properties of the maser clouds and calculated the mean and median values of the projected size (17.4 au and 5.5 au, respectively) as well as the FWHM of the line (0.373 km s$^{-1}$ and 0.315 km s$^{-1}$ for the mean and median values, respectively), testing whether it was consistent with Gaussian profile. We also found maser clouds with velocity gradients (71 per cent) that ranged from 0.005 km s$^{-1}$ au$^{-1}$ to 0.210 km s$^{-1}$ au$^{-1}$. We tested the kinematic models to explain the observed structures of the 6.7 GHz emission. There were targets where the morphology supported the scenario of a rotating and expanding disk or a bipolar outflow. Comparing the interferometric and single-dish spectra we found that, typically, 50-70 per cent of the flux was missing. This phenomena is not strongly related to the distance of the source. The EVN imaging reveals that in the complete sample of 63 sources the ring-like morphology appeared in 17 per cent of sources, arcs were seen in a further 8 per cent, and the structures were complex in 46 per cent cases. The UC HII regions coincide in position in the sky for 13 per cent of the sources. They are related both to extremely high and low luminosity masers from the sample. 0 into PostgreSQL...\n",
      "Inserting test sample 1808  This paper discusses observations of 6.7 GHz methanol masers using the European VLBI Network (EVN). We present the results of our study which spanned multiple observing sessions over a period of two years. Our observations show the distribution and kinematics of this maser emission in unprecedented detail.\n",
      "\n",
      "Our EVN measurements allowed us to image these masers with an angular resolution of less than one milliarcsecond. This resolution revealed spatial details that are not visible with single-dish telescopes. We observed strong maser emission distributed over multiple velocity components, indicating the presence of multiple regions of star formation. We also detected several new features that have not been previously observed.\n",
      "\n",
      "Investigating the kinematics of the methanol masers allowed us to infer the presence of rotating disks of gas and dust around the young stars. We find evidence of both Keplerian and non-Keplerian motion, suggesting that the disks are not simply flat and rotating. Our observations indicate that non-spherical geometry may play an important role in the formation of masers.\n",
      "\n",
      "Finally, we discuss the astrophysical implications of our results. The detection of methanol masers is indicative of high-mass star formation, and the presence of disks around young stars suggests that they may be the progenitors of massive stars. Our observations contribute to our understanding of star formation and the role of masers in this process.\n",
      "\n",
      "In conclusion, our EVN observations of 6.7 GHz methanol masers provide unprecedented detail on the distribution, kinematics, and nature of these features. Our results contribute to our understanding of high-mass star formation and the important role of rotation and non-spherical geometry in this process. 1 into PostgreSQL...\n",
      "Inserting test sample 1809  Resistivity measurements performed under pressure in the paramagnetic ground state of CeRu2Si2 are reported. They demonstrate that the relative change of effective mass through the pseudo metamagnetic transition is invariant under pressure. The results are compared with the first order metamagnetic transition due to the antiferromagnetism of Ce0.9La0.1Ru2Si2 which corresponds to the \"negative\" pressure of CeRu2Si2 by volume expansion. Finally, we describe the link between the spin-depairing of quasiparticles on CeRu2Si2 and that of Cooper pairs on the unconventional heavy fermion superconductor CeCoIn5. 0 into PostgreSQL...\n",
      "Inserting test sample 1810  This study investigates the pressure dependence of ferromagnetic fluctuations induced by a magnetic field through pseudo-metamagnetism in CeRu2Si2. Using high-resolution measurements of the magnetic susceptibility, we observed the emergence of a new phase in the temperature-pressure-magnetic field space. The pressure evolution of this phase exhibits a distinct anomalous behavior which we attribute to the emergence of a quantum critical point. Our findings not only provide a deeper understanding of the magnetic properties of complex materials but also have implications for the development of novel magnetic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 1811  To explain \\eta-distributions at RHIC energies we consider the Ornstein-Uhlenbeck process. To account for hadrons produced in the central region, we assume existence of third source located there (y \\approx 0) in addition to two sources located at the beam and target rapidities (\\pm y_{max} = \\pm \\ln[\\sqrt{s_{NN}}/m_{N}]). This results in better \\chi^2/n.d.f. than those for only two sources when analysing data. 0 into PostgreSQL...\n",
      "Inserting test sample 1812  In this paper, we present a stochastic model for the \\eta-distributions at RHIC energies. Our model takes into account the fluctuations in the transverse energy and rapidity density of the produced particles. We explore the behavior of our model and compare it with experimental data from RHIC. Our results demonstrate that the stochastic model provides an accurate description of the \\eta-distributions, and its use can shed light on the underlying physics of heavy-ion collisions at RHIC energies. 1 into PostgreSQL...\n",
      "Inserting test sample 1813  We investigate the onset of 2D time-dependent magnetic reconnection that is triggered using an external velocity driver located away from, and perpendicular to, an equilibrium Harris current sheet. Previous studies have typically utilised an internal trigger to initiate reconnection, e.g. initial conditions centred on the current sheet. Numerical simulations solving the compressible, resistive magnetohydrodynamics equations were performed to investigate the reconnection onset within different atmospheric layers of the Sun, namely the corona, chromosphere and photosphere. A reconnecting state is reached for all atmospheric heights considered, with the dominant physics being highly dependent on atmospheric conditions. The coronal case achieves a sharp rise in electric field for a range of velocity drivers. For the chromosphere, we find a larger velocity amplitude is required to trigger reconnection. For the photospheric environment, the electric field is highly dependent on the inflow speed; a sharp increase in electric field is obtained only as the velocity entering the reconnection region approaches the Alfven speed.\n",
      "\n",
      "Additionally, the role of ambipolar diffusion is investigated for the chromospheric case and we find that the ambipolar diffusion alters the structure of the current density in the inflow region. The rate at which flux enters the reconnection region is controlled by the inflow velocity. This determines all aspects of the reconnection start-up process, i.e. the early onset of reconnection is dominated by the advection term in Ohms law in all atmospheric layers. A lower plasma-$\\beta$ enhances reconnection and creates a large change in the electric field. A high plasma-$\\beta$ hinders the reconnection, yielding a sharp rise in the electric field only when the velocity flowing into the reconnection region approaches the local Alfven speed. 0 into PostgreSQL...\n",
      "Inserting test sample 1814  Magnetic reconnection is a fundamental process that occurs when oppositely oriented magnetic fields merge, releasing energy and converting magnetic energy into kinetic energy and heat. Polarimetric measurements from space-borne instrumentation have revealed dynamic magnetic activity on the Sun's surface, in the form of flares and coronal mass ejections. In this paper, we investigate the onset of two-dimensional (2D) magnetic reconnection in the solar atmosphere, where magnetic fields are most intense and therefore subject to disruption. Our analysis focuses on the photosphere, chromosphere, and corona, as these regions have shown distinctive behaviors in previous data. Using a combination of numerical simulations and statistical analysis, we investigate the magnetic properties, temperature, and velocity profiles of these three regions during magnetic reconnection. Our results show that 2D magnetic reconnection can occur in the photosphere, chromosphere, and corona, leading to heating and acceleration of the surrounding plasma and generation of electromagnetic waves. We identify several mechanisms at work during magnetic reconnection: compression, heating, and acceleration of plasma by magnetic fields, and in some cases, release of stored magnetic energy. Furthermore, we discuss the implications of our findings for future space-borne observations of magnetic reconnection and the potential for improving our understanding of the Sun's magnetic activity. Our results provide new insights into the dynamics of the solar atmosphere and the role of magnetic fields in shaping solar activity. 1 into PostgreSQL...\n",
      "Inserting test sample 1815  The detailed evolution of exoplanetary atmospheres has been the subject of decade-long studies. Only recently, investigations began on the possible atmospheric mass loss caused by the activity of galactic central engines. This question has so far been explored without using available exoplanet data. The goal of this paper is to improve our knowledge of the erosion of exoplanetary atmospheres through radiation from supermassive black holes (SMBHs) undergoing an active galactic nucleus (AGN) phase. To this end, we extended the well-known energy-limited mass-loss model to include the case of radiation from AGNs. We set the fraction of incident power $\\epsilon$ available to heat the atmosphere as either constant ($\\epsilon = 0.1$) or flux dependent ($\\epsilon = \\epsilon(F_{\\textrm{XUV}})$). We calculated the possible atmospheric mass loss for 54 known exoplanets (of which 16 are hot Jupiters residing in the Galactic bulge and 38 are Earth-like planets (EPs)) due to radiation from the Milky Way's (MW) central SMBH, Sagittarius A* (Sgr A*), and from a set of 107,220 AGNs generated using the 33,350 AGNs at $z < 0.5$ of the Sloan Digital Sky Survey database. We found that planets in the Galactic bulge might have lost up to several Earth atmospheres in mass during the AGN phase of Sgr A*, while the EPs are at a safe distance from Sgr A* ($> 7$ kpc) and have not undergone any atmospheric erosion in their lifetimes. We also found that the MW EPs might experience a mass loss up to $\\sim 15$ times the Mars atmosphere over a period of $50$ Myr as the result of exposure to the cumulative extreme-UV flux $F_{\\textrm{XUV}}$ from the AGNs up to $z = 0.5$. In both cases we found that an incorrect choice of $\\epsilon$ can lead to significant mass loss overestimates. 0 into PostgreSQL...\n",
      "Inserting test sample 1816  The effect of supermassive black holes on their surrounding environment is a topic of much interest in astrophysics. In particular, it is of great importance to study the impact of these extremely energetic objects on exoplanets. In this study, we analyze the mass loss of known exoplanets that are located in the vicinity of two highly active regions: the supermassive black hole Sgr A* and nearby active galactic nuclei (AGN).\n",
      "\n",
      "To conduct our analysis, we first collected data on exoplanets and their orbits from various astronomical databases. We then used numerical simulations to model the behavior of these exoplanets under the influence of Sgr A* and nearby AGN. Our simulations incorporated a range of factors, such as the strength of radiation and gravitational forces, the exoplanet's mass and distance from the active region, and the characteristics of its atmosphere.\n",
      "\n",
      "Our results show that exoplanets located in the vicinity of Sgr A* and nearby AGN experience significantly higher rates of mass loss compared to those located further away. This is due to the strong ionizing radiation emitted by these active regions, which erodes the exoplanet's upper atmosphere and enhances the escape of gas and plasma. Furthermore, we found that the extent of mass loss varies greatly depending on the exoplanet's specific characteristics, such as its mass and composition.\n",
      "\n",
      "In conclusion, our study provides a comparative analysis of the influence of Sgr A* and nearby AGN on the mass loss of known exoplanets. Our findings highlight the importance of considering the impact of supermassive black holes and other highly active regions when studying exoplanet atmospheres and habitability. We hope that our results can contribute to the ongoing exploration of exoplanetary systems and inform future observations and simulations of these fascinating objects. 1 into PostgreSQL...\n",
      "Inserting test sample 1817  A better understanding of the formation of large-scale structure in the Universe is arguably the most pressing question in cosmology. The most compelling and promising theoretical paradigm, Inflation + Cold Dark Matter, holds that the density inhomogeneities that seeded the formation of structure in the Universe originated from quantum fluctuations arising during inflation and that the bulk of the dark matter exists as slowing moving elementary particles (`cold dark matter') left over from the earliest, fiery moments.\n",
      "\n",
      "Large redshift surveys (such as the SDSS and 2dF) and high-resolution measurements of CBR anisotropy (to be made by the MAP and Planck Surveyor satellites) have the potential to decisively test Inflation + Cold Dark Matter and to open a window to the very early Universe and fundamental physics. 0 into PostgreSQL...\n",
      "Inserting test sample 1818  The origin of large-scale structures in the Universe has long been a subject of scientific inquiry. Recent research has shown that these structures may have originated from tiny quantum fluctuations in the very early Universe. The properties of these fluctuations can be gleaned from precision measurements of the cosmic microwave background radiation. By analyzing the statistical properties of this radiation, astrophysicists have been able to reconstruct the early Universe and simulate the formation of large-scale structures over billions of years. These simulations have demonstrated that the distribution of matter in the Universe today is strongly influenced by the initial conditions set by these quantum fluctuations, providing valuable insights into the fundamental workings of the Universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1819  In 1992 we began a precision radial velocity (RV) survey for planets around solar-like stars with the Coude Echelle Spectrograph and the Long Camera (CES LC) at the 1.4 m telescope in La Silla (Chile). We have continued the survey with the upgraded CES Very Long Camera (VLC) and HARPS, both at the 3.6 m telescope, until 2007. The observations for 31 stars cover a time span of up to 15 years and the RV precision permit a search for Jupiter analogues. We perform a joint analysis for variability, trends, periodicities, and Keplerian orbits and compute detection limits. Moreover, the HARPS RVs are analysed for correlations with activity indicators (CaII H&K and CCF shape). We achieve a long-term RV precision of 15 m/s (CES+LC, 1992-1998), 9 m/s (CES+VLC, 1999-2006), and 2.8 m/s (HARPS, 2003-2009, including archive data), resp. This enables us to confirm the known planets around Iota Hor, HR 506, and HR 3259. A steady RV trend for Eps Ind A can be explained by a planetary companion. On the other hand, we find previously reported trends to be smaller for Beta Hyi and not present for Alp Men. The candidate planet Eps Eri b was not detected despite our better precision. Also the planet announced for HR 4523 cannot be confirmed. Long-term trends in several of our stars are compatible with known stellar companions. We provide a spectroscopic orbital solution for the binary HR 2400 and refined solutions for the planets around HR 506 and Iota Hor. For some other stars the variations could be attributed to stellar activity. The occurrence of two Jupiter-mass planets in our sample is in line with the estimate of 10% for the frequency of giant planets with periods smaller than 10 yr around solar-like stars. We have not detected a Jupiter analogue, while the detections limits for circular orbits indicate at 5 AU a sensitivity for minimum mass of at least 1 M_Jup (2 M_Jup) for 13% (61%) of the stars. 0 into PostgreSQL...\n",
      "Inserting test sample 1820  The search for exoplanets has become increasingly important in recent years. The ESO CES and HARPS have developed a planet search program to find Jupiter analogues around solar-like stars. The aim of this study was to detect these exoplanets and investigate the characteristics of their host stars.\n",
      "\n",
      "In this paper, we present the results from the fourth phase of the planet search program conducted by the ESO CES and HARPS. We focused on searching for Jupiter analogues around solar-like stars, which are stars that have a similar mass, temperature, and spectral type to our own Sun.\n",
      "\n",
      "We used high-precision radial velocity measurements and performed a statistical analysis to identify any signals that corresponded to a Jupiter-like planet in our sample. We found that seven stars showed a periodic signal consistent with the presence of a Jupiter-analogue exoplanet. We then analyzed these candidates further, and confirmed that four of them are indeed Jupiter analogues around solar-like stars.\n",
      "\n",
      "We then investigated the properties of the host stars and found that they are similar to the Sun in terms of their mass and metallicity. We also found that the Jupiter analogues have orbits similar to that of Jupiter in our own solar system, with periods ranging from 3.5 to 12 years.\n",
      "\n",
      "Our study provides new insights into the occurrence and characteristics of Jupiter analogues around solar-like stars. These findings support the current planet formation theories, and suggest that the conditions favorable for the formation of Jupiter-like exoplanets are common around solar-like stars.\n",
      "\n",
      "In conclusion, we have successfully identified and confirmed four Jupiter analogues around solar-like stars, providing valuable information into the frequency and nature of exoplanets in our galaxy. These findings are crucial in advancing our understanding of the origins and evolution of planetary systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1821  We prove Gauss-Bonnet and Poincare-Hopf formulas for multi-linear valuations on finite simple graphs G=(V,E) and answer affirmatively a conjecture of Gruenbaum from 1970 by constructing higher order Dehn-Sommerville valuations which vanish for all d-graphs without boundary. An example of a quadratic valuation is the Wu characteristic w(G) which sums (-1)^(dim(x)+dim(y)) over all intersecting pairs of complete subgraphs x,y of a G. More generally, an intersection number w(A,B) sums (-1)^(dim(x)+dim(y)) over pairs x,y, where x is in A and y is in B and x,y intersect. w(G) is a quadratic Euler characteristic X(G), where X sums (-1)^dim(x) over all complete subgraphs x of G. We prove that w is multiplicative, like Euler characteristic: w(G x H) = w(G) w(H) for any two graphs and that w is invariant under Barycentric refinements. We construct a curvature K satisfying Gauss-Bonnet w(G) = sum K(a). We also prove w(G) = X(G)-X(dG) for Euler characteristic X which holds for any d-graph G with boundary dG. We also show higher order Poincare-Hopf formulas: there is for every multi-linear valuation X and function f an index i(a) such that sum i(a)=X(G). For d-graphs G and X=w it agrees with the Euler curvature. For the vanishing multi-valuations which were conjectured to exist, like for the quadratic valuation X(G) = (V X) Y with X=(1,-1,1,-1,1),Y=(0,-2,3,-4,5) on 4-graphs, discrete 4 manifolds, where V_{ij}(G) is the f-matrix counting the number of i and j-simplices in G intersecting, the curvature is constant zero.\n",
      "\n",
      "For all graphs and multi-linear Dehn-Sommerville relations, the Dehn-Sommerville curvature K(v) at a vertex is a Dehn-Sommerville valuation on the unit sphere S(v). We show X V(G) Y = v(G) Y for any linear valuation Y of a d-graph G with f-vector v(G). This provides examples for the Gruenbaum conjecture. 0 into PostgreSQL...\n",
      "Inserting test sample 1822  This paper presents a study of the Gauss-Bonnet theorem for multi-linear valuations. The theorem has been observed to hold for smooth convex bodies, but its extension to other classes of sets has not been well explored. Our work provides a framework to apply the Gauss-Bonnet theorem to more general classes of sets, such as those that are not necessarily smooth or convex.\n",
      "\n",
      "We begin by introducing the necessary mathematical background on multi-linear valuations and their properties. We then establish the Gauss-Bonnet theorem for a class of sets that includes both smooth convex bodies and a broader range of sets. Our approach is based on a novel formula for the Euler characteristic of these sets, which allows us to apply the Gauss-Bonnet theorem in a general setting.\n",
      "\n",
      "Next, we apply our results to the case of polyhedral sets, developing a series of propositions which provide a detailed analysis of their Euler characteristic. We demonstrate that these results lead to a generalization of the classical Gauss-Bonnet theorem for polyhedra, thereby extending its applicability to a wider variety of configurations.\n",
      "\n",
      "Finally, we discuss the potential applications of our work in areas such as computer graphics, optimization, and mathematical modeling. We describe several specific problems in these areas that can be analyzed using the Gauss-Bonnet theorem for multi-linear valuations, and discuss their potential benefit to real-world applications.\n",
      "\n",
      "In conclusion, this paper presents a significant contribution to the theory of Gauss-Bonnet theorem, specifically in the context of multi-linear valuations. We establish the theorem for a broad class of sets, including those that are not necessarily convex or smooth, and provide detailed analysis of its applications to polyhedral configurations. The results suggest that the Gauss-Bonnet theorem for multi-linear valuations has important implications for a variety of fields, and can lead to significant advancements in fundamental mathematical theory as well as practical applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1823  Carbon Nanotube (CNT) is one of the most significant materials for the development of faster and improved performance of nano-scaled transistors. This paper aims at analyzing a trade-off between device performance and device size of CNT based transistor. Acoustic and Optical Phonon scattering along with the elastic scattering lead to the non-ballistic performances of those transistors.\n",
      "\n",
      "The main focus of this work is mainly on finding an optimum diameter to obtain the highest degree of ballisticity from both single-walled and double-walled Carbon Nanotube Field Effect Transistors (CNT-FETs). At first, an n-type single-walled Carbon Nanotube Field Effect Transistor has been considered and the diameter dependence on degree of ballisticity has been simulated. The effects of drain voltage, gate voltage and channel length have been investigated for such characteristics followed by a comparison with double-walled CNT-FET structure. First of all, it has been found that degree of ballisticity along with the optimum diameter increases with the increase of (VDS VGS). The increase of channel length, however, degrades the ballistic performance demanding a higher diameter to reach the optimum point. Finally, it can be concluded that optimum diameter for DWCNT-FET reaches earlier than SWCNT-FET but at lower degree of ballisticity. 0 into PostgreSQL...\n",
      "Inserting test sample 1824  Carbon nanotube field effect transistors (CNTFETs) have shown great potential for high-speed, low-power electronics. One of the key factors for their performance is the diameter of the nanotubes. In this study, we investigate the impact of nanotube diameter on the ballisticity of CNTFETs. By using an analytical model, we analyze the transfer characteristics of different CNTFETs with varying diameters. Our results show that an optimal diameter exists for achieving the highest degree of ballisticity in CNTFETs. We demonstrate that the optimal diameter depends on the gate length and the tube-tube interaction. Additionally, we find that increasing the diameter beyond the optimal value can lead to a decrease in ballisticity due to increased scattering. Our findings provide valuable insights for the design and optimization of CNTFETs. Furthermore, we propose a novel method for selecting the optimal nanotube diameter based on the device specifications. Our approach can potentially facilitate the development of high-performance CNTFETs for various applications in the future. 1 into PostgreSQL...\n",
      "Inserting test sample 1825  The almost 7 decades lasting futile attempts to understand the possible physical content of the third Wigner representation class (the infinite spin class) came to a partial solution with the 2006 discovery of existence of string-localized spacetime covariantizations . This has led to a still ongoing vast generalization of renormalizability to fields with arbitrary high spin and a better understanding of the origin of partial invisibility as observed in the confinement of gluons and quarks. The present note explains the total (non-gravitational) invisibility of fields associated to the third Wigner representation class. The last section presents a critical look at the possibility that third class Wigner matter may play a role in dark matter formation. 0 into PostgreSQL...\n",
      "Inserting test sample 1826  This paper investigates the connection between dark matter and Wigner's third positive-energy representation class. The latter is a mathematical concept that has been recently explored in quantum theory. Dark matter, on the other hand, is a mysterious substance that constitutes most of the matter in the universe but cannot be directly detected. We explore how the properties of dark matter can be described by the elements of the third positive-energy representation class. Our analysis sheds light on the nature of dark matter and provides a new perspective on understanding its behavior. The results presented in this paper could have significant implications for future research in the field of astrophysics and particle physics. 1 into PostgreSQL...\n",
      "Inserting test sample 1827  The continuous-time random walk (CTRW) model is useful for alleviating the computational burden of simulating diffusion in actual media. In principle, isotropic CTRW only requires knowledge of the step-size, $P_l$, and waiting-time, $P_t$, distributions of the random walk in the medium and it then generates presumably equivalent walks in free space, which are much faster.\n",
      "\n",
      "Here we test the usefulness of CTRW to modelling diffusion of finite-size particles in porous medium generated by loose granular packs. This is done by first simulating the diffusion process in a model porous medium of mean coordination number, which corresponds to marginal rigidity (the loosest possible structure), computing the resulting distributions $P_l$ and $P_t$ as functions of the particle size, and then using these as input for a free space CTRW. The CTRW walks are then compared to the ones simulated in the actual media.\n",
      "\n",
      "In particular, we study the normal-to-anomalous transition of the diffusion as a function of increasing particle size. We find that, given the same $P_l$ and $P_t$ for the simulation and the CTRW, the latter predicts incorrectly the size at which the transition occurs. We show that the discrepancy is related to the dependence of the effective connectivity of the porous media on the diffusing particle size, which is not captured simply by these distributions.\n",
      "\n",
      "We propose a correcting modification to the CTRW model -- adding anisotropy -- and show that it yields good agreement with the simulated diffusion process.\n",
      "\n",
      "We also present a method to obtain $P_l$ and $P_t$ directly from the porous sample, without having to simulate an actual diffusion process. This extends the use of CTRW, with all its advantages, to modelling diffusion processes of finite-size particles in such confined geometries. 0 into PostgreSQL...\n",
      "Inserting test sample 1828  Modifying continuous-time random walks (CTRWs) is proposed as a novel approach to model finite-size particle diffusion in granular porous media. With many applications in geoscience and petroleum engineering, CTRWs are an essential tool in predicting the transport of fluids and solutes in porous media. However, most traditional CTRWs only consider the dynamics of infinitely small particles, which can lead to discrepancies in predicting experimental results. The proposed method incorporates finite-size effects of the diffusing particles by defining a probability density function for the particle's distribution, which systematically modifies the overall diffusion profile and leads to more accurate predictions.\n",
      "\n",
      "To demonstrate the utility of the modified CTRWs, simulations were conducted using a numerical model of a 3D granular porous medium. The results revealed significant differences in the diffusion of small and large particles, highlighting the significance of finite-size effects in porous media. This study provides an improved model that more accurately predicts the diffusion of finite-size particles in complex porous media. The methodology and results presented here have the potential to inform the development of predictive models in several scientific and engineering applications.\n",
      "\n",
      "Overall, the proposed method of modifying CTRWs to incorporate finite-size effects represents a significant advancement in the field of porous media modeling. It offers a more accurate means of predicting the transport of particles in porous media, allowing for better understanding and management of geologic and engineered systems. The findings of this study have important implications for predicting fluid flow and solute transport in porous media, thereby contributing to the development of enhanced oil recovery, and groundwater management strategies. 1 into PostgreSQL...\n",
      "Inserting test sample 1829  Wang and Ye conjectured in [22]: Let $\\Omega$ be a regular, bounded and convex domain in $\\mathbb{R}^{2}$.\n",
      "\n",
      "There exists a finite constant $C({\\Omega})>0$ such that \\[ \\int_{\\Omega}e^{\\frac{4\\pi u^{2}}{H_{d}(u)}}dxdy\\le C(\\Omega),\\;\\;\\forall u\\in C^{\\infty}_{0}(\\Omega), \\] where $H_{d}=\\int_{\\Omega}|\\nabla u|^{2}dxdy-\\frac{1}{4}\\int_{\\Omega}\\frac{u^{2}}{d(z,\\partial\\Omega)^{2}}dxdy$ and $d(z,\\partial\\Omega)=\\min\\limits_{z_{1}\\in\\partial\\Omega}|z-z_{1}|$.} The main purpose of this paper is to confirm that this conjecture indeed holds for any bounded and convex domain in $\\mathbb{R}^{2}$ via the Riemann mapping theorem (the smoothness of the boundary of the domain is thus irrelevant).\n",
      "\n",
      "We also give a rearrangement-free argument for the following Trudinger-Moser inequality on the hyperbolic space $\\mathbb{B}=\\{z=x+iy:|z|=\\sqrt{x^{2}+y^{2}}<1\\}$: \\[ \\sup_{\\|u\\|_{\\mathcal{H}}\\leq 1} \\int_{\\mathbb{B}}(e^{4\\pi u^{2}}-1-4\\pi u^{2})dV=\\sup_{\\|u\\|_{\\mathcal{H}}\\leq 1}\\int_{\\mathbb{B}}\\frac{(e^{4\\pi u^{2}}-1-4\\pi u^{2})}{(1-|z|^{2})^{2}}dxdy< \\infty, \\] by using the method employed earlier by Lam and the first author [9, 10], where $\\mathcal{H}$ denotes the closure of $C^{\\infty}_{0}(\\mathbb{B})$ with respect to the norm $$\\|u\\|_{\\mathcal{H}}=\\int_{\\mathbb{B}}|\\nabla u|^{2}dxdy-\\int_{\\mathbb{B}}\\frac{u^{2}}{(1-|z|^{2})^{2}}dxdy.$$ Using this strengthened Trudinger-Moser inequality, we also give a simpler proof of the Hardy-Moser-Trudinger inequality obtained by Wang and Ye [22]. 0 into PostgreSQL...\n",
      "Inserting test sample 1830  This research paper provides a sharp version of the Trudinger-Moser inequality on a bounded and convex planar domain. The sharp inequality states that the L2-norm of the gradient of a function in the weighted Sobolev space is bounded by a function of the L2-norm of the function itself. The inequality is proved by utilizing the radial symmetry of the domain, the Pohozaev identity, and the use of a conformal mapping. The proof highlights the importance of the convexity of the domain, which allows for the use of a double integration by parts technique. The sharpness of the inequality is demonstrated with the help of a family of conformal metrics which makes the L2-norm of the gradient blow up whenever the L2-norm of the function becomes sufficiently large. The results of the paper are shown to be optimal by proving the impossibility of finding a better constant in the inequality. Moreover, the paper discusses the application of the Trudinger-Moser inequality in the study of the regularity of solutions for singular elliptic problems with exponential nonlinearity. Finally, the paper provides some numerical examples to illustrate the sharpness of the inequality on various bounded and convex planar domains. Overall, the paper highlights the importance of the Trudinger-Moser inequality in the study of various problems in nonlinear analysis and geometry. 1 into PostgreSQL...\n",
      "Inserting test sample 1831  We report on experiments of drop impacting a hydrophobic micro-grid, of typical spacing a few tens of $\\mu$m. Above a threshold in impact speed, liquid emerges to the other side, forming micro-droplets of size about that of the grid holes. We propose a method to produce either a mono-disperse spray or a single tiny droplet of volume as small as a few picoliters corresponding to a volume division of the liquid drop by a factor of up to 10$^5$. We also discuss the discrepancy of the measured thresholds with that predicted by a balance between inertia and capillarity. 0 into PostgreSQL...\n",
      "Inserting test sample 1832  This study investigates the use of a hydrophobic micro-grid to grate a liquid into tiny droplets through the impact force. The micro-grid used a specific pattern of hydrophobicity to create a contact angle with the liquid, causing the droplets to form. The experiment took place in a controlled environment and the grating process was monitored through high-speed cameras. Results showed that the micro-grid effectively grated the liquid into tiny droplets, with the droplet size being controlled by adjusting the impact force. The findings of this study have potential applications in various fields, such as pharmaceuticals and chemical engineering. 1 into PostgreSQL...\n",
      "Inserting test sample 1833  We demonstrate strain-tuning of magnetocrystalline anisotropy over a range of more than one thousand Gauss in epitaxial Y3Fe5O12 films of excellent crystalline quality grown on lattice-mismatched Y3Al5O12 substrates.\n",
      "\n",
      "Ferromagnetic resonance (FMR) measurements reveal a linear dependence of both out-of-plane and in-plane uniaxial anisotropy on the strain-induced tetragonal distortion of Y3Fe5O12. Importantly, we find the spin mixing conductance G_r determined from inverse spin Hall effect and FMR linewidth broadening remains large: G_r = 3.33 x 10^14 Ohm^-1m^-2 in Pt/Y3Fe5O12/Y3Al5O12 heterostructures, quite comparable to the value found in Pt/Y3Fe5O12 grown on lattice-matched Gd3Ga5O12 substrates. 0 into PostgreSQL...\n",
      "Inserting test sample 1834  This paper investigates the strain-tunable magnetocrystalline anisotropy (MCA) in epitaxial Y3Fe5O12 thin films grown on different substrates. The MCA is found to be strongly dependent on the film strain due to the competition between magnetoelastic and magnetic anisotropy energies. By systematic control of the substrate-induced strain, we demonstrate that the MCA can be effectively tuned. The experimental results are further supported by density functional theory calculations. Our findings shed light on the fundamental understanding of MCA in magnetic materials and provide a promising strategy for engineering the magnetic anisotropy of epitaxial films, which is crucial for various magnetic device applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1835  Based on a detailed symmetry analysis, we state the general rules to build up the effective low energy field theory describing a system of electrons weakly interacting with the lattice degrees of freedom. The basic elements in our construction are what we call the \"memory tensors\", that keep track of the microscopic discrete symmetries into the coarse-grained action. The present approach can be applied to lattice systems in arbitrary dimensions and in a systematic way to any desired order in derivatives. We apply the method to the honeycomb lattice and re-obtain the by now well-known effective action of Dirac fermions coupled to fictitious gauge fields. As a second example, we derive the effective action for electrons in the kagom\\'e lattice, where our approach allows to obtain in a simple way the low energy electron-phonon coupling terms. 0 into PostgreSQL...\n",
      "Inserting test sample 1836  In this paper, we present a systematic approach for deriving the low energy effective action for electron-phonon interactions using symmetry analysis. Starting from the electron-phonon Hamiltonian, we utilize symmetry transformations to derive the effective action in terms of the low energy degrees of freedom. Our approach allows for the inclusion of both adiabatic and non-adiabatic electron-phonon couplings, and can be applied to a wide range of physical systems, from superconductors to molecular electronics. We demonstrate the effectiveness of our method in the case of a one-dimensional electron-phonon system, where we show that the effective action accurately captures the low energy dynamics of the system. Our results provide a powerful tool for the study of electron-phonon interactions in condensed matter systems, and open up new avenues for theoretical and experimental investigations. 1 into PostgreSQL...\n",
      "Inserting test sample 1837  Let $D$ be a bounded, connected, open set in Euclidean space $\\mathbb{R}^{2}$ with polygonal boundary. Suppose $D$ has initial temperature $1$ and the complement of $D$ has initial temperature $0$. We obtain the asymptotic behaviour of the heat content of $D$ as time $t \\downarrow 0$. We then apply this result to compute the heat content of a particular fractal polyhedron as $t \\downarrow 0$. 0 into PostgreSQL...\n",
      "Inserting test sample 1838  The heat content of a polygon has been a topic of interest in the field of mathematical analysis for many years. In this paper, we investigate the relationship between the area, perimeter, and heat content of polygons of different shapes and sizes. Our analysis reveals that the heat content of a polygon is dependent on its geometry and can be expressed as a function of its area and perimeter. These findings offer insight into the fundamental properties of geometric shapes in relation to thermal dynamics. 1 into PostgreSQL...\n",
      "Inserting test sample 1839  We measure rotational broadening in spectra taken by the Apache Point Observatory Galactic Evolution Experiment (APOGEE) survey to characterise the relationship between stellar multiplicity and rotation. We create a sample of 2786 giants and 24 496 dwarfs with stellar parameters and multiple radial velocities from the APOGEE pipeline, projected rotation speeds \\vsini\\ determined from our own pipeline, and distances, masses, and ages measured by Sanders \\& Das. We use the statistical distribution of the maximum shift in the radial velocities, \\drvm, as a proxy for the close binary fraction to explore the interplay between stellar evolution, rotation, and multiplicity. Assuming that the minimum orbital period allowed is the critical period for Roche Lobe overflow and rotational synchronization, we calculate theoretical upper limits on expected \\vsini\\ and \\drvm\\ values. These expectations agree with the positive correlation between the maximum \\drvm\\ and \\vsini\\ values observed in our sample as a function of \\logg. We find that the fast rotators in our sample have a high occurrence of short-period ($\\log(P/\\text{d})\\lesssim 4$) companions. We also find that old, rapidly-rotating main sequence stars have larger completeness-corrected close binary fractions than their younger peers.\n",
      "\n",
      "Furthermore, rapidly-rotating stars with large \\drvm\\ consistently show differences of 1-10 Gyr between the predicted gyrochronological and measured isochronal ages. These results point towards a link between rapid rotation and close binarity through tidal interactions. We conclude that stellar rotation is strongly correlated with stellar multiplicity in the field, and caution should be taken in the application of gyrochronology relations to cool stars. 0 into PostgreSQL...\n",
      "Inserting test sample 1840  Stellar multiplicity, the phenomenon of two or more stars orbiting around a common center of mass, is crucial to our understanding of the evolution of galaxies and their constituent stars. The ongoing APOGEE survey, which targets near-infrared spectroscopy of over 100,000 red giants in the Milky Way, has made it possible to investigate this phenomenon in detail. In particular, APOGEE has enabled the study of stellar multiplicity and rotation in the context of the Galactic disk and halo, as well as in clusters and associations.\n",
      "\n",
      "In this paper, we present results from a comprehensive study of 11,000 red giant stars observed by APOGEE. Our analysis reveals a significant correlation between stellar multiplicity and rotation, with binary systems showing a higher degree of rotational synchronization than single stars. Additionally, we find that the strength of this correlation varies with stellar mass and evolutionary stage, with more evolved stars exhibiting a weaker correlation than their less evolved counterparts.\n",
      "\n",
      "Our results provide important insights into the formation and evolution of binary systems, as well as the mechanisms that govern stellar rotation. In particular, they suggest that binarity plays a key role in regulating the angular momentum of stars, and that the observed trends can be explained by a combination of tidal interactions, magnetic braking, and accretion processes. We conclude by discussing the implications of our findings for future studies of star formation, Galactic dynamics, and exoplanet habitability. 1 into PostgreSQL...\n",
      "Inserting test sample 1841  We describe a noncommutative analogue of the absolute value of a regular operator acting on a noncommutative $\\mathrm{L}^p$-space. We equally prove that two classical operator norms, the regular norm and the decomposable norm are identical. We also describe precisely the regular norm of several classes of regular multipliers. This includes Schur multipliers and Fourier multipliers on some unimodular locally compact groups which can be approximated by discrete groups in various senses. A main ingredient is to show the existence of a bounded projection from the space of completely bounded $\\mathrm{L}^p$ operators onto the subspace of Schur or Fourier multipliers, preserving complete positivity. On the other hand, we show the existence of bounded Fourier multipliers which cannot be approximated by regular operators, on large classes of locally compact groups, including all infinite abelian locally compact groups. We finish by introducing a general procedure for proving positive results on selfadjoint contractively decomposable Fourier multipliers, beyond the amenable case. 0 into PostgreSQL...\n",
      "Inserting test sample 1842  This paper investigates projections, multipliers and decomposable maps on noncommutative $\\mathrm{L}^p$-spaces. We begin by providing a rigorous framework for these concepts in the context of von Neumann algebras. We then explore their applications in the study of operator-valued measures, particularly in relation to the von Neumann algebra generated by an operator system. Our main result provides a criterion for the decomposability of a map on the noncommutative $\\mathrm{L}^p$-space, and we give several examples to illustrate its usefulness. In addition, we examine the duality between the spaces of multipliers and decomposable maps, and establish a connection between the multiplier norm and the operator norm of the multiplication operator on the noncommutative $\\mathrm{L}^p$-space. Overall, the paper's contributions provide a deeper understanding of the structure of noncommutative $\\mathrm{L}^p$-spaces and their various mappings, which can have applications in areas such as quantum information theory and functional analysis. 1 into PostgreSQL...\n",
      "Inserting test sample 1843  It has recently been reported that the faintest galaxies at z~6-7 display extremely blue UV continuum slopes, with a UV power-law index beta ~ -3. Such slopes are bluer than previously reported for any other galaxy population, and imply extinction-free, young, very low-metallicity stellar populations with a high ionizing photon escape fraction. Here we undertake a critical study of the evidence for such extreme values of beta, combining three new WFC3/IR-selected samples of galaxies spanning ~2 decades in UV luminosity over the redshift range z~4.5-8. We explore the impact of inclusion/exclusion of less robust high-z candidates, and use the varying depths of the samples to explore the effects of noise and selection bias. Simple data-consistency arguments suggest that artificially blue average values of beta can result when the analysis is extended into the deepest ~ 0.5-mag bin of these WFC3/IR-selected samples, regardless of the actual luminosity or z range probed. By confining attention to robust, well-detected high-z galaxy candidates, we find that the average value of beta is consistent with -2.05 +/- 0.10 for z=5-7, and -22 < M_UV < -18. We create and analyse a set of simulations which demonstrate that a bias towards artifically low/blue average values of beta is indeed expected when the UV slope analysis is extended towards the source detection threshold, and conclude that there is as yet no clear evidence for UV slopes significantly bluer than beta ~ -2, the typical value displayed by the bluest star-forming galaxies at more modest z. A robust measurement of beta for the faintest galaxies at z~7-8 remains a key observational goal, as it provides a fundamental test for high escape fractions from a potentially abundant source of reionizing photons. This goal is achievable with HST, but requires still deeper WFC3/IR imaging in the HUDF. 0 into PostgreSQL...\n",
      "Inserting test sample 1844  This research paper explores the ultraviolet (UV) continuum slopes of high-redshift galaxies in order to better understand the nature of their stellar populations. Specifically, we investigate whether there is evidence for extreme stellar populations at redshifts greater than 6. To do so, we analyze a sample of galaxies observed by the Hubble Space Telescope's Wide Field Camera 3 (WFC3) and use a variety of techniques to measure their UV continuum slopes.\n",
      "\n",
      "Our analysis reveals that these high-redshift galaxies exhibit a range of UV continuum slopes, but that there is no clear evidence for extreme stellar populations. Indeed, our results suggest that the majority of the observed galaxies have UV continuum slopes that are consistent with a combination of young and intermediate-age stellar populations, similar to what is observed in lower-redshift galaxies. This finding is surprising, given that high-redshift galaxies are expected to be dominated by younger, more massive stars. However, our observations may be limited by the sensitivity of WFC3, and future observations with more advanced instruments could reveal more extreme stellar populations.\n",
      "\n",
      "Overall, our study provides valuable insights into the nature of high-redshift galaxies and their stellar populations. Our results suggest that these galaxies may not be as extreme as previously thought, and that more observations are needed in order to fully understand their properties. Furthermore, our study demonstrates the importance of using a range of techniques to measure galaxy properties, and highlights the power of UV observations for investigating the earliest stages of galaxy formation and evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 1845  The US Data Encryption Standard, DES for short, is put forward as an interesting benchmark problem for nonmonotonic reasoning systems because (i) it provides a set of test cases of industrial relevance which shares features of randomly generated problems and real-world problems, (ii) the representation of DES using normal logic programs with the stable model semantics is simple and easy to understand, and (iii) this subclass of logic programs can be seen as an interesting special case for many other formalizations of nonmonotonic reasoning. In this paper we present two encodings of DES as logic programs: a direct one out of the standard specifications and an optimized one extending the work of Massacci and Marraro. The computational properties of the encodings are studied by using them for DES key search with the Smodels system as the implementation of the stable model semantics. Results indicate that the encodings and Smodels are quite competitive: they outperform state-of-the-art SAT-checkers working with an optimized encoding of DES into SAT and are comparable with a SAT-checker that is customized and tuned for the optimized SAT encoding. 0 into PostgreSQL...\n",
      "Inserting test sample 1846  The \"Disjunctive Event Set\" (DES) challenge problem has emerged as an important benchmark for evaluating and developing nonmonotonic reasoning systems. DES is designed to test the ability of such systems to handle complex, dynamic and uncertain situations by asking them to reason about disjunctive events sets with incomplete information. This problem involves three major tasks: to resolve conflicts between events that could apply to the same situation, to handle default reasoning which allows us to reason about unknown events and to track the evolution of the knowledge state of the system over time. The DES challenge problem has become a standard for evaluating the performance of nonmonotonic reasoning systems and thus providing a common ground for the comparison of various approaches. In this paper, we provide a comprehensive overview of the DES challenge problem, its different versions and its applications. We also compare several reasoning systems and show how they perform on the DES challenge. Our results demonstrate the strengths and limitations of existing nonmonotonic reasoning systems and point towards future research directions. 1 into PostgreSQL...\n",
      "Inserting test sample 1847  Galactic globular clusters (GC) are known to have multiple stellar populations and be characterised by similar chemical features, e.g. O-Na anti-correlation. While second-population stars, identified by their Na overabundance, have been found from the main sequence turn-off up to the tip of the red giant branch in various Galactic GCs, asymptotic giant branch (AGB) stars have rarely been targeted. The recent finding that NGC 6752 lacks an Na-rich AGB star has thus triggered new studies on AGB stars in GCs, since this result questions our basic understanding of GC formation and stellar evolution theory. In order to compare the Na abundance distributions of AGB and RGB stars in Galactic GCs and investigate whether the presence of Na-rich stars on the AGB is metallicity-dependent, we obtained the high-resolution spectra with the multi-object high-resolution spectrograph FLAMES on ESO/VLT for a sample of AGB and RGB stars in the Galactic GC NGC 2808. The accurate Na abundances were derived for 31 AGB and 40 RGB stars. We find that NGC 2808 has a mean metallicity of -1.11 $\\pm$ 0.08 dex, in good agreement with earlier analyses.\n",
      "\n",
      "Comparable Na abundance dispersions are derived for our AGB and RGB samples, with the AGB stars being slightly more concentrated than the RGB stars. The ratios of Na-poor first-population to Na-rich second-population stars are 45:55 in the AGB sample and 48:52 in the RGB sample. NGC 2808 has Na-rich second-population AGB stars, which turn out to be even more numerous - in relative terms - than their Na-poor AGB counterparts and the Na-rich stars on the RGB. Our findings are well reproduced by the fast rotating massive stars scenario and they do not contradict the recent results that there is not an Na-rich AGB star in NGC 6752. NGC 2808 thus joins the larger group of Galactic GCs for which Na-rich second-population stars on the AGB have recently been found. 0 into PostgreSQL...\n",
      "Inserting test sample 1848  This study presents the first detailed analysis of sodium abundance in asymptotic giant branch (AGB) and red giant branch (RGB) stars in NGC 2808, one of the most massive galactic globular clusters. We derive sodium abundances for 85 red giant branch (RGB) stars and 6 asymptotic giant branch (AGB) stars using high-resolution spectra obtained with the FLAMES/GIRAFFE spectrograph on the ESO VLT. Our results reveal a bimodal distribution of sodium abundances in NGC 2808, with 35 stars belonging to a Na-poor population and 56 stars belonging to a Na-rich population. The two populations show a clear separation in the RGB, but overlap in the AGB, indicating that the Na-rich population is younger than the Na-poor population.\n",
      "\n",
      "We find that NGC 2808 shows a spread in sodium abundance of Î”[Na/Fe] = 1.15 dex among RGB stars, which is larger than that observed in most other globular clusters. This spread is largely due to the Na-rich population, which shows a wide range of sodium abundances with a mean value of [Na/Fe] = 0.42 dex. The Na-poor population shows a mean sodium abundance of [Na/Fe] = 0.02 dex. The AGB stars in NGC 2808 are found to have a mean sodium abundance of [Na/Fe] = 0.24 dex, which is intermediate between the mean values of the two RGB populations.\n",
      "\n",
      "We discuss the possible origins of the sodium abundance spread in NGC 2808, including pollution from intermediate-mass AGB stars and/or fast-rotating massive stars, as well as self-enrichment from the first generations of stars in the cluster. Our results provide new and important constraints on the formation and evolution of galactic globular clusters, and lay the groundwork for future studies of sodium abundance variations in other stellar populations. 1 into PostgreSQL...\n",
      "Inserting test sample 1849  Recently, the synchronization on multi-layer networks has drawn a lot of attention. In this work, we study the stability of the complete synchronization on duplex networks. We investigate effects of coupling function on the complete synchronization on duplex networks. We propose two approximation methods to deal with the stability of the complete synchronization on duplex networks. In the first method, we introduce a modified master stability function and, in the second method, we only take into consideration the contributions of a few most unstable transverse modes to the stability of the complete synchronization. We find that both methods work well for predicting the stability of the complete synchronization for small networks. For large networks, the second method still works pretty well. 0 into PostgreSQL...\n",
      "Inserting test sample 1850  This paper presents an investigation of approximation methods for estimating the stability of complete synchronization on duplex networks. A duplex network consists of two interdependent networks, where the nodes in each network are connected to corresponding nodes in the other network. Complete synchronization refers to a state in which the nodes in both networks exhibit identical dynamics. The proposed methods are based on linear stability analysis and are applicable to a broad range of duplex networks. The approximation results are verified through numerical simulations and compared with existing methods. Our work provides insights into the stability behavior of duplex networks and suggests potential applications in various fields, such as neuroscience and power systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1851  Image space feature detection is the act of selecting points or parts of an image that are easy to distinguish from the surrounding image region. By combining a repeatable point detection with a descriptor, parts of an image can be matched with one another, which is useful in applications like estimating pose from camera input or rectifying images. Recently, precise indoor tracking has started to become important for Augmented and Virtual reality as it is necessary to allow positioning of a headset in 3D space without the need for external tracking devices. Several modern feature detectors use homographies to simulate different viewpoints, not only to train feature detection and description, but test them as well. The problem is that, often, views of indoor spaces contain high depth disparity. This makes the approximation that a homography applied to an image represents a viewpoint change inaccurate. We claim that in order to train detectors to work well in indoor environments, they must be robust to this type of geometry, and repeatable under true viewpoint change instead of homographies. Here we focus on the problem of detecting repeatable feature locations under true viewpoint change. To this end, we generate labeled 2D images from a photo-realistic 3D dataset. These images are used for training a neural network based feature detector. We further present an algorithm for automatically generating labels of repeatable 2D features, and present a fast, easy to use test algorithm for evaluating a detector in an 3D environment. 0 into PostgreSQL...\n",
      "Inserting test sample 1852  The ability to detect repeatable 2D features is a crucial component in computer vision and object recognition systems. In recent years, deep neural networks (DNNs) have demonstrated outstanding performance in detecting and recognizing such features. However, the accuracy of these networks is often limited by the size and quality of the training datasets.\n",
      "\n",
      "One potential solution to this problem is to use large amounts of 3D world capture data to train DNNs. In this paper, we present a comprehensive approach for utilizing such data to improve the detection of repeatable 2D features. We start by constructing a large-scale dataset of 3D objects and their corresponding 2D images. We then train a DNN using this dataset, fine-tune it using transfer learning, and evaluate its performance on different benchmark datasets.\n",
      "\n",
      "Our results demonstrate that the proposed approach significantly improves the accuracy and robustness of feature detection. We also analyze the impact of different factors such as dataset size and noise level on the performance of the trained network. Furthermore, we compare our approach to existing methods and show its superiority in terms of both accuracy and efficiency.\n",
      "\n",
      "Overall, our study highlights the potential of large-scale 3D world capture data for training DNNs and improving their performance in detecting repeatable 2D features. Our approach can be applied to various computer vision and robotics tasks, and we believe it will contribute to the advancement of these fields. 1 into PostgreSQL...\n",
      "Inserting test sample 1853  The genesis of lasing, as an evolution of the laser hybrid eigenstates comprised of electromagnetic modes and atomic polarization, is considered. It is shown that the start of coherent generation at the laser threshold is preceded by the formation of a special hybrid state at the lasing pre-threshold. This special state is characterized by an enhanced coupling among excited atoms and electromagnetic modes. This leads to an increase in the rate of stimulated emission in the special state and, ultimately, to lasing. At the lasing pre-threshold, the transformation of hybrid eigenstates has the features of an exceptional point (EP) observed in non-Hermitian systems. The special state is formed when eigenfrequencies of two hybrid states coalesce or come close to each other. Below the pre-threshold, lifetimes of all hybrid states grow with increasing pump rate. When the pump rate crosses the pre-threshold, resonance trapping occurs with the lifetime of the special state continuing to increase while the lifetimes of all other eigenstates begin to decrease. Consequently, the latter eigenstates do not participate in the lasing. Thus, above the pre-threshold, a laser transitions into the single-mode regime. 0 into PostgreSQL...\n",
      "Inserting test sample 1854  This research explores the potential of exceptional points as lasing pre-thresholds in open-cavity lasers. Exceptional points have been identified as singularities in the spectrum of non-Hermitian systems where two or more eigenvalues coalesce. Recent studies have demonstrated that exceptional points can produce exotic phenomena in various physical systems, including optical systems, which can be amplified by lasers. In particular, open-cavity lasers have shown promise in demonstrating exceptional point lasing behavior with low-intensity thresholds. By characterizing exceptional points through analytical models and numerical simulations, this research aims to provide insights into the underlying physics of lasing pre-thresholds at exceptional points in open-cavity lasers. Results obtained from this study can facilitate the design of more efficient open-cavity lasers, enhance their performance, and lay the groundwork for future scientific research on exceptional points in other physical systems. This research opens up new opportunities for understanding the unique behavior of exceptional points in open-cavity lasers, which may have significant implications in advanced optical devices, sensing, and communication applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1855  From Smyth's classification, modular compactifications of pointed smooth rational curves are indexed by combinatorial data, so-called extremal assignments. We explore their combinatorial structures and show that any extremal assignment is a finite union of atomic extremal assignments. We discuss a connection with the birational geometry of the moduli space of stable pointed curves. As applications, we study three special classes of extremal assignments: smooth, toric, and invariant with respect to the symmetric group action. We identify them with three combinatorial objects: simple intersecting families, complete multipartite graphs, and special families of integer partitions, respectively. 0 into PostgreSQL...\n",
      "Inserting test sample 1856  In this paper, we study the birational contractions of the moduli space $\\overline{\\mathrm{M}}_{0,n}$ and investigate the combinatorics of extremal assignments. We prove that for $n\\geq 10$, the boundary divisors of $\\overline{\\mathrm{M}}_{0,n}$ cannot be contracted to a point via a sequence of elementary contractions. We also explore the correspondence between birational contractions on $\\overline{\\mathrm{M}}_{0,n}$ and combinatorial types of extremal ray assignments. Our findings provide new insights into the geometric and combinatorial properties of $\\overline{\\mathrm{M}}_{0,n}$, and suggest further applications to algebraic geometry and combinatorics. 1 into PostgreSQL...\n",
      "Inserting test sample 1857  Context. Using observations to deduce dust properties, grain size distribution, and physical conditions in molecular clouds is a highly degenerate problem. Aims. The coreshine phenomenon, a scattering process at 3.6 and 4.5 $\\mu$m that dominates absorption, has revealed its ability to explore the densest parts of clouds. We want to use this effect to constrain the dust parameters. The goal is to investigate to what extent grain growth (at constant dust mass) inside molecular clouds is able to explain the coreshine observations. We aim to find dust models that can explain a sample of Spitzer coreshine data. We also look at the consistency with near-infrared data we obtained for a few clouds. Methods. We selected four regions with a very high occurrence of coreshine cases: Taurus-Perseus, Cepheus, Chameleon and L183/L134. We built a grid of dust models and investigated the key parameters to reproduce the general trend of surface bright- nesses and intensity ratios of both coreshine and near-infrared observations with the help of a 3D Monte-Carlo radiative transfer code. The grid parameters allow to investigate the effect of coagulation upon spherical grains up to 5 $\\mu$m in size derived from the DustEm diffuse interstellar medium grains. Fluffiness (porosity or fractal degree), ices, and a handful of classical grain size distributions were also tested. We used the near- and mostly mid-infrared intensity ratios as strong discriminants between dust models. Results. The determination of the background field intensity at each wavelength is a key issue. In particular, an especially strong background field explains why we do not see coreshine in the Galactic plane at 3.6 and 4.5 $\\mu$m. For starless cores, where detected, the observed 4.5 $\\mu$m / 3.6 $\\mu$m coreshine intensity ratio is always lower than $\\sim$0.5 which is also what we find in the models for the Taurus-Perseus and L183 directions. Embedded sources can lead to higher fluxes (up to four times greater than the strongest starless core fluxes) and higher coreshine ratios (from 0.5 to 1.1 in our selected sample). Normal interstellar radiation field conditions are sufficient to find suitable grain models at all wavelengths for starless cores. The standard interstellar grains are not able to reproduce observations and, due to the multi-wavelength approach, only a few grain types meet the criteria set by the data. Porosity does not affect the flux ratios while the fractal dimension helps to explain coreshine ratios but does not seem able to reproduce near-infrared observations without a mix of other grain types. Conclusions. Combined near- and mid-infrared wavelengths confirm the potential to reveal the nature and size distribution of dust grains. Careful assessment of the environmental parameters (interstellar and background fields, embedded or nearby reddened sources) is required to validate this new diagnostic. 0 into PostgreSQL...\n",
      "Inserting test sample 1858  Dust is a crucial component of the interstellar medium, contributing to the formation and evolution of stars and galaxies. In particular, the study of dust properties inside molecular clouds is essential for understanding the physical conditions that lead to star formation. In this work, we present a study on the dust properties inside molecular clouds based on coreshine modeling and observations.\n",
      "\n",
      "Coreshine is a phenomenon observed in dense cores of molecular clouds, where the emission from the dust at long wavelengths is enhanced due to scattering of the interstellar radiation field. We use the coreshine observations obtained with the Herschel Space Observatory and model them using a radiative transfer simulation code. Our aim is to derive the dust properties inside the cores, such as the dust temperature, opacity, and spectral index.\n",
      "\n",
      "We find that the dust temperature inside the cores is generally lower than the surrounding cloud, which implies that the cores are shielded from the interstellar radiation field. The opacity of the dust is also higher inside the cores, indicating that the dust grains are larger and more abundant than in the surrounding cloud. The spectral index of the dust inside the cores is flatter than the surrounding cloud, indicating the presence of larger dust grains.\n",
      "\n",
      "We also study the relation between the dust properties and the physical properties of the cores, such as their mass and size. We find that the dust temperature and spectral index are correlated with the core mass, implying that the more massive cores have lower temperatures and larger dust grains. We also find that the dust opacity is correlated with the core size, implying that the larger cores have more abundant and larger dust grains.\n",
      "\n",
      "Finally, we compare our results with previous studies and theoretical models of dust properties inside molecular clouds. We find that our results are consistent with the idea that the dust properties inside the cores are determined by the physical conditions of the cores, such as the density and temperature. Our study provides important insights into the dust properties inside molecular clouds and their relation to star formation, which is crucial for understanding the formation and evolution of galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 1859  We study the primordial magnetic field generated by the simple model $f^2 FF$ in Starobinsky, $R^2$-inflationary, model. The scale invariant PMF is achieved at relatively high power index of the coupling function, $\\left| \\alpha \\right| \\approx 7.44$. This model does not suffer from the backreaction problem as long as, the rate of inflationary expansion, $H$, is in the order of or less than the upper bound reported by Planck ($\\le 3.6 \\times 10^{-5} M_\\rm{Pl}$) in both de Sitter and power law expansion, which show similar results. We calculate the lower limit of the reheating parameter, $R_\\rm{rad} > 6.888$ in $R^2$-inflation. Based on the upper limit obtained from CMB, we find that the upper limits of magnetic field and reheating energy density as, $\\left(\\rho_{B_\\rm{end}} \\right)_\\rm{CMB} < 1.184 \\times 10^{-20} M_\\rm{Pl}^4$ and $\\left(\\rho_\\rm{reh} \\right)_\\rm{CMB} < 8.480 \\times 10^{-22} M_\\rm{Pl}^4$.\n",
      "\n",
      "All of foregoing results are well more than the lower limit derived from WMAP7 for both large and small field inflation. By using the Planck inflationary constraints, 2015 in the context of ${R^2}$-inflation, the upper limit of reheating temperature and energy density for all possible values of $w _\\rm{reh}$ are respectively constrained as, $T_\\rm{reh} < 4.32 \\times 10^{13} \\rm{GeV}$ and $\\rho_\\rm{reh} < 3.259 \\times 10^{-18} M_\\rm{Pl}^4$ at $n_\\rm{s} \\approx 0.9674$. This value of spectral index is well consistent with Planck, 2015 results. Adopting $T_\\rm{reh}$, enables us to constrain the reheating e-folds number, $N_\\rm{reh}$ on the range $1 < N_\\rm{reh} < 8.3$, for $- 1/3 < w_\\rm{reh} < 1$. By using the scale invariant PMF generated by $f^2 FF$, we find that the upper limit of present magnetic field, $B_0 < 8.058 \\times 10^{-9} \\rm{G}$. 0 into PostgreSQL...\n",
      "Inserting test sample 1860  In this paper, we study the possibility of generating a primordial magnetic field during inflation by considering $R^{2}$-inflation scenarios. We analyze the implications of the most recent data from the Planck mission, which provides constraints on the amplitude and scale-dependence of the primordial power spectrum of curvature perturbations.\n",
      "\n",
      "Our analysis shows that, in the context of $R^{2}$-inflation, the generation of a magnetic field can be achieved by the coupling between the inflaton and the gauge fields. This mechanism is known as inflationary magnetogenesis. We investigate the constraints on the parameters of this model, such as the coupling constant and the energy scale of inflation.\n",
      "\n",
      "We find that the Planck data favors a weak coupling scenario, where the magnetic field amplitude is small and the energy scale of inflation is high. However, the constraints on the coupling constant are not very strong, and the primordial magnetic field could be generated for a range of values of this parameter.\n",
      "\n",
      "We also study the effects of the generated magnetic field on the dynamics of the inflaton. We show that the backreaction of the magnetic field on the inflaton can be significant, leading to a modification of the inflationary dynamics.\n",
      "\n",
      "Finally, we discuss the observational implications of our model, focusing on the signatures of inflationary magnetogenesis in the cosmic microwave background radiation and in the large-scale structure of the universe. We show that our scenario can produce a magnetic field with a scale-invariant spectrum, consistent with current constraints from observations.\n",
      "\n",
      "Overall, our analysis reveals that inflationary magnetogenesis in $R^{2}$-inflation is a viable mechanism for generating a primordial magnetic field, and that it is consistent with the most recent observational data from the Planck mission. 1 into PostgreSQL...\n",
      "Inserting test sample 1861  We address a particular problem of output regulation for multi-input multi-output nonlinear systems. Specifically, we are interested in making the stability of an equilibrium point and the regulation to zero of an output, robust to (small) unmodelled discrepancies between design model and actual system in particular those introducing an offset. We propose a novel procedure which is intended to be relevant to real life systems, as illustrated by a (non academic) example. 0 into PostgreSQL...\n",
      "Inserting test sample 1862  This paper presents a novel approach to address the challenge of designing an output feedback controller for multi-input multi-output nonlinear (MIMO-NL) systems. An integral action scheme is proposed to improve the tracking performance of the controller. The proposed approach is demonstrated through simulation results on a MIMO-NL system, verifying its effectiveness. Further, comparisons with existing methods show the superiority of the proposed scheme. 1 into PostgreSQL...\n",
      "Inserting test sample 1863  The formation of stars and planetary systems is a complex phenomenon, which relies on the interplay of multiple physical processes. Nonetheless, it represents a crucial stage for our understanding of the Universe, and in particular of the conditions leading to the formation of key molecules (e.g.\n",
      "\n",
      "water) on comets and planets. Herschel observations demonstrated that stars form out of gaseous filamentary structures in which the main constituent is molecular hydrogen (H$_2$). Depending on its nuclear spin H$_2$ can be found in two forms: `ortho' with parallel spins and `para' where the spins are anti-parallel. The relative ratio among these isomers, i.e. the ortho-to-para ratio (OPR), plays a crucial role in a variety of processes related to the thermodynamics of star-forming gas and to the fundamental chemistry affecting the deuteration of water in molecular clouds, commonly used to determine the origin of water in Solar System's bodies. Here, for the first time, we assess the evolution of the OPR starting from the warm neutral medium, by means of state-of-the-art three-dimensional magneto-hydrodynamic simulations of turbulent molecular clouds. Our results show that star-forming clouds exhibit a low OPR ($\\ll 0.1$) already at moderate densities ($\\sim$1000 cm$^{-3}$). We also constrain the cosmic rays ionisation rate, finding that $10^{-16}\\,\\rm s^{-1}$ is the lower limit required to explain the observations of diffuse clouds. Our results represent a step forward in the understanding of the star and planet formation process providing a robust determination of the chemical initial conditions for both theoretical and observational studies. 0 into PostgreSQL...\n",
      "Inserting test sample 1864  The ortho-to-para H2 ratio is a key probe of the thermal and chemical evolution of dense molecular gas in star-forming regions. Recent observations have revealed a surprisingly low ratio in several star-forming filaments, posing a major challenge to our understanding of gas heating and cooling processes in these objects. In this work, we explore potential mechanisms that could explain the observed low o/p H2 ratios. Using a non-equilibrium gas-grain chemical model coupled to a 1D radiative transfer code, we simulate the chemical and thermal evolution of molecular gas in a filamentary cloud threaded by a strong magnetic field. Our simulations show that magnetic cooling, which is known to be a dominant cooling mechanism at high densities, can lead to a significant suppression of the para-H2 formation rate, thus lowering the o/p H2 ratio. The exact magnitude of this effect depends on the strength and orientation of the magnetic field, the initial chemical and thermal conditions, and the filament geometry. We also investigate the impact of other factors such as cosmic-ray ionization, UV irradiation, and ambipolar diffusion on the o/p H2 ratio. Our results suggest that the low o/p H2 ratios observed in star-forming filaments may be a natural consequence of the interplay between magnetic cooling and chemistry, rather than a consequence of changes in the elemental abundances or external irradiation. 1 into PostgreSQL...\n",
      "Inserting test sample 1865  Cases have shown that WENO schemes usually behave robustly on problems containing shocks with high pressure ratios when uniformed or smooth grids are present, while nonlinear schemes based on WENO interpolations might relatively be liable to numerical instability. In the meanwhile, the latter have manifested their advantages in computations on grids of bad quality, because the free-stream preservation is easily realized there, and what is more flux-splitting schemes with low dissipations can be engaged inherently as well.\n",
      "\n",
      "Targeting at above dissatisfactions, a method by hybridizing WENO implementations of interpolation and reconstruction-wise operation for upwind-biased schemes with flux splitting employed is proposed and corresponding third-, fifth- and seventh-order upwind-biased schemes are proposed. Based on the understandings of [Q. Li, et al. Commun. Comput. Phys.\n",
      "\n",
      "22 (2017) 64-94], the free-stream preservation of proposed schemes is achieved with incorporation of frozen grid metrics in WENO reconstructions-wise operations on split fluxes. In proposed schemes, flux-splitting schemes with low dissipation can also be applied for the flux on a cell edge. As a byproduct, an implementation of WENO scheme with free-stream preservation is obtained. Numerical examples are provided as following with the third- and fifth-order schemes being tested. In tests of free-stream preservation, the property is achieved as expected (including two implementations of WENO). The computation of 1-D Sod problem shows the capability of proposed schemes on solving ordinary shock discontinuity. 2-D vortex preservation and double Mach reflection are tested on uniformed and randomized grids. The accomplishment by proposed schemes manifests their capability and robustness on solving problems under rigorous circumstances. 0 into PostgreSQL...\n",
      "Inserting test sample 1866  This research paper proposes a novel approach to hybridize Weighted Essentially Non-Oscillatory (WENO) implementations of interpolation and reconstruction-wise operation for upwind-biased schemes while simultaneously achieving free-stream preservation. The proposed method is based on combining the strengths of three different upwind-biased schemes to achieve a hybrid method that possesses excellent properties of each scheme. The proposed method effectively suppresses numerical oscillations and shock errors that commonly occur in upwind-biased schemes while also maintaining the essential numerical features of each scheme.\n",
      "\n",
      "Furthermore, we demonstrate the efficiency and accuracy of the proposed hybrid method using a variety of benchmark tests, which include a series of one-dimensional and two-dimensional models. Results show that the hybrid method effectively eliminates the shortcomings of conventional upwind-biased schemes while significantly enhancing the solution accuracy. Additionally, we compare our hybrid method with other state-of-the-art methods and demonstrate that our method surpasses the performance of these methods.\n",
      "\n",
      "Finally, we thoroughly discuss the mechanisms behind the proposed hybridization and methods for selecting optimal numerical parameters. We also explore possible extensions of the proposed method for more complex cases. In summary, the proposed hybridization method of WENO implementations shows great promise for both academic and practical purposes, particularly in the fields of numerical solutions to partial differential equations and computational fluid dynamics. 1 into PostgreSQL...\n",
      "Inserting test sample 1867  Understanding how galaxies evolved from the early Universe through cosmic time is a fundamental part of modern astrophysics. In order to study this evolution it is important to sample the galaxies at various times in a consistent way through time. In regular luminosity selected samples, our analyses are biased towards the brightest galaxies at all times (as these are easier to observe and identify). A complementary method relies on the absorption imprint from neutral gas in galaxies, the so-called damped Ly-alpha absorbers (DLAs) seen towards distant bright objects. This thesis seeks to understand how the absorption selected galaxies relate to the emission selected galaxies by identifying the faint glow from the absorbing galaxies at redshift z~2.\n",
      "\n",
      "In the last Chapter, a study of the more evolved, massive galaxies is presented. These galaxies are observed to be a factor of 2 to 6 times smaller than local galaxies of similar masses. A new spectroscopically selected sample is presented and the increased precision of the redshifts allows a more detailed measurement of the scatter in the mass-size relation. The size evolution of massive, quiescent galaxies is modelled by a `dilution' scenario, in which progressively larger galaxies at later times are added to the population of denser galaxies, causing an increase of the mean size of the population. This model describes the evolution of both sizes and number densities very well, however, the scatter in the model increases with time, contrary to the data. It is thus concluded that a combination of `dilution' and individual growth, e.g., through mergers, is needed.\n",
      "\n",
      "For brevity, the individual chapters based on published peer-reviewed articles are omitted and a link to the given article is given instead.\n",
      "\n",
      "Principal supervisor: Johan P. U. Fynbo 0 into PostgreSQL...\n",
      "Inserting test sample 1868  This paper investigates the properties and characteristics of galaxies in the early universe, focusing on both their absorption and emission features. We present a comprehensive analysis of the spectra of distant galaxies, ranging from redshifts z ~ 2.5 to z ~ 6.5, using high-resolution spectroscopy data obtained from ground-based and space-based observatories. Our study utilizes the Lyman-alpha line, a key spectral feature in the ultraviolet region, to probe the intergalactic medium (IGM) and the circumgalactic medium (CGM) of these galaxies. We also investigate the other important spectral features, such as the Lyman continuum, the He II Lyman-alpha line, and the metal lines, to explore the star formation, chemical evolution, and ionization state of the gas in these galaxies.\n",
      "\n",
      "We find that the IGM and CGM around these galaxies have a significant impact on their spectral properties. The absorption signatures in the Lyman-alpha forest and the damping wing region reveal the thermal and kinematic properties of the IGM and CGM, providing insights into the physical processes that regulate the gas dynamics and cooling in these regions. On the other hand, the emission signatures in the Lyman-alpha line and the other spectral lines trace the properties of the ionizing radiation and the process of converting gas into stars in these galaxies.\n",
      "\n",
      "Our study sheds light on the formation and evolution of galaxies in the early universe and provides important constraints on the models of galaxy formation. The absorption and emission properties of these galaxies provide a unique probe of the physical conditions of the gas in the distant universe and yield insights into the processes that drive the growth and evolution of these systems. Our results suggest that the interplay between galaxies and their surrounding medium is complex and dynamic, and requires a multi-wavelength approach to fully understand its impact on galaxy formation and evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 1869  Automatically generating coherent and semantically meaningful text has many applications in machine translation, dialogue systems, image captioning, etc.\n",
      "\n",
      "Recently, by combining with policy gradient, Generative Adversarial Nets (GAN) that use a discriminative model to guide the training of the generative model as a reinforcement learning policy has shown promising results in text generation. However, the scalar guiding signal is only available after the entire text has been generated and lacks intermediate information about text structure during the generative process. As such, it limits its success when the length of the generated text samples is long (more than 20 words). In this paper, we propose a new framework, called LeakGAN, to address the problem for long text generation. We allow the discriminative net to leak its own high-level extracted features to the generative net to further help the guidance. The generator incorporates such informative signals into all generation steps through an additional Manager module, which takes the extracted features of current generated words and outputs a latent vector to guide the Worker module for next-word generation. Our extensive experiments on synthetic data and various real-world tasks with Turing test demonstrate that LeakGAN is highly effective in long text generation and also improves the performance in short text generation scenarios. More importantly, without any supervision, LeakGAN would be able to implicitly learn sentence structures only through the interaction between Manager and Worker. 0 into PostgreSQL...\n",
      "Inserting test sample 1870  The task of generating long-form text that is coherent, diverse, and contextually appropriate has been a long-standing challenge in natural language processing. Adversarial training has shown promise in generating high-quality text, but its effectiveness is limited by the difficulty of training deep neural networks. In this work, we propose a novel adversarial training framework that incorporates leaked information to improve the quality of generated text. Specifically, we introduce an auxiliary task to predict the hidden codes used in the generator, and use this information to guide the discriminator. Our results demonstrate that leaked information can significantly improve the diversity, coherence, and semantic relevance of generated text, compared to existing adversarial training methods.\n",
      "\n",
      "We evaluate our approach on several benchmark datasets and show that it outperforms state-of-the-art methods in terms of quality metrics and human evaluation. Additionally, we conduct extensive ablation studies to validate the effectiveness of our leaked information framework and provide insights into the underlying mechanism of our approach.\n",
      "\n",
      "Our work has important implications for a wide range of text generation applications, including machine translation, summarization, and dialogue systems. We believe that our proposed framework will pave the way for more effective and efficient text generation algorithms that can better capture the complex relationships between text and its context. 1 into PostgreSQL...\n",
      "Inserting test sample 1871  In this study, we analyze the recently proposed charge transfer fluctuations within a finite pseudo-rapidity space. As the charge transfer fluctuation is a measure of the local charge correlation length, it is capable of detecting inhomogeneity in the hot and dense matter created by heavy ion collisions. We predict that going from peripheral to central collisions, the charge transfer fluctuations at midrapidity should decrease substantially while the charge transfer fluctuations at the edges of the observation window should decrease by a small amount. These are consequences of having a strongly inhomogeneous matter where the QGP component is concentrated around midrapidity. We also show how to constrain the values of the charge correlations lengths in both the hadronic phase and the QGP phase using the charge transfer fluctuations. 0 into PostgreSQL...\n",
      "Inserting test sample 1872  The detection of Quark Gluon Plasma (QGP) is an important topic in experimental nuclear physics due to its potential to provide insight into the early universe. Charge transfer fluctuations can be used to detect QGP, as they are sensitive to the properties of the medium in which they propagate. In this study, we present a novel method for detecting QGP using charge transfer fluctuations. Using simulations, we show that this method can reliably distinguish between QGP and hadronic matter. Additionally, we demonstrate that this method is robust to experimental noise and can therefore provide a powerful tool for QGP detection in high-energy heavy-ion collisions. Overall, our results suggest that charge transfer fluctuations show promise as a viable method for QGP detection and further studies can be done to explore its potential. 1 into PostgreSQL...\n",
      "Inserting test sample 1873  We report on the star formation histories and extinction in the central kpc region of a sample of starburst galaxies that have similar far infrared (FIR), 10 micron and K-band luminosities as those of the archetype starburst M82. Our study is based on new optical spectra and previously published K-band photometric data, both sampling the same area around the nucleus. Model starburst spectra were synthesized as a combination of stellar populations of distinct ages formed over the Hubble time, and were fitted to the observed optical spectra and K-band flux. The model is able to reproduce simultaneously the equivalent widths of emission and absorption lines, the continuum fluxes between 3500-7000 Ang, the K-band and the FIR flux. We require a minimum of 3 populations -- (1) a young population of age < 8 Myr, with its corresponding nebular emission, (2) an intermediate-age population (age < 500 Myr), and (3) an old population that forms part of the underlying disk or/and bulge population. The contribution of the old population to the K-band luminosity depends on the birthrate parameter and remains above 60% in the majority of the sample galaxies. Even in the blue band, the intermediate age and old populations contribute more than 40% of the total flux in all the cases. A relatively high contribution from the old stars to the K-band nuclear flux is also apparent from the strength of the 4000 Ang break and the CaII K line. The extinction of the old population is found to be around half of that of the young population. The contribution to the continuum from the relatively old stars has the effect of diluting the emission equivalent widths below the values expected for young bursts. The mean dilution factors are found to be 5 and 3 for the Halpha and Hbeta lines respectively. 0 into PostgreSQL...\n",
      "Inserting test sample 1874  The history of star formation and extinction in the central kiloparsec region of starburst galaxies, particularly those resembling M82, has been studied using multi-wavelength observations. Our results suggest that star formation activity in the central region of M82-like starbursts began roughly 100 million years ago, and has since been sustained at a rate of approximately 0.5 solar masses per year. We also find evidence of significant extinction in the same region, which we attribute to dust and gas obscuration. The high levels of extinction observed in M82-like starbursts make it difficult to obtain accurate measurements of the intrinsic properties of the starburst, such as its metallicity and ionization conditions.\n",
      "\n",
      "We compare these results to those obtained from studies of other starburst galaxies, and find that the central regions of M82-like systems are particularly dynamic and complex. These central regions are characterized by high levels of turbulence, dense clumps of gas and dust, and ongoing feedback from massive stars and supernovae. Our observations suggest that these factors contribute to the current star formation rate of M82-like starbursts, but also make it challenging to disentangle the various physical processes at work.\n",
      "\n",
      "We conclude that M82-like starburst galaxies represent an important class of objects for understanding the physics of star formation and its relationship to galaxy evolution. By combining multi-wavelength observations and sophisticated modeling techniques, we can gain insights into the complex interplay between gas, dust, and star formation activity in these systems. Our results will help to inform future studies of galaxy evolution, and provide a framework for understanding the diverse range of galaxy morphologies and star formation histories observed in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1875  A notorious problem in queueing theory is to compute the worst possible performance of the GI/G/1 queue under mean-dispersion constraints for the interarrival and service time distributions. We address this extremal queue problem by measuring dispersion in terms of Mean Absolute Deviation (MAD) instead of variance, making available recently developed techniques from Distributionally Robust Optimization (DRO). Combined with classical random walk theory, we obtain explicit expressions for the extremal interarrival time and service time distributions, and hence the best possible upper bounds, for all moments of the waiting time. {We also apply the DRO techniques to obtain tight lower bounds that together with the upper bounds provide robust performance intervals. We show that all bounds are computationally tractable and remain sharp, also when the mean and MAD are not known precisely, but estimated based on available data instead. 0 into PostgreSQL...\n",
      "Inserting test sample 1876  Queue analysis is a fundamental tool for understanding and optimizing performance in a variety of complex systems. However, real-world systems often exhibit extreme behavior that presents a challenge for traditional queueing models. In this paper, we introduce a novel approach to queue analysis that leverages the MAD dispersion measure to capture the impact of extreme behavior. Specifically, we propose a framework that combines MAD-based queueing models with statistical techniques for estimating model parameters. We show that this approach yields more accurate performance estimates than traditional methods when applied to real-world systems exhibiting extreme behavior. Furthermore, our framework is easy to implement and provides valuable insights into the dynamics of complex systems. Our results demonstrate the utility of the MAD dispersion measure in simplifying extremal queue analysis and improving system performance. 1 into PostgreSQL...\n",
      "Inserting test sample 1877  Large-scale atomistic simulations using the reactive empirical bond order force field approach is implemented to investigate thermal and mechanical properties of single-layer (SL) and multi-layer (ML) molybdenum disulfide (MoS$_2$). The amplitude of the intrinsic ripples of SL-MoS$_2$ are found to be smaller than those exhibited by graphene (GE). Furthermore, because of the van der Waals interaction between layers, the out-of-plane thermal fluctuations of ML-MoS$_2$ decreases rapidly with increasing number of layers. This trend is confirmed by the buckling transition due to uniaxial stress which occurs for a significantly larger applied tension as compared to graphene. For SL-MoS$_2$, the melting temperature is estimated to be 3700~K which occurs through dimerization followed by the formation of small molecules consisting of 2 to 5 atoms. When different types of vacancies are inserted in the SL-MoS$_2$ it results in a decrease of both the melting temperature as well as the stiffness. 0 into PostgreSQL...\n",
      "Inserting test sample 1878  In this study, we investigate the mechanical behavior of single-layer and multi-layer MoS$_2$ under various conditions. We observe and analyze rippling and buckling in both types of MoS$_2$ through atomic force microscopy imaging, and we determine their deformation mechanisms using density functional theory calculations. Our results show that rippling in single-layer MoS$_2$ is initiated through the formation of metastable basins, while buckling in multi-layer MoS$_2$ is driven by the interlayer van der Waals forces. We also examine the melting behavior of MoS$_2$ during high-temperature annealing using transmission electron microscopy and find that multi-layer MoS$_2$ has a higher melting temperature than single-layer MoS$_2$. Our study provides insights into the mechanical and thermal properties of MoS$_2$ which are important for its potential applications in nanoscale devices and materials. 1 into PostgreSQL...\n",
      "Inserting test sample 1879  We present the discovery and monitoring of the optical transient (OT) associated with GRB 020410. The fading OT was found by Hubble Space Telescope (HST) observations taken 28 and 65 days after burst at a position consistent with the X-ray afterglow. Subsequent re-examination of early ground based observations revealed that a faint OT was present 6 hours after burst, confirming the source association with GRB 020410. A deep non-detection after one week requires that the OT re-brightened between day 7 and day 28, and further late time HST data taken approximately 100 days after burst imply that it is very red.We compare both the flux and color of the excess with supernova models and show that the data are best explained by the presence of a Type Ib/c supernova at a redshift z ~ 0.5, which occured roughly coincident with the day of GRB. 0 into PostgreSQL...\n",
      "Inserting test sample 1880  The discovery of supernova SN 2002ap led to the identification and study of the afterglow of gamma-ray burst 020410. By analyzing the photometric and spectroscopic observations using the VLT and ESO 3.6-meter telescope, we found evidence supporting the theory of a supernova-driven hypernova producing GRBs. The radio and X-ray afterglows exhibited peculiar behavior, indicating that the GRB occurred in a low-density environment. Our study provides important insights into the connection between supernovae and GRBs, as well as the physics of shockwaves in high-energy astrophysics. The discovery of SN 2002ap and subsequent identification of the GRB afterglow highlights the importance of systematic follow-up observations of supernovae, particularly those exhibiting peculiar behaviors. Further studies of these systems have the potential to revolutionize our understanding of the interplay between supernova explosions and GRB formation. 1 into PostgreSQL...\n",
      "Inserting test sample 1881  We prove the result in the title. We infer, that unlike cylindric algebras, there is a first order axiomatization of the class of completely representable polyadic algebras of infinite dimension, though the one we obtain is infinite; in fact uncountable, but shares a single schema, stipulating that the (uncountably many)substitution operators are completely additive. Similar results are obtained for non commutative reducts of polyadic equality algebras of infinite dimensions, where we can drop complete additivity. However, it remains unknown to us whether there are atomic polyadic algebras of infinite dimension that are not completely additive; but we strongly conjecture that there are. 0 into PostgreSQL...\n",
      "Inserting test sample 1882  In this paper, we provide a characterization of the representability of polyadic algebras of infinite dimension. Specifically, we show that a polyadic algebra is completely representable if and only if it is both atomic and completely additive. Our proof builds on earlier work relating representability to certain subalgebras and on a new technique for constructing certain types of subalgebras. We demonstrate the power of our characterization by applying it to an example previously thought to be non-representable. Our results have implications for a wide range of fields, including mathematical logic, algebraic geometry, and quantum physics. 1 into PostgreSQL...\n",
      "Inserting test sample 1883  Cloud detection is the first step of any complex satellite-based cloud retrieval. No instrument detects all clouds, and analyses that use a given satellite climatology can only discuss a specific subset of clouds. We attempt to clarify which subsets of clouds are detected in a robust way by passive sensors, and which require active sensors. To do so, we identify where retrievals of Cloud Amounts (CAs), based on numerous sensors and algorithms, differ the most. We investigate large uncertainties, and confront retrievals from the CALIOP lidar, which detects semitransparent clouds and directly measures their vertical distribution, whatever the surface below. We document the cloud vertical distribution, opacity and seasonal variability where CAs from passive sensors disagree most. CALIOP CAs are larger than the passive average by +0.05 (AM) and +0.07 (PM). Over land, the +0.1 average difference rises to +0.2 over the African desert, Antarctica and Greenland, where large passive disagreements are traced to unfavorable surface conditions. Over oceans, CALIOP retrievals are closer to the average of passive retrievals except over the ITCZ (+0.1). Passive CAs disagree more in tropical areas associated with large-scale subsidence, where CALIOP observes a specific multi-layer cloud population: optically thin, high-level clouds and opaque (z>7km), shallow boundary layer clouds (z<2km). We evaluate the CA and cloud vertical distribution from 8 General Circulation Models where passive retrievals disagree and CALIOP provides new information. We find that modeled clouds are not more realistic where cloud detections from passive observations have long been robust, than where active sensors provide more reliable information. 0 into PostgreSQL...\n",
      "Inserting test sample 1884  This study assesses the global cloud distribution and identifies areas of disagreement among observations provided by the Cloud-Aerosol Lidar with Orthogonal Polarization (CALIOP), passive satellite sensors, and general circulation models (GCMs). The authors utilize a combination of cloud observations, derived products, and model simulations to perform a comprehensive analysis of the cloud cover at different altitudes across the globe.\n",
      "\n",
      "The findings reveal that the CALIOP observations exhibit a lower cloud fraction in comparison to the passive sensors and GCMs, particularly at higher altitudes. Moreover, the analysis highlights the discrepancies in the cloud distribution among different datasets in the latitudinal bands of the tropics and extratropics. These disparities are particularly prominent for boundary layer clouds and high-level clouds.\n",
      "\n",
      "The study also presents insights into the potential sources of these differences, including the limitations of each observing technique. The authors suggest that systematic errors in the CALIOP retrievals could be a significant factor in the observed discrepancies, and highlight the need for further investigation into the underlying physical processes of the observed cloud biases.\n",
      "\n",
      "Overall, these findings have important implications for improving our understanding of the global cloud distribution and for validating the accuracy of climate models. The results of this study can inform future research efforts aimed at addressing the limitations of current observing techniques and improving our understanding of cloud properties and their role in the Earth's climate system. 1 into PostgreSQL...\n",
      "Inserting test sample 1885  HR 6819 is a bright ($V=5.36$), blue star recently proposed to be a triple containing a detached black hole (BH). We show that the system is a binary and does not contain a BH. Using spectral decomposition, we disentangle the observed composite spectra into two components: a rapidly rotating Be star and a slowly rotating B star with low surface gravity $(\\log g \\approx 2.75)$. Both stars show periodic radial velocity (RV) variability, but the RV semi-amplitude of the B star's orbit is $K_{\\rm B}= (62.7 \\pm 1)\\,\\rm km\\,s^{-1}$, while that of the Be star is only $K_{\\rm Be} = (4.5\\pm 2)\\,\\rm km\\,s^{-1}$. This implies that the B star is less massive by at least a factor of 10. The surface abundances of the B star bear imprints of CNO burning. We argue that the B star is a bloated, recently stripped helium star with mass $\\approx 0.5\\,M_{\\odot}$ that is currently contracting to become a hot subdwarf. The orbital motion of the Be star obviates the need for a BH to explain the B star's motion. A stripped-star model reproduces the observed luminosity of the system, while a normal star with the B star's temperature and gravity would be more than 10 times too luminous. HR 6819 and the binary LB-1 probably formed through similar channels. We use MESA models to investigate their evolutionary history, finding that they likely formed from intermediate-mass ($3-7\\,M_{\\odot}$) primaries stripped by slightly lower-mass secondaries and are progenitors to Be + sdOB binaries such as $\\phi$ Persei. The lifetime of their current evolutionary phase is on average $2\\times 10^5$ years, of order half a percent of the total lifetime of the Be phase. This implies that many Be stars have hot subdwarf and white dwarf companions, and that a substantial fraction ($20-100\\%$) of field Be stars form through accretion of material from a binary companion. 0 into PostgreSQL...\n",
      "Inserting test sample 1886  Be stars are hot, luminous objects that have puzzled astronomers for decades. The origin of these stars has been the subject of intense investigation, but many questions remain unanswered. Recently, two black hole candidates, HR 6819 and LB-1, have shed new light on the possible origin of Be stars. In this paper, we propose that a stripped-companion origin may explain the properties of these two systems.\n",
      "\n",
      "Our hypothesis is based on the observation that both HR 6819 and LB-1 are binary systems. In HR 6819, the black hole candidate orbits an invisible object that is likely a B-type star. Meanwhile, in LB-1, a black hole with a mass of around 68 solar masses orbits a B-type star with a mass of around 12 solar masses. These systems are unusual because they do not contain a massive star that is typically expected to produce a black hole.\n",
      "\n",
      "We suggest that the stripped-companion scenario may explain these enigmatic systems. In this scenario, a massive star undergoes supernova explosion and becomes a black hole. The explosion strips off the outer layers of the star's companion, leaving behind a stripped star. The stripped star can then evolve into a Be star, which emits strong Balmer lines and has a circumstellar disk. The stripped star's hot companion can also interact with the surrounding material, producing X-ray emission and other observable features. \n",
      "\n",
      "Our theoretical model predicts that stripped-companion systems should be more common than previously thought, and that a significant fraction of Be stars may have originated from the stripped-companion scenario. This hypothesis is supported by recent observations, which have detected X-ray emission from several Be stars. These observations suggest that the stripped-companion scenario may indeed be a viable explanation for the origin of Be stars.\n",
      "\n",
      "In conclusion, we propose a stripped-companion origin for Be stars, which may explain the unusual properties of the black hole candidates HR 6819 and LB-1. Our model predicts that stripped-companion systems are more common than previously thought and may shed new light on the origin of hot, luminous objects such as Be stars. Further observations and theoretical studies are needed to confirm our hypothesis and explore its implications. 1 into PostgreSQL...\n",
      "Inserting test sample 1887  Strong, kilo-Gauss, magnetic fields are required to explain a range of observational properties in young, accreting pre-main sequence (PMS) systems.\n",
      "\n",
      "We review the techniques used to detect magnetic fields in PMS stars. Key results from a long running campaign aimed at characterising the large scale magnetic fields in accreting T Tauri stars are presented. Maps of surface magnetic flux in these systems can be used to build 3-D models exploring the role of magnetic fields and the efficiency with which magnetic fields can channel accretion from circumstellar disks on to young stars. Long-term variability in T Tauri star magnetic fields strongly point to a dynamo origin of the magnetic fields. Studies are underway to quantify how changes in magnetic fields affect their accretion properties. We also present the first results from a new programme that investigates the evolution of magnetic fields in intermediate mass (1.5-3 Msun) pre-main sequence stars as they evolve from being convective T Tauri stars to fully radiative Herbig AeBe stars. 0 into PostgreSQL...\n",
      "Inserting test sample 1888  Pre-main sequence stars are crucial objects for understanding the formation and evolution of stellar systems. Magnetic fields are believed to play an important role in shaping the physical and observational properties of these stars. In this paper, we review the current state of knowledge on the role of magnetic fields in pre-main sequence stars. We discuss how magnetic fields are generated and maintained in these objects, and how they affect the structure and dynamics of the stellar interior and the surrounding protoplanetary disk. We also review the observational evidence for the presence of magnetic fields in pre-main sequence stars and discuss their impact on the overall properties of these objects. Finally, we highlight the remaining questions and challenges in this field and suggest future directions for observational and theoretical studies. Our review provides a comprehensive overview of a key area in astrophysics that has important implications for our understanding of star and planet formation. 1 into PostgreSQL...\n",
      "Inserting test sample 1889  Recent near-IR Surveys have discovered a number of new bulge globular cluster (GC) candidates that need to be further investigated. Our main objective is to use public data from the Gaia Mission, VVV, 2MASS and WISE in order to measure the physical parameters of Minni48, a new candidate GC located in the inner bulge of the Galaxy at l=359.35 deg, b=2.79 deg. Even though there is a bright foreground star contaminating the field, the cluster appears quite bright in near- and mid-IR images. We obtain deep decontaminated optical and near-IR colour-magnitude diagrams (CMDs) for this cluster. The heliocentric cluster distance is determined from the red clump (RC) and the red giant branch (RGB) tip magnitudes in the near-IR CMD, while the cluster metallicity is estimated from the RGB slope and the fit to theoretical isochrones. The GC size is found to be r = 6' +/- 1', while reddening and extinction values are E(J-Ks)=0.60 +/- 0.05 mag, A_G=3.23 +/- 0.10 mag, A_Ks=0.45 +/- 0.05 mag. The resulting mean Gaia proper motions are PMRA=-3.5 +/- 0.5 mas/yr, PMDEC=-6.0 +/- 0.5 mas/yr.\n",
      "\n",
      "The IR magnitude of the RC yields an accurate distance modulus estimate of (m-M)_0=14.61 mag, equivalent to a distance D=8.4 +/- 1.0 kpc. This is consistent with the optical distance estimate: (m-M)_0=14.67 mag, D=8.6 +/- 1.0 kpc, and with the RGB tip distance: (m-M)_0=14.45 mag, D=7.8 +/- 1.0 kpc. The derived metallicity is [Fe/H]=-0.20 +/- 0.30 dex. A good fit to the PARSEC stellar isochrones is obtained in all CMDs using Age = 10 +/- 2 Gyr. The total absolute magnitude of this GC is estimated to be M_Ks= -9.04 +/- 0.66 mag.\n",
      "\n",
      "Based on its position, kinematics, metallicity and age, we conclude that Minni48 is a genuine GC, similar to other well known metal-rich bulge GCs. It is located at a projected Galactocentric angular distance of 2.9 deg, equivalent to 0.4 kpc, being one of the closest GCs to the Galactic centre. 0 into PostgreSQL...\n",
      "Inserting test sample 1890  The VVV survey has provided unprecedented astronomical data, revealing the presence of an intriguing globular cluster in the Galactic bulge region. This paper presents an in-depth analysis of the structural properties, kinematics, and chemical composition of this cluster, using images acquired in the near-infrared (NIR) bands by the VISTA telescope. \n",
      "\n",
      "Based on the analysis of the color-magnitude and color-colour diagrams, we infer that the cluster is metal-poor, old, and likely harbors multiple stellar populations. Its spatial distribution and kinematics indicate that it belongs to the Galactic bulge population, but it might have an external origin, possibly linked to the Sagittarius dwarf galaxy. \n",
      "\n",
      "The cluster's age and chemical composition suggest that it has played a crucial role in the formation and evolution of the Galactic bulge, providing insights into the earliest stages of the Milky Way's life. The characteristics of this cluster make it a unique target for follow-up observations with high-resolution spectroscopic instruments, to further investigate its chemical and kinematical properties.\n",
      "\n",
      "Furthermore, we explore the nature and properties of the variable sources detected within the cluster's boundaries, analyzing their light curves and pulsation periods, and infer their potential use as standard candles for distance measurements. \n",
      "\n",
      "The presence of an intriguing globular cluster in the Galactic bulge from the VVV survey constitutes a significant discovery, providing new insights into the formation and evolution of the Milky Way. The multiwavelength and multi-epoch data provided by VISTA, combined with spectroscopic follow-up observations, will allow us to deepen our understanding of the properties and origin of this intriguing object and further refine our knowledge of the Milky Way's early history. 1 into PostgreSQL...\n",
      "Inserting test sample 1891  We study the population of supermassive black holes (SMBHs) and their effects on massive central galaxies in the IllustrisTNG cosmological hydrodynamical simulations of galaxy formation. The employed model for SMBH growth and feedback assumes a two-mode scenario in which the feedback from active galactic nuclei occurs through a kinetic, comparatively efficient mode at low accretion rates relative to the Eddington limit, and in the form of a thermal, less efficient mode at high accretion rates. We show that the quenching of massive central galaxies happens coincidently with kinetic-mode feedback, consistent with the notion that active supermassive black cause the low specific star formation rates observed in massive galaxies. However, major galaxy mergers are not responsible for initiating most of the quenching events in our model. Up to black hole masses of about $10^{8.5}\\,{\\rm M}_\\odot$, the dominant growth channel for SMBHs is in the thermal mode. Higher mass black holes stay mainly in the kinetic mode and gas accretion is self-regulated via their feedback, which causes their Eddington ratios to drop, with SMBH mergers becoming the main channel for residual mass growth. As a consequence, the quasar luminosity function is dominated by rapidly accreting, moderately massive black holes in the thermal mode. We show that the associated growth history of SMBHs produces a low-redshift quasar luminosity function and a redshift zero black hole mass-stellar bulge mass relation in good agreement with observations, whereas the simulation tends to over-predict the high-redshift quasar luminosity function. 0 into PostgreSQL...\n",
      "Inserting test sample 1892  Supermassive black holes are among the most fascinating objects in astrophysics. As a consequence of their remarkable properties, they play a fundamental role in shaping the evolution of galaxies. In this study, we focus on the feedback effects that supermassive black holes have on the environment around them. To that end, we used the IllustrisTNG simulation, a state-of-the-art numerical experiment that follows the formation and evolution of galaxies in the universe.\n",
      "\n",
      "Our analysis reveals that the feedback from black holes plays a crucial role in regulating the growth of galaxies. We find that black holes inject energy and mass into their surroundings, which can heat up and expel gas from the galaxy. This process, known as AGN (active galactic nucleus) feedback, affects not only the growth of the black hole, but also the formation of stars in the galaxy. Our results suggest that AGN feedback is a key mechanism that shapes the observable properties of galaxies, such as their stellar mass and metallicity.\n",
      "\n",
      "Moreover, we investigate the dependence of AGN feedback on various physical parameters, such as the mass of the black hole and the properties of the surrounding gas. Our findings shed light on the complex interplay between black holes and their host galaxies, and provide important insights into the physics of galaxy evolution. This work highlights the importance of including AGN feedback in theoretical models of galaxy formation and represents a step forward in our understanding of the cosmic evolution of galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 1893  Enterprise Architecture defines the overall form and function of systems across an enterprise involving the stakeholders and providing a framework, standards and guidelines for project-specific architectures. Project-specific Architecture defines the form and function of the systems in a project or program, within the context of the enterprise as a whole with broad scope and business alignments. Application-specific Architecture defines the form and function of the applications that will be developed to realize functionality of the system with narrow scope and technical alignments. Because of the magnitude and complexity of any enterprise integration project, a major engineering and operations planning effort must be accomplished prior to any actual integration work. As the needs and the requirements vary depending on their volume, the entire enterprise problem can be broken into chunks of manageable pieces. These pieces can be implemented and tested individually with high integration effort.\n",
      "\n",
      "Therefore it becomes essential to analyze the economic and technical feasibility of realizable enterprise solution. It is difficult to migrate from one technological and business aspect to other as the enterprise evolves. The existing process models in system engineering emphasize on life-cycle management and low-level activity coordination with milestone verification.\n",
      "\n",
      "Many organizations are developing enterprise architecture to provide a clear vision of how systems will support and enable their business. The paper proposes an approach for selection of suitable enterprise architecture depending on the measurement framework. The framework consists of unique combination of higher order goals, non-functional requirement support and inputs-outcomes pair evaluation. The earlier efforts in this regard were concerned about only custom scales indicating the availability of a parameter in a range. 0 into PostgreSQL...\n",
      "Inserting test sample 1894  The development of enterprise architectures can be a complex and daunting task, which requires a comprehensive measurement framework to ensure their success. This paper proposes a novel approach to measuring the efficacy and efficiency of enterprise architectures through a new set of metrics which takes into account the unique needs and requirements of modern organizations. Drawing upon extensive research and case studies, we show that the traditional metrics used for measuring enterprise architectures are no longer adequate in light of new technological advancements and rapidly changing business environments. Our proposed framework seeks to address this issue by providing a more comprehensive and flexible set of metrics that can be adapted to suit the needs of any organization.\n",
      "\n",
      "The framework consists of multiple dimensions, including strategic alignment, performance management, and governance, which come together to create a holistic approach to measuring enterprise architectures. These dimensions are further broken down into measurable components, making it easier to monitor and evaluate progress over time. We demonstrate the efficacy of our framework through a case study of a multinational corporation in the banking industry, which showed significant improvements in their enterprise architecture management after implementing our framework.\n",
      "\n",
      "Moreover, this paper also highlights the importance of data quality in achieving accurate measurements. We discuss various data quality issues and provide recommendations for organizations to enhance data quality in their measurement practices.\n",
      "\n",
      "In conclusion, this paper offers a comprehensive measurement framework for enterprise architectures that takes into account the unique needs and requirements of modern organizations. Our proposed approach provides a more flexible and adaptable set of metrics that can be tailored to suit any organization, making it an indispensable tool for enterprise architecture practitioners. By utilizing our framework, organizations can monitor and evaluate their enterprise architecture programs more effectively, leading to improved business outcomes and greater success in achieving their strategic goals. 1 into PostgreSQL...\n",
      "Inserting test sample 1895  Inhomogeneous chemical evolution models of galaxies which try to reproduce the scatter seen in element-to-iron ratios of metal-poor halo stars are heavily dependent on theoretical nucleosynthesis yields of core-collapse supernovae.\n",
      "\n",
      "Hence inhomogeneous models present themselves as a test for stellar nucleosyn- thesis calculations. Applying an inhomogeneous chemical evolution model to our Galaxy reveals a number of shortcomings of existing theoretical nucleosynthesis yields. One problem is the predicted scatter in [O/Fe] and [Mg/Fe] which is too large compared to the one observed in metal-poor halo stars. This can be either due to the O or Mg yields or due to the Fe yields (or both). However, O and Mg are alpha-elements that are produced mainly during hydrostatic burning and thus are not affected by the theoretical uncertainties afflicting the collapse and explosion of a massive star. Stellar iron yields, on the other hand, depend heavily on the choice of the mass-cut between ejecta and proto neutron star and are therefore very uncertain. We present Fe yield distributions as function of progenitor mass that are consistent with the abundance distribution of metal- poor halo stars and are in agreement with observed Ni yields of SNe II with known progenitor masses. The iron yields of lower-mass SNe II (in the range 10-20 Msol) are well constrained by those observations. Present observations, however, do not allow to determine a unique solution for higher-mass SNe.\n",
      "\n",
      "Nevertheless, the main dependence of the stellar iron yield as function of progenitor mass may be derived and can be used as constraint for future supernova/hypernova models. The results are of importance for the earliest stages of galaxy formation when the ISM is dominated by chemical inhomogeneities and the instantaneous mixing approximation is not valid. 0 into PostgreSQL...\n",
      "Inserting test sample 1896  The study of elemental abundances in metal-poor halo stars is crucial for understanding the early chemical evolution of our Galaxy. In particular, O and Mg are key elements since they are predominantly produced by massive stars and their yields are sensitive to the stellar initial mass function and the extent of mass loss during stellar evolution. In this work, we investigate the implications of O and Mg abundances for the iron yields of metal-poor halo stars.\n",
      "\n",
      "To achieve this, we perform a detailed chemical evolution model which takes into account the yields of Type II supernovae, Type Ia supernovae and AGB stars. Our model shows that if O and Mg are not considered separately, the inferred stellar iron yields can be overestimated by up to a factor of two. On the other hand, if O and Mg are accounted for, the inferred iron yields are found to be consistent with recent theoretical predictions.\n",
      "\n",
      "We identify two main causes for the overestimation of iron yields when O and Mg are treated as a single element. Firstly, the O/Mg ratio varies strongly among metal-poor halo stars and this variation is not expected from theoretical models that assume a constant O/Mg ratio. Secondly, the contribution of AGB stars to the Mg yield can be important, depending on the adopted model, while their contribution to the O yield is negligible.\n",
      "\n",
      "Our study has important implications for the interpretation of observational data on metal-poor halo stars and for the calibration of theoretical models of chemical evolution. Our results highlight the need for high-quality measurements of O and Mg in this class of stars and for improved models of AGB star nucleosynthesis. 1 into PostgreSQL...\n",
      "Inserting test sample 1897  A system of non-intersecting squared Bessel processes is considered which all start from one point and they all return to another point. Under the scaling of the starting and ending points when the macroscopic boundary of the paths touches the hard edge, a limiting critical process is described in the neighbourhood of the touching point which we call the hard edge tacnode process. We derive its correlation kernel in an explicit new form which involves Airy type functions and operators that act on the direct sum of $L^2(\\mathbb R_+)$ and a finite dimensional space. As the starting points of the squared Bessel paths are set to 0, a cusp in the boundary appears. The limiting process is described near the cusp and it is called the hard edge Pearcey process. We compute its multi-time correlation kernel which extends the existing formulas for the single-time kernel. Our pre-asymptotic correlation kernel involves the ratio of two Toeplitz determinants which are rewritten using a Borodin-Okounkov type formula. 0 into PostgreSQL...\n",
      "Inserting test sample 1898  In this paper, we study the hard edge tacnode and Pearcey processes by considering non-intersecting squared Bessel paths. These processes arise in various areas of mathematics and physics, such as random matrix theory and random growth models. We investigate the behavior of the two processes in the large time limit, where the paths display long-range correlations and exhibit universal characteristics that are independent of the initial conditions. We establish explicit expressions for the corresponding correlation functions, which involve Fredholm determinants of certain integral operators. We also derive asymptotic formulas, using both analytical and numerical methods, that describe the behavior of the correlation functions near their critical points. Our results shed light on the intricate interplay between geometry and probability in the context of the hard edge tacnode and Pearcey processes, and may have potential applications in fields such as statistical mechanics and mathematical finance. 1 into PostgreSQL...\n",
      "Inserting test sample 1899  The scatter in the galaxy size versus stellar mass (Mstar) relation gets largely reduced when, rather than the half-mass radius Re, the size at a fixed surface density is used. Here we address why this happens. We show how a reduction is to be expected because any two galaxies with the same Mstar have at least one radius with identical surface density, where the galaxies have identical size. However, the reason why the scatter is reduced to the observed level is not trivial, and we pin it down to the galaxy surface density profiles approximately following Sersic profiles with their Re and Sersic index (n) anti-correlated (i.e., given Mstar, n increases when Re decreases). Our analytical results describe very well the behavior of the observed galaxies as portrayed in the NASA Sloan Atlas (NSA), which contains more than half a million local objects with 7 < log(Mstar/Msun) < 11.5. The comparison with NSA galaxies also allows us to find the optimal values for the mass surface density (2.4m0.9p1.3 Msun/pc2) and surface brightness (r-band 24.7pm0.5 mag/arcsec2) that minimize the scatter, although the actual values depend somehow on the subset of NSA galaxies used for optimization. The physical reason for the existence of optimal values is unknown but, as Trujillo+20 point out, they are close to the gas surface density threshold to form stars and thus may trace the physical end of a galaxy. Our NSA-based size--mass relation agrees with theirs on the slope as well as on the magnitude of the scatter. As a by-product of the narrowness of the size--mass relation (only 0.06 dex), we propose to use the size of a galaxy to measure its stellar mass. In terms of observing time, it is not more demanding than the usual photometric techniques and may present practical advantages in particular cases. 0 into PostgreSQL...\n",
      "Inserting test sample 1900  This study focuses on analyzing the relationship between galaxy size and stellar mass, a crucial issue in the field of astrophysics. Understanding the correlation between these two factors can provide insight into galaxy formation and evolution. We have selected a sample of galaxies from the Sloan Digital Sky Survey (SDSS) and used photometric and spectral data to calculate their available stellar mass and effective radius. Our analysis reveals a clear trend of increasing galaxy size with greater stellar mass. The results of our study can be quantified using a power law, which predicts that effective radius scales proportional to stellar mass raised to a power of 0.59. This relationship is shown to hold for all types of galaxies, from dwarf galaxies to massive ellipticals.\n",
      "\n",
      "One of the goals of our research is to investigate the potential causes of this relation. Using multiple regression analysis, we have determined that the primary factor driving the correlation is the amount of dark matter in a galaxy. Furthermore, we have found evidence supporting the idea that feedback mechanisms in the form of supernova explosions regulate the growth of galaxies. In dwarf galaxies, the relationship between galaxy size and stellar mass may be affected by the ionization and heating of gas by intense ultraviolet radiation.\n",
      "\n",
      "Our study has significant implications for our understanding of how galaxies form, develop, and evolve over time. The scaling relation for galaxy size and stellar mass indicates that there are fundamental constraints on the growth of galaxies, and that there may be specific physical processes that govern the formation and growth of galaxies. These insights will be useful in developing more accurate models for the evolution of galaxies, and in predicting the properties of galaxies that have yet to be observed. Ultimately, advances in our understanding of this relationship will lead to a deeper understanding of the universe itself. 1 into PostgreSQL...\n",
      "Inserting test sample 1901  Under the Ansatz that the occupation times of a system with finitely many states are given by the Gibbs distribution, an effective temperature is uniquely determined (up to a choice of scale), and may be computed de novo, without any reference to a Hamiltonian for empirically accessible systems. As an example, the calculation of the effective temperature for a classical Bose gas is outlined and applied to the analysis of computer network traffic. 0 into PostgreSQL...\n",
      "Inserting test sample 1902  We explore the concept of effective temperature for finite systems and its implications in statistical mechanics and thermodynamics. By extending the traditional definition of temperature to finite systems, we investigate the validity of the equipartition theorem and provide a framework for understanding thermal transport in small systems. Our results demonstrate the need for a fundamental rethinking of temperature in the context of nanoscale and mesoscale systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1903  Motivated by the insulating behavior of $\\alpha$-(BEDT-TSeF)$_2$I$_3$ at low temperatures ($T$'s), we first performed first-principles calculations based on the crystal structural data at 30 K under ambient pressure, and we constructed a two-dimensional effective model using maximally localized Wannier functions.\n",
      "\n",
      "As possible causes of the insulating behavior, we studied the effects of the on-site Coulomb interaction $U$ and spin-orbit interaction (SOI) by investigating the electronic state and the transport coefficient using the Hartree approximation and the $T$-matrix approximation. The calculations at a finite $T$ demonstrated that a spin-ordered massive Dirac electron (SMD) appeared due to the on-site Coulomb interaction. We had an interest in the anomalous competitive effect with $U$ and SOI when the SMD phase is present in $\\alpha$-(BETS)$_2$I$_3$, and we investigated these contributions to the electronic state and conductivity. The SMD is not a conventional spin order, but it exhibits the spin-valley Hall effect. Direct current resistivity in the presence of a spin order gap increased divergently and exhibited negative magnetoresistance in the low $T$ region with decreasing $T$. The charge density hardly changed below and above the $T$ at which this insulating behavior appeared. However, when considering the SOI alone, the state changed to a topological insulator phase, and the electrical resistivity is saturated by edge conduction at quite low $T$. When considering both the SMD and the SOI, the spin order gap was suppressed by the SOI, and gaps with different sizes opened in the left and right Dirac cones. This phase transition leads to distinct changes in microwave conductivity, such as a discontinuous jump and a peak structure. 0 into PostgreSQL...\n",
      "Inserting test sample 1904  In this research study, we investigate the transport properties of the organic Dirac electron system \\alpha-(BEDT-TSeF)$_2$I$_3$. The organic material used in this study has emerged as a promising candidate for creating topological states of matter due to its peculiar band structure and unique electronic properties.\\\n",
      "\n",
      "We perform a detailed investigation of magnetoresistance and resistivity measurements over a range of temperatures and pressures to gain insight into the electronic behavior of the material. Our results demonstrate a clear signature of the Dirac-like electronic band structure in the measured transport properties, indicating the presence of a Dirac cone at the Fermi level.\\\n",
      "\n",
      "Furthermore, our analysis suggests that the presence of strong electron-electron interactions leads to pronounced quantum corrections to the semiclassical picture of electronic transport. The observed anomalies in the magnetoresistance and resistivity measurements can be attributed to these fluctuations, which result in an enhancement of weak-localization effects and a breakdown of the semiclassical picture at low temperatures.\\\n",
      "\n",
      "Finally, we compare our findings to previous studies on other organic materials and highlight the significance of our results for future research in the field of Dirac and topological materials. Overall, our investigation suggests that \\alpha-(BEDT-TSeF)$_2$I$_3$ is a promising candidate for the study and engineering of topological states of matter, with potential applications in quantum computing and high-speed electronics. 1 into PostgreSQL...\n",
      "Inserting test sample 1905  We propose an automatic video inpainting algorithm which relies on the optimisation of a global, patch-based functional. Our algorithm is able to deal with a variety of challenging situations which naturally arise in video inpainting, such as the correct reconstruction of dynamic textures, multiple moving objects and moving background. Furthermore, we achieve this in an order of magnitude less execution time with respect to the state-of-the-art. We are also able to achieve good quality results on high definition videos. Finally, we provide specific algorithmic details to make implementation of our algorithm as easy as possible. The resulting algorithm requires no segmentation or manual input other than the definition of the inpainting mask, and can deal with a wider variety of situations than is handled by previous work. 1. Introduction.\n",
      "\n",
      "Advanced image and video editing techniques are increasingly common in the image processing and computer vision world, and are also starting to be used in media entertainment. One common and difficult task closely linked to the world of video editing is image and video \" inpainting \". Generally speaking, this is the task of replacing the content of an image or video with some other content which is visually pleasing. This subject has been extensively studied in the case of images, to such an extent that commercial image inpainting products destined for the general public are available, such as Photoshop's \" Content Aware fill \" [1]. However, while some impressive results have been obtained in the case of videos, the subject has been studied far less extensively than image inpainting. This relative lack of research can largely be attributed to high time complexity due to the added temporal dimension. Indeed, it has only very recently become possible to produce good quality inpainting results on high definition videos, and this only in a semi-automatic manner. Nevertheless, high-quality video inpainting has many important and useful applications such as film restoration, professional post-production in cinema and video editing for personal use. For this reason, we believe that an automatic, generic video inpainting algorithm would be extremely useful for both academic and professional communities. 0 into PostgreSQL...\n",
      "Inserting test sample 1906  Video inpainting is the process of removing undesired elements from a video by filling in the created gaps with plausible information. When dealing with complex scenes, where the background is cluttered or constantly changing, traditional inpainting techniques fall short. Complex scenes demand a more robust and efficient inpainting algorithm that can handle the complexity and variability introduced by the dynamic nature of the scene. In this paper, we present a novel video inpainting algorithm capable of handling complex scenes.\n",
      "\n",
      "Our proposed algorithm is founded on a combination of deep learning techniques, which allow us to capture the relevant information about the scene and generate a plausible new content that blends seamlessly with the surrounding region. Our algorithm harnesses the power of deep neural networks to extract the most relevant features of the scene, including both the static and dynamic background information. Using this extracted information, our algorithm generates a plausible content that effectively fills the gaps created by the removed objects.\n",
      "\n",
      "To validate the effectiveness of our algorithm, we conducted several experiments on a diverse set of complex scenes, including those with cluttered backgrounds and dynamic elements. Our algorithm demonstrated superior performance compared to traditional inpainting techniques and previous state-of-the-art models. Our results show that our algorithm can effectively and efficiently handle the complexity and variability of complex scenes and outperform traditional algorithms.\n",
      "\n",
      "In conclusion, this paper presented a novel video inpainting algorithm that demonstrates superior performance in the context of complex scenes. Our approach is based on deep learning techniques that effectively leverage the relevant information in the scene to generate plausible content that blends seamlessly with the background. Our algorithm represents a significant advancement in the field of video inpainting and has potentially important applications in a wide range of fields, including video editing, virtual reality, and surveillance systems, among others. 1 into PostgreSQL...\n",
      "Inserting test sample 1907  The low efficiency of metal halide perovskite light-emitting diodes (PeLEDs) with blue emission block their potential applications in large-area full-color displays and solid-state lighting. A delicate control over the entire electroluminescence process is indispensable to overcome the efficiency limitations of blue PeLEDs. Here, we demonstrate an efficient device architecture to synergistically reduce the energetic losses during electron-photon conversion and boost the extraction of trapped light in the device. An interfacial nucleation seeding scheme is proposed to control the crystallization process of highly emissive perovskite nanocrystals and suppress the trap-mediated non-radiative recombination losses due to interfacial hydrogen bonding interactions. This manipulation results in a record external quantum efficiency (EQE) of 12.8% for blue PeLEDs emitting at 486 nm, along with the improved spectral stability and operation lifetime. Additionally, the maximum EQE reaches 16.8% after combining an internal outcoupling structure without spectral distortion, which can be further raised to 27.5% when using a lens-based structure on top of the device. We anticipate that our work provides an effective method for its application in high-performance PeLEDs. 0 into PostgreSQL...\n",
      "Inserting test sample 1908  The development of high efficiency perovskite light-emitting diodes (PeLEDs) is a goal of increasing importance for a wide range of modern optoelectronic applications. However, the low luminescence efficiency of blue-emitting PeLEDs represents a significant bottleneck to their commercial viability. To address this limitation, we present a novel approach based on the synergistic combination of electroluminescent manipulation with interfacial nucleation seeding. Through a careful selection of different molecular architectures and engineering of thin film interfaces, we demonstrate the successful creation of efficient PeLEDs emitting bright blue light. Our approach exploits the injection and confinement of charge carriers used in concert with the growth of optimized organic-inorganic perovskite crystals. This synergistic manipulation reduces the operating voltage and enhances the luminance and efficiency of the PeLEDs while simultaneously suppressing nonradiative recombination pathways. Our work provides an innovative blueprint for overcoming the current challenges associated with blue-emitting PeLEDs and advancing their performance towards the next generation of high-performance optoelectronics. 1 into PostgreSQL...\n",
      "Inserting test sample 1909  This paper proposes an agent-based model that combines both spot and balancing electricity markets. From this model, we develop a multi-agent simulation to study the integration of the consumers' flexibility into the system. Our study identifies the conditions that real-time prices may lead to higher electricity costs, which in turn contradicts the usual claim that such a pricing scheme reduces cost. We show that such undesirable behavior is in fact systemic. Due to the existing structure of the wholesale market, the predicted demand that is used in the formation of the price is never realized since the flexible users will change their demand according to such established price. As the demand is never correctly predicted, the volume traded through the balancing markets increases, leading to higher overall costs. In this case, the system can sustain, and even benefit from, a small number of flexible users, but this solution can never upscale without increasing the total costs. To avoid this problem, we implement the so-called \"exclusive groups.\" Our results illustrate the importance of rethinking the current practices so that flexibility can be successfully integrated considering scenarios with and without intermittent renewable sources. 0 into PostgreSQL...\n",
      "Inserting test sample 1910  This paper investigates the implementation of flexible demand through either a real-time pricing or market integration approach. Flexible demand refers to the ability of consumers to adjust their energy consumption patterns in response to changes in energy supply or prices. Both real-time pricing and market integration have been proposed as ways to incentivize flexible demand, but their effectiveness and practicality have yet to be fully analyzed. This study employs a simulation model to compare the outcomes of real-time pricing and market integration on energy consumption, costs, and greenhouse gas emissions. The results suggest that market integration could potentially lead to a lower overall cost of energy for consumers and reduced emissions, but it may also require significant changes in the energy market structure. Real-time pricing, on the other hand, could offer more flexibility and simplicity for consumers, but its effectiveness may depend on the level of price elasticity and the availability of real-time data. The findings of this study provide important insights for policymakers and energy stakeholders as they consider different strategies for encouraging flexible demand and achieving energy sustainability. 1 into PostgreSQL...\n",
      "Inserting test sample 1911  Magnetic confinement in graphene has been of recent and growing interest because its potential applications in nanotechnology. In particular, the observation of the so called magnetic edge states in graphene has opened the possibility to deepen into the generation of spin currents and its applications in spintronics. We study the magnetic edge states of quasi-particles arising in graphene monolayers due to an inhomogeneous magnetic field of a magnetic barrier in the formalism of the two-dimensional massless Dirac equation. We also show how the solutions of such states in each of both triangular sublattices of the graphene are related through a supersymmetric transformation in the quantum mechanical sense. 0 into PostgreSQL...\n",
      "Inserting test sample 1912  Graphene, a two-dimensional material that exhibits unique electronic properties, has attracted intensive research interest in recent years. One of its fascinating features is the presence of magnetic edge states, which arise from the interplay between the spin and the motion of electrons along the boundaries of graphene. These edge states are robust against backscattering and have potential applications in spintronics and quantum computing. In this paper, we review recent theoretical and experimental advances in understanding and manipulating magnetic edge states in graphene. We also discuss future prospects and challenges in this rapidly evolving field. Overall, this work sheds light on the fundamental physics of graphene and showcases its promising potential for practical applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1913  Affective computing has been largely limited in terms of available data resources. The need to collect and annotate diverse in-the-wild datasets has become apparent with the rise of deep learning models, as the default approach to address any computer vision task. Some in-the-wild databases have been recently proposed. However: i) their size is small, ii) they are not audiovisual, iii) only a small part is manually annotated, iv) they contain a small number of subjects, or v) they are not annotated for all main behavior tasks (valence-arousal estimation, action unit detection and basic expression classification). To address these, we substantially extend the largest available in-the-wild database (Aff-Wild) to study continuous emotions such as valence and arousal. Furthermore, we annotate parts of the database with basic expressions and action units. As a consequence, for the first time, this allows the joint study of all three types of behavior states. We call this database Aff-Wild2. We conduct extensive experiments with CNN and CNN-RNN architectures that use visual and audio modalities; these networks are trained on Aff-Wild2 and their performance is then evaluated on 10 publicly available emotion databases. We show that the networks achieve state-of-the-art performance for the emotion recognition tasks. Additionally, we adapt the ArcFace loss function in the emotion recognition context and use it for training two new networks on Aff-Wild2 and then re-train them in a variety of diverse expression recognition databases. The networks are shown to improve the existing state-of-the-art. The database, emotion recognition models and source code are available at http://ibug.doc.ic.ac.uk/resources/aff-wild2. 0 into PostgreSQL...\n",
      "Inserting test sample 1914  This research paper investigates the performance of the Aff-Wild2 dataset on the tasks of expression, affect, and action unit recognition, utilizing multi-task learning and ArcFace. The Aff-Wild2 dataset is a challenging dataset due to its diverse set of emotions and the use of unconstrained videos. The multi-task learning approach was implemented to simultaneously recognize facial expressions, affect, and action units, which resulted in improved performance compared to a single task learning method. Additionally, the ArcFace method was used to enhance the feature embedding by introducing angular margin loss, which can better discriminate similar-looking expressions.\n",
      "\n",
      "Various deep neural network models were trained on the dataset, including ResNet-18, ResNet-34, and ResNet-50, and their performance evaluated. Results showed that ResNet-50 achieved the highest scores due to its deeper architecture, which is able to learn more complex features. Moreover, the study also analyzed the effect of dataset size on model performance, and found that as the dataset size increases, the performance of the model also improves.\n",
      "\n",
      "In addition to the quantitative analysis, a qualitative analysis was performed to investigate which emotions were challenging for the model to recognize. It was found that the model struggled with emotions that were often expressed subtly, such as contempt and boredom, as well as emotions that were similar in appearance, like happiness and surprise. The results of this study have important applications in the field of affective computing and human-computer interaction, where accurate emotion recognition is crucial. 1 into PostgreSQL...\n",
      "Inserting test sample 1915  In an effort to explore high-throughput processing of microscopic image data, a method based on deep convolutional neural network is proposed. The state-of-the-art computer vision algorithm, Faster R-CNN, was trained for the detection of iron (II) phthalocyanines on Se-terminated Au(111) platform resolved by scanning probe microscopy. The construction of the feature pyramid enables the multi-scale molecule detection in images of different scales from 10 nm to 50 nm. After the detection, the orientation of each molecule is measured by a following program. Based on the statistical distribution of the orientation angles, the preferred adsorption configurations of iron (II) phthalocyanine on the platform are revealed. This method yields high accuracy and recall with F1 score close to 1 after optimization of hyperparameters and training. It is expected to be a feasible solution in the scenarios where autonomous and high-throughput processing of microscopic image data is needed. 0 into PostgreSQL...\n",
      "Inserting test sample 1916  This study presents a novel approach to autonomously detecting molecular configurations in microscopic images utilizing a deep convolutional neural network. We propose a technique that is able to automatically identify molecular configurations to assist in the analysis of complex biological systems. Our method involves training a neural network on a dataset of labeled images to enable it to classify different molecular configurations accurately. To demonstrate the efficacy of our approach, we conducted experiments on a collection of microscopic images of biological samples, achieving an accuracy rate of over 95%. The proposed technique can help improve molecular analysis, enabling greater insights into biological systems, and thus opening up new areas of research. We conclude that deep convolutional neural networks provide an effective means of detecting molecular configurations, and that this approach holds great potential in a wide range of applications in the field of biology. 1 into PostgreSQL...\n",
      "Inserting test sample 1917  By example of a particle interacting with ideal gas, it is shown that statistics of collisions in statistical mechanics at any degree of the gas rarefaction qualitatively differs from that conjugated with Boltzmann's hypothetical molecular chaos and kinetic equation. In reality, probability of the particle collisions in itself is random, which results in power-law asymptotic of the particle velocity relaxation. An estimate of its exponent is suggested basing on simple kinematic reasonings 0 into PostgreSQL...\n",
      "Inserting test sample 1918  This research paper investigates the relaxation statistics of gases. By analyzing the dynamics of gas particles and their collisions, we develop a theoretical model for characterizing the frequency of relaxation events. Numerical simulations based on this model verify its accuracy, demonstrating that the statistics of true relaxation are distinct from those of mere collisions. Understanding these differences has important implications for applications such as heat transfer and fluid dynamics in gas-based systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1919  Strong gravitational lensing leads to an occurrence of multiple images, with different magnifications, of a lensed source. Those magnifications can in turn be modified by microlensing on smaller mass scales within the lens. Recently, measurements of the changes in the magnification ratio of the individual images have been proposed as a powerful tool for estimation of the size and velocity of the emission region in the lensed source. The changes of the magnification ratios in blazars PKS1830-211 and QSO B0218+357, if interpreted as caused by a microlensing on individual stars, put strong constraints on those two variables. These constraints are difficult to accommodate with the current models of gamma-ray emission in blazars. In this paper we study if similar changes in the magnification ratio can be caused by microlensing on intermediate size structures in the lensing galaxy. We investigate in details three classes of possible lenses: globular clusters (GC), open clusters (OC) and giant molecular clouds (GMC). We apply this scenario to the case of QSO B0218+357. Our numerical simulations show that changes in magnifi- cations with similar time scales can be obtained for relativistically moving emission regions with sizes up to 0.01 pc in the case of microlensing on the cores of GCs or clumps in GMCs. From the density of such structures in spiral galaxies we estimate however that lensing in giant molecular clouds would be more common. 0 into PostgreSQL...\n",
      "Inserting test sample 1920  Quasar B0218+357 is an object of high interest in the field of astrophysics, as its observational analysis provides valuable insight into the mechanisms that govern the behavior of intermediate size structures and microlensing effects. In this study, we investigate the variability of GeV gamma-ray emission in the quasar B0218+357 and its possible correlation with microlensing phenomena on intermediate size structures. We analyze the multi-wavelength observations gathered from Fermi-LAT, Swift, and the Liverpool Telescope, alongside the optical observations obtained from the KVA and Nordic Optical Telescope. Our results reveal that the variability of gamma-ray flux observed in B0218+357 is consistent with microlensing on intermediate size structures, with a time delay between GeV gamma-rays and optical emission, confirming the microlensing hypothesis. Additionally, we perform numerical simulations to compare the obtained light curve with model predictions, which further strengthens our conclusions and provides valuable insights for future research. Our findings shed light on the complex interplay between microlensing and intermediate size structures and offer new opportunities to study the physics behind these phenomena. This research contributes to the ongoing efforts to understand the complex mechanisms governing gamma-ray emission in quasar systems and provides new insights into the fundamental properties of intermediate size structures in the Universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1921  We present a new Hubble Space Telescope (HST) Cosmic Origins Spectrograph (COS) absorption-line survey to study halo gas around 16 luminous red galaxies (LRGs) at z=0.21-0.55. The LRGs are selected uniformly with stellar mass Mstar>1e11 Msun and no prior knowledge of the presence/absence of any absorption features. Based on observations of the full Lyman series, we obtain accurate measurements of neutral hydrogen column density N(HI) and find that high-N(HI) gas is common in these massive quiescent halos with a median of <log N(HI)> = 16.6 at projected distances d<~160 kpc. We measure a mean covering fraction of optically-thick gas with log N(HI)>~17.2 of <kappa>LLS=0.44^{+0.12}_{-0.11} at d<~160 kpc and <kappa>LLS=0.71^{+0.11}_{-0.20} at d<~100 kpc. The line-of-sight velocity separations between the HI absorbing gas and LRGs are characterized by a mean and dispersion of <v_{gas-gal}>=29 km/s and \\sigma_v_{gas-gal}=171 km/s.\n",
      "\n",
      "Combining COS FUV and ground-based echelle spectra provides an expanded spectral coverage for multiple ionic transitions, from low-ionization MgII and SiII, to intermediate ionization SiIII and CIII, and to high-ionization OVI absorption lines. We find that intermediate ions probed by CIII and SiIII are the most prominent UV metal lines in LRG halos with a mean covering fraction of <kappa(CIII)>_{0.1}=0.75^{+0.08}_{-0.13} for W(977)>=0.1 Ang at d<160 kpc, comparable to what is seen for CIII in L* and sub-L* star-forming and red galaxies but exceeding MgII or OVI in quiescent halos. The COS-LRG survey shows that massive quiescent halos contain widespread chemically-enriched cool gas and that little distinction between LRG and star-forming halos is found in their HI and CIII content. 0 into PostgreSQL...\n",
      "Inserting test sample 1922  In this paper, we present the initial results of our study on characterizing the circumgalactic gas around massive ellipticals at a redshift of approximately 0.4. Using a combination of imaging and spectroscopic observations from the Keck Observatory, we conducted a comprehensive analysis of the gas properties in the vicinity of these galaxies. Our study is motivated by the need to better understand the role of circumgalactic gas in regulating the growth and evolution of galaxies.\n",
      "\n",
      "Our analysis shows that the circumgalactic gas exhibits a range of physical and chemical properties, including kinematics, ionization, and metallicity gradients. We find that the gas is typically enriched with metals, supporting the hypothesis that it is being continuously enriched by the stellar activity of the host galaxy. Moreover, the ionization state of the gas appears to be correlated with the morphological type of the host galaxy, suggesting a strong connection between the star formation history and the gas properties.\n",
      "\n",
      "We also investigate the properties of the gas in relation to the underlying dark matter halo. Our results show that the gas is predominantly found within a few hundred kiloparsecs of the host galaxy, but extends out to several times the virial radius of the halo. We also observe a clear correlation between the gas properties and the dark matter halo mass, indicating that the gas properties are strongly influenced by the gravitational potential of the host halo.\n",
      "\n",
      "Overall, our study represents one of the first systematic investigations of the circumgalactic medium around massive ellipticals at this redshift. Our initial results provide valuable insights into the physical and chemical processes governing the growth and evolution of these galaxies. We anticipate that future studies will build upon our findings and further enhance our understanding of the intricate interplay between galaxies and their surrounding gas reservoirs. 1 into PostgreSQL...\n",
      "Inserting test sample 1923  In this paper, the preconditioned TBiCOR and TCORS methods are presented for solving the Sylvester tensor equation. A tensor Lanczos $\\mathcal{L}$-Biorthogonalization algorithm (TLB) is derived for solving the Sylvester tensor equation. Two improved TLB methods are presented. One is the biconjugate $\\mathcal{L}$-orthogonal residual algorithm in tensor form (TBiCOR), which implements the $LU$ decomposition for the triangular coefficient matrix derived by the TLB method. The other is the conjugate $\\mathcal{L}$-orthogonal residual squared algorithm in tensor form (TCORS), which introduces a square operator to the residual of the TBiCOR algorithm. A preconditioner based on the nearest Kronecker product is used to accelerate the TBiCOR and TCORS algorithms, and we obtain the preconditioned TBiCOR algorithm (PTBiCOR) and preconditioned TCORS algorithm (PTCORS). The proposed algorithms are proved to be convergent within finite steps of iteration without roundoff errors. Several examples illustrate that the preconditioned TBiCOR and TCORS algorithms present excellent convergence. 0 into PostgreSQL...\n",
      "Inserting test sample 1924  The Sylvester tensor equation is a fundamental mathematical problem that arises in control theory and signal processing. This paper presents two new algorithms, called Preconditioned TBiCOR and TCORS, for solving this equation. Both algorithms are based on the iterative BiCOR and CORS methods, respectively, but incorporate preconditioning to improve efficiency. We provide a detailed analysis of the convergence properties of the algorithms and prove that they converge under reasonable assumptions. We also demonstrate through numerical experiments that both Preconditioned TBiCOR and TCORS outperform their non-preconditioned counterparts in terms of computational efficiency and accuracy. Our results suggest that these algorithms can be highly useful for solving large-scale Sylvester tensor equations that arise in practical applications. 1 into PostgreSQL...\n",
      "Inserting test sample 1925  Following our result that for the final state of continued spherical gravitational collapse, the gravitational mass of the fluid, $M_f\\to 0$, we show that for a physical fluid the eventual value of $2GM_f/R_f\\to 1$ rather than $2GM_f/R_f <1$, indicating approach to a zero-mass black hole. We also indicate that as the final state would be approached, the curvature components tend to blow up, and the proper radial distance $l$ and the proper time $\\tau \\to \\infty$. This indicates that actually the singularity is never attained for the collapse of an isolated body. We also identify that, the final state may correspond to the local 3-speed $v\\to c$, eventhough the circumference speed $U\\to 0$. However, at a finite observation epoch, such Eternally Collapsing Objects (ECOs) may have a modest local speed of collapse $v \\ll c$, and the lab frame speed of collapse should practically be zero because of their extremely high surface gravitational red-shifts. 0 into PostgreSQL...\n",
      "Inserting test sample 1926  The final state of spherical gravitational collapse has been an active area of research for decades due to its implications for the understanding of gamma-ray bursts (GRBs). In this study, we investigate the likely source of GRBs by considering the collapse of a spherical star to a black hole through numerical simulations. Our findings reveal that the final state of the gravitational collapse of a massive star is a highly relativistic outflow of material known as a jet. This jet collides with the surrounding medium, resulting in a GRB. Our simulations suggest that the energy of the jet is order of magnitude larger than that of a supernova explosion, explaining the extreme brightness of GRBs. These findings present a crucial milestone in understanding the physics behind GRBs, and may potentially lead to new insights into the behavior of massive stars and the role they play in shaping our universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1927  This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as 'pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext' tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce. 0 into PostgreSQL...\n",
      "Inserting test sample 1928  This research delves into the problem of learning features solely by observing object movements, which is a key challenge posed by unsupervised learning. Our proposed approach learns dynamic object representations from unlabeled videos using a convolutional autoencoder trained over consecutive video frames. Our results demonstrate that it is possible to capture high-level object features by simply observing their behavior through time. Furthermore, we present experiments showing how the features learned in an unsupervised manner can be transfered to downstream tasks such as image classification, outperforming existing methods in certain scenarios. Lastly, we investigate the learned representations and show how they can reveal qualitatively meaningful object features. Overall, our study offers a promising alternative to classical learning methods and reveals important insights on how object perceptual features are learned through movement information alone. 1 into PostgreSQL...\n",
      "Inserting test sample 1929  This paper presents a novel parallel-in-time algorithm able to compute time-periodic solutions of problems where the period is not given. Exploiting the idea of the multiple shooting method, the proposed approach calculates the initial values at each subinterval as well as the corresponding period iteratively. As in the Parareal method, parallelization in the time domain is performed using discretization on a two-level grid. A special linearization of the time-periodic system on the coarse grid is introduced to speed up the computations. The iterative algorithm is verified via its application to the Colpitt oscillator model. 0 into PostgreSQL...\n",
      "Inserting test sample 1930  This paper presents a method for calculating time-periodic solutions with unknown periods in a parallel-in-time manner. The proposed method leverages the concept of inertial manifold, which provides an approximate phase space that accounts for the long-term dynamics of a system. By exploiting the properties of inertial manifold, the time-periodic solution can be efficiently computed through a parareal algorithm with multiple processors. Numerical experiments demonstrate the effectiveness and scalability of the proposed method for capturing time-periodic solutions in various nonlinear systems, including the Lorenz system and the Navier-Stokes equations. 1 into PostgreSQL...\n",
      "Inserting test sample 1931  Automation of brain tumor segmentation in 3D magnetic resonance images (MRIs) is key to assess the diagnostic and treatment of the disease. In recent years, convolutional neural networks (CNNs) have shown improved results in the task.\n",
      "\n",
      "However, high memory consumption is still a problem in 3D-CNNs. Moreover, most methods do not include uncertainty information, which is especially critical in medical diagnosis. This work studies 3D encoder-decoder architectures trained with patch-based techniques to reduce memory consumption and decrease the effect of unbalanced data. The different trained models are then used to create an ensemble that leverages the properties of each model, thus increasing the performance. We also introduce voxel-wise uncertainty information, both epistemic and aleatoric using test-time dropout (TTD) and data-augmentation (TTA) respectively. In addition, a hybrid approach is proposed that helps increase the accuracy of the segmentation. The model and uncertainty estimation measurements proposed in this work have been used in the BraTS'20 Challenge for task 1 and 3 regarding tumor segmentation and uncertainty estimation. 0 into PostgreSQL...\n",
      "Inserting test sample 1932  This paper presents a novel approach for MRI brain tumor segmentation and uncertainty estimation in medical imaging using 3D-UNet architectures. The method aims to effectively manipulate large 3D MR images and extract high-level features to segment the regions of the brain affected by the tumor. In particular, our approach employs a probabilistic framework based on Monte Carlo simulation to robustly estimate the uncertainty in model predictions. We extensively evaluate our method using a publicly available dataset and compare it with state-of-the-art methods. Our experiments show that the proposed framework provides accurate segmentation results and significantly higher uncertainty estimation performance than other methods. Furthermore, we demonstrate the usefulness of our approach for delineating tumor boundaries and predicting tumor characteristics. Overall, our method can help improve diagnostic accuracy and treatment planning for patients diagnosed with brain tumors. 1 into PostgreSQL...\n",
      "Inserting test sample 1933  There is a long-standing discrepancy between galaxy cluster masses determined from X-ray and gravitational lensing observations of which Abell 1689 is a well-studied example. In this work we take advantage of 180 ks of Chandra X-ray observations and a new weak gravitational study based on a Hubble Space Telescope mosaic covering the central 1.8 Mpc x 1.4 Mpc to eliminate the mass discrepancy. In contrast to earlier X-ray analyses where the very circular surface brightness has been inferred as Abell 1689 being spherically symmetric and in hydrostatic equilibrium, a hardness ratio map analysis reveals a regular and symmetric appearing main clump with a cool core plus some substructure in the North Eastern part of the cluster. The gravitational lensing mass model supports the interpretation of Abell 1689 being composed of a main clump, which is possibly a virialized cluster, plus some substructure. In order to avoid complications and mis-interpretations due to X-ray emission from the substructure, we exclude it from the mass reconstruction. Comparing X-ray and lensing mass profiles of the regular main part only, shows no significant discrepancy between the two methods and the obtained mass profiles are consistent over the full range where the mass can be reconstructed from X-rays (out to approx. 1 Mpc). The obtained cluster mass within approx. 875 kpc derived from X-rays alone is 6.4 plus/minus 2.1 x 10^14 solar masses compared to a weak lensing mass of 8.6 plus/minus 3.0 x 10^14 solar masses within the same radius. 0 into PostgreSQL...\n",
      "Inserting test sample 1934  Galaxy clusters are one of the most massive objects in the universe, whose properties can reveal crucial information regarding their formation and evolution. In this paper, we address the mismatch between the observations of the complex galaxy cluster Abell 1689 using two different methods: X-ray and gravitational lensing. While X-ray observations act as a probe of the hot intra-cluster gas, gravitational lensing is sensitive to the total mass of the cluster. \n",
      "\n",
      "We have improved on previous studies by analyzing the available Chandra X-ray and Hubble Space Telescope lensing data simultaneously. We find that there is a significant discrepancy between these two estimates of the mass of Abell 1689. Our results reveal that this cluster is more massive than previously thought via X-rays, but more importantly, they show that the observed amount of lensing cannot entirely explain the cluster's total mass. \n",
      "\n",
      "To investigate potential sources of uncertainty, we conducted an extensive set of tests that change the assumptions and parameters in our analysis. We found that none of these tests can account for the observed discrepancy. Our results suggest that the cluster's total mass distribution is significantly more extended than previously thought, which may indicate the presence of additional structures. \n",
      "\n",
      "This study highlights the limitations of using only one method for estimating galaxy cluster masses and demonstrates the importance of combining observational techniques. Our results stress the need for further study, particularly with upcoming observational facilities, to unveil the underlying physical processes that manifest as the complex mass distribution in galaxy clusters. 1 into PostgreSQL...\n",
      "Inserting test sample 1935  Spectroscopy in the infrared provides a means to assess important properties of the plasma in gaseous nebulae. We present some of our own work that illustrates the need for interactions between the themes of this conference - astronomical data, atomic data, and plasma simulations. We undertook Infrared Space Observatory (ISO) observations with the intent of better understanding the effects of density variations in nebulae, particularly planetary nebulae (PNs), by determining average electron densities from the flux ratios of several fine-structure, IR emission lines. Instead, we are able to ascertain only minor density information because of several instances of the observed line flux ratios being out of range of the theoretical predictions using current atomic data. In these cases, the ISO data cannot presently be used to derive electron density, but rather provide direction for needed improvements in the atomic collision strengths.\n",
      "\n",
      "We have detected an unidentified (uid) strong emission line in an ISO/SWS spectrum of the Orion Nebula. The line has a rest wavelength 2.89350$\\pm$0.00003 $\\mu$m. A long-slit UKIRT observation confirms the presence of this line and shows that the emission is spatially extended and appears to be coincident with the brightest part of the ionized region. We do not detect the uid line in our SWS02 spectra of any of the several bright PNs which we observed for a comparable time. The need for basic atomic data, in this case wavelengths to aid species identification, is paramount for future progress.\n",
      "\n",
      "We look toward the future with a brief synopsis of upcoming or planned IR missions. 0 into PostgreSQL...\n",
      "Inserting test sample 1936  Infrared spectroscopy has allowed for the detection and analysis of atomic lines in gaseous nebulae, providing valuable insights into the physical and chemical characteristics of these interstellar structures. This paper reviews the current state of knowledge regarding infrared spectral lines and their properties in gaseous nebulae. Specifically, we examine the emission spectra of atomic hydrogen (H), helium (He), nitrogen (N), and oxygen (O), which represent the dominant species in such nebulae. \n",
      "\n",
      "Through analysis of the infrared spectra, astronomers are able to determine the temperature, density, and chemical composition of the gas present in these interstellar regions. These measurements are essential for our understanding of star formation, as they reveal the conditions required for the initiation and continuation of the process. Additionally, infrared spectroscopy is used to identify the presence of molecules in the nebulae, such as carbon monoxide (CO), water (H2O), and ammonia (NH3). These molecular species provide further insights into the physical and chemical conditions of the nebulae, and may even play a role in the process of forming planets around newly-formed stars.\n",
      "\n",
      "In conclusion, the study of infrared spectral lines in gaseous nebulae has greatly contributed to our understanding of the physical and chemical properties of these interstellar structures. With advancements in infrared spectroscopy technology, astronomers are able to make increasingly precise measurements of these spectral lines, leading to new discoveries and furthering our understanding of the universe around us. 1 into PostgreSQL...\n",
      "Inserting test sample 1937  We present the results of UBVJHKLM photometry of R CrB spanning the period from 1976 to 2001. Studies of the optical light curve have shown no evidence of any stable harmonics in the variations of the stellar emission. In the L band we found semi-regular oscillations with the two main periods of ~3.3 yr and 11.9 yr and the full amplitude of ~0.8 mag and ~0.6 mag, respectively. The colors of the warm dust shell (resolved by Ohnaka et al. 2001) are found to be remarkably stable in contrast to its brightness. This indicates that the inner radius is a constant, time-independent characteristic of the dust shell. The observed behavior of the IR light curve is mainly caused by the variation of the optical thickness of the dust shell within the interval \\tau(V)= 0.2-0.4.\n",
      "\n",
      "Anticorrelated changes of the optical brightness (in particular with P ~ 3.3 yr) have not been found. Their absence suggests that the stellar wind of R CrB deviates from spherical symmetry. The light curves suggest that the stellar wind is variable. The variability of the stellar wind and the creation of dust clouds may be caused by some kind of activity on the stellar surface. With some time lag, periods of increased mass-loss cause an increase in the dust formation rate at the inner boundary of the extended dust shell and an increase in its IR brightness. We have derived the following parameters of the dust shell (at mean brightness) by radiative transfer modeling: inner dust shell radius r_in ~ 110 R_*, temperature T_dust(r_in) ~ 860 K, dust density \\rho_dust(r_in) ~ 1.1x10^{-20} g cm^-3, optical depth \\tau(V) ~ 0.32 at 0.55 micron, mean dust formation rate [dM/dt]_dust ~ 3.1x10^-9 M_sun / yr, mass-loss rate [dM/dt]_gas ~ 2.1x10^-7 M_sun / yr, size of the amorphous carbon grains <(~) 0.01 micron, and B-V ~ -0.28. 0 into PostgreSQL...\n",
      "Inserting test sample 1938  R Coronae Borealis stars are characterized by their unpredictable declines in brightness, often by several magnitudes. UBVJHKLM photometry is a valuable tool for studying these stars, as it allows us to observe their spectral energy distribution throughout multiple wavelengths. In this paper, we present a detailed analysis of R Coronae Borealis using UBVJHKLM photometry.\n",
      "\n",
      "Our observations show that the star's brightness declines by more than six magnitudes during deep minima and recovers after several months to years. We developed a model to explain the behavior of R Coronae Borealis during these events, which we believe is linked to the expulsion of carbon dust from the star's envelope. Our modeling indicates that the temperature of the expelled carbon dust is around 1100 K. Furthermore, we found that the expelled dust is rich in C2, CN, and possibly O2 molecules.\n",
      "\n",
      "We also investigated the periodic behavior of R Coronae Borealis. We discovered a period of about 48 days, which is likely caused by the rotation of the star. We also detected a change in the star's color during the decline in brightness, likely due to the formation of dust. The model we developed to explain the behavior of R Coronae Borealis suggests that the star's envelope experiences a complex expansion/contraction cycle, which results in carbon dust being expelled during the contraction phase.\n",
      "\n",
      "To conclude, our observations and modeling of R Coronae Borealis using UBVJHKLM photometry provide important insights into the behavior of this unique star. The model we developed suggests that the expulsion of carbon dust from the star's envelope plays an essential role in its variability. We hope that our study will contribute to a better understanding of R Coronae Borealis and other stars with similar variability. 1 into PostgreSQL...\n",
      "Inserting test sample 1939  This paper presents the results from the first wide and deep dual narrow-band survey to select H-alpha (Ha) and [OII] line emitters at z=1.47+-0.02 (using matched narrow-band filters in the H and z' bands), exploiting synergies between the UKIRT and Subaru telescopes. The Ha survey at z=1.47 reaches a flux limit of ~7x10^-17 erg/s/cm^2 and detects ~200 Ha emitters over 0.7deg^2, while the much deeper [OII] survey reaches an effective flux of ~7x10^-18 erg/s/cm^2, detecting ~1400 z=1.47 [OII] emitters in a matched co-moving volume of ~2.5x10^5 Mpc^3. The combined survey results in the identification of 190 simultaneous Ha and [OII] emitters at z=1.47. Ha and [OII] luminosity functions are derived and both are shown to evolve significantly from z~0 in a consistent way. The star formation rate density of the Universe at z=1.47 is evaluated, with the Ha analysis yielding 0.16+-0.05 M_sun/yr/Mpc^3 and the [OII] analysis 0.17+-0.04 M_sun/yr/Mpc^3. The measurements are combined with other studies, providing a self-consistent measurement of the star formation history of the Universe over the last ~11Gyrs. By using a large comparison sample at z~0.1 (from the SDSS), [OII]/Ha line ratios are calibrated as probes of dust-extinction. Ha emitters at z~1.47 show on average 1 mag of extinction at Ha, similar to the SDSS sources at z~0. Although we find that dust extinction correlates with SFR, the relation evolves by about ~0.5 mag from z~1.5 to z~0, with z~0 relations over-predicting the dust extinction corrections at high-z by that amount. Stellar mass is found to be a much more fundamental extinction predictor, with the relation between mass and extinction being valid at both z~0 and z~1.5. Dust extinction corrections as a function of optical colours are also derived, offering simpler mechanisms for estimating extinction in moderately star-forming systems over the last ~9Gyrs [Abridged]. 0 into PostgreSQL...\n",
      "Inserting test sample 1940  In this study, we investigate the process of star formation at redshift z=1.47 through a double-blind analysis of H{\\alpha} and [OII] emission lines in the High-redshift(Z) Emission Line Survey (HiZELS). The primary goal of the study is to understand the physical conditions and mechanisms associated with star formation in distant galaxies, which can ultimately provide insights into the evolution of galaxies over cosmic time. \n",
      "\n",
      "The data used in this study were obtained from the Subaru Telescope's Suprime-Cam instrument. Our sample consists of galaxies with H{\\alpha} and [OII] emission lines detected at a high signal-to-noise ratio. We employ the double-blind technique to overcome potential biases in the selection of galaxies and to ensure an objective and unbiased analysis of the data. \n",
      "\n",
      "We find that the majority of the star-forming galaxies in our sample have a relatively low stellar mass, consistent with previous studies. In addition, we estimate the star formation rates (SFRs) in our sample using the H{\\alpha} and [OII] emission lines, and find that our SFR estimates are consistent with previous studies, indicating the reliability of our approach. \n",
      "\n",
      "Furthermore, we investigate the relation between the SFR and the galaxy's specific star formation rate (sSFR), and find that the sSFR(S) of distant galaxies is much higher than that of nearby galaxies. We also study the relation between the SFR and the galaxy's age, and find that the median age of the star-forming galaxies in our sample is relatively young, suggesting that the star-forming activity in these galaxies is ongoing. \n",
      "\n",
      "Finally, we discuss the potential implications of our results for our understanding of galaxy evolution at high redshifts. Our findings provide support for the hypothesis that distant galaxies evolve through continued star formation, rather than a single short-lived burst of star formation at an earlier epoch. Overall, our study offers valuable insights into the processes and conditions associated with star formation in distant galaxies and provides groundwork for further research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 1941  We report the distribution of planets as a function of planet radius (R_p), orbital period (P), and stellar effective temperature (Teff) for P < 50 day orbits around GK stars. These results are based on the 1,235 planets (formally \"planet candidates\") from the Kepler mission that include a nearly complete set of detected planets as small as 2 Earth radii (Re). For each of the 156,000 target stars we assess the detectability of planets as a function of R_p and P.\n",
      "\n",
      "We also correct for the geometric probability of transit, R*/a. We consider first stars within the \"solar subset\" having Teff = 4100-6100 K, logg = 4.0-4.9, and Kepler magnitude Kp < 15 mag. We include only those stars having noise low enough to permit detection of planets down to 2 Re. We count planets in small domains of R_p and P and divide by the included target stars to calculate planet occurrence in each domain. Occurrence of planets varies by more than three orders of magnitude and increases substantially down to the smallest radius (2 Re) and out to the longest orbital period (50 days, ~0.25 AU) in our study. For P < 50 days, the radius distribution is given by a power law, df/dlogR= k R^\\alpha. This rapid increase in planet occurrence with decreasing planet size agrees with core-accretion, but disagrees with population synthesis models. We fit occurrence as a function of P to a power law model with an exponential cutoff below a critical period P_0. For smaller planets, P_0 has larger values, suggesting that the \"parking distance\" for migrating planets moves outward with decreasing planet size. We also measured planet occurrence over Teff = 3600-7100 K, spanning M0 to F2 dwarfs. The occurrence of 2-4 Re planets in the Kepler field increases with decreasing Teff, making these small planets seven times more abundant around cool stars than the hottest stars in our sample. [abridged] 0 into PostgreSQL...\n",
      "Inserting test sample 1942  The study presented in this research paper aims to investigate the occurrence of planets within a short distance from solar-type stars. The observational data used for this study were collected with NASA's Kepler space telescope, providing a comprehensive survey of potential planetary systems. In particular, we focused on planets located within 0.25 astronomical units (AU) from their host stars.\n",
      "\n",
      "We analyzed the light curve data of over 42,000 stars within the Kepler database and identified several hundred planetary candidates associated with solar-type stars. To validate our findings, we employed a range of statistical methods, including calculating false positive probabilities, assessing completeness and detecting any potential biases in our sample.\n",
      "\n",
      "Our results suggest that planets within 0.25 AU of solar-type stars are relatively common, with a frequency of occurrence of 26.5% among the surveyed stars. The majority of these planets are smaller than Neptune, with a size range between 0.5 and 4 times that of Earth's radius. Moreover, the occurrence rate seems to be significantly higher for stars with lower masses and cooler temperatures, which may suggest a correlation between the formation of planets and stellar properties.\n",
      "\n",
      "Regarding the orbital properties of the identified planets, the majority had short orbital periods, with a median value of 4 days, and very low eccentricities. This result indicates that such planets may have migrated from their initial locations, as they would have been subject to tidal forces that tend to circularize their orbits.\n",
      "\n",
      "In conclusion, our study provides comprehensive observational evidence for the occurrence of planets within 0.25 AU of solar-type stars, with implications for the formation and evolution of planetary systems. Future research using new instruments, such as the Transiting Exoplanet Survey Satellite, may improve our understanding of exoplanet properties and help to refine our estimates of their occurrence rates. 1 into PostgreSQL...\n",
      "Inserting test sample 1943  We derive the low redshift galaxy stellar mass function (GSMF), inclusive of dust corrections, for the equatorial Galaxy And Mass Assembly (GAMA) dataset covering 180 deg$^2$. We construct the mass function using a density-corrected maximum volume method, using masses corrected for the impact of optically thick and thin dust. We explore the galactic bivariate brightness plane ($M_\\star-\\mu$), demonstrating that surface brightness effects do not systematically bias our mass function measurement above 10$^{7.5}$ M$_{\\odot}$.\n",
      "\n",
      "The galaxy distribution in the $M-\\mu$-plane appears well bounded, indicating that no substantial population of massive but diffuse or highly compact galaxies are systematically missed due to the GAMA selection criteria. The GSMF is {fit with} a double Schechter function, with $\\mathcal M^\\star=10^{10.78\\pm0.01\\pm0.20}M_\\odot$, $\\phi^\\star_1=(2.93\\pm0.40)\\times10^{-3}h_{70}^3$Mpc$^{-3}$, $\\alpha_1=-0.62\\pm0.03\\pm0.15$, $\\phi^\\star_2=(0.63\\pm0.10)\\times10^{-3}h_{70}^3$Mpc$^{-3}$, and $\\alpha_2=-1.50\\pm0.01\\pm0.15$. We find the equivalent faint end slope as previously estimated using the GAMA-I sample, although we find a higher value of $\\mathcal M^\\star$. Using the full GAMA-II sample, we are able to fit the mass function to masses as low as $10^{7.5}$ $M_\\odot$, and assess limits to $10^{6.5}$ $M_\\odot$. Combining GAMA-II with data from G10-COSMOS we are able to comment qualitatively on the shape of the GSMF down to masses as low as $10^{6}$ $M_\\odot$. Beyond the well known upturn seen in the GSMF at $10^{9.5}$ the distribution appears to maintain a single power-law slope from $10^9$ to $10^{6.5}$. We calculate the stellar mass density parameter given our best-estimate GSMF, finding $\\Omega_\\star= 1.66^{+0.24}_{-0.23}\\pm0.97 h^{-1}_{70} \\times 10^{-3}$, inclusive of random and systematic uncertainties. 0 into PostgreSQL...\n",
      "Inserting test sample 1944  This paper presents the results of the Galaxy And Mass Assembly (GAMA) project on the galaxy stellar mass function up to a redshift of $z = 0.1$. The data was collected from the r-band selected equatorial regions and covers an area of approximately 180 square degrees. The analysis is based on a sample of over $400,000$ galaxies with robust and accurate photometric redshifts calculated from the GAMA input catalogue. Our study finds a steep decline in the number density of low-mass galaxies, while the high-mass end follows a power-law distribution. We use a Schechter function fit to estimate the characteristic stellar mass, $M_*$, a parameter that marks the transition between these two regimes. Our analysis resulted in $log_{10}(M_*/M_{\\odot}) = 10.67 \\pm 0.02$, which is consistent with past observational work. \n",
      "\n",
      "We also probe galaxy morphologies, distinguishing between early- and late-type galaxies. The mass function for the early-types follows the same trends observed for the full sample, while late-type galaxies exhibit a steeper faint-end slope, and a less steep high-mass slope. We investigate Sersic index distributions, and find that lower-mass early-types are more centrally concentrated, with brighter central surface brightnesses. \n",
      "\n",
      "Furthermore, we characterize the mass-metallicity relation of our sample, and find an increase in metallicity as a function of mass that is in alignment with previous observations. Finally, our results reveal an excess of massive, blue galaxies beyond $z=0.07$. It is likely that these objects are formed from recent, gas-rich mergers and will evolve into massive ellipticals over time.\n",
      "\n",
      "In conclusion, our analysis presents a unique and robust dataset for studying the galaxy stellar mass function up to $z=0.1$. The results agree with previous findings, and provide valuable insights into the morphologies and metallicity distributions of early- and late-type galaxies. These findings can be used to test and refine models of galaxy formation and evolution, and inspire future observational and theoretical work in the field. 1 into PostgreSQL...\n",
      "Inserting test sample 1945  The adsorbed atoms exhibit tendency to occupy a triangular lattice formed by periodic potential of the underlying crystal surface. Such a lattice is formed by, e.g., a single layer of graphane or the graphite surfaces as well as (111) surface of face-cubic center crystals. In the present work, an extension of the lattice gas model to $S=1/2$ fermionic particles on the two-dimensional triangular (hexagonal) lattice is analyzed. In such a model, each lattice site can be occupied not by only one particle, but by two particles, which interact with each other by onsite $U$ and intersite $W_{1}$ and $W_{2}$ (nearest and next-nearest-neighbor, respectively) density-density interaction. The investigated hamiltonian has a form of the extended Hubbard model in the atomic limit (i.e., the zero-bandwidth limit). In the analysis of the phase diagrams and thermodynamic properties of this model with repulsive $W_{1}>0$, the variational approach is used, which treats the onsite interaction term exactly and the intersite interactions within the mean-field approximation. The ground state ($T=0$) diagram for $W_{2}\\leq0$ as well as finite temperature ($T>0$) phase diagrams for $W_{2}=0$ are presented. Two different types of charge order within $\\sqrt{3} \\times \\sqrt{3}$ unit cell can occur. At $T=0$, for $W_{2}=0$ phase separated states are degenerated with homogeneous phases (but $T>0$ removes this degeneration), whereas attractive $W_2<0$ stabilizes phase separation at incommensurate fillings. For $U/W_{1}<0$ and $U/W_{1}>1/2$ only the phase with two different concentrations occurs (together with two different phase separated states occurring), whereas for small repulsive $0<U/W_{1}<1/2$ the other ordered phase also appears (with tree different concentrations in sublattices). The qualitative differences with the model considered on hypercubic lattices are also discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 1946  In this study, we investigate the charge-order phenomenon in a $S=1/2$ fermionic gas on a triangular lattice through mean-field theory. This lattice is known to be a candidate for hosting exotic states of matter such as spin liquid and superconductivity. Our analysis is based on constructing the appropriate Hamiltonian for such a system, which involves terms that account for lattice sites' occupations and nearest-neighbor interactions. We utilize the Hartree-Fock approximation to investigate the formation of charge modulations within the lattice, which influence its electronic and transport properties. Our calculations show that competing interactions between neighboring electrons lead to charge-density excitations, which can disrupt the system's Fermi surface and lead to non-Fermi liquid behavior. We find that the doping level and the coupling strength play important roles in determining the charge-order patterns, which can vary across different regions of the lattice. Furthermore, we investigate the effect of lattice deformation and frustration on the formation of charge-order, which could lead to the emergence of novel quantum phases. Our findings shed light on how charge-order can arise in complex many-body systems and provide insights into the electronic properties of materials with triangular lattices. 1 into PostgreSQL...\n",
      "Inserting test sample 1947  Owing to the unusual geometry of kagome lattices-lattices made of corner-sharing triangles-their electrons are useful for studying the physics of frustrated, correlated and topological quantum electronic states. In the presence of strong spin-orbit coupling, the magnetic and electronic structures of kagome lattices are further entangled, which can lead to hitherto unknown spin-orbit phenomena. Here we use a combination of vector-magnetic-field capability and scanning tunnelling microscopy to elucidate the spin-orbit nature of the kagome ferromagnet Fe3Sn2 and explore the associated exotic correlated phenomena. We discover that a many-body electronic state from the kagome lattice couples strongly to the vector field with three-dimensional anisotropy, exhibiting a magnetization-driven giant nematic (two-fold-symmetric) energy shift. Probing the fermionic quasi-particle interference reveals consistent spontaneous nematicity-a clear indication of electron correlation-and vector magnetization is capable of altering this state, thus controlling the many-body electronic symmetry. These spin-driven giant electronic responses go well beyond Zeeman physics and point to the realization of an underlying correlated magnetic topological phase. The tunability of this kagome magnet reveals a strong interplay between an externally applied field, electronic excitations and nematicity, providing new ways of controlling spin-orbit properties and exploring emergent phenomena in topological or quantum materials. 0 into PostgreSQL...\n",
      "Inserting test sample 1948  We investigate the magnetic properties of a strongly correlated, three-dimensional kagome lattice, where anisotropy and giant spin-orbit coupling are present. Our results indicate that the spin-orbit coupling in this system can be tuned by applying an external magnetic field, leading to a significant modification of the magnetic properties. Moreover, we show that the anisotropy of the system is governed by the strength and direction of the spin-orbit coupling and the external field. We analyze the characteristics of the spin and magnetization states at different field strengths and find a strong dependence of the magnetic ground state on the coupling strength. Our study reveals that the kagome magnet exhibits a unique and robust tunability of its spin-orbit coupling and a rich phase diagram, offering a platform for exploring exotic magnetic behavior and topological states of matter. Our findings provide a deeper understanding of the interplay between strong electronic correlations, spin-orbit coupling, and magnetism, with potential applications in spintronics, quantum computation, and energy harvesting technologies. 1 into PostgreSQL...\n",
      "Inserting test sample 1949  This paper is concerned with a complete asymptoticanalysis as $\\mathfrak{E} \\to 0$ of the stationary Munk equation $\\partial\\_x\\psi-\\mathfrak{E} \\Delta^2 \\psi=\\tau$ in a domain $\\Omega\\subset \\mathbf{R}^2$, supplemented with boundaryconditions for $\\psi $ and $\\partial\\_n \\psi$. This equation is a simplemodel for the circulation of currents in closed basins, the variables$x$ and $y$ being respectively the longitude and the latitude. A crudeanalysis shows that as $\\mathfrak{E} \\to 0$, the weak limit of $\\psi$ satisfiesthe so-called Sverdrup transport equation inside the domain, namely$\\partial\\_x \\psi^0=\\tau$, while boundary layers appear in the vicinity ofthe boundary.These boundary layers, which are the main center of interest of thepresent paper, exhibit several types of peculiar behaviour. First, thesize of the boundary layer on the western and eastern boundary, whichhad already been computed by several authors, becomes formally verylarge as one approaches northern and southern portions of the boudary,i.e. pieces of the boundary on which the normal is vertical. Thisphenomenon is known as geostrophic degeneracy. In order to avoid suchsingular behaviour, previous studies imposed restrictive assumptionson the domain $\\Omega$ and on the forcing term $\\tau$. Here, we provethat a superposition of two boundary layers occurs in the vicinity ofsuch points: the classical western or eastern boundary layers, andsome northern or southern boundary layers, whose mathematicalderivation is completely new. The size of northern/southern boundarylayers is much larger than the one of western boundary layers($\\mathfrak{E}^{1/4}$ vs. $\\mathfrak{E}^{1/3}$). We explain in detail how the superpositiontakes place, depending on the geometry of the boundary.Moreover, when the domain $\\Omega$ is not connex in the $x$ direction,$\\psi^0$ is not continuous in $\\Omega$, and singular layers appear inorder to correct its discontinuities. These singular layers areconcentrated in the vicinity of horizontal lines, and thereforepenetrate the interior of the domain $\\Omega$. Hence we exhibit some kindof boundary layer separation.\n",
      "\n",
      "However, we emphasize that we remainable to prove a convergence theorem, so that the singular layerssomehow remain stable, in spite of the separation.Eventually, the effect of boundary layers is non-local in severalaspects. On the first hand, for algebraic reasons, the boundary layerequation is radically different on the west and east parts of theboundary.\n",
      "\n",
      "As a consequence, the Sverdrup equation is endowed with aDirichlet condition on the East boundary, and no condition on the Westboundary. Therefore western and eastern boundary layers have in factan influence on the whole domain $\\Omega$, and not only near theboundary. On the second hand, the northern and southern boundary layerprofiles obey a propagation equation, where the space variable $x$plays the role of time, and are therefore not local. 0 into PostgreSQL...\n",
      "Inserting test sample 1950  The oceanic circulation is a complex system involving a number of physical processes that operate on a vast range of spatial and temporal scales. In many cases, these processes are strongly influenced by the presence of boundaries, such as coastlines or oceanic ridges, which can give rise to boundary layers that have a significant impact on the overall circulation pattern. In this paper, we present a mathematical study of one particular type of boundary layer: the degenerate boundary layer that occurs in large-scale ocean circulation problems.\n",
      "\n",
      "The degenerate boundary layer arises when the thickness of the boundary layer becomes comparable to the characteristic length scales of the problem. In the context of large-scale ocean circulation, this can occur in regions where the Coriolis force is weak or vanishes, such as the equatorial regions or certain parts of the Southern Ocean. Under such conditions, the momentum equations that govern the flow exhibit a singular behavior, with the boundary layer becoming extremely thin and the velocity gradients becoming very large.\n",
      "\n",
      "Our study focuses on the mathematical analysis of these degenerate boundary layers and their role in large-scale ocean circulation. We begin by deriving a set of simplified equations that capture the essential features of the degenerate boundary layer, including the dominance of advection over diffusion and the presence of strong lateral shear. We then carry out a detailed analysis of these simplified equations, using both analytical and numerical methods to investigate the behavior of the flow.\n",
      "\n",
      "Our results show that the presence of degenerate boundary layers can have a significant impact on the overall circulation pattern in large-scale ocean systems. In particular, we find that the boundary layers can give rise to highly localized eddies and jets that are not captured by traditional models of the oceanic circulation. These features have important implications for a wide range of processes, including the transport of heat, salt, and nutrients, as well as the distribution of marine organisms.\n",
      "\n",
      "Overall, our study highlights the important role played by degenerate boundary layers in large-scale ocean circulation problems and underscores the need for more accurate and detailed models to capture their behavior. Our results provide a foundation for further research in this area and have important implications for our understanding of the dynamics of the global ocean system. 1 into PostgreSQL...\n",
      "Inserting test sample 1951  Acronyms are the short forms of phrases that facilitate conveying lengthy sentences in documents and serve as one of the mainstays of writing. Due to their importance, identifying acronyms and corresponding phrases (i.e., acronym identification (AI)) and finding the correct meaning of each acronym (i.e., acronym disambiguation (AD)) are crucial for text understanding. Despite the recent progress on this task, there are some limitations in the existing datasets which hinder further improvement. More specifically, limited size of manually annotated AI datasets or noises in the automatically created acronym identification datasets obstruct designing advanced high-performing acronym identification models. Moreover, the existing datasets are mostly limited to the medical domain and ignore other domains. In order to address these two limitations, we first create a manually annotated large AI dataset for scientific domain. This dataset contains 17,506 sentences which is substantially larger than previous scientific AI datasets. Next, we prepare an AD dataset for scientific domain with 62,441 samples which is significantly larger than the previous scientific AD dataset. Our experiments show that the existing state-of-the-art models fall far behind human-level performance on both datasets proposed by this work. In addition, we propose a new deep learning model that utilizes the syntactical structure of the sentence to expand an ambiguous acronym in a sentence. The proposed model outperforms the state-of-the-art models on the new AD dataset, providing a strong baseline for future research on this dataset. 0 into PostgreSQL...\n",
      "Inserting test sample 1952  The use of acronyms is prevalent in many fields, as it facilitates communication and saves time. However, the proliferation of different acronyms and their ambiguous nature can pose a challenge for both novice and expert readers. To address this issue, we present a new dataset for acronym identification and disambiguation. Our dataset consists of text from various sources and domains, such as academic papers, news articles, and social media. We manually annotated the text at the acronym level, providing information about the expansion of the acronym and its meaning in context. The dataset covers a wide range of acronyms, including those with multiple meanings and those that are domain-specific, thus reflecting the complexity of the issue at hand.\n",
      "\n",
      "To evaluate the effectiveness of our dataset, we experimented with different machine learning models and feature sets. Our results indicate that the use of a combination of lexical, syntactic, and semantic features yields the best performance. Moreover, we show that our dataset is effective not only for acronym identification but also for disambiguation, as it allows for the disentanglement of co-occurring acronyms with similar expansions.\n",
      "\n",
      "In conclusion, our dataset provides a valuable resource for researchers interested in acronym identification and disambiguation. We hope that it will foster the development of new models and approaches for addressing this important problem, and ultimately enhance the clarity and effectiveness of communication across various fields. 1 into PostgreSQL...\n",
      "Inserting test sample 1953  Thermal dark matter scenarios based on light (sub-GeV) fermions typically require the presence of an extra dark sector containing both a massive dark photon along with a dark Higgs boson. The latter generates both the dark photon mass and an additional mass term for the dark sector fermions. This simple setup has both rich phenomenology and bright detection prospects at high-intensity accelerator experiments. We point out that in addition to the well studied pseudo-Dirac regime, this model can achieve the correct relic density in three different scenarios, and examine in details their properties and experimental prospects. We emphasize in particular the effect of the dark Higgs boson on both detection prospects and cosmological bounds. 0 into PostgreSQL...\n",
      "Inserting test sample 1954  The dark Higgs boson is a hypothetical particle predicted by various extensions of the Standard Model of particle physics. In this study, we investigate the signatures of the dark Higgs boson in scenarios with light fermionic dark matter. We analyze the modified production and decay rates of the Higgs boson, as well as the possible channels for dark matter annihilation. Our results indicate that the dark Higgs boson can serve as a mediator of interactions between dark matter particles, and its effects can be detected through deviations from the Standard Model predictions. This research sheds light on the potential discovery of dark matter particles and the underlying fundamental physics that governs their interactions. 1 into PostgreSQL...\n",
      "Inserting test sample 1955  We study the surface transport properties of stationary localized configurations of relativistic fluids to the first two non-trivial orders in a derivative expansion. By demanding that these finite lumps of relativistic fluid are described by a thermal partition function with arbitrary stationary background metric and gauge fields, we are able to find several constraints among surface transport coefficients. At leading order, besides recovering the surface thermodynamics, we obtain a generalization of the Young-Laplace equation for relativistic fluid surfaces, by considering a temperature dependence in the surface tension, which is further generalized in the context of superfluids. At the next order, for uncharged fluids in 3+1 dimensions, we show that besides the 3 independent bulk transport coefficients previously known, a generic localized configuration is characterized by 3 additional surface transport coefficients, one of which may be identified with the surface modulus of rigidity. Finally, as an application, we study the effect of temperature dependence of surface tension on some explicit examples of localized fluid configurations, which are dual to certain non-trivial black hole solutions via the AdS/CFT correspondence. 0 into PostgreSQL...\n",
      "Inserting test sample 1956  Plasma-balls, also known as ball lightning, are a fascinating and elusive phenomenon that have been studied for centuries. In this research, we investigate the surface transport processes occurring in plasma-balls. Using high-speed imaging and spectroscopic measurements, we are able to observe and analyze the surface dynamics of plasma-balls. Our results show that the transport of charged particles occurs predominantly on the surface of the plasma-balls, leading to interesting and complex behavior such as surface waves and oscillations. We propose a model to explain these surface transport phenomena based on the interaction of plasma-balls with their surrounding environment. Our findings have important implications for understanding the dynamics of plasma-balls as well as for their potential applications such as energy storage and transport. In conclusion, this research provides new insights into the surface transport processes in plasma-balls and opens up new avenues for further studies in this fascinating field. 1 into PostgreSQL...\n",
      "Inserting test sample 1957  High dynamic range (HDR) photography is becoming increasingly popular and available by DSLR and mobile-phone cameras. While deep neural networks (DNN) have greatly impacted other domains of image manipulation, their use for HDR tone-mapping is limited due to the lack of a definite notion of ground-truth solution, which is needed for producing training data.\n",
      "\n",
      "In this paper we describe a new tone-mapping approach guided by the distinct goal of producing low dynamic range (LDR) renditions that best reproduce the visual characteristics of native LDR images. This goal enables the use of an unpaired adversarial training based on unrelated sets of HDR and LDR images, both of which are widely available and easy to acquire.\n",
      "\n",
      "In order to achieve an effective training under this minimal requirements, we introduce the following new steps and components: (i) a range-normalizing pre-process which estimates and applies a different level of curve-based compression, (ii) a loss that preserves the input content while allowing the network to achieve its goal, and (iii) the use of a more concise discriminator network, designed to promote the reproduction of low-level attributes native LDR possess.\n",
      "\n",
      "Evaluation of the resulting network demonstrates its ability to produce photo-realistic artifact-free tone-mapped images, and state-of-the-art performance on different image fidelity indices and visual distances. 0 into PostgreSQL...\n",
      "Inserting test sample 1958  High dynamic range (HDR) imaging has emerged as a promising technique for capturing high-quality images with a wide range of luminance levels. However, the process of tone mapping is crucial for rendering HDR images into the limited dynamic range of standard monitors. In this paper, we propose an unpaired learning framework for HDR image tone mapping, which doesn't require the paired training data for the mapping function. Our approach involves learning a generator network that can approximate the tone mapping function, and a discriminator network that can differentiate the generated output from real images. We also introduce a novel loss function that helps the generator to produce high-quality and consistent images. Our experiments show that the proposed method produces realistic, high-quality output while preserving the details in the input images. Compared to the existing paired learning methods, our technique is more efficient, scalable, and can be applied to any arbitrary images without additional manual work. Additionally, we show that our framework outperforms the state-of-the-art methods in terms of both objective and subjective evaluations. Our finding opens up new opportunities for the HDR imaging community to explore more efficient and effective tone mapping techniques. 1 into PostgreSQL...\n",
      "Inserting test sample 1959  Using the submm array camera SCUBA on the 15-m JCMT it is now possible to conduct unbiased submm surveys and quantify the level of star-formation activity in the young Universe by observing the rest-frame FIR thermal emission from dust in high-redshift galaxies. However an accurate interpretation of these existing submm cosmological surveys is prevented by uncertainties in the redshifts of the submm-selected galaxies, the ambiguities in the identifications of their optical/IR and radio counterparts and the restricted range of flux density over which the submm source-counts are measured. This paper outlines the future observations required to overcome these deficiencies. 0 into PostgreSQL...\n",
      "Inserting test sample 1960  In recent years, millimetre and submillimetre observations have allowed us to unveil exciting new insights into the evolution of our universe. With upcoming advancements in technology, we stand on the brink of an even greater understanding of the cosmos. This paper presents an overview of the latest research in millimetre and submillimetre cosmology, discussing the latest breakthroughs in cosmological studies from the origin of the cosmic microwave background radiation to the search for new galaxies. In addition, this paper explores the current and future potential of millimeter and submillimeter astronomy that will allow us to further our understanding of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1961  Late on July 23, 2012, the STEREO-A spacecraft encountered a fast forward shock driven by a coronal mass ejection launched from the Sun earlier that same day. The estimated travel time of the disturbance ($\\sim 20$ hrs), together with the massive magnetic field strengths measured within the ejecta ($> 100$nT), made it one of the most extreme events observed during the space era.\n",
      "\n",
      "In this study, we examine the properties of the shock wave. Because of an instrument malfunction, plasma measurements during the interval surrounding the CME were limited, and our approach has been modified to capitalize on the available measurements and suitable proxies, where possible. We were able to infer the following properties. First, the shock normal was pointing predominantly in the radial direction (${\\bf n} = 0.97 {\\bf e}_r -0.09 {\\bf e}_t -0.23 {\\bf e}_n$). Second, the angle between ${\\bf n}$ and the upstream magnetic field, $\\theta_{Bn}$, was estimated to be $\\approx 34^{\\circ}$, making the shock \"quasi-parallel,\" and supporting the idea of an earlier \"preconditioning\" ICME. Third, the shock speed was estimated to be $\\approx 3300$ km s$^{-1}$. Fourth, the sonic Mach number, $M_s$, for this shock was $\\sim 28$. We support these results with an idealized numerical simulation of the ICME. Finally, we estimated the change in ram pressure upstream of the shock to be $\\sim 5$ times larger than the pressure from the energetic particles, suggesting that this was not a standard \"steady-state\" cosmic-ray modified shock (CRMS). Instead it might represent an early, transient phase in the evolution of the CRMS. 0 into PostgreSQL...\n",
      "Inserting test sample 1962  In this paper, we present an analysis of the properties of the fast forward shock driven by the extreme coronal mass ejection (CME) that occurred on July 23, 2012. Our investigation is based on observations from multiple instruments, including the Solar Dynamics Observatory and the Solar and Heliospheric Observatory. We utilize data from the Advanced Composition Explorer to analyze the plasma and magnetic field measurements at the shock front.\n",
      "\n",
      "Our results show that the fast forward shock driven by the July 23, 2012 CME was associated with energetic particle events that were observed both in situ and at the Earth's surface. We find that the shock was capable of accelerating particles up to energies of several MeV, and that this acceleration was driven by the shock's interaction with the ambient solar wind.\n",
      "\n",
      "Furthermore, we analyze the effects of the shock on the surrounding magnetic field. We find that the fast forward shock caused significant disturbances in the magnetic field, leading to enhanced magnetic reconnection and the formation of new quasi-steady reconnection regions. Our analysis suggests that these quasi-steady reconnection regions may have played a role in energizing the particles observed at the shock.\n",
      "\n",
      "Overall, our investigation provides insights into the properties of the fast forward shock driven by extreme CMEs and highlights the complex interplay between shocks, energetic particles, and the ambient solar wind. 1 into PostgreSQL...\n",
      "Inserting test sample 1963  Background initialization is an important step in many high-level applications of video processing,ranging from video surveillance to video inpainting.However,this process is often affected by practical challenges such as illumination changes,background motion,camera jitter and intermittent movement,etc.In this paper,we develop a co-occurrence background model with superpixel segmentation for robust background initialization. We first introduce a novel co-occurrence background modeling method called as Co-occurrence Pixel-Block Pairs(CPB)to generate a reliable initial background model,and the superpixel segmentation is utilized to further acquire the spatial texture Information of foreground and background.Then,the initial background can be determined by combining the foreground extraction results with the superpixel segmentation information.Experimental results obtained from the dataset of the challenging benchmark(SBMnet)validate it's performance under various challenges. 0 into PostgreSQL...\n",
      "Inserting test sample 1964  Background initialization in video processing is a crucial task that aims to identify the background of a scene before any object or motion is detected. In this paper, we propose a co-occurrence background model with superpixels for robust background initialization. Our method combines the advantages of both co-occurrence matrices and superpixels, allowing for more accurate and efficient background modeling. We use the co-occurrence matrices to capture the spatial relationships between pixels, and the superpixel segmentation to reduce the computational cost and improve the robustness of the model to noise and outliers. We evaluate our method on several benchmark datasets and demonstrate its superiority over existing state-of-the-art methods. Our work provides a promising approach for improving the performance of background initialization in video surveillance systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1965  Observations of 28SiO v=0 J=1-0 line emission (7-mm wavelength) from AGB stars show in some cases peculiar profiles, composed of a central intense component plus a wider plateau. Very similar profiles have been observed in CO lines from some AGB stars and most post-AGB nebulae and, in these cases, they are clearly associated with the presence of conspicuous axial symmetry and bipolar dynamics.\n",
      "\n",
      "We present systematic observations of 28SiO v=0 J=1-0 emission in 28 evolved stars, performed with the 40~m radio telescope of the IGN in Yebes, Spain. We find that the composite core plus plateau profiles are almost always present in O-rich Miras, OH/IR stars, and red supergiants. They are also found in one S-type Mira ($\\chi$ Cyg), as well as in two semiregular variables (X Her and RS Cnc) that are known to show axial symmetry. In the other objects, the profiles are simpler and similar to those of other molecular lines. The composite structure appears in the objects in which SiO emission is thought to come from the very inner circumstellar layers, prior to dust formation. The central spectral feature is found to be systematically composed of a number of narrow spikes, except for X Her and RS Cnc, in which it shows a smooth shape that is very similar to that observed in CO emission. These spikes show a significant (and mostly chaotic) time variation, while in all cases the smooth components remain constant within the uncertainties. The profile shape could come from the superposition of standard wide profiles and a group of weak maser spikes.\n",
      "\n",
      "Alternatively, we speculate that the very similar profiles detected in objects that are axisymmetric may be indicative of the systematic presence of a significant axial symmetry in the very inner circumstellar shells around AGB stars; the presence of such symmetry would be independent of the probable weak maser effects in the central spikes. 0 into PostgreSQL...\n",
      "Inserting test sample 1966  In this study, we investigate the 28SiO v=0 J=1-0 emission from evolved stars. Our goal is to understand the properties and characteristics of this emission, and to explore its potential use as a diagnostic tool in stellar astrophysics.\n",
      "\n",
      "To achieve this goal, we analyze a sample of evolved stars using a combination of observational and theoretical techniques. We start by using the IRAM 30-m telescope to obtain high-resolution spectra of the 28SiO v=0 J=1-0 emission from a sample of asymptotic giant branch (AGB) stars. We then compare these observations with theoretical predictions to determine the physical parameters of the emission, such as its temperature, density, and velocity structure.\n",
      "\n",
      "Our analysis reveals that the 28SiO v=0 J=1-0 emission is a valuable diagnostic tool for studying evolved stars. We find that the emission is sensitive to the mass-loss rate, pulsation period, and stellar atmosphere of AGB stars. Furthermore, we demonstrate that the line profile and intensity of the emission can be used to infer the presence of circumstellar shells and disks around evolved stars.\n",
      "\n",
      "Our results have important implications for understanding the evolution of evolved stars and their contribution to the chemical enrichment of the universe. In particular, we show that the 28SiO v=0 J=1-0 emission provides a powerful probe of the mass-loss process and the formation of dust in AGB stars. Our work also has practical applications, as the 28SiO v=0 J=1-0 line can be used as a tracer of molecular gas in star-forming regions and galaxies.\n",
      "\n",
      "In conclusion, our study demonstrates the importance of the 28SiO v=0 J=1-0 emission as a diagnostic tool in stellar astrophysics. We provide a comprehensive analysis of the physical properties of the emission, and show how it can be used to probe the evolution and structure of evolved stars. Our work has important implications for understanding the contribution of evolved stars to the chemical evolution of the universe, and for using the 28SiO v=0 J=1-0 line as a tracer of molecular gas in star-forming regions and galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 1967  The Brexit referendum took place in the UK in June, 2016. The unweighted percentage of leavers over the whole population was 51.9%. In this paper, first, we demonstrate that a 52%-48% split represents only a difference that is not sufficiently different from a 50-50 split to claim a majority for either side. Second, and most important, on this basis of the unweighted percentage, statement like: The country voted to leave the EU, were made. When a statement about a population is made based on a subset of it (the turnout rate for Brexit was only 72% and therefore 37% of the eligible population voted Leave), it comes with an element of uncertainty that should not be ignored. The unweighted average disregards, not only between-region heterogeneity but also within-region variability. Our analysis, controlling for both, finds that the split of the Brexit is of negligible material significance and do not indicate majority for either side. 0 into PostgreSQL...\n",
      "Inserting test sample 1968  The 2016 Brexit referendum was a pivotal moment in British politics and triggered a surge of interest in the voting patterns of the population. This research paper aims to analyze the data collected during the referendum and provide new insights into what it really says about the attitudes of the voters. The data, which covers demographics, geography, and socio-economic status, is analyzed using statistical methods to reveal patterns and correlations. The results show that certain groups, such as older voters and those in rural areas, were more likely to vote for Brexit. Moreover, the findings suggest that the referendum result may have been influenced by factors beyond just the core issues of Brexit. This paper offers a new perspective on the 2016 referendum and opens up avenues for further research on the complex interplay between politics and society. 1 into PostgreSQL...\n",
      "Inserting test sample 1969  Eruptive flares are sudden releases of magnetic energy that involve many phenomena, several of which can be explained by the standard 2D flare model and its realizations in three-dimensions. We analyze a three-dimensional magnetohydrodynamics simulation in the framework of this model that naturally explains the contraction of coronal loops in the proximity of the flare sites, as well as the inflow towards the region above the cusp-shaped loops.\n",
      "\n",
      "We find that two vorticity arcs located along the flanks of the erupting magnetic flux rope are generated as soon as the eruption begins. The magnetic arcades above the flux-rope legs are then subjected to expansion, rotation or contraction depending on which part of the vortex-flow advects them. In addition to the vortices, an inward-directed magnetic pressure gradient exists in the current sheet below the magnetic flux rope. It results in the formation of a sink that is maintained by reconnection.\n",
      "\n",
      "We conclude that coronal loop apparent implosions observed during eruptive flares are the result of hydro-magnetic effects related to the generation of vortex- and sink-flows when a flux rope moves in a magnetized environment. 0 into PostgreSQL...\n",
      "Inserting test sample 1970  Coronal mass ejections (CMEs), which are explosive releases of plasma and magnetic field from the solar corona, pose a significant threat to space weather. To better understand this phenomenon, we investigate the dynamics of vortex and sink flows in eruptive flares as a model for coronal implosions. Our study employs state-of-the-art numerical simulations and analytical modeling, which highlight the role of vortex and sink flows in shaping the magnetic topology and triggering the explosive release of energy. We find that vortex and sink flows can drive the formation of twisted magnetic structures, which act as precursor events to CMEs. Additionally, we show that the interaction between vortex and sink flows can lead to current sheets and reconnection events, which facilitate the release of magnetic energy. Overall, our results provide a new perspective on the physics of CMEs and lay the foundation for the development of improved space weather forecasting models. 1 into PostgreSQL...\n",
      "Inserting test sample 1971  We investigate CP-violation effect in the long-baseline neutrino oscillation in the four-neutrino model with mass scheme of the two nearly degenerate pairs separated with the order of 1 eV, by using the data from the solar neutrino deficit, the atmospheric neutrino anomaly and the LSND experiments along with the other accelerator and reactor experiments. By use of the most general parametrization of the mixing matrix with six angles and six phases, we show that the genuine CP-violation effect could attain as large as 0.3 for $\\Delta P(\\nu_\\mu\\to\\nu_\\tau) \\equiv P(\\nu_\\mu\\to\\nu_\\tau) - P(\\bar{\\nu_\\mu}\\to\\bar{\\nu_\\tau})$ and that the matter effect is negligibly small such as at most 0.01 for $\\Delta P(\\nu_\\mu\\to\\nu_\\tau)$ for $\\Delta m^2 = (1-5)\\times 10^{-3} {\\rm eV}^2$, which is the mass-squared difference relevant to the long-baseline oscillation. 0 into PostgreSQL...\n",
      "Inserting test sample 1972  This paper investigates the impact of CP violation on long-baseline neutrino oscillation in the four-neutrino model. The phenomenon of neutrino oscillation has been studied extensively in recent years and has become a central topic in the field of particle physics. In the four-neutrino model, the mixing matrix can be more complex than in the simpler three-neutrino models, and CP violation can play a significant role. Specifically, we examine the case of a long-baseline experiment utilizing a neutrino beam from a distant source, and we analyze the parameter space where the effect of CP violation is the strongest. Our calculations demonstrate that significant CP violation effects can occur if certain conditions are satisfied. Our results serve as a theoretical foundation for current and future long-baseline neutrino experiments, which seek to uncover more about the nature of neutrinos and the fundamental laws of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1973  The Riemann $\\Xi(z)$ function (even in $z$) admits a Fourier transform of an even kernel $\\Phi(t)=4e^{9t/2}\\theta''(e^{2t})+6e^{5t/2}\\theta'(e^{2t})$. Here $\\theta(x):=\\theta_3(0,ix)$ and $\\theta_3(0,z)$ is a Jacobi theta function, a modular form of weight $\\frac{1}{2}$. (A) We discover a family of functions $\\{\\Phi_n(t)\\}_{n\\geqslant 2}$ whose Fourier transform on compact support $(-\\frac{1}{2}\\log n, \\frac{1}{2}\\log n)$, $\\{F(n,z)\\}_{n\\geqslant2}$, converges to $\\Xi(z)$ uniformly in the critical strip $S_{1/2}:=\\{|\\Im(z)|< \\frac{1}{2}\\}$. (B) Based on this we then construct another family of functions $\\{H(14,n,z)\\}_{n\\geqslant 2}$ and show that it uniformly converges to $\\Xi(z)$ in the critical strip $S_{1/2}$. (C) Based on this we construct another family of functions $\\{W(n,z)\\}_{n\\geqslant 8}:=\\{H(14,n,2z/\\log n)\\}_{n\\geqslant 8}$ and show that if all the zeros of $\\{W(n,z)\\}_{n\\geqslant 8}$ in the critical strip $S_{1/2}$ are real, then all the zeros of $\\{H(14,n,z)\\}_{n\\geqslant 8}$ in the critical strip $S_{1/2}$ are real. (D) We then show that $W(n,z)=U(n,z)-V(n,z)$ and $U(n,z^{1/2})$ and $V(n,z^{1/2})$ have only real, positive and simple zeros. And there exists a positive integer $N\\geqslant 8$ such that for all $n\\geqslant N$, the zeros of $U(n,x^{1/2})$ are strictly left-interlacing with those of $V(n,x^{1/2})$. Using an entire function equivalent to Hermite-Kakeya Theorem for polynomials we show that $W(n\\geqslant N,z^{1/2})$ has only real, positive and simple zeros. Thus $W(n\\geqslant N,z)$ have only real and imple zeros. (E) Using a corollary of Hurwitz's theorem in complex analysis we prove that $\\Xi(z)$ has no zeros in $S_{1/2}\\setminus\\mathbb{R}$, i.e., $S_{1/2}\\setminus \\mathbb{R}$ is a zero-free region for $\\Xi(z)$. Since all the zeros of $\\Xi(z)$ are in $S_{1/2}$, all the zeros of $\\Xi(z)$ are in $\\mathbb{R}$, i.e., all the zeros of $\\Xi(z)$ are real. 0 into PostgreSQL...\n",
      "Inserting test sample 1974  The Riemann zeta function, denoted as $\\zeta(z)$, is one of the most essential functions in mathematics, and its properties have been studied for centuries. It is known that $\\zeta(z)$ has non-trivial zeros, which lie on the critical line $\\Re(z)=\\frac{1}{2}$. Another function that is closely related to $\\zeta(z)$ is the Riemann $\\Xi(z)$ function, which is defined as the derivative of the logarithm of the Riemann zeta function.\n",
      "\n",
      "The zeros of the Riemann $\\Xi(z)$ function are closely tied to those of the Riemann zeta function, as they correspond to the critical points of the zeta function. In this paper, we investigate the properties of the zeros of the Riemann $\\Xi(z)$ function and their relationship to the Riemann zeta function.\n",
      "\n",
      "One of the most important results that we obtain is a formula for the number of zeros of the Riemann $\\Xi(z)$ function that lie on a given vertical line in the complex plane. Our formula is expressed in terms of the values of the Riemann zeta function and the Hurwitz zeta function, which are closely related to each other. We also derive other formulas that give us a better understanding of the distribution of the zeros of the Riemann $\\Xi(z)$ function.\n",
      "\n",
      "Moreover, we investigate the behavior of the zeros of the Riemann $\\Xi(z)$ function near the critical line $\\Re(z)=\\frac{1}{2}$. We show that the zeros exhibit a critical slowing down as they approach the critical line, which is a phenomenon known in physics as a 'critical point'. We also investigate the possible existence of zeros on the critical line and establish a lower bound for the number of zeros on the critical line.\n",
      "\n",
      "Our results provide significant insight into the properties of the Riemann zeta function and its related functions. They shed light on the behavior of the non-trivial zeros of these functions and their connection to number theory and physics. The analysis presented in this paper has applications in a variety of fields, from cryptography to quantum mechanics. Overall, this is an important contribution to the ongoing study of the Riemann $\\Xi(z)$ function and its zeros. 1 into PostgreSQL...\n",
      "Inserting test sample 1975  Among many topological indices of trees the sum of distances $\\sigma(T)$ and the number of subtrees $F(T)$ have been a long standing pair of graph invariants that are well known for their negative correlation. That is, among various given classes of trees, the extremal structures maximizing one usually minimize the other, and vice versa. By introducing the \"local\" versions of these invariants, $\\sigma_T(v)$ for the sum of distance from $v$ to all other vertices and $F_T(v)$ for the number of subtrees containing $v$, extremal problems can be raised and studied for vertices within a tree. This leads to the concept of \"middle parts\" of a tree with respect to different indices. A challenging problem is to find extremal values of the ratios between graph indices and corresponding local functions at middle parts or leaves. This problem also provides new opportunities to further verify the the correlation between different indices such as $\\sigma(T)$ and $F(T)$. Such extremal ratios, along with the extremal structures, were studied and compared for the distance and subtree problems for general trees In this paper this study is extended to binary trees, a class of trees with numerous practical applications in which the extremal ratio problems appear to be even more complicated. After justifying some basic properties on the distance and subtree problems in trees and binary trees, characterizations are provided for the extremal structures achieving two extremal ratios in binary trees of given order. The generalization of this work to $k$-ary trees is also briefly discussed. The findings are compared with the previous established extremal structures in general trees. Lastly some potential future work is mentioned. 0 into PostgreSQL...\n",
      "Inserting test sample 1976  This paper analyzes two well-studied computational problems on binary trees: the distance problem and the subtree problem. We consider a collection of extremal ratios that shed light on the difficulty of these problems. Our results show that these ratios vary considerably between the two problems, and that they can differ by several orders of magnitude. In particular, we provide tight bounds on the largest and smallest possible values of each ratio in both problems. Our analysis reveals that the distance problem is generally more difficult than the subtree problem, in the sense that its extremal ratios are larger and more variable. We also investigate the effect of various tree parameters on these ratios, including tree size, height, degree, and balance. Our main findings suggest that the extremal ratios depend primarily on the depth of the tree and the distribution of its degree sequence. Finally, we propose several open problems and directions for future research, including randomized and approximation algorithms for the distance and subtree problems, and the analysis of these problems in other classes of trees. Our work provides new insights into the computational complexity of fundamental problems in tree algorithms and opens up new avenues for investigation in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 1977  We performed a detailed study of the evolution of the star formation rate (SFR) and stellar mass of the brightest group galaxies (BGGs) and their relative contribution to the total baryon budget within $R_{200}$ ($f^{BGG}_{b,200}$). The sample comprises 407 BGGs selected from X-ray galaxy groups ($M_{200}=10^{12.8}-10^{14} \\;M_{\\odot}$) out to $z\\sim1.3$ identified in the COSMOS, XMM-LSS, and AEGIS fields. We find that BGGs constitute two distinct populations of quiescent and star-forming galaxies and their mean SFR is $\\sim2$ dex higher than the median SFR at $ z<1.3 $. Both the mean and the median SFRs decline with time by $>2$ dex. The mean (median) of stellar mass of BGGs has grown by $0.3$ dex since $z=1.3$ to the present day. We show that up to $\\sim45\\% $ of the stellar mass growth in a star-forming BGG can be due to its star-formation activity. With respect to $f^{BGG}_{b,200}$, we find it to increase with decreasing redshift by $\\sim0.35$ dex while decreasing with halo mass in a redshift dependent manner. We show that the slope of the relation between $f^{BGG}_{b,200}$ and halo mass increases negatively with decreasing redshift. This trend is driven by an insufficient star-formation in BGGs, compared to the halo growth rate. We separately show the BGGs with the 20\\% highest $f^{BGG}_{b,200}$ are generally non-star-forming galaxies and grow in mass by processes not related to star formation (e.g., dry mergers and tidal striping). We present the $ M_\\star-M_h $ and $ M_\\star/M_h-M_h $ relations and compare them with semi-analytic model predictions and a number of results from the literature. We quantify the intrinsic scatter in stellar mass of BGGs at fixed halo mass ($\\sigma_{log M_{\\star}}$) and find that $\\sigma_{log M_{\\star}}$ increases from 0.3 dex at $ z\\sim0.2 $ to 0.5 dex at $ z\\sim1.0 $ due to the bimodal distribution of stellar mass. 0 into PostgreSQL...\n",
      "Inserting test sample 1978  Galaxies are not isolated entities, but often exist in groups composed of a few to hundreds of galaxies. The Brightest Group Galaxies (BGGs) have been studied in the context of understanding the formation and evolution of galaxy groups. In this study, we investigate the relative contribution of BGGs to the total baryon content of groups at z<1.3.\n",
      "\n",
      "By analyzing the Multi-Unit Spectroscopic Explorer (MUSE) data, we investigate the amount of baryons, or the normal matter that makes up stars, gas, and everything else, within groups of galaxies. Our analysis shows that the baryon fraction, or the ratio of baryonic matter to the total matter, is higher in groups that contain BGGs. This indicates that BGGs are important contributors to the total baryon content of galaxy groups.\n",
      "\n",
      "Further analysis reveals that the relative contribution of BGGs to the total baryon content varies with group mass. In particular, the fraction of baryons in BGGs is higher in lower mass groups. This suggests that the BGGs in lower mass groups may have a more important role in the formation and evolution of these systems compared to those in higher mass groups.\n",
      "\n",
      "Our findings have implications for understanding the formation and evolution of galaxy groups, as well as the role of BGGs in this process. In particular, our results highlight the importance of BGGs in contributing to the baryon content of galaxy groups, and suggest that the properties of BGGs may vary with group mass. We hope that this study will provide further insights into the complex processes that govern the evolution of galaxies and the structures they form. 1 into PostgreSQL...\n",
      "Inserting test sample 1979  We calculate the dispersion of a hole in ${\\rm CuO_2}$ bilayers and 3D antiferromagnets, using the self-consistent Born approximation (SCBA) to the $t-J$ model. Superconductivity and the symmetry of its order parameter are studied introducing a nearest-neighbor density-density attraction induced by short range antiferromagnetic (AF) fluctuations, as described in recent studies for the single layer cuprates. The well-known pairing in the ${ d_{x^2-y^2}}$ channel observed for one plane remains robust when three dimensional interactions are turned on. In bilayers, as the exchange along the direction perpendicular to the planes grows, eventually a transition to a ``s-wave'' state is observed with an order parameter mixture of ${ d_{3z^2-r^2}}$ and ${ s_{x^2+y^2 + z^2}}$. For an isotropic 3D antiferromagnet the $d_{x^2 -y^2}$ and $d_{3z^2 - r^2}$ channels are degenerate. 0 into PostgreSQL...\n",
      "Inserting test sample 1980  This research investigates hole dispersion and the symmetry of the superconducting order parameter for underdoped CuO$_2$ bilayers and 3D antiferromagnets. The aim is to determine the correlation between hole concentration, superconducting transition temperature (T$_c$), and the antiferromagnetic order parameter in these materials. We employ scanning tunneling microscopy and angle-resolved photoemission spectroscopy to gather data on the electronic structure. We identify a correlation between the hole concentration and T$_c$ in CuO$_2$ bilayers. However, in 3D antiferromagnets, we observe an unexpected suppression of T$_c$ as hole concentration increases. Furthermore, we find that the superconducting gap symmetry is consistent with a nodal d-wave gap. These observations suggest that antiferromagnetic order in high-T$_c$ superconductors may be critical to their understanding, and highlight the importance of symmetry in these materials. 1 into PostgreSQL...\n",
      "Inserting test sample 1981  In this work, we investigate the exclusive production of particles in scattering processes in the so-called saturation region. Within this scheme the phenomenon of geometric scaling takes place: cross sections are functions only of a dimensionless combination of the relevant kinematic variables, which happens both in inclusive and diffractive cases, as in the production of vector mesons. In particular, the scaling variable is given in general by $\\tau = Q^2/Q_s^2$, where $Q^2$ is the photon virtuality and $Q_s$ represents the saturation scale, which drives the energy dependence and the corresponding nuclear effects. Based on the scaling property, we are able to derive a universal expression for the cross sections for the exclusive vector meson production and deeply virtual Compton scattering (DVCS) in both photon-proton and photon-nucleus interactions. This phenomenological result describes all available data from DESY-HERA for $\\rho$, $\\phi$ and $J/\\psi$ production and DVCS measurements. A discussion is also carried out on the size of nuclear shadowing corrections on photon-nucleus interaction. 0 into PostgreSQL...\n",
      "Inserting test sample 1982  This paper investigates the application of the Color Glass Condensate (CGC) formalism and QCD factorization to the exclusive production of vector mesons and deeply virtual Compton scattering. By utilizing the geometrical scaling property, which states the scaling behavior of amplitudes with respect to the energy of the process and its virtuality, we derive a universal scaling function that applies to both vector meson production and deeply virtual Compton scattering. Our calculations fit well with the numerical results in the existing literature and provide valuable insights into the fundamental mechanism of these processes. We also demonstrate that the high-energy behavior of the scattering amplitude in deeply virtual Compton scattering is related to the dynamics of the quarks and gluons involved in the process, which allows for a better understanding of the underlying physics. The results presented in this paper offer novel contributions to the theoretical understanding and experimental investigations of vector meson production and deeply virtual Compton scattering. 1 into PostgreSQL...\n",
      "Inserting test sample 1983  We consider the optimal design of sequential transmission over broadcast channel with nested feedback. Nested feedback means that the channel output of the outer channel is also available at the decoder of the inner channel. We model the communication system as a decentralized team with three decision makers---the encoder and the two decoders. Structure of encoding and decoding strategies that minimize a total distortion measure over a finite horizon are determined. The results are applicable for real-time communication as well as for the information theoretic setup. 0 into PostgreSQL...\n",
      "Inserting test sample 1984  We propose an algorithm for the optimal sequential transmission of messages over a broadcast channel, where the receiver provides feedback to the transmitter using nested codes. Our algorithm utilizes the feedback to adaptively improve the transmission rate and select the best code for each message. We analyze the achievable rate region of the feedback channel and provide bounds on the transmission rate. Simulation results show the effectiveness of the proposed algorithm in reducing the number of transmission rounds required to deliver each message with a high probability of error-free reception. 1 into PostgreSQL...\n",
      "Inserting test sample 1985  The two-dimensional nature of graphene makes it an ideal platform to explore proximity-induced unconventional planar superconductivity and the possibility of topological superconductivity. Using Green's functions techniques, we study the transport properties of a finite size ballistic graphene layer placed between a normal state electrode and a graphene lead with proximity-induced unconventional superconductivity. Our microscopic description of such a junction allows us to consider the effect of edge states in the graphene layer and the imperfect coupling to the electrodes. The tunnel conductance through the junction and the spectral density of states feature a rich interplay between graphene's edge states, interface bound states formed at the graphene-superconductor junction, Fabry-P\\'erot resonances originated from the finite size of the graphene layer, and the characteristic Andreev surface states of unconventional superconductors. Within our analytical formalism, we identify the separate contribution from each of these subgap states to the conductance and density of states. Our results show that graphene provides an advisable tool to determine experimentally the pairing symmetry of proximity-induced unconventional superconductivity. 0 into PostgreSQL...\n",
      "Inserting test sample 1986  This work investigates the properties of unconventional superconductors using two-dimensional spectroscopy techniques applied to a graphene substrate. We study the behavior of subgap states in order to further understand the underlying mechanisms of unconventional superconductivity. Our results show that subgap states are strongly affected by the interaction of the superconducting order parameter with other degrees of freedom, such as the spin or charge density waves. We found that the density of states associated with these subgap states can be enhanced or suppressed depending on the specific experimental conditions. We also observe the emergence of new spectral features at energies below the superconducting gap, which further suggests the presence of unconventional superconductivity. Our findings provide insight into the role of subgap states in superconductivity and have implications for the design of future experiments that aim to uncover the properties of unconventional superconductors. 1 into PostgreSQL...\n",
      "Inserting test sample 1987  Four months of quasi-simultaneous spectroscopic and photometric observations were used to study the variations of the photometric light curve, the evolution of the chromospheric activity from the H$\\alpha$ and H$\\beta$ lines, and the distribution of cool spots from Doppler maps.\n",
      "\n",
      "During our observations one side of the star was more active than the other.\n",
      "\n",
      "The equivalent width of the H$\\alpha$ line from the least active hemisphere increased from ~0.7 {\\AA} at the beginning of the observation to 1.0 {\\AA} at the end. The basal emission of the most active hemisphere remained roughly constant at a H$\\alpha$ EW of ~1.0 {\\AA}. Intense flare activity was observed during the first twenty days, where at least four different events were detected. The line asymmetries of the H$\\alpha$ line suggest that one of the flares could have produced a mass ejection with a maximum projected speed of $\\mathrm{70\\thinspace km\\thinspace s^{-1}}$. The rotational modulation of the V-band photometry showed clear anti-correlation with the chromospheric activity. The difference in brightness between the opposite hemispheres decreased from 0.16 to 0.09 mag in two months. Three spots gradually moving apart from each other dominated the photospheric Doppler maps. The comparison between the maps and the H$\\alpha$ line as the star rotates reveals the spatial coexistence of chromospheric H$\\alpha$ emission and photospheric spots.\n",
      "\n",
      "Our results indicate that the active regions of LQ Hya can live for at least four months. The detected changes in the photometric light curve and the spectroscopic Doppler images seem to be more a consequence of the spatial redistribution of the active regions rather than due to changes in their strength. Only one of the active regions shows significant changes in its chromospheric emission. 0 into PostgreSQL...\n",
      "Inserting test sample 1988  LQ Hydrae is known for its complex magnetic activity, not uncommon for a young star like it. Using high-resolution spectroscopy and imaging observations, we explore the coexistence and evolution of spots, plages, and flare activity on the surface of LQ Hydrae over short time frames. Our results reveal that spots and plages have a tendency to emerge and decay in close proximity, though not entirely correlated. We found that the formation of plages often preceded flare activity, suggesting that the presence of active regions on the surface of LQ Hydrae may increase the likelihood of flaring. Furthermore, we detected rapid fluctuations in the magnetic and velocity fields of the star, likely caused by small-scale instabilities. Interestingly, we observed an inverse correlation between the brightness of plages and the strength of magnetic fields within them. We attribute this to a variation in the temperature of these regions, as well as potential changes in their coverage by magnetic fields. Through these observations, we were able to map the evolutionary paths of spots and plages in detail, and through their movements, we monitored the dynamical evolution of LQ Hydrae's magnetic field. Our study provides new insights into the magnetic activity of young stars, which is essential to our understanding of the star-planet interaction and the habitability of exoplanets in these systems. 1 into PostgreSQL...\n",
      "Inserting test sample 1989  Daany is .NET and cross platform data analytics and linear algebra library written in C# supposed to be a tool for data preparation, feature engineering and other kind of data transformations and feature engineering. The library is implemented on top of .NET Standard 2.1 and supports .NET Core 3.0 and above separated on several Visual Studio projects that can be installed as a NuGet package. The library implements DataFrame as the core component with extensions of a set of data science and linear algebra features. The library contains several implementation of time series decomposition (SSA, STL ARIMA), optimization methods (SGD) as well as plotting support. The library also implements set of features based on matrix, vectors and similar linear algebra operations. The main part of the library is the Daany.DataFrame with similar implementation that can be found in python based Pandas library. The paper presents the main functionalities and the implementation behind the Daany packages in the form of developer guide and can be used as manual in using the Daany in every-day work. To end this the paper shows the list of papers used the library. 0 into PostgreSQL...\n",
      "Inserting test sample 1990  In recent years, the field of data analytics has been of great importance in multiple industries, providing insights into various aspects of business operations and decision making. The emergence of .NET technology has further facilitated the process of data analytics. This paper presents the Daany system, which is a framework for data processing and analysis on the .NET platform. Daany combines multiple features, including easy-to-use APIs, efficient parallel processing capabilities, and optimized memory management, to provide a highly customizable and scalable solution for data analytics. The framework offers flexible and intuitive data representation, enabling users to manipulate data in a variety of formats. Moreover, Daany proposes a set of core statistical and machine learning algorithms for advanced analytics tasks such as clustering, classification, and regression. The efficiency and versatility of Daany make it a powerful tool for a broad range of data analytics applications, from small-scale data analysis to large-scale data processing on distributed systems. We demonstrate the functionality of Daany by applying it to several real-world datasets, highlighting its potential and effectiveness in modern data analytics. 1 into PostgreSQL...\n",
      "Inserting test sample 1991  The Perseus arm has a gap in Galactic longitudes (l) between 50 and 80 deg (hereafter the Perseus arm gap) where the arm has little star formation activity. To better understand the gap, we conducted astrometric observations with VERA and analyzed archival H I data. We report on parallax and proper motion results from four star-forming regions, of which G050.28-00.39 and G070.33+01.59 are likely associated with the gap. The measured parallaxes are 0.140+/-0.018 (mas), 0.726+/-0.038 (mas), 0.074+/-0.037 (mas), and 0.118+/-0.035 (mas) for G050.28-00.39, G053.14+00.07, G070.33+01.59, and G079.08+01.33, respectively. Since the fractional parallax error of G070.33+01.59 is large (0.5), we estimated a 3D kinematic distance of the source to be 7.7+/-1.0 kpc using both the LSR velocity (VLSR) and the measured proper motion. Perseus-arm sources G049.41+00.32 and G050.28-00.39 lag relative to a Galactic rotation by 77+/-17 km/s and 31+/-10 km/s, respectively. The noncircular motion of G049.41+00.32 cannot be explained by the gravitational potential of the Perseus arm. We discovered rectangular holes with integrated brightness temperatures of < 30 K arcdeg in l vs. VLSR of the H I data. One of the holes is centered near (l, VLSR) = (47 deg, -15 km/s), and G049.41+00.32 is associated with the rim of the hole. However, G050.28-00.39 is not associated with the hole. We found extended H I emission on one side of the Galactic plane when integrating the H I data over the velocity range covering the hole (i.e., VLSR = [-25, -5] km/s). G049.41+00.32 and G050.28-00.39 are moving toward the emission. The Galactic H I disk at the same velocity range showed an arc structure, indicating that the disk was pushed from the lower side of the disk.\n",
      "\n",
      "All the observational results might be explained by a cloud collision with the Galactic disk. 0 into PostgreSQL...\n",
      "Inserting test sample 1992  This study focuses on the astrometry of VERA towards the Perseus arm gap region of the galaxy. The purpose is to investigate the dynamics of the Milky Way and the Perseus arm. The research involves the analysis of high-precision astrometry data obtained from VERA. The proper motions of maser sources were measured using a parallax-based method and the Galactic rotation curve was constructed based on the derived velocities. By comparing the results with existing models of the Milky Way, we aim to improve our understanding of the Galactic structure and the Perseus arm gap.\n",
      "\n",
      "The data used in this study was collected over a period of 13 years (2004-2017) using a dedicated interferometric array comprised of four 20-m radio telescopes. These measurements allowed the authors to construct a dense and accurate map of the Perseus arm with a resolution of 0.1 milliarcseconds.\n",
      "\n",
      "From the results obtained, the study confirms the existence of a gap in the Perseus arm. The gap, which spans a region of about 8 kpc in Galactic longitude, is found to be caused by the strong spiral shock in the region. By analyzing the shape and location of the gap, the authors conclude that it is most likely a part of the Perseus spiral arm that has been separated from the main section due to the shock.\n",
      "\n",
      "Furthermore, the derived Galactic rotation curve shows a gradual increase in rotational velocity towards the Perseus arm. The results are in good agreement with previous studies and support the idea that the Milky Way consists of four spiral arms with the Perseus arm being one of them.\n",
      "\n",
      "In summary, this study presents a detailed analysis of the astrometry data obtained from VERA towards the Perseus arm gap. The results provide new insights into the dynamics of the Milky Way and the Perseus arm. The research endorses the notion that the formation and evolution of the Milky Way is a complex and ongoing process that requires further investigation. 1 into PostgreSQL...\n",
      "Inserting test sample 1993  The three dimensional (3D) Dirac semimetal, which has been predicted theoretically, is a new electronic state of matter. It can be viewed as 3D generalization of graphene, with a unique electronic structure in which conduction and valence band energies touch each other only at isolated points in momentum space (i.e. the 3D Dirac points), and thus it cannot be classified either as a metal or a semiconductor. In contrast to graphene, the Dirac points of such a semimetal are not gapped by the spin-orbit interaction and the crossing of the linear dispersions is protected by crystal symmetry. In combination with broken time-reversal or inversion symmetries, 3D Dirac points may result in a variety of topologically non-trivial phases with unique physical properties. They have, however, escaped detection in real solids so far. Here we report the direct observation of such an exotic electronic structure in cadmium arsenide (Cd3As2) by means of angle-resolved photoemission spectroscopy (ARPES). We identify two momentum regions where electronic states that strongly disperse in all directions form narrow cone-like structures, and thus prove the existence of the long sought 3D Dirac points. This electronic structure naturally explains why Cd3As2 has one of the highest known bulk electron mobilities. This realization of a 3D Dirac semimetal in Cd3As2 not only opens a direct path to a wide spectrum of applications, but also offers a robust platform for engineering topologically-nontrivial phases including Weyl semimetals and Quantum Spin Hall systems. 0 into PostgreSQL...\n",
      "Inserting test sample 1994  The realization of three-dimensional Dirac semimetals represents a key breakthrough in the quest for new materials that possess exotic electronic properties. In this work, we report on the experimental realization of a three-dimensional Dirac semimetal using a novel crystal growth technique and measurements of transport and optical properties. Our results demonstrate that the material exhibits a highly unusual electronic dispersion that is described by a three-dimensional version of the relativistic Dirac equation. The use of a high-resolution angle-resolved photoemission spectroscopy revealed the existence of a pair of three-dimensional Dirac cones in the bulk electronic band structure. These cones are protected by crystal symmetry, making the material immune to disorder-induced backscattering, and thereby providing a robust platform for further study of intrinsic transport properties in this exciting new phase of matter.\n",
      "\n",
      "Our findings offer a new avenue for the exploration of Fermi arcs, Weyl fermions, and other topological electronic states in three dimensions. Furthermore, the unique optical properties we discovered in the material present exciting possibilities for the development of new optoelectronic devices. Our results also have significant implications for the broader fields of condensed matter physics, materials science, and nanotechnology. Overall, our work represents a significant step forward in the experimental realization of three-dimensional Dirac semimetals and opens the door to further investigation of their electronic and optical properties. 1 into PostgreSQL...\n",
      "Inserting test sample 1995  The star formation history (SFH) and the properties of the dwarf galaxy LGS3 are analyzed using color-magnitude (CM) diagrams plotted from VRI photometry of 736 stars. The distance to the galaxy is estimated through the position of the tip or the red giant branch. Two acceptable results have been obtained: 0.77+/-0.07 Mpc and 0.96+/-0.07 Mpc, although the first value is favored by complementary considerations on the stellar content of the galaxy. Both values make LGS3 a possible satellite of M31 or of M33. The SFH is investigated for each of the two adopted distances comparing the observed CM diagrams with model CM diagrams computed for different star formation rates (psi(t)) and chemical enrichment laws (Z(t)).\n",
      "\n",
      "The results are compatible with LGS3 having been forming stars since an early epoch, 15-12 Gyr ago, at an almost constant rate if distance is 0.77 Mpc or at an exponentially decreasing rate if distance is 0.96 Mpc. According to our models, the current metallicity would range from Z~0.0007 to Z~0.002. Other results are the current psi(t): (0.55+/-0.04)x10^(-10) Mo yr^(-1) pc^(-2) or (0.47+/-0.07)x10^(-10) Mo yr^(-1) pc^(-2), depending of the distance, and its average for the entire life of the galaxy, <psi>=(1.4+/-0.1)x10^(-10) Mo yr^(-1) pc^(-2). At the present psi(t), the probability of LGS3 having an HII region is 0.2, which is compatible with the fact that no HII regions have been found in the galaxy. Its fraction of gas relative to the mass intervening in the chemical evolution is about 0.40 and its percentage of dark matter (that which cannot be explained as stellar remnants or by extrapolation of the used IMF to low masses) is 95%. The results for psi(t) and Z(t) for d=0.77 Mpc are compatible with a moderate outflow of well mixed material (lambda=3), but large 0 into PostgreSQL...\n",
      "Inserting test sample 1996  The Local Group dwarf galaxy LGS 3 is a fascinating target for research as it provides a unique opportunity to study the properties and evolution of low-mass galaxies. In this study, we present a detailed analysis of the stellar content and star formation history of LGS 3 using deep photometric data from the Hubble Space Telescope. \n",
      "\n",
      "We find that LGS 3 has a complex stellar population that consists of both old and young stars. The old stars are distributed in a diffuse halo-like structure around the galaxy, while the young stars form compact clusters that are distributed along the galaxy's main axis. The presence of these young star clusters suggests that LGS 3 has experienced recent bursts of star formation, possibly triggered by interactions with nearby galaxies or gas accretion events.\n",
      "\n",
      "Using the stellar population synthesis technique, we derive the star formation history of LGS 3 over the past 12 billion years. We find that the galaxy has been steadily forming stars at a low rate over this period, and that the recent bursts of star formation have contributed significantly to its stellar mass. Moreover, we find evidence for a recent decline in the star formation rate of LGS 3, indicating that the galaxy may be transitioning from a star-forming to a quiescent phase.\n",
      "\n",
      "We also compare the stellar content and star formation history of LGS 3 with those of other Local Group dwarf galaxies. We find that LGS 3 is distinct from other dwarf galaxies in having a relatively high fraction of young stars, indicating that it has been more active in forming stars over the past few hundred million years. Moreover, we find evidence for a metallicity gradient in LGS 3, which may reflect its formation history and potential interactions with nearby galaxies.\n",
      "\n",
      "Our analysis of the stellar content and star formation history of LGS 3 provides important insights into the formation and evolution of low-mass galaxies in the Local Group. Further studies of LGS 3 and other dwarf galaxies will help to improve our understanding of the processes that govern the formation and evolution of galaxies in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 1997  Magnetic transition metal dichalcogenide (TMD) films have recently emerged as promising candidates to host novel magnetic phases relevant to next-generation spintronic devices. However, systematic control of the magnetization orientation, or anisotropy, and its thermal stability, characterized by Curie temperature (Tc) remains to be achieved in such films. Here we present self-intercalated epitaxial Cr1+{\\delta}Te2 films as a platform for achieving systematic/smooth magnetic tailoring in TMD films. Using a molecular beam epitaxy (MBE) based technique, we have realized epitaxial Cr1+{\\delta}Te2 films with smoothly tunable over a wide range (0.33-0.82), while maintaining NiAs-type crystal structure. With increasing {\\delta}, we found monotonic enhancement of Tc from 160 to 350 K, and the rotation of magnetic anisotropy from out-of-plane to in-plane easy axis configuration for fixed film thickness.\n",
      "\n",
      "Contributions from conventional dipolar and orbital moment terms are insufficient to explain the observed evolution of magnetic behavior with {\\delta}. Instead, ab initio calculations suggest that the emergence of antiferromagnetic interactions with {\\delta}, and its interplay with conventional ferromagnetism, may play a key role in the observed trends. To our knowledge, this constitutes the first demonstration of tunable Tc and magnetic anisotropy across room temperature in TMD films, and paves the way for engineering novel magnetic phases for spintronic applications. 0 into PostgreSQL...\n",
      "Inserting test sample 1998  This research investigates the possibility of tailoring the magnetic properties of Cr1+{\\delta}Te2 epitaxial films through self-intercalation. The films were grown using molecular beam epitaxy, and their structures were characterized through X-ray diffraction and high-resolution scanning transmission electron microscopy. The study reveals that self-intercalation can modify the thickness of the Te layers in the films, leading to changes in their structural and magnetic properties. It was found that the magnetic properties of the films are influenced by Cr vacancies that are formed during the self-intercalation process. The results show that self-intercalation can tune the magnetic moment per Cr atom, as well as the magnetic anisotropy energy, by controlling the amount of Cr vacancies in the films. The research provides a useful insight into the physical mechanisms of magnetism in Cr1+{\\delta}Te2 films, which can inform the design of new materials with tailored magnetic properties. Overall, this study demonstrates a promising approach to engineering magnetic materials through self-intercalation. 1 into PostgreSQL...\n",
      "Inserting test sample 1999  We prove L^p estimates for the Walsh model of the \"biest\", a trilinear multiplier with singular symbol which arises naturally in the expansion of eigenfunctions of a Schrodinger operator, and which is also related to the bilinear Hilbert transform. The corresponding estimates for the Fourier model will be obtained in the sequel of this paper. 0 into PostgreSQL...\n",
      "Inserting test sample 2000  In this paper, we prove $L^p$ estimates for the biest I in the Walsh case. We establish the boundedness of certain oscillatory singular integral operators associated with Walsh functions in the endpoint $L^1$ setting. Our approach is based on a modification of the tent space methodology introduced by Hyt\\\"{o}nen. Applications to the bi-parameter paraproduct decomposition are included. 1 into PostgreSQL...\n",
      "Inserting test sample 2001  We compute several coefficients needed for O(a) improvement of currents in perturbation theory, using the Brodsky-Lepage-Mackenzie prescription for choosing an optimal scale q*. We then compare the results to non-perturbative calculations. Normalization factors of the vector and axial vector currents show good agreement, especially when allowing for small two-loop effects. On the other hand, there are large discrepancies in the coefficients of O(a) improvement terms. We suspect that they arise primarily from power corrections inherent in the non-perturbative methods. 0 into PostgreSQL...\n",
      "Inserting test sample 2002  In order to improve the accuracy of lattice QCD simulations, a perturbative calculation of O(a) improvement coefficients is necessary. This allows for the reduction of lattice artifacts, which can lead to more precise calculations of physical quantities. In this study, we present the results of such a calculation, using a non-perturbative method on quark bilinears. Our approach provides an improvement upon previous calculations, as it includes contributions from all orders of the lattice spacing and features a complete matching with the continuum limit. 1 into PostgreSQL...\n",
      "Inserting test sample 2003  We present a detailed spectroscopic and photometric analysis of DA and DB white dwarfs drawn from the Sloan Digital Sky Survey with trigonometric parallax measurements available from the Gaia mission. The temperature and mass scales obtained from fits to $ugriz$ photometry appear reasonable for both DA and DB stars, with almost identical mean masses of $\\langle M \\rangle = 0.617~M_\\odot$ and $0.620~M_\\odot$, respectively. The comparison with similar results obtained from spectroscopy reveals several problems with our model spectra for both pure hydrogen and pure helium compositions. In particular, we find that the spectroscopic temperatures of DA stars exceed the photometric values by $\\sim$10% above $T_{\\rm eff}\\sim14,000$~K, while for DB white dwarfs, we observe large differences between photometric and spectroscopic masses below $T_{\\rm eff}\\sim16,000$~K. We attribute these discrepancies to the inaccurate treatment of Stark and van der Waals broadening in our model spectra, respectively. Despite these problems, the mean masses derived from spectroscopy --- $\\langle M \\rangle = 0.615~M_\\odot$ and $0.625~M_\\odot$ for the DA and DB stars, respectively --- agree extremely well with those obtained from photometry. Our analysis also reveals the presence of several unresolved double degenerate binaries, including DA+DA, DB+DB, DA+DB, and even DA+DC systems. We finally take advantage of the Gaia parallaxes to test the theoretical mass-radius relation for white dwarfs. We find that 65% of the white dwarfs are consistent within the 1$\\sigma$ confidence level with the predictions of the mass-radius relation, thus providing strong support to the theory of stellar degeneracy. 0 into PostgreSQL...\n",
      "Inserting test sample 2004  The aim of this study is to perform a comprehensive spectroscopic and photometric analysis of DA and DB white dwarfs from the Sloan Digital Sky Survey (SDSS) and the Gaia mission. White dwarfs, the compact, hot remnants of low-mass stars, are important probes of stellar evolution and the structure of the Milky Way. The SDSS and Gaia surveys have provided large samples of these objects, allowing for detailed analyses of their physical properties. \n",
      "\n",
      "We analyzed a sample of 15,000 white dwarfs from SDSS DR14 and Gaia DR2 using a combination of spectroscopic and photometric techniques. We fitted the SDSS spectra of DA and DB white dwarfs with model atmospheres to determine their effective temperatures, surface gravities, and chemical compositions. These parameters were used to estimate the white dwarfs' masses and radii. We also measured the white dwarfs' photometric colors and used them to obtain independent estimates of their temperatures and gravities.\n",
      "\n",
      "Our analysis revealed a number of interesting results. We found that the DA and DB white dwarfs differ significantly in their chemical compositions, with DB stars showing a higher incidence of helium and metal pollution. We also identified a subset of DA white dwarfs with unusually thick hydrogen layers, possibly indicating recent accretion. Our measurements of the white dwarfs' masses and radii allowed us to test theoretical models of white dwarf evolution and constrain the underlying physics of the degenerate matter equation of state.\n",
      "\n",
      "Our findings have important implications for our understanding of both stellar evolution and the structure of the Milky Way. The combination of SDSS and Gaia data has enabled us to perform a detailed and comprehensive study of these fascinating objects, and we expect that future surveys will yield even greater insights into their nature and origin. 1 into PostgreSQL...\n",
      "Inserting test sample 2005  We investigate dynamics of large scale and slow deformations of layered structures. Starting from the respective model equations for a non-conserved system, a conserved system and a binary fluid, we derive the interface equations which are a coupled set of equations for deformations of the boundaries of each domain. A further reduction of the degrees of freedom is possible for a non-conserved system such that internal motion of each domain is adiabatically eliminated. The resulting equation of motion contains only the displacement of the center of gravity of domains, which is equivalent to the phase variable of a periodic structure. Thus our formulation automatically includes the phase dynamics of layered structures. In a conserved system and a binary fluid, however, the internal motion of domains turns out to be a slow variable in the long wavelength limit because of concentration conservation.\n",
      "\n",
      "Therefore a reduced description only involving the phase variable is not generally justified. 0 into PostgreSQL...\n",
      "Inserting test sample 2006  This paper presents a study on the interface dynamics of layered structures. Such structures are present in various physical systems, such as geological formations, biological tissues, thin films in electronics, and numerous engineering applications. In many cases, the properties of the layered structures depend crucially on the dynamics of their interfaces. In this work, we investigate several aspects of interface dynamics, including the effects of external forces, the role of fluctuations, and the behavior of defects and dislocations. We use a combination of theoretical and computational techniques, including molecular dynamics simulations and continuum modeling, to explore the behavior of the interface under various conditions. Our results provide insights into the fundamental mechanisms that govern the behavior of layered structures, and have implications for a wide range of practical applications. 1 into PostgreSQL...\n",
      "Inserting test sample 2007  We examine the wave-functionals describing the collapse of a self-gravitating dust ball in an exact quantization of the gravity-dust system. We show that ingoing (collapsing) dust shell modes outside the apparent horizon must necessarily be accompanied by outgoing modes inside the apparent horizon, whose amplitude is suppressed by the square root of the Boltzmann factor at the Hawking temperature. Likewise, ingoing modes in the interior must be accompanied by outgoing modes in the exterior, again with an amplitude suppressed by the same factor. A suitable superposition of the two solutions is necessary to conserve the dust probability flux across the apparent horizon, thus each region contains both ingoing and outgoing dust modes. If one restricts oneself to considering only the modes outside the apparent horizon then one should think of the apparent horizon as a partial reflector, the probability for a shell to reflect being given by the Boltzmann factor at the Hawking temperature determined by the mass contained within it. However, if one considers the entire wave function, the outgoing wave in the exterior is seen to be the transmission through the horizon of the interior outgoing wave that accompanies the collapsing shells. This transmission could allow information from the interior to be transferred to the exterior. 0 into PostgreSQL...\n",
      "Inserting test sample 2008  The collapse of a massive object due to gravitational forces has been the focus of many investigations aimed at providing a description of the physics involved. In particular, the so-called \"apparent horizon\" - a theoretical boundary that marks the point of no return - has been an area of intense interest with research suggesting that it plays a key role in how gravitational collapse occurs. In this study, we examine the behavior of particles at the apparent horizon during the collapse process. We show that under certain conditions, the apparent horizon can act as a boundary through which particles are reflected back to the outside environment from which they came. Furthermore, we find that some particles can penetrate the apparent horizon and continue towards the singularity, providing an opportunity to study the behavior of particles under extreme gravitational conditions. Our results have important implications for the understanding of black hole formation in the universe and could ultimately lead to new insights into the nature of gravity itself. 1 into PostgreSQL...\n",
      "Inserting test sample 2009  Many experimental and field studies have shown that adaptation can occur very rapidly. Two qualitatively different modes of fast adaptation have been proposed: selective sweeps wherein large shifts in the allele frequencies occur at a few loci and evolution via small changes in the allele frequencies at many loci. While the first process has been thoroughly investigated within the framework of population genetics, the latter is based on quantitative genetics and is much less understood. Here we summarize results from our recent theoretical studies of a quantitative genetic model of polygenic adaptation that makes explicit reference to population genetics to bridge the gap between the two frameworks. Our key results are that polygenic adaptation may be a rapid process and can proceed via subtle or dramatic changes in the allele frequency depending on the sizes of the phenotypic effects relative to a threshold value. We also discuss how the signals of polygenic selection may be detected in the genome. While powerful methods are available to identify signatures of selective sweeps at loci controling quantitative traits, the development of statistical tests for detecting small shifts of allele frequencies at quantitative trait loci is still in its infancy. 0 into PostgreSQL...\n",
      "Inserting test sample 2010  Polygenic adaptation refers to the process by which complex traits are modified through natural selection acting on multiple genetic variants of small effect. Understanding the modes of rapid polygenic adaptation is essential for predicting the evolution of complex traits, such as susceptibility to diseases. Multiple modes of rapid polygenic adaptation have been proposed, including shifts in allele frequencies, changes in the frequency of haplotypes, and changes in the frequency of polygenic scores. However, the relative importance of these modes of adaptation remains poorly understood. In this study, we explore the different modes of rapid polygenic adaptation using both analytical and simulation approaches. Our results suggest that the relative importance of each mode depends on the underlying genetic architecture of the polygenic trait. In particular, we find that the importance of allele frequency changes is highest when the trait is influenced by a small number of loci of large effect, whereas changes in haplotype and polygenic score frequencies become more important when the trait is influenced by many loci of small effect. Our findings have important implications for predicting and managing the evolution of complex traits. 1 into PostgreSQL...\n",
      "Inserting test sample 2011  We study the 2p-core level x-ray photoemission spectra in ferromagnetic transition metals, Fe, Co, and Ni using a recently developed ab initio method.The excited final states are set up by distributing electrons on the one-electron states calculated under the fully screened potential in the presence of the core hole. We evaluate the overlap between these excited states and the ground state by using one-electron wave functions, and obtain the spectral curves as a function of binding energy. The calculated spectra reproduce well the observed spectra displaying interesting dependence on the element and on the spin of the removed core electron. The origin of the spectral shapes is elucidated in terms of the one-electron states screening the core hole. The magnetic splitting of the threshold energy is also estimated by using the coherent potential approximation within the fully screened potential approximation. It decreases more rapidly than the local spin moment with moving from Fe to Ni. It is estimated to be almost zero for Ni despite the definite local moment about 0.6\\mu_B, in agreement with the experiment. 0 into PostgreSQL...\n",
      "Inserting test sample 2012  This study employs an ab initio approach to investigate the 2p-core level x-ray photoemission spectra (XPS) in ferromagnetic transition metals. Utilizing density functional theory (DFT), we perform a systematic analysis of the electronic structure and its influence on the XPS spectra for Fe, Co, and Ni. We examine the contribution of various electron states to the XPS intensity and the effect of spin polarization on the spectra. Our results show distinctive features in the XPS spectra for each of the considered elements, providing insight into the fundamental mechanisms and complexities of the XPS process in ferromagnetic systems. Additionally, we investigate the role of crystal symmetry on the XPS spectra and compare our theoretical results with experimental data. This study may have implications for the improved understanding and interpretation of XPS spectra in ferromagnetic materials and could contribute to the development of more accurate theoretical models for XPS analysis. 1 into PostgreSQL...\n",
      "Inserting test sample 2013  Visual Human Activity Recognition (HAR) and data fusion with other sensors can help us at tracking the behavior and activity of underground miners with little obstruction. Existing models, such as Single Shot Detector (SSD), trained on the Common Objects in Context (COCO) dataset is used in this paper to detect the current state of a miner, such as an injured miner vs a non-injured miner. Tensorflow is used for the abstraction layer of implementing machine learning algorithms, and although it uses Python to deal with nodes and tensors, the actual algorithms run on C++ libraries, providing a good balance between performance and speed of development. The paper further discusses evaluation methods for determining the accuracy of the machine-learning and an approach to increase the accuracy of the detected activity/state of people in a mining environment, by means of data fusion. 0 into PostgreSQL...\n",
      "Inserting test sample 2014  This research paper proposes a novel approach to recognizing human activities using visual object detection. The proposed method combines the power of convolutional neural networks for object detection with traditional machine learning algorithms for activity recognition. We evaluate the proposed approach on popular public datasets and achieve state-of-the-art performance for both object detection and activity recognition. Moreover, we show that our approach is robust to variations in object size, view angle, and occlusion. Our results suggest that visual object detection can be used as a powerful tool to recognize human activities with high accuracy. This research is particularly relevant for applications such as surveillance, assisted living, and human-robot interaction. Our approach has the potential to be expanded for a variety of activity recognition tasks and to be integrated into real-world systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2015  In [22] Milnor proved that a real analytic map $f\\colon (R^n,0) \\to (R^p,0)$, where $n \\geq p$, with an isolated critical point at the origin has a fibration on the tube $f|\\colon B_\\epsilon^n \\cap f^{-1}(S_\\delta^{p-1}) \\to S_\\delta^{p-1}$. Constructing a vector field such that, (1) it is transverse to the spheres, and (2) it is transverse to the tubes, he \"inflates\" the tube to the sphere, to get a fibration $\\varphi\\colon S_\\epsilon^{n-1} \\setminus f^{-1}(0) \\to S^{p-1}$, but the projection is not necessarily given by $f/ \\|f\\|$ as in the complex case.\n",
      "\n",
      "In the case $f$ has isolated critical value, in [9] it was proved that if the fibres inside a small tube are transverse to the sphere $S_\\epsilon$, then it has a fibration on the tube. Also in [9], the concept of $d$-regularity was defined, it turns out that $f$ is $d$-regular if and only if the map $f/|f\\|\\colon S_\\epsilon^{n-1} \\setminus f^{-1}(0) \\to S^{p-1}$ is a fibre bundle equivalent to the one on the tube.\n",
      "\n",
      "In this article, we prove the corresponding facts in a more general setting: if a locally surjective map $f$ has a linear discriminant $\\Delta$ and a fibration on the tube $f|\\colon B_\\epsilon^n \\cap f^{-1}(S_\\delta^{p-1} \\setminus \\Delta) \\to S_\\delta^{p-1} \\setminus \\Delta$, then $f$ is $d$-regular if and only if the map $f/ \\|f\\|\\colon S_\\epsilon^{n-1} \\setminus f^{-1}(\\Delta) \\to S^{p-1} \\setminus \\mathcal{A}$ (with $\\mathcal{A}$ the radial projection of $\\Delta$ on $S^{p-1}$) is a fibre bundle equivalent to the one on the tube. We do this by constructing a vector field $\\tilde{w}$ which inflates the tube to the sphere in a controlled way, it satisfies properties analogous to the vector field constructed by Milnor in the complex setting: besides satisfying (1) and (2) above, it also satisfies that $f/ \\|f\\|$ is constant on the integral curves of $\\tilde{w}$. 0 into PostgreSQL...\n",
      "Inserting test sample 2016  In this paper, we investigate the equivalence of Milnor and Milnor-L\\^e fibrations for real analytic maps. Milnor fibrations and their higher dimensional counterparts, Milnor-L\\^e fibrations, play a crucial role in the study of singularities and topology of real analytic maps. Specifically, they provide a combinatorial tool for understanding the nature of singularities in the fibers of a map.\n",
      "\n",
      "Our main result establishes a complete equivalence between the two types of fibrations for a broad class of real analytic maps. We show that any Milnor fibration can be transformed into a Milnor-L\\^e fibration via a homotopy that preserves the underlying topology. Conversely, any Milnor-L\\^e fibration can be transformed into a Milnor fibration by adding and removing certain critical points in a controlled way.\n",
      "\n",
      "Our proof relies on a combination of techniques from algebraic geometry, topology, and singularity theory. In particular, we use a version of desingularization theory to construct a smooth manifold from a given real analytic map. We then use the resulting manifold to construct a homotopy that transforms the Milnor fibration into a Milnor-L\\^e fibration.\n",
      "\n",
      "The implications of our result are far-reaching. It allows researchers to study singularities of real analytic maps using the combinatorial language of Milnor-L\\^e fibrations, which are often easier to work with than their Milnor counterparts. Moreover, it provides a new perspective on the relationship between singularity theory and algebraic geometry.\n",
      "\n",
      "Overall, our work provides a deeper understanding of the fundamental properties of Milnor and Milnor-L\\^e fibrations for real analytic maps. It opens up new avenues for research in singularity theory, topology, and algebraic geometry, and may have important applications in areas such as robotics, computer graphics, and computational biology. 1 into PostgreSQL...\n",
      "Inserting test sample 2017  Due to ionosphere absorption and the interference by natural and artificial radio emissions, astronomical observation from the ground becomes very difficult at the wavelengths of decametre or longer, which we shall refer as the ultralong wavelengths. This unexplored part of electromagnetic spectrum has the potential of great discoveries, notably in the study of cosmic dark ages and dawn, but also in heliophysics and space weather, planets and exoplanets, cosmic ray and neutrinos, pulsar and interstellar medium (ISM), extragalactic radio sources, and so on. The difficulty of the ionosphere can be overcome by space observation, and the Moon can shield the radio frequency interferences (RFIs) from the Earth. A lunar orbit array can be a practical first step of opening up the ultralong wave band. Compared with a lunar surface observatory on the far side, the lunar orbit array is simpler and more economical, as it does not need to make the risky and expensive landing, can be easily powered with solar energy, and the data can be transmitted back to the Earth when it is on the near-side part of the orbit. Here I describe the Discovering Sky at the Longest wavelength (DSL) project, which will consist of a mother satellite and 6~9 daughter satellites, flying on the same circular orbit around the Moon, and forming a linear interferometer array. The data are collected by the mother satellite which computes the interferometric cross-correlations (visibilities) and transmits the data back to the Earth. The whole array can be deployed on the lunar orbit with a single rocket launch. The project is under intensive study in China. 0 into PostgreSQL...\n",
      "Inserting test sample 2018  This paper presents a novel approach for exploring the sky at longer wavelengths using a lunar orbit array. The Moon's inherent stability offers an ideal platform for telescopes and receivers operating at frequencies ranging from several hundred megahertz to tens of gigahertz.\n",
      "\n",
      "Unlike previous attempts to study longer wavelengths, this approach is based on a distributed array of receivers rather than a monolithic telescope. This approach enables us to study an unprecedentedly large area of the sky with high sensitivity and angular resolution. The lunar orbit array consists of many small, individual elements that are spaced apart and working in concert.\n",
      "\n",
      "The scientific potential for this approach is vast, including the ability to observe the early universe and the formation of the first stars and galaxies. Additionally, the lunar orbit array will enable us to study the properties of cosmic magnetic fields and the behavior of the Sun's corona. Investigations into the chemical and physical properties of the interstellar medium and the search for extraterrestrial life are also possible with this unique platform.\n",
      "\n",
      "The challenges associated with building and deploying a lunar orbit array are significant, but advancements in technology and processing power make this approach feasible. We discuss the design considerations and the key technologies necessary for this approach to be successful. We also highlight the benefits of studying longer wavelengths and present several scientific investigations that can be carried out with the lunar orbit array.\n",
      "\n",
      "Overall, the proposed lunar orbit array represents a transformative approach to exploring the sky at longer wavelengths and holds great promise for groundbreaking scientific discoveries. 1 into PostgreSQL...\n",
      "Inserting test sample 2019  It is well known that multigrid methods are optimally efficient for solution of elliptic equations (O(N)), which means that effort is proportional to the number of points at which the solution is evaluated). Thus this is an ideal method to solve the initial data/constraint equations in General Relativity for (for instance) black hole interactions, or for other strong-field gravitational configurations. Recent efforts have produced finite difference multigrid solvers for domains with holes (excised regions). We present here the extension of these concepts to higher order (fourth-, sixth- and eigth-order). The high order convergence allows rapid solution on relatively small computational grids. Also, general relativity evolution codes are moving to typically fourth-order; data have to be computed at least as accurately as this same order for straightfoward demonstration of the proper order of convergence in the evolution.\n",
      "\n",
      "Our vertex-centered multigrid code demonstrates globally high-order-accurate solutions of elliptic equations over domains containing holes, in two spatial dimensions with fixed (Dirichlet) outer boundary conditions, and in three spatial dimensions with {\\it Robin} outer boundary conditions. We demonstrate a ``real world'' 3-dimensional problem which is the solution of the conformally flat Hamiltonian constraint of General Relativity. The success of this method depends on: a) the choice of the discretization near the holes; b) the definition of the location of the inner boundary, which allows resolution of the hole even on the coarsest grids; and on maintaining the same order of convergence at the boundaries as in the interior of the computational domain. 0 into PostgreSQL...\n",
      "Inserting test sample 2020  The numerical solution of elliptic partial differential equations on domains containing holes presents significant challenges, particularly in the context of black hole initial data. Recently developed multigrid methods tackle these challenges by employing high-order discretizations that preserve convergence rates in the presence of singularities. This study builds upon such methods to develop a high-order convergent multigrid technique tailored specifically for black hole initial data. The proposed method is shown to maintain the theoretical convergence rates even on domains containing holes, which represent the most challenging cases. Numerical experiments demonstrate that our method achieves rapid computational convergence on a variety of test problems, including for domains with multiple holes. Moreover, this study explores the performance of several different transfer operators and concludes that the use of â€œfull-weightingâ€ transfer operators leads to faster convergence. The implications of these findings are promising, suggesting that the developed technique could enable significant advancements toward realistic and accurate simulations of black hole initial data. This study additionally touches on possible directions for future research, including the extension of the method to incorporate Adaptive Mesh Refinement techniques and the design of accurate numerical truncation error estimators. Ultimately, this work showcases the potential of high-order convergent multigrid methods for solving fundamentally-challenging partial differential equations on complex domains. 1 into PostgreSQL...\n",
      "Inserting test sample 2021  We discuss the Fermi-liquid properties of hadronic matter derived from a chiral Lagrangian field theory in which Brown-Rho (BR) scaling is incorporated.\n",
      "\n",
      "We identify the BR scaling as a contribution to Landau's Fermi liquid fixed-point quasiparticle parameter from \"heavy\" isoscalar meson degrees of freedom that are integrated out from a low-energy effective Lagrangian. We show that for the vector (convection) current, the result obtained in the chiral Lagrangian approach agrees precisely with that obtained in the semi-phenomenological Landau-Migdal approach. This precise agreement allows one to determine the Landau parameter that enters in the effective nucleon mass in terms of the constant that characterizes BR scaling. When applied to the weak axial current, however, these two approaches differ in a subtle way. While the difference is small numerically, the chiral Lagrangian approach implements current algebra and low-energy theorems associated with the axial response that the Landau method misses and hence is expected to be more predictive. 0 into PostgreSQL...\n",
      "Inserting test sample 2022  This paper explores the relationship between chiral Lagrangians and Landau Fermi liquid theory in the context of dense hadronic matter. Using a scaling approach, we demonstrate that the chiral symmetry breaking scale is proportional to the Fermi wave vector in this regime. We also analyze the interplay between these two frameworks and the role of the pion decay constant as an indicator of the crossover between them. Furthermore, we investigate the impact of isospin asymmetry and the strange quark mass on the chiral phase transition in this system. Our findings suggest that the combination of chiral Lagrangians and Landau Fermi liquid theory provides a comprehensive and consistent description of the thermodynamics and transport properties of dense hadronic matter. This work has important implications for the study of compact stars and heavy ion collisions, where the understanding of the equation of state and the QCD phase diagram is crucial. 1 into PostgreSQL...\n",
      "Inserting test sample 2023  The latest ATLAS results for processes with a top quark pair and an associated vector boson are presented here. The measurement of the production cross sections for these processes is important for the direct determination of the top quark couplings to gauge bosons and for constraints on new physics models, in particular for models which go beyond the Standard Model regarding the mechanism for the mass generation. 0 into PostgreSQL...\n",
      "Inserting test sample 2024  The production of top quark-antiquark pairs in association with a vector boson, $t\\bar{t}+V$, is a rare process that occurs at high energies. This paper presents the latest ATLAS measurements of $t\\bar{t}+V$ production using the LHC data collected for proton-proton collisions at a center-of-mass energy of 13 TeV. The results are in agreement with the Standard Model predictions and provide important input for the exploration of the electroweak sector of particle physics. 1 into PostgreSQL...\n",
      "Inserting test sample 2025  We show stability of spherical caps (SCs) lying on a flat surface, where the motion is governed by the volume-preserving Mean Curvature Flow (MCF).\n",
      "\n",
      "Moreover, we introduce a dynamic boundary condition that models a line tension effect on the boundary. The proof is based on the generalized principle of linearized stability 0 into PostgreSQL...\n",
      "Inserting test sample 2026  This paper studies the stability of spherical caps under the volume-preserving mean curvature flow with line tension. The authors propose a method for proving the stability of a cap by considering the linearized stability of its profile. The results show that the cap is stable if the line tension is small, concave, and with finite height. 1 into PostgreSQL...\n",
      "Inserting test sample 2027  Higher-order representations of objects such as programs, proofs, formulas and types have become important to many symbolic computation tasks. Systems that support such representations usually depend on the implementation of an intensional view of the terms of some variant of the typed lambda-calculus.\n",
      "\n",
      "Various notations have been proposed for lambda-terms to explicitly treat substitutions as basis for realizing such implementations. There are, however, several choices in the actual reduction strategies. The most common strategy utilizes such notations only implicitly via an incremental use of environments.\n",
      "\n",
      "This approach does not allow the smaller substitution steps to be intermingled with other operations of interest on lambda-terms. However, a naive strategy explicitly using such notations can also be costly: each use of the substitution propagation rules causes the creation of a new structure on the heap that is often discarded in the immediately following step. There is thus a tradeoff between these two approaches. This thesis describes the actual realization of the two approaches, discusses their tradeoffs based on this and, finally, offers an amalgamated approach that utilizes recursion in rewrite rule application but also suspends substitution operations where necessary. 0 into PostgreSQL...\n",
      "Inserting test sample 2028  Lambda Calculus (Î»-Calculus) is a highly expressive formalism to study computability and functional programming. However, evaluating Î»-terms can be problematic for large input sizes, especially in terms of heap usage. This paper presents a detailed exploration of the effects of various reduction strategies on the heap usage in Î» term normalization. Reduction strategies such as Call-by-Name (CBN), Call-by-Value (CBV), and Normal Order (NO) evaluation have been studied in depth. Additionally, two new strategies are introduced, namely, Applicative Order Leftmost Outermost (AOLLO) and Applicative Order Rightmost Outermost (AORRO). We have conducted empirical evaluations of these strategies on both microbenchmarks and a larger benchmark suite. Our experiments show that the reduction strategies significantly affect the heap usage of normalization. We show that the NO strategy has the highest memory usage while CBV and CBN use comparatively less memory. Furthermore, we demonstrate that the new strategies, AOLLO and AORRO, exhibit promising behavior in terms of heap usage and evaluation performance. Our findings encourage further exploration of Î» term reduction strategies for efficient normalization. 1 into PostgreSQL...\n",
      "Inserting test sample 2029  The fractal shape and multi-component nature of the interstellar medium together with its vast range of dynamical scales provides one of the great challenges in theoretical and numerical astrophysics. Here we will review recent progress in the direct modelling of interstellar hydromagnetic turbulence, focusing on the role of energy injection by supernova explosions.\n",
      "\n",
      "The implications for dynamo theory will be discussed in the context of the mean-field approach. Results obtained with the test field-method are confronted with analytical predictions and estimates from quasilinear theory. The simulation results enforce the classical understanding of a turbulent Galactic dynamo and, more importantly, yield new quantitative insights. The derived scaling relations enable confident global mean-field modelling. 0 into PostgreSQL...\n",
      "Inserting test sample 2030  The phenomenon of supernova-driven interstellar turbulence is a crucial factor in understanding the behavior of galactic magnetic fields. The immense energy released by supernovae creates shock waves that propagate and generate turbulence in the surrounding interstellar medium. This turbulence, in turn, plays a critical role in the creation of galactic magnetic fields through the so-called \"galactic dynamo\" mechanism which amplifies random seed fields into a coherent large-scale magnetic field. Despite decades of study, our understanding of both supernova-driven turbulence and the galactic dynamo mechanism remains far from complete. This paper presents recent advances in both observational and theoretical work aimed at shedding light on these fundamental processes and their role in shaping galactic structure. 1 into PostgreSQL...\n",
      "Inserting test sample 2031  To study implementations and optimisations of interaction net systems we propose a calculus to allow us to reason about nets, a concrete data-structure that is in close correspondence with the calculus, and a low-level language to create and manipulate this data structure. These work together so that we can describe the compilation process for interaction nets, reason about the behaviours of the implementation, and study the efficiency and properties. 0 into PostgreSQL...\n",
      "Inserting test sample 2032  We propose an implementation model for the reduction of interaction nets. We present a combination of strategies for controlling reduction sequences and a formalization of their properties in terms of an abstract machine. Our approach combines a set of rules with a method for dynamic memoization to reduce the overhead associated with computing the system configurations. We demonstrate the effectiveness of our method with several examples, indicating substantial reduction in computational complexity. 1 into PostgreSQL...\n",
      "Inserting test sample 2033  Nikulin has classified all finite abelian groups acting symplectically on a K3 surface and he has shown that the induced action on the K3 lattice $U^3\\oplus E_8(-1)^2$ depends only on the group but not on the K3 surface. For all the groups in the list of Nikulin we compute the invariant sublattice and its orthogonal complement by using some special elliptic K3 surfaces. 0 into PostgreSQL...\n",
      "Inserting test sample 2034  In this paper, we study elliptic fibrations and symplectic automorphisms on K3 surfaces. We first review the basic definitions and properties of K3 surfaces. Then, we discuss elliptic fibrations and show how they arise on K3 surfaces. Finally, we investigate the behavior of symplectic automorphisms on K3 surfaces and provide some examples. Our work sheds light on the geometry of K3 surfaces and their relationships with algebraic structures. 1 into PostgreSQL...\n",
      "Inserting test sample 2035  The purpose of this study was to evaluate the setup uncertainties for single-fraction stereotactic radiosurgery (SF-SRS) based on the clinical data with the two different mask-creation methods using pretreatment CBCT imaging guidance. Dedicated frameless fixation BrainLAB masks for 23 patients were created as a routine mask (R-mask) making method, as explained in the BrainLAB user manual. The alternative masks (A-mask) which were created by modifying the cover range of the R-mask for the patient head were used for 23 patients. The systematic errors including the each mask and stereotactic target localizer were analyzed and the errors were calculated as the mean and standard deviation (SD) from the LR, SI, AP, and yaw setup corrections. In addition, the frequency of three-dimensional (3D) vector length were also analyzed. The values of the mean setup corrections for the R-mask in all directions were small; < 0.7 mm and < 0.1 degree, whereas the magnitudes of the SDs were relatively large compared to the mean values. In contrast to the R-mask, the means and SDs of the A-mask were smaller than those for the R-mask with the exception of the SD in the AP direction. The mean and SD in the yaw rotational direction in the R-mask and A-mask system were comparable. The 3D vector shifts of a larger magnitude occurred more frequently for the R-mask than the A-mask. The setup uncertainties for each mask with the stereotactic localizing system had an asymmetric offset towards the positive AP direction. The A-mask-creation method, which is capable of covering the top of the patient head is superior to that for R-mask, and thereby the use of the A-mask is encouraged for SF-SRS to reduce the setup uncertainties. Moreover, the careful mask making is required to prevent the possible setup uncertainties. 0 into PostgreSQL...\n",
      "Inserting test sample 2036  This study investigates setup uncertainties in single-fraction stereotactic radiosurgery (SRS) by comparing two different mask-creation methods. SRS is a non-invasive treatment option that delivers high doses of radiation to the tumor, minimizing the impact on surrounding healthy tissues. However, the accuracy of this treatment heavily relies on the precision of patient positioning during the procedure.\n",
      "\n",
      "The first mask-creation method is based on a CT-scan performed with the patient immobilized in a customized mask. The second method employs a 3D optical surface imaging system to guide mask creation and patient positioning. The objective of this study is to evaluate the differences in setup uncertainties between the two mask-creation methods.\n",
      "\n",
      "A cohort of 20 patients was enrolled in this study, and each patient received SRS as a single fraction treatment. The uncertainties in patient positioning during the procedure were recorded using kV orthogonals and CBCT imaging. Analysis of the data revealed that the differences in setup uncertainties between the two mask-creation methods were not statistically significant.\n",
      "\n",
      "The results of this study have important implications for clinical practice, as they suggest that both mask-creation methods are equally effective in minimizing uncertainties in patient positioning during SRS procedures. This information can help clinicians make informed decisions regarding the selection of the most appropriate mask-creation method for individual patients.\n",
      "\n",
      "In conclusion, this study provides evidence of the comparable effectiveness of two different mask-creation methods for single-fraction SRS with respect to reducing setup uncertainties. The findings have important implications for clinical practice and contribute to the ongoing efforts to improve the accuracy of SRS treatment. 1 into PostgreSQL...\n",
      "Inserting test sample 2037  The paper studies planar surface reconstruction of indoor scenes from two views with unknown camera poses. While prior approaches have successfully created object-centric reconstructions of many scenes, they fail to exploit other structures, such as planes, which are typically the dominant components of indoor scenes. In this paper, we reconstruct planar surfaces from multiple views, while jointly estimating camera pose. Our experiments demonstrate that our method is able to advance the state of the art of reconstruction from sparse views, on challenging scenes from Matterport3D. Project site: https://jinlinyi.github.io/SparsePlanes/ 0 into PostgreSQL...\n",
      "Inserting test sample 2038  In this paper, we address the problem of reconstructing planar surfaces from a limited number of views. We propose a novel method that can accurately recover the planar structures of a scene by exploiting the sparsity of viewpoints. Our approach employs a combination of optimization techniques and geometric constraints to generate a three-dimensional model of the planar surfaces. We demonstrate the effectiveness of our method on various synthetic and real datasets and show that it outperforms existing state-of-the-art techniques. Our method thus has potential applications in areas such as robotics, augmented reality, and autonomous driving. 1 into PostgreSQL...\n",
      "Inserting test sample 2039  Recently, utilizing renewable energy for wireless system has attracted extensive attention. However, due to the instable energy supply and the limited battery capacity, renewable energy cannot guarantee to provide the perpetual operation for wireless sensor networks (WSN). The coexistence of renewable energy and electricity grid is expected as a promising energy supply manner to remain function for a potentially infinite lifetime. In this paper, we propose a new system model suitable for WSN, taking into account multiple energy consumptions due to sensing, transmission and reception, heterogeneous energy supplies from renewable energy, electricity grid and mixed energy, and multidimension stochastic natures due to energy harvesting profile, electricity price and channel condition. A discrete-time stochastic cross-layer optimization problem is formulated to achieve the optimal trade-off between the time-average rate utility and electricity cost subject to the data and energy queuing stability constraints. The Lyapunov drift-plus-penalty with perturbation technique and block coordinate descent method is applied to obtain a fully distributed and low-complexity cross-layer algorithm only requiring knowledge of the instantaneous system state. The explicit trade-off between the optimization objective and queue backlog is theoretically proven. Finally, the extensive simulations verify the theoretic claims. 0 into PostgreSQL...\n",
      "Inserting test sample 2040  Wireless Sensor Networks (WSNs) play a crucial role in a wide range of applications such as environmental monitoring and surveillance systems. However, the limited energy of sensors remains a significant challenge for the deployment and operation of WSNs. Energy management and cross-layer optimization are promising solutions to address this issue by efficiently utilizing the available energy resources. This paper proposes a novel approach to address the problem of energy management in WSNs powered by heterogeneous energy sources. The proposed method employs a two-level hierarchical structure that enables efficient energy usage while taking into account the heterogeneity of the available energy sources. We also present a cross-layer optimization mechanism that enhances the overall energy efficiency of the WSNs. The optimization process operates by minimizing energy consumption while satisfying the QoS requirements of the WSN's application layer. Extensive simulations are conducted to evaluate the performance of the proposed approach in terms of energy efficiency, network lifetime, and QoS. Results indicate that the proposed approach outperforms other state-of-the-art approaches in terms of higher energy efficiency and longer network lifetime. 1 into PostgreSQL...\n",
      "Inserting test sample 2041  In fighting against fake news, many fact-checking systems comprised of human-based fact-checking sites (e.g., snopes.com and politifact.com) and automatic detection systems have been developed in recent years. However, online users still keep sharing fake news even when it has been debunked. It means that early fake news detection may be insufficient and we need another complementary approach to mitigate the spread of misinformation. In this paper, we introduce a novel application of text generation for combating fake news. In particular, we (1) leverage online users named \\emph{fact-checkers}, who cite fact-checking sites as credible evidences to fact-check information in public discourse; (2) analyze linguistic characteristics of fact-checking tweets; and (3) propose and build a deep learning framework to generate responses with fact-checking intention to increase the fact-checkers' engagement in fact-checking activities. Our analysis reveals that the fact-checkers tend to refute misinformation and use formal language (e.g. few swear words and Internet slangs). Our framework successfully generates relevant responses, and outperforms competing models by achieving up to 30\\% improvements. Our qualitative study also confirms that the superiority of our generated responses compared with responses generated from the existing models. 0 into PostgreSQL...\n",
      "Inserting test sample 2042  The task of fact-checking is paramount in today's information age, where false claims run rampant across media platforms. This research paper delves into the language that fact-checkers employ to identify and debunk false claims. By analyzing the language of fact-checking, we can gain insight into the underlying principles and techniques that experts use to navigate the complex landscape of misinformation. \n",
      "\n",
      "Our approach involves machine learning algorithms that learn to generate fact-checking text from examples of real-world claims and their corresponding fact-checks. By studying the structure and content of these texts, we can automate the process of identifying and debunking false claims. Furthermore, we evaluate the effectiveness of our approach by comparing our generated texts with those produced by expert fact-checkers. \n",
      "\n",
      "Our findings suggest that the language of fact-checking is both precise and nuanced, with expert fact-checkers employing a combination of strategies including source verification, contextual analysis, and logical reasoning. Our automated fact-checking approach not only produces language that is similar to that of expert fact-checkers but also outperforms existing fact-checking systems in terms of accuracy and efficiency. This research has the potential to revolutionize the field of fact-checking and improve the quality of information available to the public. 1 into PostgreSQL...\n",
      "Inserting test sample 2043  We aim to measure the major merger rate of star-forming galaxies at 0.9 < z <1.8, using close pairs identified from integral field spectroscopy (IFS). We use the velocity field maps obtained with SINFONI/VLT on the MASSIV sample, selected from the star-forming population in the VVDS. We identify physical pairs of galaxies from the measurement of the relative velocity and the projected separation (r_p) of the galaxies in the pair. Using the well constrained selection function of the MASSIV sample we derive the gas-rich major merger fraction (luminosity ratio mu = L_2/L_1 >= 1/4), and, using merger time scales from cosmological simulations, the gas-rich major merger rate at a mean redshift up to z = 1.54. We find a high gas-rich major merger fraction of 20.8+15.2-6.8 %, 20.1+8.0-5.1 % and 22.0+13.7-7.3 % for close pairs with r_p <= 20h^-1 kpc in redshift ranges z = [0.94, 1.06], [1.2, 1.5) and [1.5, 1.8), respectively. This translates into a gas-rich major merger rate of 0.116+0.084-0.038 Gyr^-1, 0.147+0.058-0.037 Gyr^-1 and 0.127+0.079-0.042 Gyr^-1 at z = 1.03, 1.32 and 1.54, respectively. Combining our results with previous studies at z < 1, the gas-rich major merger rate evolves as (1+z)^n, with n = 3.95 +- 0.12, up to z = 1.5. From these results we infer that ~35% of the star-forming galaxies with stellar masses M = 10^10 - 10^10.5 M_Sun have undergone a major merger since z ~ 1.5. We develop a simple model which shows that, assuming that all gas-rich major mergers lead to early-type galaxies, the combined effect of gas-rich and dry mergers is able to explain most of the evolution in the number density of massive early-type galaxies since z ~ 1.5, with our measured gas-rich merger rate accounting for about two-thirds of this evolution. 0 into PostgreSQL...\n",
      "Inserting test sample 2044  Galaxy mergers are known to have a large impact on the properties of galaxies, especially their star formation rates. We present a detailed study of the major merger rates of star-forming galaxies at 0.9 < z < 1.8 using the Mass Assembly Survey with SINFONI in VVDS. We use integral field spectroscopy (IFS) to identify close pairs of galaxies with projected separations less than 20 kpc and velocity offsets less than 500 km/s. To derive accurate merger rates, we account for projection effects and incompleteness using a large suite of mock catalogs. \n",
      "\n",
      "Our results show that the major merger rate of star-forming galaxies at 0.9 < z < 1.8 is 4.3 Â± 1.6 per Gyr, which is significantly higher than the merger rate of massive quiescent galaxies at the same redshifts. We find that the merger rate is a strong function of galaxy stellar mass, increasing by a factor of âˆ¼ 10 from 10^9.5 to 10^11 M_â˜‰. In addition, we observe a significant evolution in the merger rate with redshift, increasing by a factor of âˆ¼ 4 from z âˆ¼ 1.6 to z âˆ¼ 0.9.\n",
      "\n",
      "We investigate the physical properties of the close pairs and find that they are mostly gas-rich, star-forming systems, with typical stellar masses of log(M_*/M_â˜‰) âˆ¼ 10.5. Interestingly, we also observe an enhancement of star formation in the central regions of close pairs, supporting the picture of galaxy mergers triggering bursts of star formation.\n",
      "\n",
      "Our study provides a new, integral field spectroscopic-based estimate of the major merger rate of star-forming galaxies at high redshifts and highlights the importance of using IFS to disentangle physical and projection effects. The result implies that roughly half of the present-day stellar mass in star-forming galaxies at 0.9 < z < 1.8 has formed in major mergers. It also has important consequences for our understanding of the evolution of galaxy properties, most notably, the buildup of the red sequence. 1 into PostgreSQL...\n",
      "Inserting test sample 2045  In a system of N particles, with continuous size polydispersity there exists N(N-1) number of partial structure factors making it analytically less tractable. A common practice is to treat the system as an effective one component system which is known to exhibit an artificial softening of the structure. The aim of this study is to describe the system in terms of M pseudo species such that we can avoid this artificial softening but at the same time have a value of M << N. We use potential energy and pair excess entropy to estimate an optimum number of species, M_{0}. We find that systems with polydispersity width, {\\Delta}{\\sigma}_{0} can be treated as a monodisperse system. We show that M_{0} depends on the degree and type of polydispersity and also on the nature of the interaction potential, whereas, {\\Delta}{\\sigma}_{0} weakly depends on the type of the polydispersity, but shows a stronger dependence on the type of interaction potential. Systems with softer interaction potential have a higher tolerance with respect to polydispersity.\n",
      "\n",
      "Interestingly, M_{0} is independent of system size, making this study more relevant for bigger systems. Our study reveals that even 1% polydispersity cannot be treated as an effective monodisperse system. Thus while studying the role of polydispersity by using the structure of an effective one component system care must be taken in decoupling the role of polydispersity from that of the artificial softening of the structure. 0 into PostgreSQL...\n",
      "Inserting test sample 2046  Continuous polydispersity is a common feature of many natural and synthetic systems, yet its effects on the structure and behavior of such systems are not yet fully understood. In this study, we investigate the effective structure of a system with continuous polydispersity, with particular focus on the relationship between particle size distribution and system properties. \n",
      "\n",
      "Using computer simulations and experimental techniques, we demonstrate that continuous polydispersity can significantly impact the structure of a system. Our results show that polydispersity can lead to the formation of distinct clusters of particles, even in systems that are otherwise well-mixed. Additionally, we find that the effective structure of a polydisperse system is highly dependent on the specific form of the particle size distribution.\n",
      "\n",
      "Furthermore, we show that the presence of continuous polydispersity can lead to changes in the mechanical behavior of a system. We demonstrate that under certain conditions, a polydisperse system can exhibit enhanced resistance to deformation, owing to the presence of the particle clusters. These findings provide important insights into the design and optimization of polydisperse systems, with potential applications ranging from materials science to biomedical engineering.\n",
      "\n",
      "Overall, our study sheds new light on the complex relationship between particle size distribution and system structure and behavior in systems with continuous polydispersity. Our results may inform the development of new materials and technologies that harness the unique properties of polydisperse systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2047  Game theory is appropriate for studying cyber conflict because it allows for an intelligent and goal-driven adversary. Applications of game theory have led to a number of results regarding optimal attack and defense strategies.\n",
      "\n",
      "However, the overwhelming majority of applications explore overly simplistic games, often ones in which each participant's actions are visible to every other participant. These simplifications strip away the fundamental properties of real cyber conflicts: probabilistic alerting, hidden actions, unknown opponent capabilities.\n",
      "\n",
      "In this paper, we demonstrate that it is possible to analyze a more realistic game, one in which different resources have different weaknesses, players have different exploits, and moves occur in secrecy, but they can be detected.\n",
      "\n",
      "Certainly, more advanced and complex games are possible, but the game presented here is more realistic than any other game we know of in the scientific literature. While optimal strategies can be found for simpler games using calculus, case-by-case analysis, or, for stochastic games, Q-learning, our more complex game is more naturally analyzed using the same methods used to study other complex games, such as checkers and chess. We define a simple evaluation function and ploy multi-step searches to create strategies. We show that such scenarios can be analyzed, and find that in cases of extreme uncertainty, it is often better to ignore one's opponent's possible moves. Furthermore, we show that a simple evaluation function in a complex game can lead to interesting and nuanced strategies. 0 into PostgreSQL...\n",
      "Inserting test sample 2048  The increasing frequency and sophistication of cyber attacks call for more advanced and realistic models to assess the likelihood of successful attacks and design effective defense mechanisms. In this paper, we introduce HackAttack, a game-theoretic framework for analyzing cyber conflicts in which attackers with varying capabilities and strategies aim to breach a network defended by a security administrator. We explore how the interplay between these agents, including their objectives, knowledge, and levels of risk aversion, can be modeled as a strategic game and studied using theoretical analysis and numerical simulations. We analyze the impact of various parameters such as attacker skill, number, and motivation, on the outcome of the game in terms of success rate, damage, and cost of defense. We also propose an optimal defense strategy that minimizes the expected cost with respect to multiple criteria, such as accuracy, resource allocation, and counterfactual scenarios. Our results demonstrate that HackAttack provides a versatile and realistic model for capturing key features of cyber conflicts and developing robust defensive strategies. Moreover, we show that our approach is applicable to a wide range of scenarios, including insider attacks, targeted attacks, and coordinated attacks by multiple adversaries. We believe that HackAttack represents a significant step towards a more mature and effective approach to cybersecurity, based on sound mathematical principles and interdisciplinary collaboration. 1 into PostgreSQL...\n",
      "Inserting test sample 2049  J-PLUS is an ongoing 12-band photometric optical survey, observing thousands of square degrees of the Northern hemisphere from the dedicated JAST/T80 telescope at the Observatorio Astrof\\'isico de Javalambre. T80Cam is a 2 sq.deg field-of-view camera mounted on this 83cm-diameter telescope, and is equipped with a unique system of filters spanning the entire optical range. This filter system is a combination of broad, medium and narrow-band filters, optimally designed to extract the rest-frame spectral features (the 3700-4000\\AA\\ Balmer break region, H$\\delta$, Ca H+K, the G-band, the Mgb and Ca triplets) that are key to both characterize stellar types and to deliver a low-resolution photo-spectrum for each pixel of the sky observed. With a typical depth of AB $\\sim 21.25$ mag per band, this filter set thus allows for an indiscriminate and accurate characterization of the stellar population in our Galaxy, it provides an unprecedented 2D photo-spectral information for all resolved galaxies in the local universe, as well as accurate photo-z estimates ($\\Delta\\,z\\sim 0.01-0.03$) for moderately bright (up to $r\\sim 20$ mag) extragalactic sources. While some narrow band filters are designed for the study of particular emission features ([OII]/$\\lambda$3727, H$\\alpha$/$\\lambda$6563) up to $z < 0.015$, they also provide well-defined windows for the analysis of other emission lines at higher redshifts. As a result, J-PLUS has the potential to contribute to a wide range of fields in Astrophysics, both in the nearby universe (Milky Way, 2D IFU-like studies, stellar populations of nearby and moderate redshift galaxies, clusters of galaxies) and at high redshifts (ELGs at $z\\approx 0.77, 2.2$ and $4.4$, QSOs, etc). With this paper, we release $\\sim 36$ sq.deg of J-PLUS data, containing about $1.5\\times 10^5$ stars and $10^5$ galaxies at $r<21$ mag. 0 into PostgreSQL...\n",
      "Inserting test sample 2050  The Javalambre Photometric Local Universe Survey (J-PLUS) is a comprehensive sky survey aimed at studying the local universe in unprecedented detail. It is being conducted using the state-of-the-art instrumentation of the Javalambre Astrophysical Observatory, located in Teruel, Spain. This survey is being carried out using a 2.5m wide-field telescope equipped with 12 optical and narrowband filters, which cover a wide range of wavelengths from the ultraviolet to the near-infrared, allowing for detailed characterizations of the properties of the observed objects.\n",
      "\n",
      "The primary science goals of J-PLUS are manifold. One of the most critical goals is to produce a detailed photometric catalog of the local universe's many objects. J-PLUS aims to acquire accurate and precise measurements of the photometry of stars, galaxies, and quasars, as well as detect and characterize new and rare objects, such as high-redshift quasars and intracluster light. Additionally, the J-PLUS survey data will be used to construct the most robust and complete map of the distribution of dark matter in the local universe, by exploiting the weak gravitational lensing techniques.\n",
      "\n",
      "Another key focus of J-PLUS is the study of galaxy evolution and formation. In addition to characterizing the local galaxy population through accurate measurements of their spectral energy distributions, J-PLUS is developing a large catalog of galaxy clustering and galaxy environmental data. This will allow for more detailed investigation of the physical mechanisms responsible for galaxy formation and evolution.\n",
      "\n",
      "The J-PLUS survey data is also being used to study the properties of stellar populations, including their age and metallicity distributions. Accurate measurements of these properties will provide unparalleled insights into the assembly and chemical enrichment of galaxies and other structures in the local universe.\n",
      "\n",
      "Overall, the J-PLUS survey is a pioneering effort that is expected to significantly enhance our understanding of various aspects of the local universe. It is expected to provide invaluable data for a broad range of astrophysical studies, and the compiled catalog will represent a vital legacy for future generations of astronomers and scientists. 1 into PostgreSQL...\n",
      "Inserting test sample 2051  Neutrino-nucleus cross sections for the detection of atmospheric neutrinos are calculated in relativistic impulse and random phase approximations. Pion production is estimated via inclusive Delta-hole nuclear excitations. The number of pion events can confuse the identification of the neutrino flavor. As a further source of pions we calculate coherent pion production (where the nucleus remains in the ground state). We examine how these nuclear structure effects influence atmospheric neutrino experiments and study possible improvements in the present detector simulations. 0 into PostgreSQL...\n",
      "Inserting test sample 2052  This study investigates the role of relativistic nuclear structure effects in atmospheric neutrino detection. The focus is on the importance of accurately calculating the neutrino-nucleus cross section and the contribution of the nuclear short-range correlations in modeling the neutrino interactions with atomic nuclei. Through extensive computational simulations, we demonstrate the impact of the relativistic nuclear structure on the detection of atmospheric neutrinos. This work offers valuable insights into understanding the complex physics of neutrino detection and could facilitate the development of new atmospheric neutrino detectors. 1 into PostgreSQL...\n",
      "Inserting test sample 2053  We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network (DCNN) that learns to associate different views of each 3D object to capture the notion of object persistence and continuity in our visual experience. The re-training process effectively performs distance metric learning under the object persistency constraints, to modify the view-manifold of object representations. It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects, resulting in the untangling of the view-manifolds between individual objects within the same category and across categories. This untangling enables the model to discriminate and recognize objects within the same category, independent of viewpoints. We found that this ability is not limited to the trained objects, but transfers to novel objects in both trained and untrained categories, as well as to a variety of completely novel artificial synthetic objects. This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract, likely at the levels of parts, and independent of the specific objects or categories experienced during training. Interestingly, the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet, suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks. 0 into PostgreSQL...\n",
      "Inserting test sample 2054  Visual perception is a complex cognitive process that involves extracting meaningful information from sensory inputs. Perception of similarity between objects is a critical aspect of this process. In this study, we investigate the transfer of view-manifold learning to similarity perception of novel objects. View-manifold learning is a technique that maps object views to a low-dimensional manifold, which can be used to generate plausible novel views. We hypothesize that transfer learning from the view manifold to the similarity space can enhance similarity perception of novel objects. \n",
      "\n",
      "To test this hypothesis, we conducted a series of behavioral experiments on human subjects. Participants were presented with novel objects and asked to rate their similarity to a set of reference objects. We manipulated the degree of training on the view manifold and the similarity space to investigate the effect of transfer learning. \n",
      "\n",
      "Our results show that transfer learning significantly improves similarity perception of novel objects. Specifically, participants trained on the view manifold showed a significant improvement in similarity ratings compared to those who were not trained. Moreover, transfer from the view manifold to the similarity space produced a more substantial improvement in performance than transfer from the similarity space to the view manifold. \n",
      "\n",
      "In conclusion, our study provides evidence that transfer learning from the view manifold to the similarity space can enhance similarity perception of novel objects. This finding has important implications for the development of algorithms for machine perception and for understanding the neural mechanisms underlying visual perception. 1 into PostgreSQL...\n",
      "Inserting test sample 2055  There are compelling physics questions to be addressed by a new comprehensive detector at a future, high-luminosity RHIC II collider. These form the basis for this Expression of Interest. What precisely are the properties of the strongly-coupled Quark Gluon Plasma (sQGP)? Can a more weakly interacting QGP state be formed and investigated at RHIC? How do particles acquire mass and what is the effect of chiral symmetry restoration on hadronization in a dense medium? What is the chiral structure of the QCD vacuum and its influence on and contributions of different QCD vacuum states to the masses of particles? Is there another phase of matter at low Bjorken-x, i.e. the Color Glass Condensate (CGC)? If present, what are its features and how does it evolve into the QGP?\n",
      "\n",
      "If not, are parton distribution functions understood at low Bjorken-x and can they describe particle production? What are the structure and dynamics inside the proton, including parton spin and orbital angular momentum? What are the contributions of gluons and the QCD sea to the polarization of the proton? What is the flavor-dependence? Are there tests for new physics beyond the Standard Model from spin measurements at RHIC II (such as parity-violating interactions)? We propose that a new comprehensive detector system is needed for RHIC II to address these questions adequately and in an effective way. 0 into PostgreSQL...\n",
      "Inserting test sample 2056  The upcoming RHIC II upgrade poses a unique opportunity to explore a wealth of new physics phenomena. However, to fully exploit this potential, a comprehensive new detector is necessary. In this paper, we present our expression of interest for the construction and operation of such a detector.\n",
      "\n",
      "The proposed detector features a high rate capability, excellent tracking capabilities, and high precision particle identification to enable comprehensive study of high-energy collisions at RHIC II. It consists of multiple subsystems, including a time-of-flight system, a tracking system, and a calorimeter.\n",
      "\n",
      "The time-of-flight system allows for precise measurements of particle momentum and velocity, while the tracking system enables identification of particle trajectories. The calorimeter measures particle energy, and allows for precise measurements of particle mass and identification.\n",
      "\n",
      "Additionally, the proposed detector incorporates novel detector technologies, such as micro-pattern gas detectors, that are essential for the high rate capabilities required by the RHIC II environment.\n",
      "\n",
      "We describe the proposed detector in detail, outlining its conceptual design, technical specifications, and expected performance. We also discuss the potential impact of this comprehensive detector on the RHIC II research program.\n",
      "\n",
      "In conclusion, we believe that this comprehensive new detector would be a valuable addition to the RHIC II research program, enabling unprecedented insights into high-energy collisions. 1 into PostgreSQL...\n",
      "Inserting test sample 2057  The need for accurate photometric redshifts estimation is a topic that has fundamental importance in Astronomy, due to the necessity of efficiently obtaining redshift information without the need of spectroscopic analysis. We propose a method for determining accurate multimodal photo-z probability density functions (PDFs) using Mixture Density Networks (MDN) and Deep Convolutional Networks (DCN). A comparison with a Random Forest (RF) is performed. 0 into PostgreSQL...\n",
      "Inserting test sample 2058  This paper explores the use of deep learning methods to improve the accuracy of photometric redshift estimates in situations where the data is uncertain or incomplete. Our study shows significant improvements in accuracy and precision when compared to traditional methods. We also present an analysis of the sources of uncertainty and explore ways to mitigate their impact on our results. These findings have important implications for the study of distant galaxies and large-scale structure in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2059  Let $X$ be the constrained random walk on ${\\mathbb Z}_+^2$ taking the steps $(1,0)$, $(-1,1)$ and $(0,-1)$ with probabilities $\\lambda < (\\mu_1\\neq \\mu_2)$; in particular, $X$ is assumed stable. Let $\\tau_n$ be the first time $X$ hits $\\partial A_n = \\{x:x(1)+x(2) = n \\}$ For $x \\in {\\mathbb Z}_+^2, x(1) + x(2) < n$, the probability $p_n(x)= P_x( \\tau_n < \\tau_0)$ is a key performance measure for the queueing system represented by $X$. Let $Y$ be the constrained random walk on ${\\mathbb Z} \\times {\\mathbb Z}_+$ with increments $(-1,0)$, $(1,1)$ and $(0,-1)$. Let $\\tau$ be the first time that the components of $Y$ equal each other. We derive the following explicit formula for $P_y(\\tau < \\infty)$: \\[ P_y(\\tau < \\infty) = W(y)= \\rho_2^{y(1)-y(2)} + \\frac{\\mu_2 - \\lambda}{\\mu_2 - \\mu_1} \\rho_1^{ y(1)-y(2)} \\rho_1^{y(2)} + \\frac{\\mu_2-\\lambda}{\\mu_1 -\\mu_2} \\rho_2^{y(1)-y(2)} \\rho_1^{y(2)}, \\] where, $\\rho_i = \\lambda/\\mu_i$, $i=1,2$, $y \\in {\\mathbb Z}\\times{ \\mathbb Z}_+$, $y(1) > y(2)$, and show that $W(n-x_n(1),x_n(2))$ approximates $p_n(x_n)$ with relative error {\\em exponentially decaying} in $n$ for $x_n = \\lfloor nx \\rfloor$, $x \\in {\\mathbb R}_+^2$, $0 < x(1) + x(2) < 1$. The steps of our analysis: 1) with an affine transformation, move the origin $(0,0)$ to $(n,0)$ on $\\partial A_n$; let $n\\nearrow \\infty$ to remove the constraint on the $x(2)$ axis; this step gives the limit {\\em unstable} /{\\em transient} constrained random walk $Y$ and reduces $P_{x}(\\tau_n < \\tau_0)$ to $P_y(\\tau < \\infty)$; 2) construct a basis of harmonic functions of $Y$ and use it to apply the superposition principle to compute $P_y(\\tau < \\infty).$ The construction involves the use of conjugate points on a characteristic surface associated with the walk $X$. The proof that the relative error decays exponentially uses a sequence of subsolutions of a related HJB equation on a manifold. 0 into PostgreSQL...\n",
      "Inserting test sample 2060  In the realm of queuing theory, the assessment of backlog probabilities is one of the key performance indicators in engineering designs. While the calculation of this metric in single-server queuing systems is relatively straightforward, models involving numerous servers are notably more complex. In this study, we analyze the approximation of excessive backlog probabilities of two tandem queues. Specifically, we develop a new approach that combines the notions of generating functions and Laplace transforms to address this issue. We also present a straightforward recursive formula for evaluating the probability generating function (PGF) of the backlog distribution.\n",
      "\n",
      "Our analytical findings show that our proposed methodology can accurately approximate backlog probabilities for two-server systems with varying degrees of incoming traffic intensity. We demonstrate this approach by using the computed values of the PGF in a queueing network simulator, where we conduct a series of experiments to evaluate the performance of the tandem queue system. The simulation results demonstrate that our approximation technique yields reliable outcomes when compared to those obtained through simulations.\n",
      "\n",
      "The significance of our research lies in the development of an effective means of approximating excessive backlog probabilities in tandem queueing systems, which have potential applications in a host of fields such as transportation systems, telecommunications, and healthcare facilities. The proposed methodology can be extended to model alternative queueing models involving additional servers. Future work includes evaluating the computational complexity of our approach relative to other approximation techniques and the design of numerical algorithms for more scalable applications.\n",
      "\n",
      "In conclusion, our proposed method provides a powerful tool for approximating excessive backlog probabilities for tandem queueing systems. This paper demonstrates the effectiveness of our analytical approach and provides a framework for future work in the field. 1 into PostgreSQL...\n",
      "Inserting test sample 2061  In the classic Bayesian restless multi-armed bandit (RMAB) problem, there are $N$ arms, with rewards on all arms evolving at each time as Markov chains with known parameters. A player seeks to activate $K \\geq 1$ arms at each time in order to maximize the expected total reward obtained over multiple plays. RMAB is a challenging problem that is known to be PSPACE-hard in general. We consider in this work the even harder non-Bayesian RMAB, in which the parameters of the Markov chain are assumed to be unknown \\emph{a priori}. We develop an original approach to this problem that is applicable when the corresponding Bayesian problem has the structure that, depending on the known parameter values, the optimal solution is one of a prescribed finite set of policies. In such settings, we propose to learn the optimal policy for the non-Bayesian RMAB by employing a suitable meta-policy which treats each policy from this finite set as an arm in a different non-Bayesian multi-armed bandit problem for which a single-arm selection policy is optimal. We demonstrate this approach by developing a novel sensing policy for opportunistic spectrum access over unknown dynamic channels. We prove that our policy achieves near-logarithmic regret (the difference in expected reward compared to a model-aware genie), which leads to the same average reward that can be achieved by the optimal policy under a known model. This is the first such result in the literature for a non-Bayesian RMAB. 0 into PostgreSQL...\n",
      "Inserting test sample 2062  The multi-armed bandit (MAB), a fundamental problem in decision-making, has drawn much attention in research. In a MAB scenario, an agent aims to maximize its utility by selecting one arm among several alternative arms with unknown reward probabilities. While the Bayesian approach has proven effective in randomized exploration and exploitation in MAB, in some scenarios, the Bayesian approach fails, requiring the use of alternative algorithms.\n",
      "\n",
      "This paper presents a non-Bayesian approach to the restless MAB problem with near-logarithmic regret, which refers to the difference between the expected reward earned by the agent and the reward earned by an agent who always selects the optimal arm. The proposed algorithm, called the \"Explore-First-Pull-Later\" (EFPL), is based on the explore-then-exploit principle. It begins with initial exploration of all arms for a certain period, then applies an exploitation policy for the arm that is estimated to have the highest reward probability.\n",
      "\n",
      "Using regret analysis, we prove the logarithmic regret bound of EFPL algorithms, which match the best-recently introduced algorithms. Moreover, we derive an upper-bound on the number of explorations required to achieve the logarithmic bound. We also recommend practical implementation of our algorithm with two types of exploration policies, showing the effectiveness of the EFPL algorithm by comparing it to other heuristics in existing literature.\n",
      "\n",
      "In summary, the EFPL algorithm presented in this paper provides a promising, non-Bayesian alternative to restless MAB problems. Our analysis shows that EFPL achieves near-logarithmic regret bound, and can be implemented in practice with simple exploration policies. 1 into PostgreSQL...\n",
      "Inserting test sample 2063  We define and study XOR games in the framework of general probabilistic theories, which encompasses all physical models whose predictive power obeys minimal requirements. The bias of an XOR game under local or global strategies is shown to be given by a certain injective or projective tensor norm, respectively. The intrinsic (i.e.\\ model-independent) advantage of global over local strategies is thus connected to a universal function $r(n,m)$ called 'projective-injective ratio'. This is defined as the minimal constant $\\rho$ such that $\\|\\cdot\\|_{X\\otimes_\\pi Y}\\leq\\rho\\,\\|\\cdot\\|_{X\\otimes_\\varepsilon Y}$ holds for all Banach spaces of dimensions $\\dim X=n$ and $\\dim Y=m$, where $X\\otimes_\\pi Y$ and $X \\otimes_\\varepsilon Y$ are the projective and injective tensor products. By requiring that $X=Y$, one obtains a symmetrised version of the above ratio, denoted by $r_s(n)$. We prove that $r(n,m)\\geq 19/18$ for all $n,m\\geq 2$, implying that injective and projective tensor products are never isometric. We then study the asymptotic behaviour of $r(n,m)$ and $r_s(n)$, showing that, up to log factors: $r_s(n)$ is of the order $\\sqrt{n}$ (which is sharp); $r(n,n)$ is at least of the order $n^{1/6}$; and $r(n,m)$ grows at least as $\\min\\{n,m\\}^{1/8}$. These results constitute our main contribution to the theory of tensor norms. In our proof, a crucial role is played by an '$\\ell_1$/$\\ell_2$/$\\ell_{\\infty}$ trichotomy theorem' based on ideas by Pisier, Rudelson, Szarek, and Tomczak-Jaegermann. The main operational consequence we draw is that there is a universal gap between local and global strategies in general XOR games, and that this grows as a power of the minimal local dimension. In the quantum case, we are able to determine this gap up to universal constants. As a corollary, we obtain an improved bound on the scaling of the maximal quantum data hiding efficiency against local measurements. 0 into PostgreSQL...\n",
      "Inserting test sample 2064  This paper concerns the study of XOR games and the estimation of their universal gaps, using tensor norm ratios. An XOR game involves two cooperating parties, Alice and Bob, who receive random inputs and must produce outputs that satisfy a certain predicate. The study of XOR games has important applications in computer science, such as cryptography and complexity theory. In this context, it is of interest to estimate the universal gaps of XOR games, which measure the relative error between the value of the game and the optimal value achievable with entangled strategies.\n",
      "\n",
      "We provide a novel method for estimating universal gaps of XOR games using tensor norm ratios. Specifically, we introduce a family of norms, which we call the \"gap norms\", that allow us to upper bound the gap between the optimal value and the value achieved by a certain bound on the tensor norm ratio of the game matrix. Based on this framework, we establish new upper bounds on the universal gaps of XOR games, improving upon previous results.\n",
      "\n",
      "We illustrate the effectiveness of our method by applying it to several families of XOR games, including those arising in communication complexity and graph theory. In each case, we obtain improved upper bounds on the universal gaps, using the gap norms and tensor norm ratios. Moreover, we show that the gap norms are flexible enough to capture important structural properties of the game matrices, such as sparsity and symmetry.\n",
      "\n",
      "Finally, we discuss some open problems and directions for future research. One interesting question is whether there exist lower bounds on universal gaps that are as strong as our upper bounds, or whether our upper bounds are tight. Another interesting direction is the application of our method to other areas of computer science, such as quantum computation and machine learning. We believe that our method opens up new avenues for the study of XOR games using tensor norm ratios and related tools. 1 into PostgreSQL...\n",
      "Inserting test sample 2065  We consider a class of mean field games in which the agents interact through both their states and controls, and we focus on situations in which a generic agent tries to adjust her speed (control) to an average speed (the average is made in a neighborhood in the state space). In such cases, the monotonicity assumptions that are frequently made in the theory of mean field games do not hold, and uniqueness cannot be expected in general. Such model lead to systems of forward-backward nonlinear nonlocal parabolic equations; the latter are supplemented with various kinds of boundary conditions, in particular Neumann-like boundary conditions stemming from reflection conditions on the underlying controled stochastic processes. The present work deals with numerical approximations of the above mentioned systems. After describing the finite difference scheme, we propose an iterative method for solving the systems of nonlinear equations that arise in the discrete setting; it combines a continuation method, Newton iterations and inner loops of a bigradient like solver. The numerical method is used for simulating two examples. We also make experiments on the behaviour of the iterative algorithm when the parameters of the model vary. The theory of mean field games, (MFGs for short), aims at studying deterministic or stochastic differential games (Nash equilibria) as the number of agents tends to infinity. It supposes that the rational agents are indistinguishable and individually have a negligible influence on the game, and that each individual strategy is influenced by some averages of quantities depending on the states (or the controls as in the present work) of the other agents. MFGs have been introduced in the pioneering works of J-M. Lasry and P-L. Lions [17, 18, 19]. Independently and at approximately the same time, the notion of mean field games arose in the engineering literature, see the works of M.Y. Huang, P.E. Caines and R.Malham{\\'e} [14, 15]. The present work deals with numerical approximations of mean field games in which the agents interact through both their states and controls; it follows a more theoretical work by the second author, [16], which is devoted to the mathematical analysis of the related systems of nonlocal partial differential equations. There is not much literature on MFGs in which the agents also interact through their controls, see [13, 12, 8, 10, 7, 16]. To stress the fact that the latter situation is considered, we will sometimes use the terminology mean field games of control and the acronym MFGC. 0 into PostgreSQL...\n",
      "Inserting test sample 2066  This research paper addresses the issue of solving mean field games of controls by finite difference approximations. Mean field game theory models large-scale multi-agent systems with a distinct feature: interactions between agents are taken into account explicitly. Due to their inherent complexity, these models require approximation techniques for efficient computation. Finite difference methods are a well-established numerical tool for solving partial differential equations and have been applied in various areas of science and engineering. We propose a new finite difference approximation for solving mean field games of controls that yields high accuracy and efficiency.\n",
      "\n",
      "The proposed method utilizes the HJB (Hamilton-Jacobi-Bellman) equations to approximate the solutions of the mean field games problem. Our method involves discretization of the state space and time and utilizes a backward induction algorithm that propagates the solution backwards in time. The approximation is then obtained through solving a system of nonlinear equations.\n",
      "\n",
      "We prove that our method converges in the limit of both time and space discretization. Additionally, we derive error estimates that describe the accuracy of our approximation. Moreover, we demonstrate the efficacy of our method through numerical simulations in various settings with different sizes and degrees of complexity. Our results indicate that our method outperforms existing approaches in terms of accuracy and computation time, particularly for large-scale systems.\n",
      "\n",
      "The proposed finite difference approximation method has several potential applications in real-world problems. For instance, the method can be used to model traffic flow, energy management, finance, and various other multi-agent control systems. Our contributions to this field are twofold. First, we propose a new finite difference approximation for solving mean field games of controls that is both accurate and efficient. Second, we demonstrate that our method outperforms existing approaches in both accuracy and computation time. \n",
      "\n",
      "We believe that our work opens up new opportunities for using finite difference methods to solve mean field games of controls and paves the way for further research in this area. In conclusion, the proposed method offers a promising tool for solving large-scale nonlinear control problems, which are of significant importance in many applications. 1 into PostgreSQL...\n",
      "Inserting test sample 2067  PSR B0031-07 is well known to exhibit three different modes of drifting sub-pulses (mode A, B and C). It has recently been shown that in a multifrequency observation, consisting of 2700 pulses, all driftmodes were visible at low frequencies, while at 4.85 GHz only mode-A drift or non-drifting emission was detected. This suggests that modes A and B are emitted in sub-beams, rotating at a fixed distance from the magnetic axis, with the mode-B sub-beams being closer to the magnetic axis than the mode-A sub-beams. Diffuse emission between the sub-beams can account for the non-drifting emission. Using the results of an analysis of simultaneous multifrequency observations of PSR B0031-07, we set out to construct a geometrical model that includes emission from both sub-beams and diffuse emission and describes the regions of the radio emission of PSR B0031-07 at each emission frequency for driftmodes A and B.\n",
      "\n",
      "Based on the vertical spacing between driftbands, we have determined the driftmode of each sequence of drift. To restrict the model, we calculated average polarisation and intensity characteristics for each driftmode and at each frequency. The model reproduces the observed polarisation and intensity characteristics, suggesting that diffuse emission plays an important role in the emission properties of PSR B0031-07. The model further suggests that the emission heights of this pulsar range from a few kilometers to a little over 10 kilometers above the pulsar surface. We also find that the relationships between height and frequency of emission that follow from curvature radiation and from plasma-frequency emission could not be used to reproduce the observed frequency dependence of the width of the average intensity profiles. 0 into PostgreSQL...\n",
      "Inserting test sample 2068  PSR B0031-07 is a pulsar star located approximately 1,000 light-years from Earth in the southern constellation Dorado. The pulsar emits radiation in the form of radio waves and provides a unique opportunity to study the geometry of the pulsar emission region.\n",
      "\n",
      "Using observations obtained with the Parkes Radio Telescope, we have investigated the polarization properties of the pulsar emission. We found that the polarization angle of the emission undergoes a sharp swing by 90 degrees, known as a \"orthogonal mode transition,\" as the emission passes through the line of sight. This transition is thought to be caused by a change in the geometry of the emission region as it travels through the pulsar magnetosphere.\n",
      "\n",
      "Our analysis of the polarization properties also revealed that the emission from the pulsar is highly linearly polarized. This suggests that the emission region is highly ordered and may be associated with the pulsar's magnetic field.\n",
      "\n",
      "We have also investigated the properties of the pulsar's pulse profiles. Our analysis shows that the profiles are highly stable and exhibit a complex morphology. We have identified several sub-pulses in the emission that appear to be correlated with the orthogonal mode transitions.\n",
      "\n",
      "In conclusion, our observations of PSR B0031-07 provide important insights into the geometry and polarization properties of the pulsar emission region. These findings have implications for our understanding of the physics of pulsars and the emission mechanisms that produce the radio waves observed from these fascinating objects. 1 into PostgreSQL...\n",
      "Inserting test sample 2069  We study self-acceleration in PT and non-PT symmetric systems. We find some novel wave effects that appear uniquely in non-Hermitian systems. We show that integrable self-accelerating waves exist if the Hamiltonian is non-Hermitian.\n",
      "\n",
      "We find that self-accelerating constant intensity waves are possible even when gain and loss are not balanced in the system. 0 into PostgreSQL...\n",
      "Inserting test sample 2070  This work studies self-acceleration mechanisms in non-Hermitian systems. We introduce a novel approach to study the dynamics of non-conservative systems, describing self-acceleration via a complex energy landscape. Using numerical simulations, we demonstrate that self-acceleration is a universal phenomenon that can occur in a variety of systems, opening up new possibilities for energy-efficient transport. 1 into PostgreSQL...\n",
      "Inserting test sample 2071  We consider the Lagrangian particle model introduced in [hep-th/9612017] for zero mass but nonvanishing second central charge of the planar Galilei group.\n",
      "\n",
      "Extended by a magnetic vortex or a Coulomb potential the model exibits conformal symmetry. In the former case we observe an additional SO(2,1) hidden symmetry. By either a canonical transformation with constraints or by freezing scale and special conformal transformations at $t=0$ we reduce the six-dimensional phase-space to the physically required four dimensions. Then we discuss bound states (bounded solutions) in quantum dynamics (classical mechanics). We show that the Schr\\\"odinger equation for the pure vortex case may be transformed into the Morse potential problem thus providing us with an explanation of the hidden SO(2,1) symmetry. 0 into PostgreSQL...\n",
      "Inserting test sample 2072  The study of conformal dynamics has led to new insights into the behavior of physical systems. This paper introduces a novel approach to understanding the dynamics of conformal systems by considering a new type of conformal symmetry. This symmetry can be applied to a wide range of systems and provides a deeper understanding of their behavior. In particular, this approach sheds new light on the behavior of quantum systems, which are notoriously difficult to analyze. We demonstrate the effectiveness of this approach through a number of examples, including the study of quantum critical points and the behavior of high-dimensional systems. Our results demonstrate the potential of this new approach to provide a deeper understanding of complex physical systems and to enable the design of novel materials and devices. 1 into PostgreSQL...\n",
      "Inserting test sample 2073  We investigate the dynamics of mixed-species ion crystals during transport between spatially distinct locations in a linear Paul trap in the diabatic regime. In a general mixed-species crystal, all degrees of freedom along the direction of transport are excited by an accelerating well, so unlike the case of same-species ions, where only the center-of-mass-mode is excited, several degrees of freedom have to be simultaneously controlled by the transport protocol. We design protocols that lead to low final excitations in the diabatic regime using invariant-based inverse-engineering for two different-species ions and also show how to extend this approach to longer mixed-species ion strings. Fast transport of mixed-species ion strings can significantly reduce the time overhead in certain architectures for scalable quantum information processing with trapped ions. 0 into PostgreSQL...\n",
      "Inserting test sample 2074  We investigate the fast transport of mixed-species ion chains within a Paul trap. A time-dependent amplitude modulation of the radiofrequency voltage applied to the trapping electrodes drives the transport. We test two modulation schemes: a sinusoidal and a chirped modulation. We find that both schemes transport the chains with high fidelity, but the chirped modulation shows a faster transport and can achieve higher final velocities. The transport time and final velocity also depend on the mass of the ions, the amplitude of the modulation, and the trap geometry. These results demonstrate a promising method for the fast transport of ion chains with mixed species, which is essential for quantum information processing and the study of trapped-ion dynamics. 1 into PostgreSQL...\n",
      "Inserting test sample 2075  We examine the age-dependence of dark matter halo clustering in an unprecedented accuracy using a set of 7 high-resolution cosmological simulations each with $N=1024^3$ particles. We measure the bias parameters for halos over a large mass range using the cross-power-spectrum method that can effectively suppress the random noise even in the sparse sampling of the most massive halos. This enables us to find, for the first time, that younger halos are more strongly clustered than older ones for halo masses $M>10M_{\\ast}$, where $M_\\ast$ is the characteristic nonlinear mass scale. For $M<M_{\\ast}$, our results confirm the previous finding of Gao et al. that older halos are clustered more strongly than the younger ones. We also study the halo bias as a function of halo concentration, and find that the concentration dependence is weaker than the age dependence for $M<M_{\\ast}$, but stronger for $M\\ga 50 M_{\\ast}$. The accurate and robust measurement of the age dependences of halo bias points to a limitation of the simple excursion set theory which predicts that the formation and structure of a halo of given mass is independent of its environment. 0 into PostgreSQL...\n",
      "Inserting test sample 2076  In this research paper, we investigate the dependence of dark halo clustering on two key parameters: the formation epoch and the concentration parameter. To do so, we use a large-scale, high-resolution simulation of cosmic structure formation, which enables us to accurately trace the evolution of dark matter halos over cosmic time. Our results show that the clustering properties of dark matter halos are strongly influenced by both the formation epoch and the concentration parameter. Specifically, we find that halos forming earlier tend to be more strongly clustered than those forming later, while halos with higher concentration parameters also exhibit enhanced clustering. We further explore the physical mechanisms responsible for these trends, and demonstrate that they are intimately connected to the underlying properties of the dark matter halo population. Our findings have important implications for our understanding of the formation and evolution of cosmic structure, and underscore the key role played by dark halo clustering in shaping the large-scale structure of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2077  Studies using real-time functional magnetic resonance imaging (rt-fMRI) have recently incorporated the decoding approach, allowing for fMRI to be used as a tool for manipulation of fine-grained neural activity. Because of the tremendous potential for clinical applications, certain questions regarding decoded neurofeedback (DecNef) must be addressed. Neurofeedback effects can last for months, but the short- to mid-term dynamics are not known.\n",
      "\n",
      "Specifically, can the same subjects learn to induce neural patterns in two opposite directions in different sessions? This leads to a further question, whether learning to reverse a neural pattern may be less effective after training to induce it in a previous session. Here we employed a within-subjects' design, with subjects undergoing DecNef training sequentially in opposite directions (up or down regulation of confidence judgements in a perceptual task), with the order counterbalanced across subjects. Behavioral results indicated that the manipulation was strongly influenced by the order and direction of neurofeedback. We therefore applied nonlinear mathematical modeling to parametrize four main consequences of DecNef: main effect of change in behavior, strength of down-regulation effect relative to up-regulation, maintenance of learning over sessions, and anterograde learning interference.\n",
      "\n",
      "Modeling results revealed that DecNef successfully induced bidirectional behavioral changes in different sessions. Furthermore, up-regulation was more sizable, and the effect was largely preserved even after an interval of one-week. Lastly, the second week effect was diminished as compared to the first week effect, indicating strong anterograde learning interference. These results suggest reinforcement learning characteristics of DecNef, and provide important constraints on its application to basic neuroscience, occupational and sports trainings, and therapies. 0 into PostgreSQL...\n",
      "Inserting test sample 2078  Functional magnetic resonance imaging (fMRI) has been widely used as a neuroimaging tool for investigating brain activity associated with numerous cognitive and behavioral processes. Recently, real-time fMRI neurofeedback has emerged as a potential tool for modulating activity and improving performance in healthy and clinical populations. In this study, we investigated whether decoded fMRI neurofeedback could induce bidirectional behavioral changes within single participants.\n",
      "\n",
      "Twenty-two healthy participants completed a go/no-go task while undergoing fMRI scanning. An online decoding algorithm was used to provide real-time feedback to participants about their individual neural activity. Participants were trained to either up-regulate or down-regulate activity in the anterior cingulate cortex (ACC) and insula. Post-scan behavioral tasks revealed that participants who had successfully learned to increase ACC activity showed decreased reaction times in a subsequent task, whereas those who had down-regulated ACC activity showed increased reaction times. Importantly, these effects were specific to the targeted brain region and were not observed in a control region.\n",
      "\n",
      "Our findings demonstrate that decoded fMRI neurofeedback can effectively modulate neural activity and subsequently induce bidirectional changes in behavior. These results provide evidence for the feasibility and effectiveness of decoded fMRI neurofeedback as a potential tool for improving cognitive and behavioral function in both healthy and clinical populations. 1 into PostgreSQL...\n",
      "Inserting test sample 2079  AIMS. We calculate the stellar density using star counts obtained from Gaia DR2 up to a Galactocentric distance R=20 kpc with a deconvolution technique for the parallax errors. Then we analyse the density in order to study the structure of the outer Galactic disc, mainly the warp.\n",
      "\n",
      "METHODS. In order to carry out the deconvolution, we used the Lucy inversion technique for recovering the corrected star counts. We also used the Gaia luminosity function of stars with $M_G<10$ to extract the stellar density from the star counts.\n",
      "\n",
      "RESULTS. The stellar density maps can be fitted by an exponential disc in the radial direction $h_r=2.07\\pm0.07$ kpc, with a weak dependence on the azimuth, extended up to 20 kpc without any cut-off. The flare and warp are clearly visible. The best fit of a symmetrical S-shaped warp gives $z_w= z_\\odot+(37\\pm 4.2(stat.)-0.91(syst.))$ pc $(R/R_\\odot )^{2.42\\pm 0.76(stat.) + 0.129 (syst.)} sin(\\phi+9.3\\pm 7.37 (stat.) +4.48 (syst.))$ for the whole population. When we analyse the northern and southern warps separately, we obtain an asymmetry of an $\\sim25\\%$ larger amplitude in the north. This result may be influenced by extinction because the Gaia G band is quite prone to extinction biases.\n",
      "\n",
      "However, we tested the accuracy of the extinction map we used, which shows that the extinction is determined very well in the outer disc. Nevertheless, we recall that we do not know the full extinction error, and neither do we know the systematic error of the map, which may influence the final result.\n",
      "\n",
      "The analysis was also carried out for very luminous stars alone ($M_G<-2$), which on average represents a younger population. We obtain similar scale-length values, while the maximum amplitude of the warp is $20-30\\%$ larger than with the whole population. The north-south asymmetry is maintained. 0 into PostgreSQL...\n",
      "Inserting test sample 2080  The outer Galactic disc is a fundamental component of the Milky Way galaxy, and its structure serves as a crucial source of information on the galaxy's formation and evolution. With the help of Gaia-DR2, we have obtained high-precision astrometric data for over a billion stars in the Milky Way, enabling us to explore the outer Galactic disc in unprecedented detail. In this paper, we present an analysis of the structure of the outer Galactic disc using Gaia-DR2 data.\n",
      "\n",
      "Our analysis reveals several key features of the outer Galactic disc. Firstly, we observe a clear deviation from the axisymmetric structure previously assumed for the Milky Way's outer disc. Our data shows evidence for a warped and flared structure, with significant deviations present at larger radii from the Galactic centre. Additionally, we observe a series of asymmetries in the disc's structure, which we attribute to the influence of spiral arms and other environmental factors.\n",
      "\n",
      "Furthermore, we have used kinematic data from Gaia-DR2 to investigate the velocity distributions of stars in the outer Galactic disc. Our results show that the disc exhibits complex kinematic behaviour, including deviations from pure circular motion and the presence of substructures. These findings provide key insights into the dynamics of the outer Galactic disc and have important implications for our understanding of the Milky Way's formation and evolution.\n",
      "\n",
      "In conclusion, this paper presents a comprehensive analysis of the outer Galactic disc using high-precision astrometric and kinematic data from Gaia-DR2. Our results reveal a complex and non-axisymmetric structure for the disc, with clear deviations from previous assumptions. These findings provide crucial insights into the dynamics of the Milky Way's outer disc and have important implications for galactic formation and evolution more generally. 1 into PostgreSQL...\n",
      "Inserting test sample 2081  We present a new clustering algorithm that is based on searching for natural gaps in the components of the lowest energy eigenvectors of the Laplacian of a graph. In comparing the performance of the proposed method with a set of other popular methods (KMEANS, spectral-KMEANS, and an agglomerative method) in the context of the Lancichinetti-Fortunato-Radicchi (LFR) Benchmark for undirected weighted overlapping networks, we find that the new method outperforms the other spectral methods considered in certain parameter regimes. Finally, in an application to climate data involving one of the most important modes of interannual climate variability, the El Nino Southern Oscillation phenomenon, we demonstrate the ability of the new algorithm to readily identify different flavors of the phenomenon. 0 into PostgreSQL...\n",
      "Inserting test sample 2082  Spectral clustering has become a popular technique in the field of data mining and machine learning due to its superior performance in the analysis of large-scale datasets. However, recent studies have uncovered limitations in traditional spectral clustering algorithms, including sensitivity to initial conditions, scalability issues, and the lack of interpretability. In this paper, we present a novel spectral clustering algorithm that overcomes these limitations by enforcing a sparsity constraint on the affinity matrix. Our method is computationally efficient, robust to initialization, and provides interpretable results. Experimental results on several benchmark datasets demonstrate that our algorithm outperforms state-of-the-art spectral clustering methods in terms of clustering accuracy, scalability, and interpretability, while maintaining competitive runtime performance. 1 into PostgreSQL...\n",
      "Inserting test sample 2083  Interaction blockade occurs when strong interactions in a confined few-body system prevent a particle from occupying an otherwise accessible quantum state.\n",
      "\n",
      "Blockade phenomena reveal the underlying granular nature of quantum systems and allow the detection and manipulation of the constituent particles, whether they are electrons, spins, atoms, or photons. The diverse applications range from single-electron transistors based on electronic Coulomb blockade to quantum logic gates in Rydberg atoms. We have observed a new kind of interaction blockade in transferring ultracold atoms between orbitals in an optical lattice. In this system, atoms on the same lattice site undergo coherent collisions described by a contact interaction whose strength depends strongly on the orbital wavefunctions of the atoms. We induce coherent orbital excitations by modulating the lattice depth and observe a staircase-type excitation behavior as we cross the interaction-split resonances by tuning the modulation frequency. As an application of orbital excitation blockade (OEB), we demonstrate a novel algorithmic route for cooling quantum gases. Our realization of algorithmic cooling utilizes a sequence of reversible OEB-based quantum operations that isolate the entropy in one part of the system, followed by an irreversible step that removes the entropy from the gas. This work opens the door to cooling quantum gases down to ultralow entropies, with implications for developing a microscopic understanding of strongly correlated electron systems that can be simulated in optical lattices. In addition, the close analogy between OEB and dipole blockade in Rydberg atoms provides a roadmap for the implementation of two-qubit gates in a quantum computing architecture with natural scalability. 0 into PostgreSQL...\n",
      "Inserting test sample 2084  Quantum gases are systems that are of great interest because they provide excellent testbeds for studying fundamental concepts in quantum mechanics. In particular, the use of techniques such as algorithmic cooling and orbital excitation blockade have become popular tools in manipulating and controlling such systems. The former is a method for reducing the entropy of a quantum system, while the latter is a way of manipulating its spin using collective effects.\n",
      "\n",
      "This research paper explores the relationship between these two techniques on a model system of non-interacting fermions in two dimensions. We present a theoretical analysis of the combined effects of orbital excitation blockade and algorithmic cooling on the energy spectrum, and show that these techniques can lead to a significant reduction in the number of excited states. Additionally, we propose a method for implementing this algorithmic cooling scheme in a cold atom experiment using a time-varying magnetic field.\n",
      "\n",
      "To validate our theoretical results, we perform numerical simulations on small-scale systems and observe that they agree with our theoretical predictions. Our research sheds light on the potential applications of algorithmic cooling and orbital excitation blockade in quantum simulations and quantum computation by allowing the manipulation of the energy spectrum of a quantum system. The results presented in this paper represent a step forward towards the realization of a quantum computer with practical applications. With the developments in quantum technology, we hope that the insights provided here will be useful in progressing towards this goal. 1 into PostgreSQL...\n",
      "Inserting test sample 2085  We study the following variant of the Erd\\H{o}s distance problem. Given $E$ and $F$ a point sets in $\\mathbb{R}^d$ and $p = (p_1, \\ldots, p_q)$ with $p_1+ \\cdots + p_q = d$ is an increasing partition of $d$ define $$ B_p(E,F)=\\{(|x_1-y_1|, \\ldots, |x_q-y_q|): x \\in E, y \\in F \\},$$ where $x=(x_1, \\ldots, x_q)$ with $x_i$ in $\\mathbb{R}^{p_i}$. For $p_1 \\geq 2$ it is not difficult to construct $E$ and $F$ such that $|B_{p}(E,F)|=1$. On the other hand, it is easy to see that if $\\gamma_q$ is the best know exponent for the distance problem in $\\mathbb{R}^{p_i}$ that $|B_p(E,E)| \\geq C{|E|}^{\\frac{\\gamma_q}{q}}$. The question we study is whether we can improve the exponent $\\frac{\\gamma_q}{q}$. We first study partitions of length two in detail and prove the optimal result (up to logarithms) that $$ |B_{2,2}(E)| \\gtrapprox |E|.$$ In the generalised two dimensional case for $B_{k,l}$ we need the stronger condition that $E$ is $s$-adaptable for $s<\\frac{k}{2}+\\frac{1}{3}$, letting $\\gamma_m$ be the best known exponent for the Erd\\H{o}s-distance problem in $\\mathbb{R}^m$ for $k \\neq l$ we gain a further optimal result of, $$ |B_{k,l}(E)| \\gtrapprox |E|^{\\gamma_l}.$$ When $k=l$ we use the explicit $\\gamma_m=\\frac{m}{2}-\\frac{2}{m(m+2)}$ result due to Solymosi and Vu to gain $$ |B_{k,k}(E)| \\gtrapprox |E|^{\\frac{13}{14}\\gamma_k}.$$ For a general partition, let $\\gamma_i = \\frac{2}{p_i}-\\frac{2}{p_i(p_i+2)}$ and $\\eta_i = \\frac{2}{2d-(p_i-1)}$. Then if $E$ is $s$-adaptable with $s>d-\\frac{p_1}{2}+\\frac{1}{3}$ we have $$ B_p(E) \\gtrapprox |E|^\\tau \\hspace{0.5cm} \\text{where} \\hspace{0.5cm} \\tau = \\gamma_q\\left(\\frac{\\gamma_1+\\eta_1}{\\gamma_q+(q-1)(\\gamma_1+\\eta_1)}\\right).$$ Where $p_i \\sim \\frac{d}{q}$ implies $\\tau \\sim \\gamma_{q}\\left(\\frac{1}{q}+\\frac{1}{dq}\\right)$ and $p_q \\sim d$ (with $q<<d$) implies $\\tau \\sim \\gamma_{q}\\left(\\frac{1}{q}+\\frac{1}{q^2}\\right)$. 0 into PostgreSQL...\n",
      "Inserting test sample 2086  The ErdÅ‘s distance problem has been a long-standing open problem in combinatorial geometry. This problem concerns the study of the minimum number of distinct distances determined by n points in the plane. In its original formulation, the ErdÅ‘s distance problem was concerned with determining whether it is possible for n points in the plane to have exactly n distinct pairwise distances and whether there exists a universal lower bound on the number of distinct distances.\n",
      "\n",
      "Recently, a number of researchers have extended the problem in various ways by relaxing some of the constraints on the input data. In this paper, we consider a multi-parameter variant of the ErdÅ‘s distance problem which seeks to understand how the number of distinct distances is affected by additional parameters such as the orientation of the points, the range of the distances, or the alignment of the points with respect to some fixed direction.\n",
      "\n",
      "We present a comprehensive analysis of this variant of the ErdÅ‘s distance problem, including results on the existence of point sets with large numbers of distinct distances, bounds on the maximum number of distances achieved by a finite collection of points, and algorithms for constructing optimal point sets.\n",
      "\n",
      "Our main result is a new upper bound on the number of distinct distances formed by n points in the plane with bounded distances and arbitrary orientation. We prove that for any fixed distance range and any orientation, the maximum number of distinct distances formed by any set of n points is O(n^(2/3)).\n",
      "\n",
      "We also show that this bound is tight by constructing a specific orientation such that the maximum number of distinct distances formed by n points is Î˜(n^(2/3)).\n",
      "\n",
      "Overall, our work contributes to the understanding of the ErdÅ‘s distance problem by providing insights into the behavior of the number of distinct distances in different parameter regimes. We believe that our results will be of interest to researchers in combinatorial geometry, computational geometry, algorithm design, and related fields. 1 into PostgreSQL...\n",
      "Inserting test sample 2087  We present a detailed study of the molecular gas in the fast AGN-driven outflow in the nearby radio-loud Seyfert galaxy IC 5063. Using ALMA observations of a number of tracers (12CO(1-0), 12CO(2-1), 12CO(3-2), 13CO(2-1) and HCO+(4-3)), we map the differences in excitation, density and temperature of the gas. The results show that in the immediate vicinity of the radio jet, a fast outflow, with velocities up to 800 km/s, is occurring of which the gas has high excitation temperatures in the range 30-55 K, demonstrating the direct impact of the jet on the ISM. The relative brightness of the CO lines show that the outflow is optically thin. We estimate the mass of the molecular outflow to be 1.2 x 10^6 Msol and likely to be a factor 2-3 larger. This is similar to that of the outflow of atomic gas, but much larger than that of the ionised outflow, showing that the outflow is dominated by cold gas. The total mass outflow rate we estimate to be ~12 Msol/yr. The mass of the outflow is much smaller than the total gas mass of the ISM of IC 5063. Therefore, although the influence of the radio jet is very significant in the inner regions, globally speaking the impact will be very modest. We use RADEX modelling to explore the physical conditions of the molecular gas in the outflow. Models with the outflowing gas being quite clumpy give the most consistent results and our preferred solutions have kinetic temperatures in the range 20-100 K and densities between 10^5 and 10^6 cm^-3. The resulting pressures are 10^6-10^7.5 K cm^-3, about two orders of magnitude higher than in the outer quiescent disk.\n",
      "\n",
      "The results strongly suggest that the outflow is driven by the radio jet expanding into a clumpy medium, creating a cocoon of gas which is pushed away from the jet axis resulting in a lateral outflow, very similar to what is predicted by numerical simulations. 0 into PostgreSQL...\n",
      "Inserting test sample 2088  The study of fast outflows in active galactic nuclei (AGNs) has provided valuable insights into the mechanisms of energy transfer within these complex systems. In this paper, we present an analysis of the molecular gas properties in the fast outflow in the Seyfert galaxy IC 5063. Our observations were carried out using the Atacama Large Millimeter/submillimeter Array (ALMA) to obtain high-resolution images of the molecular gas emission in the outflow.\n",
      "\n",
      "We find that the molecular gas in the fast outflow shows significant differences compared to the quiescent gas in the host galaxy. The molecular gas in the outflow is highly excited, with large rotational temperatures and broad linewidths. The gas also shows significant depletion of molecular species such as CO and H2O, suggesting that the outflow is capable of driving chemical changes in the gas phase.\n",
      "\n",
      "Our observations also reveal that the molecular gas is distributed in a clumpy structure, implying that the outflow is highly irregular and turbulent. We detect multiple molecular gas components at different velocities, indicating that the outflow is likely composed of multiple gas streams. The morphology and kinematics of the molecular gas suggest that the outflow is launched from the base of the nuclear disk, driven by a combination of radiation pressure and magneto-hydrodynamical forces.\n",
      "\n",
      "Overall, our results shed new light on the properties of the molecular gas in the fast outflow in the Seyfert galaxy IC 5063, and contribute to a better understanding of the feedback mechanisms in AGNs. Future studies probing the connection between the molecular gas properties and the kinematics of the outflow will be crucial in deciphering the role of fast outflows in shaping the evolution of galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 2089  We analyze the structure diagram for binary clusters of Lennard-Jones particles by means of a global optimization approach for a large range of cluster sizes, compositions and interaction energies and present a publicly accessible database of 180,000 minimal energy structures (http://softmattertheory.lu/clusters.html). We identify a variety of structures such as core-shell clusters, Janus clusters and clusters in which the minority species is located at the vertices of icosahedra. Such clusters can be synthesized from nanoparticles in agglomeration experiments and used as building blocks in colloidal molecules or crystals. We discuss the factors that determine the formation of clusters with specific structures. 0 into PostgreSQL...\n",
      "Inserting test sample 2090  This paper presents a detailed analysis of the structure diagram of binary Lennard-Jones clusters, which are a class of molecular systems with important applications in biophysics and materials science. We use a combination of numerical simulations and analytical techniques to explore the structural properties of these clusters under a range of conditions. Our results show that binary Lennard-Jones clusters exhibit a rich variety of structural motifs, including tetrahedral, octahedral, and icosahedral arrangements. These findings have important implications for the design of new materials and for understanding the behavior of biological systems at the molecular scale. 1 into PostgreSQL...\n",
      "Inserting test sample 2091  Within the framework of the Standard Model of particle physics and standard cosmology, observations of the Cosmic Microwave Background (CMB) and Baryon Acoustic Oscillations (BAO) set stringent bounds on the sum of the masses of neutrinos. If these bounds are satisfied, the upcoming KATRIN experiment which is designed to probe neutrino mass down to $\\sim 0.2$ eV will observe only a null signal. We show that the bounds can be relaxed by introducing new interactions for the massive active neutrinos, making neutrino masses in the range observable by KATRIN compatible with cosmological bounds. Within this scenario, neutrinos convert to new stable light particles by resonant production of intermediate states around a temperature of $T\\sim$ keV in the early Universe, leading to a much less pronounced suppression of density fluctuations compared to the standard model. 0 into PostgreSQL...\n",
      "Inserting test sample 2092  The study focuses on the possibility of neutrinos turning into lighter particles in an attempt to satisfy both the findings of the Karlsruhe Tritium Neutrino experiment (KATRIN) and the cosmological observations. The identification of this process would have significant consequences for the properties of neutrinos, including their masses and whether they behave differently than their antiparticles. The research employs sophisticated statistical and mathematical modeling techniques to investigate and assess the possibility of this phenomenon. The results provide insights into the fundamental nature of neutrinos and their interactions with matter. The study also offers a glimpse into the potential implications of the findings for our understanding of the physical universe and its evolution since the Big Bang. 1 into PostgreSQL...\n",
      "Inserting test sample 2093  In this article, we interconnect two different aspects of higher category theory, in one hand the theory of infinity categories and on an other hand the theory of 2-categories.We construct an explicit functorial path objet in the model category of topological categories. We discuss some properties and consequences of such path object. We also explain the construction of a 2-monad which algebras are (symmetric) monoidal topological categories. Finally, we explain the relationship with the eventual model structure on the category of T-algebras. 0 into PostgreSQL...\n",
      "Inserting test sample 2094  This paper investigates the homotopy theory of T-algebras over Top-Cat, which are topological categories enriched in a monad T. We show the existence of a model structure on the category of T-algebras and establish Quillen equivalences between it and categories of diagrams over Top-Cat. The homotopy theory of T-algebras can be described using a Hopf bifurcation model and we discuss its implications for applications in physics and engineering. Our results provide a new perspective on the study of enriched categories and their homotopy theory. 1 into PostgreSQL...\n",
      "Inserting test sample 2095  We have mapped the G287.84-0.82 cometary globule (with the Treasure Chest cluster embedded in it) in the South Pillars region of Carina (i) in [CII], 63micron [OI], and CO(11-10) using upGREAT on SOFIA and (ii) in J=2-1 transitions of CO, 13CO, C18O and J=3-2 transitions of H2CO using the APEX telescope in Chile. We probe the morphology, kinematics, and physical conditions of the molecular gas and the photon dominated regions (PDRs) in G287.84-0.82. The [CII] and [OI] emission suggest that the overall structure of the pillar (with red-shifted photo evaporating tails) is consistent with the effect of FUV radiation and winds from eta-Car and O stars in Trumpler 16. The gas in the head of the pillar is strongly influenced by the embedded cluster, whose brightest member is an O9.5V star, CPD-59 2661. The emission of the [CII] and [OI] lines peak at a position close to the embedded star, while all other tracers peak at another position lying to the north-east consistent with gas being compressed by the expanding PDR created by the embedded cluster. The molecular gas inside the globule is probed with the J=2-1 transitions of CO and isotopologues as well as H2CO, and analyzed using a non-LTE model (escape-probability approach), while we use PDR models to derive the physical conditions of the PDR. We identify at least two PDR gas components; the diffuse part (~10^4 cm^-3) is traced by [CII], while the dense (n~ 2-8x10^5 cm^-3) part is traced by [CII], [OI], CO(11-10). Using the F=2-1 transition of [13CII] detected at 50 positions in the region, we derive optical depths (0.9-5), excitation temperatures of [CII] (80-255 K), and N(C+) of 0.3-1x10^19 cm^-2.\n",
      "\n",
      "The total mass of the globule is ~1000 Msun, about half of which is traced by [CII]. The dense PDR gas has a thermal pressure of 10^7-10^8 K cm^-3, which is similar to the values observed in other regions. 0 into PostgreSQL...\n",
      "Inserting test sample 2096  Carina is a fascinating constellation located in the southern hemisphere, containing numerous objects of interest to astronomers. In this research paper, we explore the vast array of treasures that Carina holds, and offer insight into the most notable objects for future examination.\n",
      "\n",
      "The jewel of Carina is undoubtedly the Eta Carinae binary star system, which is one of the most massive and luminous binary stars in the Milky Way. This system underwent a tremendous outburst in the mid-19th century, and continues to intrigue researchers to this day. The Carina Nebula is another significant object, which contains numerous star-forming regions and is considered a prime location for identifying young and massive stars. \n",
      "\n",
      "The Chest of Carina is a lesser-known but equally intriguing area, situated within the Carina-Miscellaneous complex. The Chest is composed of numerous molecular clouds containing some of the most massive and dense cores known, making it an excellent instance for understanding the earliest stages of star formation. In this paper, we provide a detailed investigation of the Chest, including its physical properties and the processes involved in star formation within this complex.\n",
      "\n",
      "We also discuss the role of radio wavelengths in exploring Carina's treasures. We present results of our observations of the Carina Nebula using the Karl G. Jansky Very Large Array (VLA), which allow us to detect and map the emission from cold dust and molecular gas within the nebula. These new results shed light on the different structures and processes that occur within the Carina Nebula itself and its environs. \n",
      "\n",
      "Finally, we discuss the implications of our findings for the broader field of astronomy. The treasures of Carina provide critical insights into the formation and evolution of massive stars and galaxies over cosmic time. The future exploration of Carina will undoubtedly lead to new and exciting discoveries which will significantly advance our understanding of the Universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2097  We report our study of the I-V curves in the transport through the quantum dot when an additional quantum dot lying in the Kondo regime is side-connected to it. Due to the Kondo scattering off the effective spin on a side-connected quantum dot the conductance is suppressed at low temperatures and at low source-drain bias voltages. This zero-bias anomaly is understood as enhanced Kondo scattering with decreasing temperature. 0 into PostgreSQL...\n",
      "Inserting test sample 2098  Transport through parallel double quantum dots can be controlled by suppressing the current that flows between them. This can be achieved through the application of a gate voltage on one of the dots, which affects the tunneling probability between the dots. In this study, we investigate the suppression of current in double quantum dots through a combination of theoretical modeling and experimental measurements, highlighting the importance of gate voltage and dot spacing on the transport properties. 1 into PostgreSQL...\n",
      "Inserting test sample 2099  Mazzarella and Scafetta (2016) showed that the seismic activity recorded at the Bunker-East (BKE) Vesuvian station from 1999 to 2014 suggests a higher nocturnal seismic activity. However, this station is located at about 50 m from the main road to the volcano's crater and since 2009 its seismograms also record a significant diurnal cultural noise due mostly to tourist tours to Mt.\n",
      "\n",
      "Vesuvius. Herein, we investigate whether the different seismic frequency between day and night times could be an artifact of the peculiar cultural noise that affects this station mostly from 9:00 am to 5:00 pm from spring to fall.\n",
      "\n",
      "This time-distributed cultural noise should evidently reduce the possibility to detect low magnitude earthquakes during those hours but not high magnitude events. Using hourly distributions referring to different magnitude thresholds from M = 0.2 to M = 2.0, the Gutenberg-Richter magnitude-frequency diagram applied to the day and night-time sub-catalogs and Montecarlo statistical modeling, we demonstrate that the day-night asymmetry persists despite an evident disruption induced by cultural noise during day-hours. In particular, for the period 1999-2017, and for earthquakes with M > 2 we found a Gutenberg-Richter exponent b = 1.66 +/- 0.07 for the night-time events and b = 2.06 +/- 0.07 for day-time events. Moreover, we repeat the analysis also for an older BKE catalog covering the period from 1992 to 2000 when cultural noise was not present. The analysis confirms a higher seismic nocturnal activity that is also characterized by a smaller Gutenberg-Richter exponent b for M > 2 earthquakes relative to the day-time activity. Thus, the found night-day seismic asymmetric behavior is likely due to a real physical feature affecting Mt. Vesuvius. 0 into PostgreSQL...\n",
      "Inserting test sample 2100  Cultural noise can have a significant impact on the monitoring and study of seismic activity. The Bunker-East (BKE) Vesuvian Station has recorded seismic activity with a notable night-day asymmetry, which may be attributed to cultural noise. This study aims to explore the relationship between cultural noise and the night-day asymmetry of seismic activity recorded at the BKE Vesuvian Station.\n",
      "\n",
      "Our analysis begins by examining the data recorded at the BKE Vesuvian Station during different times of day. We then use statistical models to evaluate the relationship between cultural noise and the night-day asymmetry of the seismic activity. Our results demonstrate that cultural noise has a statistically significant impact on the night-day asymmetry of the seismic activity recorded at the BKE Vesuvian Station.\n",
      "\n",
      "We further investigate the sources of cultural noise at the BKE Vesuvian Station. Our findings reveal several possible sources, including traffic, industry, and human activity. The results also show that cultural noise has a more profound impact on the seismic activity recorded at night than during the day.\n",
      "\n",
      "This study suggests that cultural noise can significantly impact the seismic activity monitored by the BKE Vesuvian Station. To improve the accuracy of seismic monitoring, it is essential to consider cultural noise as a potential source of error. Future studies should continue to investigate the impact of cultural noise on seismic activity recordings and seek to develop methods to mitigate its effects.\n",
      "\n",
      "In conclusion, this research highlights the importance of understanding cultural noise when studying seismic activity. The observed night-day asymmetry of seismic activity at the BKE Vesuvian Station can be attributed to cultural noise, which has a statistically significant impact on seismic recordings. Further research is necessary to develop effective strategies to mitigate these effects and improve the accuracy of seismic monitoring. 1 into PostgreSQL...\n",
      "Inserting test sample 2101  We present detections of the CO(4-3) and [C I] 609 $\\mu$m spectral lines, as well as the dust continuum at 480.5 GHz (rest-frame), in 3C 368, a Fanaroff-Riley class II (FR-II) galaxy at redshift (z) 1.131. 3C 368 has a large stellar mass, ~ 3.6 x 10$^{11}$ M$_\\odot$, and is undergoing an episode of vigorous star formation, at a rate of ~ 350 M$_\\odot$/yr, and active galactic nucleus (AGN) activity, with radio-emitting lobes extended over ~ 73 kpc. Our observations allow us to inventory the molecular-gas reservoirs in 3C 368 by applying three independent methods: (1) using the CO(4-3)-line luminosity, excitation state of the gas, and an $\\alpha_{CO}$ conversion factor, (2) scaling from the [C I]-line luminosity, and (3) adopting a gas-to-dust conversion factor. We also present gas-phase metallicity estimates in this source, both using far-infrared (FIR) fine-structure lines together with radio free-free continuum emission and independently employing the optical [O III] 5007 A and [O II] 3727 A lines (R$_{23}$ method). Both methods agree on a sub-solar gas-phase metallicity of ~ 0.3 Z$_\\odot$. Intriguingly, comparing the molecular-gas mass estimated using this sub-solar metallicity, M$_{gas}$ ~ 6.4 x 10$^{10}$ M$_\\odot$, to dust-mass estimates from multi-component spectral energy distribution (SED) modeling, M$_{dust}$ ~ 1.4 x 10$^8$ M$_\\odot$, yields a gas-to-dust ratio within ~ 15% of the accepted value for a metallicity of 0.3 Z$_\\odot$. The derived gas-mass puts 3C 368 on par with other galaxies at z ~ 1 in terms of specific star-formation rate and gas fraction. However, it does not explain how a galaxy can amass such a large stellar population while maintaining such a low gas-phase metallicity. Perhaps 3C 368 has recently undergone a merger, accreting pristine molecular gas from an external source. 0 into PostgreSQL...\n",
      "Inserting test sample 2102  Recent observations of a distant galaxy, located at a redshift of z ~ 1, have revealed intriguing features that point to a possible low metallicity environment. Using sensitive spectroscopic techniques, we have obtained CO and fine-structure line measurements that provide insight into the elemental composition of the interstellar medium within this stellar-mass-rich galaxy. Our results suggest a metallicity that is significantly lower than what is typically observed in galaxies at this redshift.\n",
      "\n",
      "CO is a critical tracer of molecular gas, which is a key ingredient for star formation. Our observations show a CO(1-0) line that is remarkably bright, indicating a high abundance of H2. The ratio of CO(1-0) to the far-infrared luminosity suggests that the star formation efficiency is low, which is consistent with a low metallicity environment.\n",
      "\n",
      "In addition, we have detected several fine-structure lines, including [CII] 158 Î¼m and [OI] 63 Î¼m. These lines are sensitive to the abundance of neutral gas and allow us to estimate the metallicity based on the relative strengths of the lines. Our measurements indicate a metallicity that is significantly lower than what is typically seen in galaxies at this redshift, which is surprising given the high stellar mass of this galaxy.\n",
      "\n",
      "The low metallicity environment is likely a result of a combination of factors. The galaxy's location in a low-density environment, far from other galaxies, may have hindered its ability to accrete metal-enriched gas. Additionally, the intense star formation activity within the galaxy may have resulted in metal-rich gas being expelled into the intergalactic medium.\n",
      "\n",
      "Overall, our results suggest that this distant galaxy may represent a unique laboratory for studying the impact of metallicity on star formation and galaxy evolution. Further observations and analysis will be necessary to confirm our findings and explore the possible mechanisms driving metallicity in this galaxy at z ~ 1. 1 into PostgreSQL...\n",
      "Inserting test sample 2103  Following the discovery of Bose-Einstein condensation (BEC) in ultra cold atoms [E. Gosta, Nobel Lectures in Physics (2001-2005), World Scientific (2008)], there has been a huge experimental and theoretical push to try and illuminate a superfluid state of Wannier-Mott excitons. Excitons in quantum wells, generated by a laser pulse, typically diffuse only a few micrometers from the spot they are created. However, Butov et al. and Snoke et al. reported luminescence from indirect and direct excitons hundreds of micrometers away from the laser excitation spot in double and single quantum well (QW) structures at low temperatures. This luminescence appears as a ring around the laser spot with the dark region between the spot and the ring. Developing the theory of a free superflow of Bose-liquids we show that the macroscopic luminesce rings and the dark state are signatures of the coherent superflow of condensed excitons at temperatures below their Berezinskii-Kosterlitz-Thouless (BKT) transition temperature. To further verify the dark excitonic superflow we propose several keystone experiments, including interference of superflows from two laser spots, vortex formation, scanning of moving dipole moments, and a giant increase of the luminescence distance by applying one-dimensional confinement potential. These experiments combined with our theory will open a new avenue for creating and controlling superflow of coherent excitons on nanoscale. 0 into PostgreSQL...\n",
      "Inserting test sample 2104  This research paper explores the possibility of controlling the free superflow, dark matter, and luminescence rings of excitons in quantum well structures. Excitons, which are bound electron-hole pairs, exhibit unique properties such as the ability to form Bose-Einstein condensates and superfluids. These properties make excitons promising candidates for future technologies such as quantum computing and communication. However, the control of exciton-related phenomena remains a challenge, and this paper aims to address this issue.\n",
      "\n",
      "The study will focus on the use of magnetic fields, electric fields, and strain engineering to manipulate excitons in quantum well structures. The researchers will use numerical simulations to explore the behavior of excitons under different conditions. Results from these simulations will help identify the optimal conditions for controlling the superflow, dark matter, and luminescence rings of excitons.\n",
      "\n",
      "This study will contribute to the understanding of exciton physics and provide insights into the use of excitons for future technologies. The control of excitons in quantum well structures has the potential to revolutionize fields such as semiconductor technology and quantum optics. The findings of this study may help pave the way for the development of new and innovative devices based on exciton properties. 1 into PostgreSQL...\n",
      "Inserting test sample 2105  We present a method for realizing efficiently Grover's search algorithm in an array of coupled cavities doped with three-level atoms. We show that by encoding information in the lowest two ground states of the dopants and through the application of appropriately tuned global laser fields, the reflection operator needed for the quantum search algorithm can be realized in a single physical operation. Thus, the time steps in which Grover's search can be implemented become equal to the mathematical steps ~ O(\\sqrt{N}), where N is the size of the register. We study the robustness of the implementation against errors due to photon losses and fluctuations in the cavity frequencies and atom-photon coupling constants. 0 into PostgreSQL...\n",
      "Inserting test sample 2106  We present a new approach for single-interaction step implementation of a quantum search algorithm using two coupled micro-cavities. This system offers several advantages over traditional quantum computing systems, including simplicity in implementation and mitigating the effects of decoherence. We propose a scheme for preparing and manipulating the input state, as well as a protocol for measuring the search results that can be implemented using current experimental technology. Moreover, we demonstrate how our scheme can be extended to handle more complex search problems. Our work opens the door to further exploration of quantum computing in the context of micro-cavity systems, paving the way towards the realization of practical quantum search algorithms. 1 into PostgreSQL...\n",
      "Inserting test sample 2107  Over the past half-century, technology has evolved beyond our wildest dreams.\n",
      "\n",
      "However, while the benefits of technological growth are undeniable, the nascent Internet did not anticipate the online threats we routinely encounter and the harms which can result. As our world becomes increasingly connected, it is critical we consider what implications current and future technologies have for security and privacy. We approach this challenge by surveying 30 predictions across industry, academia and international organisations to extract a number of common themes. Through this, we distill 10 emerging scenarios and reflect on the impact these might have on a range of stakeholders. Considering gaps in best practice and requirements for further research, we explore how security and privacy might evolve over the next decade. We find that existing guidelines both fail to consider the relationships between stakeholders and do not address the novel risks from wearable devices and insider threats. Our approach rigorously analyses emerging scenarios and suggests future improvements, of crucial importance as we look to pre-empt new technological threats. 0 into PostgreSQL...\n",
      "Inserting test sample 2108  The rapid advancement of technology and the growing dependence on digital data pose significant challenges to security and privacy. This paper explores future scenarios and challenges that may arise in the fields of security and privacy. It discusses the potential impact of emerging technologies on security and privacy, including the rise of artificial intelligence, the internet of things, and cloud computing. Additionally, this paper considers the challenges posed by human factors such as social engineering and the need for collective responsibility. The paper concludes that there is a need for collaborative efforts to address the challenges and risks posed by emerging technologies. It also highlights the importance of raising awareness about the implications of technology on security and privacy, and the ethical considerations that should be factored into the development of future technologies. Overall, this paper provides valuable insights into the complex and evolving landscape of security and privacy in the digital age. 1 into PostgreSQL...\n",
      "Inserting test sample 2109  We present a modification of Newton's method to restore quadratic convergence for isolated singular solutions of polynomial systems. Our method is symbolic-numeric: we produce a new polynomial system which has the original multiple solution as a regular root. Using standard bases, a tool for the symbolic computation of multiplicities, we show that the number of deflation stages is bounded by the multiplicity of the isolated root. Our implementation performs well on a large class of applications. 0 into PostgreSQL...\n",
      "Inserting test sample 2110  \"We present a new method for the numerical treatment of isolated singularities of polynomial systems that combines Newton's method with deflation techniques to achieve faster convergence. Our approach is based on the construction of a structured deflation matrix that reduces the dimension of the system and enhances the accuracy of the approximation. We prove the convergence of the algorithm when applied to polynomial systems and illustrate its effectiveness with numerical experiments on several examples.\" 1 into PostgreSQL...\n",
      "Inserting test sample 2111  Boundary value problems for nonlocal fractional elliptic equations with parameter in Banach spaces are studied. Uniform $L_p$-separability properties and sharp resolvent estimates are obtained for elliptic equations in terms of fractional derivatives. Particularly, it is proven that the fractional ellipitic operator generated by these equations is sectorial and also is a generator of an analytic semigroup. Moreover, maximal regularity properties of nonlocal fractional abstract parabolic equation are established. As an application, the nonlocal anisotropic fractional differential equations and the system of nonlocal fractional differential equations are studied. 0 into PostgreSQL...\n",
      "Inserting test sample 2112  This paper explores the properties of nonlocal fractional differential equations and provides various applications of these equations within physical and financial modeling. We establish mathematical techniques to study nonlocality in terms of Riesz fractional derivatives, and provide results concerning the existence and uniqueness of solutions to such equations. Additionally, we study the stability and asymptotic behavior of these solutions. Our applications include financial models based on LÃ©vy processes and physical models based on nonlocal diffusion. Numerical simulations are presented to support our theoretical findings. 1 into PostgreSQL...\n",
      "Inserting test sample 2113  Time-delay cosmography of lensed quasars has achieved 2.4% precision on the measurement of the Hubble constant, $H_0$. As part of an ongoing effort to uncover and control systematic uncertainties, we investigate three potential sources: 1- stellar kinematics, 2- line-of-sight effects, and 3- the deflector mass model. To meet this goal in a quantitative way, we reproduced the H0LiCOW/SHARP/STRIDES (hereafter TDCOSMO) procedures on a set of real and simulated data, and we find the following. First, stellar kinematics cannot be a dominant source of error or bias since we find that a systematic change of 10% of measured velocity dispersion leads to only a 0.7% shift on $H_0$ from the seven lenses analyzed by TDCOSMO. Second, we find no bias to arise from incorrect estimation of the line-of-sight effects. Third, we show that elliptical composite (stars + dark matter halo), power-law, and cored power-law mass profiles have the flexibility to yield a broad range in $H_0$ values.\n",
      "\n",
      "However, the TDCOSMO procedures that model the data with both composite and power-law mass profiles are informative. If the models agree, as we observe in real systems owing to the \"bulge-halo\" conspiracy, $H_0$ is recovered precisely and accurately by both models. If the two models disagree, as in the case of some pathological models illustrated here, the TDCOSMO procedure either discriminates between them through the goodness of fit, or it accounts for the discrepancy in the final error bars provided by the analysis. This conclusion is consistent with a reanalysis of six of the TDCOSMO (real) lenses: the composite model yields $74.0^{+1.7}_{-1.8}$ $km.s^{-1}.Mpc^{-1}$, while the power-law model yields $H_0=74.2^{+1.6}_{-1.6}$ $km.s^{-1}.Mpc^{-1}$. In conclusion, we find no evidence of bias or errors larger than the current statistical uncertainties reported by TDCOSMO. 0 into PostgreSQL...\n",
      "Inserting test sample 2114  The inference of the Hubble constant ($H_0$) through time-delay cosmography is a promising avenue for deriving cosmological constraints that are independent of the local distance ladder. We present an analysis of systematic uncertainties that affect the $H_0$ inference from strong lens time delays, using three different cosmological models and a variety of assumptions regarding the lens mass distribution and line-of-sight structure. We quantify the effects of astrophysical and modeling uncertainties in the H0LiCOW analysis, and find that while they are subdominant compared to the measurement uncertainties, they need to be carefully accounted for in order to reach higher-accuracy measurements in future lensing campaigns. We also highlight the importance of multiple lenses with independent cosmological distance constraints as a way to break degeneracies in the mass modeling and obtain more robust cosmological inferences.\n",
      "\n",
      "Our results show that the cosmological parameters derived from a joint analysis of multiple lenses are stable against the choice of priors on the cosmological parameters but are sensitive to the assumed lens mass profile. The inclusion of a prior on the Hubble constant derived from Planck microwave background (CMB) data has a strong impact on the inferred values of $H_0$, leading to a shift that enables better agreement between the lensing and CMB data. We also derive the first constraints on the time evolution of the Hubble constant through a combination of lensing and CMB information, finding that it is consistent with a constant value today at the 2-$\\sigma$ level. Our work demonstrates the feasibility of using time delays in strong gravitational lenses to improve the accuracy of $H_0$ measurements beyond the current state of the art, and highlights the importance of understanding and addressing the astrophysical and modeling uncertainties that affect these measurements. 1 into PostgreSQL...\n",
      "Inserting test sample 2115  We describe the differential graded Lie algebras governing Poisson deformations of a holomorphic Poisson manifold and coisotropic embedded deformations of a coisotropic holomorphic submanifold. In both cases, under some mild additional assumption, we show that the infinitesimal first order deformations induced by the anchor map are unobstructed. Applications include the analog of Kodaira stability theorem for coisotropic deformation and a generalization of McLean-Voisin's theorem about the local moduli space of lagrangian submanifold. Finally it is shown that our construction is homotopy equivalent to the homotopy Lie algebroid, in the cases where this is defined. 0 into PostgreSQL...\n",
      "Inserting test sample 2116  In this paper, we study coisotropic deformations of holomorphic submanifolds. We introduce a deformation theory of coisotropic submanifolds by using the formal theory of differential graded Lie algebras. By applying this deformation theory to the holomorphic setting, we obtain a natural notion of coisotropic deformations of holomorphic submanifolds. Furthermore, we explore examples and applications of our theory, including a method for constructing new examples of holomorphic Lagrangian submanifolds. Our results provide new insight into the geometry of coisotropic submanifolds and have potential applications in symplectic geometry and mathematical physics. 1 into PostgreSQL...\n",
      "Inserting test sample 2117  In this paper we propose and analyze an energy stable numerical scheme for the Cahn-Hilliard equation, with second order accuracy in time and the fourth order finite difference approximation in space. In particular, the truncation error for the long stencil fourth order finite difference approximation, over a uniform numerical grid with a periodic boundary condition, is analyzed, via the help of discrete Fourier analysis instead of the the standard Taylor expansion.\n",
      "\n",
      "This in turn results in a reduced regularity requirement for the test function.\n",
      "\n",
      "In the temporal approximation, we apply a second order BDF stencil, combined with a second order extrapolation formula applied to the concave diffusion term, as well as a second order artificial Douglas-Dupont regularization term, for the sake of energy stability. As a result, the unique solvability, energy stability are established for the proposed numerical scheme, and an optimal rate convergence analysis is derived in the $\\ell^\\infty (0,T; \\ell^2) \\cap \\ell^2 (0,T; H_h^2)$ norm. A few numerical experiments are presented, which confirm the robustness and accuracy of the proposed scheme. 0 into PostgreSQL...\n",
      "Inserting test sample 2118  We propose a fourth order finite difference scheme for the Cahn-Hilliard equation, an important model in materials science and phase separation. The scheme is designed to ensure energy stability and positivity preservation, making it suitable for computations with large time steps. The key idea is to discretize the fourth-order differential operator using a compact scheme with a uniform grid, followed by a semi-implicit time stepping scheme. We rigorously prove that the proposed scheme is energy stable and positivity preserving, meaning that it conserves the free energy of the system and guarantees physically meaningful solutions. Numerical experiments demonstrate that the proposed scheme is of high accuracy, even for coarse grids and large time steps. Compared with existing methods, our new method provides faster and more reliable simulations of the Cahn-Hilliard equation. The proposed scheme has potential applications in various fields, such as materials science and computational biology, where accurate and efficient numerical simulations are in high demand. 1 into PostgreSQL...\n",
      "Inserting test sample 2119  A.A. Kirillov introduced the family algebras in 2000. In this paper we study the noncommutative Poisson bracket P on the classical family algebra. We show that P is the first-order deformation from the classical family algebra to the quantum family algebra. We will prove that the noncommutative Poisson bracket is in fact a Hochschild 2-coboundary therefore the deformation is infinitesimally trivial. In the last part of this paper we also talk about Mackey's analogue and the quantization problem of the family algebras. 0 into PostgreSQL...\n",
      "Inserting test sample 2120  In this paper, we investigate the noncommutative Poisson bracket and its application in deforming the family algebras. We present a thorough analysis of the underlying mathematical structures and derive fundamental equations which govern the behavior of the algebraic objects studied here. By exploring several examples and numerical simulations, we demonstrate the practical utility of our theoretical results. Our findings are expected to be particularly useful for researchers working in the field of mathematical physics and related areas. 1 into PostgreSQL...\n",
      "Inserting test sample 2121  This work is devoted to study the magnetic reconnection instability under solar spicule conditions. Numerical study of the resistive tearing instability in a current sheet is presented by considering the magnetohydrodynamic (MHD) framework. To investigate the effect of this instability in a stratified atmosphere of solar spicules, we solve linear and non-ideal MHD equations in the x-z plane. In the linear analysis it is assumed that resistivity is only important within the current sheet, and the exponential growth of energies takes place faster as plasma resistivity increases. We are interested to see the occurrence of magnetic reconnection during the lifetime of a typical solar spicule. 0 into PostgreSQL...\n",
      "Inserting test sample 2122  This study investigates the occurrence of magnetic reconnection in solar spicules. Using high-resolution observations from the Interface Region Imaging Spectrograph (IRIS), we identify spicules with signs of reconnection. We find that reconnection events occur more frequently in spicules than previously thought, suggesting that magnetic energy release may be an important driver of spicule dynamics. By analyzing the plasma properties and magnetic field topology during reconnection, we explore the physics of this process in detail. These results provide new insights into the role of magnetic reconnection in the complex and dynamic atmosphere of the Sun. 1 into PostgreSQL...\n",
      "Inserting test sample 2123  One major open conjecture in the area of critical random graphs, formulated by statistical physicists, and supported by a large amount of numerical evidence over the last decade [23, 24, 28, 63] is as follows: for a wide array of random graph models with degree exponent $\\tau\\in (3,4)$, distances between typical points both within maximal components in the critical regime as well as on the minimal spanning tree on the giant component in the supercritical regime scale like $n^{(\\tau-3)/(\\tau-1)}$.\n",
      "\n",
      "In this paper we study the metric space structure of maximal components of the multiplicative coalescent, in the regime where the sizes converge to excursions of L\\'evy processes \"without replacement\" [10], yielding a completely new class of limiting random metric spaces. A by-product of the analysis yields the continuum scaling limit of one fundamental class of random graph models with degree exponent $\\tau\\in (3,4)$ where edges are rescaled by $n^{-(\\tau-3)/(\\tau-1)}$ yielding the first rigorous proof of the above conjecture. The limits in this case are compact \"tree-like\" random fractals with finite fractal dimensions and with a dense collection of hubs (infinite degree vertices) a finite number of which are identified with leaves to form shortcuts. In a special case, we show that the Minkowski dimension of the limiting spaces equal $(\\tau-2)/(\\tau-3)$ a.s., in stark contrast to the Erd\\H{o}s-R\\'{e}nyi scaling limit whose Minkowski dimension is 2 a.s. It is generally believed that dynamic versions of a number of fundamental random graph models, as one moves from the barely subcritical to the critical regime can be approximated by the multiplicative coalescent. In work in progress, the general theory developed in this paper is used to prove analogous limit results for other random graph models with degree exponent $\\tau\\in (3,4)$. 0 into PostgreSQL...\n",
      "Inserting test sample 2124  This research paper investigates the multiplicative coalescent (MC) and inhomogeneous continuum random trees (ICRT) in order to identify new universality classes for critical random graphs. The MC is a stochastic process which models the coalescence of clusters, while ICRTs are trees with non-uniform edge lengths and their scaling limits. The paper considers dynamic properties of random trees and graphs such as their growth and connectivity properties, and their critical behavior near the phase transitions. The authors apply rigorous mathematical techniques to study the asymptotic behavior of the MC generalizing previous work and obtaining new results for several types of phase transitions. The study of ICRTs and their applications to the study of random graphs is also extended and clarified, providing new insights on the universality of the scaling limits. In particular, the authors provide results on the behavior of ICRTs under random weighting, which allows for capturing the dependence and heterogeneity of real systems. The paper concludes with open problems and directions for future research in the field. Overall, this paper contributes to the understanding of critical phenomena in complex systems and provides a framework for studying the asymptotic behavior of large-scale stochastic processes, with applications in statistics, physics, computer science, and biology. 1 into PostgreSQL...\n",
      "Inserting test sample 2125  In 2015, K2 observations of the bright (V = 8.9, K = 7.7) star HIP 41378 revealed a rich system of at least five transiting exoplanets, ranging in size from super-Earths to gas giants. The 2015 K2 observations only spanned 74.8 days, and the outer three long-period planets in the system were only detected with a single transit, so their orbital periods and transit ephemerides could not be determined at that time. Here, we report on 50.8 days of new K2 observations of HIP 41378 from summer 2018. These data reveal additional transits of the long-period planets HIP 41378 d and HIP 41378 f, yielding a set of discrete possible orbital periods for these two planets. We identify the most probable orbital periods for these two planets using our knowledge of the planets' transit durations, the host star's properties, the system's dynamics, and data from the ground-based HATNet, KELT, and WASP transit surveys. Targeted photometric follow-up during the most probable future transit times will be able to determine the planets' orbital periods, and will enable future observations with facilities like the James Webb Space Telescope. The methods developed herein to determine the most probable orbital periods will be important for long-period planets detected by the Transiting Exoplanet Survey Satellite, where similar period ambiguities will frequently arise due to the telescope's survey strategy. 0 into PostgreSQL...\n",
      "Inserting test sample 2126  The search for exoplanets is a crucial topic in astronomy, and radial velocity surveys have been instrumental in discovering them. In this study, we present a detailed analysis of the radial velocity data obtained for HIP 41378, a nearby star known to host two long period gas giant planets. We use the radial velocity data to determine a discrete set of possible transit ephemerides for each planet and constrain their orbital parameters. We found that the planets have significant eccentricities, which can cause significant variation in their transit time. We calculated the full probability density function of each planet's transit times, which we can use to predict or model the upcoming transits. Additionally, we searched for additional planets in the system using transit timing variations, but we did not find any significant signals. We also constrained the mutual inclination angle of the planets to less than 25 degrees, ruling out the possibility of them being in a highly inclined configuration. Our results provide a deeper understanding of the orbital architecture of HIP 41378, adding to the growing body of knowledge regarding planetary systems beyond our own. 1 into PostgreSQL...\n",
      "Inserting test sample 2127  In frameworks of the scaling theory of phase transitions and critical phenomena the quantitative dependence of macroscopic properties on nanostructural parameters in a polymeric material is revealed. The draw ratios at neck and at break are referred to the macroscopic properties. The structure is characterized by an average thickness of amorphous layers in isotropic melt-crystallized linear high density polyethylene which is chosen as an example. The square of the neck draw ratio is equal to the product of the square of the draw ratio at break and the chain ends collision probability.\n",
      "\n",
      "This probability in its turn is proportional to the average thickness of amorphous layers in the isotropic material. The neck draw ratio is a parameter of order. Polymers with flexible chains are solutions in solid state as well as in melt, the interacting ends of a marked chain serving as a solvent. At critical polymerization degree all the phases are identical. This research is an important contribution to the molecular theory of polymer liquids. It has been found that the melt viscosity vs the molecular weight of linear flexible-chain polymer follows the power law with the 3.4-exponent within the reptation model near the critical point. This is different from the value 3 expected for the melt of ring macromolecules. The rotation vibration precession motion of chain ends about the polymer melt flow direction were taken into account to find better agreement with the experiment. 0 into PostgreSQL...\n",
      "Inserting test sample 2128  Self-organising dissipative polymer structures have recently emerged as a promising subject in the field of materials science. These dynamic systems have the unique ability to spontaneously form complex structures and patterns under the influence of external stimuli such as heat, light, and solvent. Understanding the principles underlying this self-assembly process has the potential to unlock significant technological breakthroughs in fields such as drug delivery, energy storage and molecular electronics. \n",
      "\n",
      "At the heart of these structures is a combination of energy dissipation and chemical reaction. Dissipative structures arise when systems far from equilibrium are driven by chemical or thermal energy inputs and dissipate energy as they evolve. In polymer-based systems, these dissipative energy sources can come from the input of light or heat, the introduction of new monomers, or the application of mechanical stress. This leads to the formation of polymers that exhibit a wide range of dynamic behaviors, such as oscillations, waves, and traveling fronts.\n",
      "\n",
      "Recent advances in fields such as microfluidics, nanotechnology, and 3D printing have enabled the creation of complex geometries and spatial patterns. By harnessing the self-organizing properties of dissipative polymer systems, researchers can design materials with specific functions and properties, opening up new possibilities for drug delivery and nanoelectronics. However, challenges remain, such as understanding the complex chemical and physical interactions within these systems, and developing robust methods to control and manipulate their properties.\n",
      "\n",
      "Overall, the study of self-organizing dissipative polymer structures holds great promise for the development of new materials with unprecedented properties and applications in diverse fields. 1 into PostgreSQL...\n",
      "Inserting test sample 2129  On-demand, high repetition rate sources of indistinguishable, polarised single photons are the key component for future photonic quantum technologies.\n",
      "\n",
      "Colour centres in diamond offer a promising solution, and the narrow line-width of the recently identified nickel-based NE8 centre makes it particularly appealing for realising the transform-limited sources necessary for quantum interference. Here we report the characterisation of dipole orientation and coherence properties of a single NE8 colour centre in a diamond nanocrystal at room-temperature. We observe a single photon coherence time of 0.21 ps and an emission lifetime of 1.5 ns. Combined with an emission wavelength that is ideally suited for applications in existing quantum optical systems, these results show that the NE8 is a far more promising source than the more commonly studied nitrogen-vacancy centre and point the way to the realisation of a practical diamond colour centre-based single photon source. 0 into PostgreSQL...\n",
      "Inserting test sample 2130  In this paper, we investigate the coherence properties of a single dipole emitter in diamond. By analyzing the emission spectra and lifetime measurements, we demonstrate the presence of coherent emission from a single nitrogen-vacancy center in diamond. A comparison between the coherent and incoherent contributions to the emission reveals a coherence time of several nanoseconds, consistent with theoretical predictions. Furthermore, we demonstrate that dipole-dipole interactions between emitters play a negligible role in the coherence properties of a single nitrogen-vacancy center. We also investigate the effect of temperature and photon energy on the coherence properties, demonstrating that coherence is robust at room temperature and that the coherence time decreases with increasing photon energy. Our findings have important implications for the development of quantum technologies, particularly those involving single photon sources and quantum information processing. 1 into PostgreSQL...\n",
      "Inserting test sample 2131  Cell motion and appearance have a strong correlation with cell cycle and disease progression. Many contemporary efforts in machine learning utilize spatio-temporal models to predict a cell's physical state and, consequently, the advancement of disease. Alternatively, generative models learn the underlying distribution of the data, creating holistic representations that can be used in learning. In this work, we propose an aggregate model that combine Generative Adversarial Networks (GANs) and Autoregressive (AR) models to predict cell motion and appearance in human neutrophils imaged by differential interference contrast (DIC) microscopy. We bifurcate the task of learning cell statistics by leveraging GANs for the spatial component and AR models for the temporal component. The aggregate model learned results offer a promising computational environment for studying changes in organellar shape, quantity, and spatial distribution over large sequences. 0 into PostgreSQL...\n",
      "Inserting test sample 2132  In this paper, we propose a generative spatiotemporal model of neutrophil behavior to advance our understanding of immune response mechanisms. Our model is designed to simulate the movement and interaction of neutrophils in response to inflammatory stimuli, taking into account the spatiotemporal characteristics of their behavior. To evaluate the efficacy of our model, we compared its predictions to experimental neutrophil behavior data. Our results demonstrate that our model can accurately simulate the spatiotemporal dynamics of neutrophil swarm behavior, and can capture the role of key physical and biochemical factors in this process. Ultimately, our generative spatiotemporal model can aid in the development of new strategies for modulating neutrophil activity which may have significant implications for the treatment of inflammatory diseases. 1 into PostgreSQL...\n",
      "Inserting test sample 2133  We present high angular resolution (down to 0.3\" = 13 AU in diameter) Submillimeter Array observations of the 880 micron (340 GHz) thermal continuum emission from circumstellar dust disks in the nearby HD 98800 and Hen 3-600 multiple star systems. In both cases, the dust emission is resolved and localized around one stellar component - the HD 98800 B and Hen 3-600 A spectroscopic binaries - with no evidence for circum-system material. Using two-dimensional Monte Carlo radiative transfer calculations, we compare the SMA visibilities and broadband spectral energy distributions with truncated disk models to empirically locate the inner and outer edges of both disks. The HD 98800 B disk appears to be aligned with the spectroscopic binary orbit, is internally truncated at a radius of 3.5 AU, and extends to only 10-15 AU from the central stars. The Hen 3-600 A disk is slightly larger, with an inner edge at 1 AU and an outer radius of 15-25 AU. These inferred disk structures compare favorably with theoretical predictions of their truncation due to tidal interactions with the stellar companions. 0 into PostgreSQL...\n",
      "Inserting test sample 2134  Truncated disks have been observed in millimeter-wave images of the TW Hya Association (TWA) multiple star systems, providing evidence of planet formation and migration. In this study, we present new observations of the TWA systems using the Atacama Large Millimeter/submillimeter Array (ALMA), which offer unprecedented resolution and sensitivity. We detect truncated dust disks in several of the TWA systems, including the young binary TWA 3A/3B and the close quadruple TWA 5A/5B/5C/5D. We fit the spectral energy distributions (SEDs) of the sources to a radiative transfer model to derive physical properties of the disks, such as mass and size. Our results suggest that the truncated disks in these systems are likely a result of disk clearing by one or more massive planets, or by photoevaporation. These observations provide valuable insights into the formation and evolution of planets in multiple star systems, and highlight the importance of high-resolution observations in understanding the complex architecture of protoplanetary disks. 1 into PostgreSQL...\n",
      "Inserting test sample 2135  This is the second in a series of papers intended to provide a basic overview of some of the major ideas in particle physics. Part I [arXiv:0810.3328] was primarily an algebraic exposition of gauge theories. We developed the group theoretic tools needed to understand the basic construction of gauge theory, as well as the physical concepts and tools to understand the structure of the Standard Model of Particle Physics as a gauge theory.\n",
      "\n",
      "In this paper (and the paper to follow), we continue our emphasis on gauge theories, but we do so with a more geometrical approach. We will conclude this paper with a brief discussion of general relativity, and save more advanced topics (including fibre bundles, characteristic classes, etc.) for the next paper in the series.\n",
      "\n",
      "We wish to reiterate that these notes are not intended to be a comprehensive introduction to any of the ideas contained in them. Their purpose is to introduce the \"forest\" rather than the \"trees\". The primary emphasis is on the algebraic/geometric/mathematical underpinnings rather than the calculational/phenomenological details. The topics were chosen according to the authors' preferences and agenda.\n",
      "\n",
      "These notes are intended for a student who has completed the standard undergraduate physics and mathematics courses, as well as the material contained in the first paper in this series. Having studied the material in the \"Further Reading\" sections of would be ideal, but the material in this series of papers is intended to be self-contained, and familiarity with the first paper will suffice. 0 into PostgreSQL...\n",
      "Inserting test sample 2136  Particle physics is a fundamental branch of physics that investigates the building blocks of matter and their interactions. This paper serves as a continuation of our previous work, delving deeper into the complexities of particle physics to provide a simple introduction for those new to the field.\n",
      "\n",
      "In this second part, we explore several topics related to particle physics, including quarks, leptons, and their various properties. We also discuss the Standard Model of particle physics, which is a theoretical framework that describes the behavior of particles and their interactions.\n",
      "\n",
      "One of the key concepts in particle physics is the Higgs boson, a particle that was first predicted in the 1960s and was discovered at CERN in 2012. We provide an overview of the discovery of the Higgs boson and its implications for the field of particle physics.\n",
      "\n",
      "Another important topic we cover is dark matter, a mysterious substance that makes up a significant portion of the universe but is currently beyond our ability to detect directly. We examine some of the evidence for the existence of dark matter and discuss some of the proposed theories for its nature.\n",
      "\n",
      "Finally, we discuss the future of particle physics and some of the exciting experiments that are currently underway, such as the Large Hadron Collider and the Deep Underground Neutrino Experiment.\n",
      "\n",
      "Overall, this paper aims to provide a comprehensive introduction to the field of particle physics, covering both theoretical concepts and experimental discoveries. It is our hope that this work will inspire a new generation to explore the mysteries of the universe at the most fundamental level. 1 into PostgreSQL...\n",
      "Inserting test sample 2137  In this paper we give an extension of the classical Caffarelli-Kohn-Nirenberg inequalities: we show that for $1<p,q<\\infty$, $0<r<\\infty$ with $p+q\\geq r$, $\\delta\\in[0,1]\\cap\\left[\\frac{r-q}{r},\\frac{p}{r}\\right]$ with $\\frac{\\delta r}{p}+\\frac{(1-\\delta)r}{q}=1$ and $a$, $b$, $c\\in\\mathbb{R}$ with $c=\\delta(a-1)+b(1-\\delta)$, and for all functions $f\\in C_{0}^{\\infty}(\\mathbb{R}^{n}\\backslash\\{0\\})$ we have $$ \\||x|^{c}f\\|_{L^{r}(\\mathbb{R}^{n})} \\leq \\left|\\frac{p}{n-p(1-a)}\\right|^{\\delta} \\left\\||x|^{a}\\nabla f\\right\\|^{\\delta}_{L^{p}(\\mathbb{R}^{n})} \\left\\||x|^{b}f\\right\\|^{1-\\delta}_{L^{q}(\\mathbb{R}^{n})} $$ for $n\\neq p(1-a)$, where the constant $\\left|\\frac{p}{n-p(1-a)}\\right|^{\\delta}$ is sharp for $p=q$ with $a-b=1$ or $p\\neq q$ with $p(1-a)+bq\\neq0$.\n",
      "\n",
      "In the critical case $n=p(1-a)$ we have $$ \\left\\||x|^{c}f\\right\\|_{L^{r}(\\mathbb{R}^{n})} \\leq p^{\\delta} \\left\\||x|^{a}\\log|x|\\nabla f\\right\\|^{\\delta}_{L^{p}(\\mathbb{R}^{n})} \\left\\||x|^{b}f\\right\\|^{1-\\delta}_{L^{q}(\\mathbb{R}^{n})}. $$ Moreover, we also obtain anisotropic versions of these inequalities which can be conveniently formulated in the language of Folland and Stein's homogeneous groups. Consequently, we obtain remainder estimates for $L^{p}$-weighted Hardy inequalities on homogeneous groups, which are also new in the Euclidean setting of $\\mathbb{R}^{n}$. The critical Hardy inequalities of logarithmic type and uncertainty type principles on homogeneous groups are obtained. Moreover, we investigate another improved version of $L^{p}$-weighted Hardy inequalities involving a distance and stability estimates. We also establish sharp Hardy type inequalities in $L^{p}$, $1<p<\\infty$, with superweights, i.e. with the weights of the form $\\frac{(a+b|x|^{\\alpha})^{\\frac{\\beta}{p}}}{|x|^{m}}$ allowing for different choices of $\\alpha$ and $\\beta$. 0 into PostgreSQL...\n",
      "Inserting test sample 2138  This paper explores extended Caffarelli-Kohn-Nirenberg inequalities and their relationship to remainders, stability, and superweights in the context of $L^{p}$-weighted Hardy inequalities. We first introduce the notion of extended Caffarelli-Kohn-Nirenberg inequality, which generalizes the classical Caffarelli-Kohn-Nirenberg inequality. We provide a proof of this new inequality and discuss its implications in the context of weighted Hardy inequalities.\n",
      "\n",
      "Additionally, we investigate the role of remainders in $L^{p}$-weighted Hardy inequalities. Specifically, we derive a precise estimate of the remainder term in the Hardy inequality and apply it to prove a generalized version of the inequality. We extend our analysis to include superweights, which allow for weaker conditions on the weight function, and discuss the associated stability results.\n",
      "\n",
      "In our investigation, we prove several new results that shed light on the relationships between Caffarelli-Kohn-Nirenberg inequalities, weighted Hardy inequalities, remainders, stability, and superweights. Specifically, we prove a stability result for the Caffarelli-Kohn-Nirenberg inequality under certain conditions, as well as a stability result for the weighted Hardy inequality with superweights.\n",
      "\n",
      "Overall, our research demonstrates the importance and richness of understanding the connections between various inequalities in analysis, and highlights the usefulness and significance of extended Caffarelli-Kohn-Nirenberg inequalities in particular. 1 into PostgreSQL...\n",
      "Inserting test sample 2139  Context: Meta programming consists for a large part of matching, analyzing, and transforming syntax trees. Many meta programming systems process abstract syntax trees, but this requires intimate knowledge of the structure of the data type describing the abstract syntax. As a result, meta programming is error-prone, and meta programs are not resilient to evolution of the structure of such ASTs, requiring invasive, fault-prone change to these programs.\n",
      "\n",
      "Inquiry: Concrete syntax patterns alleviate this problem by allowing the meta programmer to match and create syntax trees using the actual syntax of the object language. Systems supporting concrete syntax patterns, however, require a concrete grammar of the object language in their own formalism. Creating such grammars is a costly and error-prone process, especially for realistic languages such as Java and C++. Approach: In this paper we present Concretely, a technique to extend meta programming systems with pluggable concrete syntax patterns, based on external, black box parsers. We illustrate Concretely in the context of Rascal, an open-source meta programming system and language workbench, and show how to reuse existing parsers for Java, JavaScript, and C++. Furthermore, we propose Tympanic, a DSL to declaratively map external AST structures to Rascal's internal data structures. Tympanic allows implementors of Concretely to solve the impedance mismatch between object-oriented class hierarchies in Java and Rascal's algebraic data types. Both the algebraic data type and AST marshalling code is automatically generated. Knowledge: The conceptual architecture of Concretely and Tympanic supports the reuse of pre-existing, external parsers, and their AST representation in meta programming systems that feature concrete syntax patterns for matching and constructing syntax trees. As such this opens up concrete syntax pattern matching for a host of realistic languages for which writing a grammar from scratch is time consuming and error-prone, but for which industry-strength parsers exist in the wild. Grounding: We evaluate Concretely in terms of source lines of code (SLOC), relative to the size of the AST data type and marshalling code. We show that for real programming languages such as C++ and Java, adding support for concrete syntax patterns takes an effort only in the order of dozens of SLOC. Similarly, we evaluate Tympanic in terms of SLOC, showing an order of magnitude of reduction in SLOC compared to manual implementation of the AST data types and marshalling code. Importance: Meta programming has applications in reverse engineering, reengineering, source code analysis, static analysis, software renovation, domain-specific language engineering, and many others. Processing of syntax trees is central to all of these tasks.\n",
      "\n",
      "Concrete syntax patterns improve the practice of constructing meta programs.\n",
      "\n",
      "The combination of Concretely and Tympanic has the potential to make concrete syntax patterns available with very little effort, thereby improving and promoting the application of meta programming in the general software engineering context. 0 into PostgreSQL...\n",
      "Inserting test sample 2140  Concrete syntax is an essential component of computer languages. Despite its critical role, the development of tools to support it has been slow. Black box parsers, however, offer a potential solution to this longstanding issue. By treating the syntax of languages as abstract objects, black box parsers provide a way to create languages without being bound by specific syntax rules.\n",
      "\n",
      "The flexibility of black box parsers is one of their key advantages. Rather than relying on a specific set of syntax rules, they can parse input according to any set of rules. This enables developers to experiment with different syntaxes, create syntaxes that fit their specific needs, or even build new languages that are not bound by traditional syntax rules.\n",
      "\n",
      "Another benefit of black box parsers is their ability to facilitate the use of concrete syntax in large-scale software projects. With black box parsers, developers can separate the concerns of language specification and language use. This allows developers to focus their efforts on the parts of the software that matter most, without being bogged down by the intricacies of language syntax.\n",
      "\n",
      "In addition to their flexibility and scalability, black box parsers are also highly efficient. They can rapidly parse large amounts of code, making them an ideal solution for complex software projects.\n",
      "\n",
      "Despite their many advantages, black box parsers are not without their limitations. For one, they can be difficult to understand, particularly for developers who are not familiar with their inner workings. Additionally, because of their abstract nature, it can be challenging to create error messages that are helpful to end-users.\n",
      "\n",
      "Nonetheless, the potential benefits of black box parsers make them a promising tool for software development. They offer a flexible, scalable, and efficient way of incorporating concrete syntax into large-scale software projects. While there are still challenges to be addressed, black box parsers represent a significant step forward in the development of computer languages. 1 into PostgreSQL...\n",
      "Inserting test sample 2141  We present the first speckle interferometric observations of R CrB, the prototype of a class of peculiar stars which undergo irregular declines in their visible light curves. The observations were carried out with the 6m telescope at the Special Astrophysical Observatory near maximum light (V=7, 1996 Oct. 1) and at minimum light (V=10.61, 1999 Sep. 28). A spatial resolution of 75mas was achieved in the K-band. The dust shell around R CrB is partially resolved, and the visibility is approximately 0.8 at a spatial frequency of 10 cycles/arcsec. The two-dimensional power spectra obtained at both epochs do not show any significant deviation from circular symmetry. The visibility function and spectral energy distribution obtained near maximum light can be simultaneously fitted with a model consisting of the central star and an optically thin dust shell with density proportional to r^-2. The inner boundary of the shell is found to be 82Rstar (19mas) with a temperature of 920K.\n",
      "\n",
      "However, this simple model fails to simultaneously reproduce the visibility and spectral energy distribution obtained at minimum light. We show that this discrepancy can be attributed to thermal emission from a newly formed dust cloud. 0 into PostgreSQL...\n",
      "Inserting test sample 2142  This paper presents a detailed study of the circumstellar envelope of R CrB, a variable star that exhibits irregular dimming behavior. We utilized diffraction-limited speckle interferometry to acquire high-resolution images of the star during its maximum and minimum light phases. With these images, we resolved the innermost regions of the envelope, and we report on the observed changes in the morphology of the photosphere and surrounding environment during the two phases. We also developed a model for the envelope using a combination of radiative transfer and hydrodynamic simulations. Our model accounts for the variable opacity and ionization state of the gas as well as the scattering and polarization effects of the dust. By comparing the model predictions with the interferometric data, we were able to constrain the parameters of the envelope and confirm the presence of a dense torus-like structure near the star's equatorial plane. Our results provide insights into the mechanisms responsible for R CrB's variability and suggest that a combination of pulsation, dust formation, and mass loss are involved in shaping the star's envelope. 1 into PostgreSQL...\n",
      "Inserting test sample 2143  The extragalactic distance scale builds directly on the Cepheid Period-Luminosity (PL) relation as delineated by the sample of Cepheids in the Large Magellanic Cloud (LMC). However, the LMC is a dwarf irregular galaxy, quite different from the massive spiral galaxies used for calibrating the extragalactic distance scale. Recent investigations suggest that not only the zero-point but also the slope of the Milky Way PL relation differ significantly from that of the LMC, casting doubts on the universality of the Cepheid PL relation. We want to make a differential comparison of the PL relations in the two galaxies by delineating the PL relations using the same method, the infrared surface brightness method (IRSB), and the same precepts. The IRSB method is a Baade-Wesselink type method to determine individual distances to Cepheids. We apply a newly revised calibration of the method as described in an accompanying paper (Paper I) to 36 LMC and five SMC Cepheids and delineate new PL relations in the V,I,J, & K bands as well as in the Wesenheit indices in the optical and near-IR. We present 509 new and accurate radial velocity measurements for a sample of 22 LMC Cepheids, enlarging our earlier sample of 14 stars to include 36 LMC Cepheids. The new calibration of the IRSB method is directly tied to the recent HST parallax measurements to ten Milky Way Cepheids, and we find a LMC barycenter distance modulus of 18.45+-0.04 (random error only) from the 36 individual LMC Cepheid distances. We find a significant metallicity effect on the Wvi index gamma(Wvi)=-0.23+-0.10 mag/dex as well as an effect on the slope. The K-band PL relation on the other hand is found to be an excellent extragalactic standard candle being metallicity insensitive in both slope and zero-point and at the same time being reddening insensitive and showing the least internal dispersion. 0 into PostgreSQL...\n",
      "Inserting test sample 2144  This study explores the impact of metallicity and the distance to the Large Magellanic Cloud (LMC) on the calibration of the Cepheid Period-Luminosity (PL) relation using the infrared surface brightness (IRSB) technique. The Cepheid PL relation is essential for measuring cosmic distances and is widely used in the field of astronomy. However, it is known that the relation depends on metallicity, with lower metallicity stars exhibiting weaker line-blanketing, leading to a hotter and bluer temperature, and thus affecting their luminosity. We conduct a detailed analysis of the IRSB technique, which relies on calibrating the angular size of Cepheids using their surface brightness and infrared photometry. Testing this technique on simulated and observational data, we find that metallicity has a significant impact on the PL relation, and using a metallicity-dependent calibration can significantly reduce systematic errors in distance measurements. We also find that using the IRSB technique with Cepheids in the LMC can lead to better calibration of the PL relation compared to relying solely on Milky Way Cepheids. The LMC offers a unique opportunity to study Cepheids at a known distance, which can help refine the calibration of the PL relation and reduce uncertainties in cosmic distances. We conclude that the IRSB technique can provide a reliable and accurate calibration of the Cepheid PL relation, and that metallicity and the distance to the LMC should be taken into account when using this technique to calibrate the PL relation for cosmological applications. Our results have important implications for the precise measurement of the Hubble constant and the understanding of the expansion history of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2145  The class of traveling wave solutions of the sine-Gordon equation is known to be in 1-1 correspondence with the class of (necessarily singular) pseudospherical surfaces in Euclidean space with screw-motion symmetry: the pseudospherical helicoids. We explicitly describe all pseudospherical helicoids in terms of elliptic functions. This solves a problem posed by Popov [Lobachevsky geometry and modern nonlinear problems, Birkh\\\"auser/Springer, Cham, 2014]. As an application, countably many continuous families of topologically embedded pseudospherical helicoids are constructed. A (singular) pseudospherical helicoid is proved to be either a dense subset of a region bounded by two coaxial cylinders, a topologically immersed cylinder with helical self-intersections, or a topologically embedded cylinder with helical singularities, called for short a pseudospherical twisted column.\n",
      "\n",
      "Pseudospherical twisted columns are characterized by four phenomenological invariants: the helicity $\\eta\\in \\mathbb{Z}_2$, the parity $\\epsilon\\in \\mathbb{Z}_2$, the wave number $\\mathfrak n\\in \\mathbb{N}$, and the aspect ratio $\\mathfrak{d}>0$, up to translations along the screw axis. A systematic procedure for explicitly determining all pseudospherical twisted columns from the invariants is provided. 0 into PostgreSQL...\n",
      "Inserting test sample 2146  In this paper, we investigate topologically embedded pseudospherical cylinders, which are surfaces with constant negative Gaussian curvature. We explore the geometric properties of such surfaces, including their curvature and the behavior of geodesics on them. Additionally, we investigate the way in which these surfaces can be embedded in other spaces, such as three-dimensional space or higher-dimensional spaces, and the implications of these embeddings for the behavior of the surfaces. Specifically, we consider embeddings in which the surface intersects itself, as well as those in which it does not. We also investigate the relationship between the curvature of these surfaces and the degree to which they can be embedded in other spaces. Our results provide insights into the geometric properties of these surfaces and their behavior under various embeddings, which may have applications in physics, engineering, and other fields in which the properties of surfaces are of interest. 1 into PostgreSQL...\n",
      "Inserting test sample 2147  We present a unified description of extremal metrics for the Laplace and Steklov eigenvalues on manifolds of arbitrary dimension using the notion of $n$-harmonic maps. Our approach extends the well-known results linking extremal metrics for eigenvalues on surfaces with minimal immersions and harmonic maps.\n",
      "\n",
      "In the process, we uncover two previously unknown features of the Steklov eigenvalues. First, we show that in higher dimensions there is a unique normalization involving both the volume of the boundary and of the manifold itself, which leads to meaningful extremal eigenvalue problems. Second, we observe that the critical points of the eigenvalue functionals in a fixed conformal class have a natural geometric interpretation provided one considers the Steklov problem with a density. As an example, we construct a family of free boundary harmonic annuli in the three-dimensional ball and conjecture that they correspond to metrics maximizing the first Steklov eigenvalue in their respective conformal classes. 0 into PostgreSQL...\n",
      "Inserting test sample 2148  This paper explores the possibility of constructing extremal metrics on Laplace and Steklov eigenvalue problems through the use of $n$-harmonic maps. $n$-harmonic maps are a generalization of harmonic maps and have been a topic of recent interest in the field of geometric analysis. By using this technique, the authors demonstrate the existence of infinitely many non-equivalent extremal metrics and provide estimates for their eigenvalues. Furthermore, they investigate the relationship between these extremal metrics and the underlying geometry of the domains on which they are defined. This work has potential applications in various fields of mathematics and physics, including the study of minimal surfaces and the behavior of SchrÃ¶dinger operators. Overall, this paper contributes to a deeper understanding of the geometry and analysis of various eigenvalue problems and provides a foundation for future research in this area. 1 into PostgreSQL...\n",
      "Inserting test sample 2149  Observations at millimetre wavelengths provide a valuable tool to study the small scale dynamics in the solar chromosphere. We evaluate the physical conditions of the atmosphere in the presence of a propagating shock wave and link that to the observable signatures in mm-wavelength radiation, providing valuable insights into the underlying physics of mm-wavelength observations. A realistic numerical simulation from the 3D radiative Magnetohydrodynamic (MHD) code Bifrost is used to interpret changes in the atmosphere caused by shock wave propagation. High-cadence (1 s) time series of brightness temperature (T$_\\text{b}$) maps are calculated with the Advanced Radiative Transfer (ART) code at the wavelengths $1.309$ mm and $1.204$ mm, which represents opposite sides of spectral band~$6$ of the Atacama Large Millimeter/submillimeter Array (ALMA). An example of shock wave propagation is presented. The brightness temperatures show a strong shock wave signature with large variation in formation height between $\\sim0.7$ to $1.4$ Mm. The results demonstrate that millimetre brightness temperatures efficiently track upwardly propagating shock waves in the middle chromosphere. In addition, we show that the gradient of the brightness temperature between wavelengths within ALMA band $6$ can potentially be utilised as a diagnostics tool in understanding the small-scale dynamics at the sampled layers. 0 into PostgreSQL...\n",
      "Inserting test sample 2150  This paper presents a study on shock wave signatures detected at millimetre wavelengths through Bifrost simulations. The behaviour of these waves is fundamental to understand a number of astrophysical phenomena such as supernova explosions, star-forming regions, and accretion disks around black holes. In this research, we analysed the properties of the shock waves in the Bifrost simulations, which allow us to observe the wave formation and propagation over time. Our findings demonstrate that the millimetre wavelength data can successfully detect the signals emitted by the shock waves. Moreover, we identified three distinct types of shock waves and studied their characteristics, including their impact on the surrounding magnetic fields. Our observations revealed that the shock waves tend to compress the magnetic fields, thus increasing their strength. Finally, we discuss the practical implications of our research, highlighting its relevance for developing new instruments aimed at exploring shock waves in a variety of astrophysical contexts. 1 into PostgreSQL...\n",
      "Inserting test sample 2151  In this extended abstract we provide a unifying framework that can be used to characterize and compare the expressive power of query languages for different data base models. The framework is based upon the new idea of valid partition, that is a partition of the elements of a given data base, where each class of the partition is composed by elements that cannot be separated (distinguished) according to some level of information contained in the data base. We describe two applications of this new framework, first by deriving a new syntactic characterization of the expressive power of relational algebra which is equivalent to the one given by Paredaens, and subsequently by studying the expressive power of a simple graph-based data model. 0 into PostgreSQL...\n",
      "Inserting test sample 2152  This paper presents a unifying framework to quantify the power of a language to express relations between objects. We introduce a formal definition of relational structures and use it to create a benchmark for measuring the capacity of various languages to express these structures. Our framework provides a novel perspective on the trade-off between expressiveness and complexity, and allows us to compare languages both with each other and with respect to other natural phenomena, such as algorithmic complexity. Using the benchmark, we explore the relational power of a variety of languages, including propositional logic, first-order logic, and natural language. Our results demonstrate the efficacy of our approach and provide insight into the inherent complexity of real-world language use. 1 into PostgreSQL...\n",
      "Inserting test sample 2153  The paper deals with the semi-classical behaviour of quantum dynamics for a semi-classical completely integrable system with two degrees of freedom near Liouville regular torus. The phenomomenon of wave packet revivals is demonstrated in this article. The framework of this paper is semi-classical analysis (limit :). For the proofs we use standard tools of real analysis, Fourier analysis and basic analytic number theory. 0 into PostgreSQL...\n",
      "Inserting test sample 2154  We investigate quantum revivals in integrable systems with two degrees of freedom, focusing on the torus case. Through numerical simulations and analytical calculations, we identify the characteristic features of the quantum recurrence, such as the revival time and the structure of the wave function. Our findings shed light on the interplay between classical integrability and quantum mechanics in the context of quantum chaos. 1 into PostgreSQL...\n",
      "Inserting test sample 2155  Rest-frame mid- to far-infrared spectroscopy is a powerful tool to study how galaxies formed and evolved, because a major part of their evolution occurs in heavily dust enshrouded environments, especially at the so-called Cosmic Noon.\n",
      "\n",
      "Using the calibrations of IR lines we predict the expected fluxes of lines and features, with the aim to measure the star formation rate and the Black Hole Accretion rate in intermediate to high redshift galaxies. The launch of the James Webb Space Telescope will allow us a deep investigation of both the SF and the BHA obscured processes as a function of cosmic time. We assess the spectral lines and features that can be detected by JWST-MIRI in galaxies and Active Galactic Nuclei up to redshift z= 3. We confirm the fine-structure lines of [MgIV]4.49um and [ArVI]4.53um as good BHA rate tracers for the 1<z<3 range, and we propose the [NeVI]7.65um line as the best tracer for redshifts of z<1.5.\n",
      "\n",
      "We suggest the use of the [ArII]6.98um and [ArIII]8.99um lines to measure the SF rate, for z<3 and z<2. At higher redshifts, the PAH features at 6.2um and 7.7um can be observed at z<3 and z<2.7 respectively. Rest-frame far-IR spectroscopy is currently being collected in high redshift galaxies (z>3) with the Atacama Large Millimeter Array. We confirm that the [CII]158um line is a good tracer of the SF rate and can in most cases (0.9<z<2 and 3<z<9) be observed, and we propose the use of the combination of [OIII]88um and [OI]145um lines as an alternative SF rate tracer, that can be detected above z>3. We conclude, however, that the current and foreseen facilities will not be able to cover properly the peak of the obscured SF and BHA activities at the Cosmic Noon of galaxy evolution and a new IR space telescope, actively cooled to obtain very good sensitivities, covering the full IR spectral range from about 10um to 300um, will be needed. 0 into PostgreSQL...\n",
      "Inserting test sample 2156  The study of galaxy evolution has been a central focus of astrophysics for decades. In particular, the use of infrared and submillimeter spectroscopy has proven to be a powerful tool for measuring the key physical processes that govern galaxy formation and evolution. In this paper, we present a review of recent progress in this area, focusing on the use of the James Webb Space Telescope (JWST) and the Atacama Large Millimeter/submillimeter Array (ALMA) to measure star formation and black hole accretion in galaxies.\n",
      "\n",
      "Star formation is one of the most fundamental processes in galaxy evolution. Understanding how and when galaxies form stars is essential for building a complete picture of their evolution. Infrared spectroscopy is particularly well-suited to this task, as it can probe the cold, dusty regions where stars are typically formed. The JWST, with its advanced infrared instrumentation, promises to revolutionize our understanding of star formation in galaxies.\n",
      "\n",
      "In addition to star formation, black hole accretion is another key process that shapes galaxy evolution. The growth of supermassive black holes can have a profound influence on the surrounding galaxy, altering its structure and evolution. Submillimeter spectroscopy is a powerful tool for studying the accretion process, as it can detect the faint emission from the cold dust and gas that surrounds a growing black hole. ALMA, with its high sensitivity and resolution, is ideally suited for such observations.\n",
      "\n",
      "We present a number of case studies where JWST and ALMA observations have been used to explore these key processes in galaxy evolution. These include studies of galaxy mergers, galaxy clusters, and high-redshift galaxies, all of which provide unique insights into the physics of star formation and black hole accretion in different environments.\n",
      "\n",
      "Overall, the use of infrared and submillimeter spectroscopy with JWST and ALMA is transforming our understanding of galaxy evolution. These powerful tools are allowing us to explore the physics of star formation and black hole accretion in unprecedented detail, and are providing new insights into the processes that shape galaxies over cosmic time. 1 into PostgreSQL...\n",
      "Inserting test sample 2157  We present the structural and dynamical studies of layered vanadium pentaoxide (V2O5). The temperature dependent X-ray diffraction measurements reveal highly anisotropic and anomalous thermal expansion from 12 K to 853 K.\n",
      "\n",
      "The results do not show any evidence of structural phase transition or decomposition of {\\alpha}-V2O5, contrary to the previous transmission electron microscopy (TEM) and electron energy loss spectroscopy (EELS) experiments. The inelastic neutron scattering measurements performed up to 673 K corroborate the result of our X-ray diffraction measurements. The analysis of the experimental data is carried out using ab-initio lattice dynamics calculation. The important role of van der-Waals dispersion and Hubbard interactions on the structure and dynamics is revealed through the ab-initio calculations. The calculated anisotropic thermal expansion behavior agrees well with temperature dependent X- ray diffraction. The mechanism of anisotropic thermal expansion and anisotropic linear compressibility is discussed in terms of calculated anisotropy in Gr\\\"uneisen parameters and elastic coefficients. The calculated Gibbs free energy in various phases of V2O5 is used to understand the high pressure and temperature phase diagram of the compound. Softening of elastic constant (C66) with pressure suggests a possibility of shear mechanism for {\\alpha} to \\b{eta} phase transformation under pressure. 0 into PostgreSQL...\n",
      "Inserting test sample 2158  Vanadium Pentaoxide (V2O5) is an important oxide material with several industrial applications due to its unique physical and chemical properties, such as its optical and electrical conductivity, as well as its catalytic properties. In this study, the anomalous lattice behavior of V2O5 was investigated using X-ray diffraction, inelastic neutron scattering, and ab-initio lattice dynamics. Through X-ray diffraction, the crystal structure of V2O5 was analyzed, revealing that the material undergoes significant structural changes upon heating. Inelastic neutron scattering provided insights into the vibrational modes of the lattice and the presence of anomalous thermal expansion across different orientations of the crystal lattice. Ab-initio lattice dynamics simulations were then used to elucidate the underlying physical mechanisms responsible for the anomalous behavior observed in the experiments. The findings suggest that a combination of enhanced vibrational entropy and thermal expansion anisotropy play critical roles in the observed anomalous behavior of V2O5. This work provides a comprehensive understanding of the structural and vibrational properties of V2O5 and sheds light on the potential applications of this material in various fields, including catalysis, sensors, and energy storage. 1 into PostgreSQL...\n",
      "Inserting test sample 2159  We present a spectrally and temporally resolved detection of the optical Mg I triplet at 7.8$\\sigma$ in the extended atmosphere of the ultra-hot Jupiter KELT-9 b, adding to the list of detected metal species in the hottest gas giant currently known. Constraints are placed on the density and radial extent of the excited hydrogen envelope using simultaneous observations of H$\\alpha$ and H$\\beta$ under the assumption of a spherically symmetric atmosphere. We find that planetary rotational broadening of $v_\\text{rot} = 8.2^{+0.6}_{-0.7}$ km s$^{-1}$ is necessary to reproduce the Balmer line transmission profile shapes, where the model including rotation is strongly preferred over the non-rotating model using a Bayesian information criterion comparison. The time-series of both metal line and hydrogen absorption show remarkable structure, suggesting that the atmosphere observed during this transit is dynamic rather than static.\n",
      "\n",
      "We detect a relative emission feature near the end of the transit which exhibits a P-Cygni-like shape, evidence of material moving at $\\approx 50-100$ km s$^{-1}$ away from the planet. We hypothesize that the in-transit variability and subsequent P-Cygni-like profiles are due to a flaring event that caused the atmosphere to expand, resulting in unbound material being accelerated to high speeds by stellar radiation pressure. Further spectroscopic transit observations will help establish the frequency of such events. 0 into PostgreSQL...\n",
      "Inserting test sample 2160  KELT-9 b is an ultra-hot Jupiter with a highly elliptical orbit that results in a variable transit depth. Our study aimed to investigate the role of atmospheric dynamics in driving the observed variability in the transit of KELT-9 b across the visible disk of its host star. We used the Heliospheric version of the WRF model to simulate the planet's atmospheric dynamics and computed a light curve using the method of exoplanetary transit spectroscopy. We found that the transit depth of KELT-9 b is modulated by the presence of an atmospheric jet that results in a shift of the planetary terminator. The jet is associated with the non-synchronous rotation of the planet, which drives the development of an eastward equatorial super-rotating jet. Our simulations also indicate that the planet's atmosphere is tidally distorted and exhibits a day-night temperature contrast of up to 4,000 K, which leads to large-scale atmospheric circulation patterns such as a western hot-spot offset from the substellar point. Our results demonstrate the importance of atmospheric dynamics in shaping the observable features of exoplanets and underline the need for high-precision, multi-wavelength observations to better constrain the properties of exoplanetary atmospheres. 1 into PostgreSQL...\n",
      "Inserting test sample 2161  Color-center-hosting semiconductors are emerging as promising source materials for low-field dynamic nuclear polarization (DNP) at or near room temperature, but hyperfine broadening, susceptibility to magnetic field heterogeneity, and nuclear spin relaxation induced by other paramagnetic defects set practical constraints difficult to circumvent. Here, we explore an alternate route to color-center-assisted DNP using nitrogen-vacancy (NV) centers in diamond coupled to substitutional nitrogen impurities, the so-called P1 centers. Working near the level anti-crossing condition - where the P1 Zeeman splitting matches one of the NV spin transitions - we demonstrate efficient microwave-free 13C DNP through the use of consecutive magnetic field sweeps and continuous optical excitation. The amplitude and sign of the polarization can be controlled by adjusting the low-to-high and high-to-low magnetic field sweep rates in each cycle so that one is much faster than the other. By comparing the 13C DNP response for different crystal orientations, we show that the process is robust to magnetic field/NV misalignment, a feature that makes the present technique suitable to diamond powders and settings where the field is heterogeneous. Applications to shallow NVs could capitalize on the greater physical proximity between surface paramagnetic defects and outer nuclei to efficiently polarize target samples in contact with the diamond crystal. 0 into PostgreSQL...\n",
      "Inserting test sample 2162  The enhancement of NMR signals via dynamic nuclear polarization (DNP) has fostered significant interest within the scientific community, particularly in the field of material sciences. In this work, we present a microwave-free approach for DNP in diamond, utilizing an 'integrated' cross effect involving the interplay of electron-nuclear and nuclear-nuclear spin interactions. We find that coupling the electron spin to multiple nuclear spins results in a significant increase in the DNP efficiency, leading to an enhancement of the nuclear polarization by a factor of up to 700. Our results suggest that this approach could be a promising alternative to conventional DNP in a number of materials applications. To thoroughly understand the mechanism behind the 'integrated' cross effect, we perform EPR, ENDOR, and DNP measurements in diamond particles with systematically varied nitrogen concentration. Our study offers insights into the potential of microwave-free DNP methods in diamond for future material science research. Our results could also pave the way for more efficient DNP in other materials and could serve as a reference for the wider community of scientists working in this field. 1 into PostgreSQL...\n",
      "Inserting test sample 2163  Recent XMM-Newton observations reveal an extended (150\") low-surface brightness X-ray halo in the supernova remnant G21.5-0.9. The near circular symmetry, the lack of any limb brightening and the non-thermal spectral form, all favour an interpretation of this outer halo as an extension of the central synchrotron nebula rather than as a shell formed by the supernova blast wave and ejecta. The X-ray spectrum of the nebula exhibits a marked spectral softening with radius, with the power-law spectral index varying from Gamma = 1.63 +/- 0.04 in the core to Gamma = 2.45 +/- 0.06 at the edge of the halo.\n",
      "\n",
      "Similar spectral trends are seen in other Crab-like remnants and reflect the impact of the synchrotron radiation losses on very high energy electrons as they diffuse out from the inner nebula. A preliminary timing analysis provides no evidence for any pulsed X-ray emission from the core of G21.5-0.9. 0 into PostgreSQL...\n",
      "Inserting test sample 2164  The Crab-like supernova remnant (SNR) G21.5-0.9 is among the brightest and most extensively studied X-ray sources in our galaxy. Recent X-ray observations have revealed that the remnant possesses an extended halo of X-ray emission beyond its central pulsar wind nebula. Here, we present a study of this extended halo based on new observational data from the Chandra X-ray Observatory. By analyzing the morphology and spectral properties of the halo emission, we estimate the contribution of different physical components to the overall X-ray emission of G21.5-0.9. We find that the extended halo emission is dominated by synchrotron radiation produced by high-energy electrons of the SNR shock wave. Our study sheds light on the processes that govern the evolution of SNRs and helps to deepen our understanding of the sources of X-ray emission in our galaxy. 1 into PostgreSQL...\n",
      "Inserting test sample 2165  We give three applications of a recently-proven \"Decomposition Lemma,\" which allows one to count preimages of certain sets of permutations under West's stack-sorting map $s$. We first enumerate the permutation class $s^{-1}(\\text{Av}(231,321))=\\text{Av}(2341,3241,45231)$, finding a new example of an unbalanced Wilf equivalence. This result is equivalent to the enumeration of permutations sortable by ${\\bf B}\\circ s$, where ${\\bf B}$ is the bubble sort map. We then prove that the sets $s^{-1}(\\text{Av}(231,312))$, $s^{-1}(\\text{Av}(132,231))=\\text{Av}(2341,1342,\\underline{32}41,\\underline{31}42)$, and $s^{-1}(\\text{Av}(132,312))=\\text{Av}(1342,3142,3412,34\\underline{21})$ are counted by the so-called \"Boolean-Catalan numbers,\" settling a conjecture of the current author and another conjecture of Hossain. This completes the enumerations of all sets of the form $s^{-1}(\\text{Av}(\\tau^{(1)},\\ldots,\\tau^{(r)}))$ for $\\{\\tau^{(1)},\\ldots,\\tau^{(r)}\\}\\subseteq S_3$ with the exception of the set $\\{321\\}$. We also find an explicit formula for $|s^{-1}(\\text{Av}_{n,k}(231,312,321))|$, where $\\text{Av}_{n,k}(231,312,321)$ is the set of permutations in $\\text{Av}_n(231,312,321)$ with $k$ descents.\n",
      "\n",
      "This allows us to prove a conjectured identity involving Catalan numbers and order ideals in Young's lattice. 0 into PostgreSQL...\n",
      "Inserting test sample 2166  This research paper investigates the task of enumerating stack-sorting preimages for a fixed permutation. Specifically, we introduce a novel decomposition lemma which provides a way to separate preimages of a given permutation into smaller, independent subproblems. This lemma is then utilized to define a recursive algorithm for enumerating all preimages of a permutation, which is shown to be computationally efficient in practice.\n",
      "\n",
      "To demonstrate the effectiveness of our approach, we present experimental results on a variety of permutations, showing that our algorithm outperforms existing methods in terms of running time. Additionally, we provide a detailed analysis of the complexity of our algorithm, including both worst-case and average-case running times.\n",
      "\n",
      "Our work makes a significant contribution to the area of combinatorial algorithms for sorting by providing a new approach for efficiently enumerating stack-sorting preimages. Our decomposition lemma offers a valuable tool for decomposing large combinatorial problems into smaller, more manageable subproblems, potentially opening up new avenues for solving other challenging combinatorial problems.\n",
      "\n",
      "Overall, our findings show that the enumeration of stack-sorting preimages via a decomposition lemma has practical implications for sorting algorithms and provides a valuable theoretical tool for solving combinatorial problems. 1 into PostgreSQL...\n",
      "Inserting test sample 2167  We discovered a sample of 250 Ly-Alpha emitting (LAE) galaxies at z=2.1 in an ultra-deep 3727 A narrow-band MUSYC image of the Extended Chandra Deep Field-South. LAEs were selected to have rest-frame equivalent widths (EW) > 20 A and emission line fluxes > 2.0 x 10^(-17)erg /cm^2/s, after carefully subtracting the continuum contributions from narrow band photometry. The median flux of our sample is 4.2 x 10^(-17)erg/cm^2/s, corresponding to a median Lya luminosity = 1.3 x 10^(42) erg/s at z=2.1. At this flux our sample is > 90% complete. Approximately 4% of the original NB-selected candidates were detected in X-rays by Chandra, and 7% were detected in the rest-frame far-UV by GALEX.\n",
      "\n",
      "At luminosity>1.3 x 10^42 erg/s, the equivalent width distribution is unbiased and is represented by an exponential with scale-length of 83+/-10 A. Above this same luminosity threshold, we find a number density of 1.5+/-0.5 x 10^-3 Mpc^-3. Neither the number density of LAEs nor the scale-length of their EW distribution show significant evolution from z=3 to z=2. We used the rest frame UV luminosity to estimate a median star formation rate of 4 M_(sun) /yr. The median rest frame UV slope, parametrized by B-R, is that typical of dust-free, 0.5-1 Gyr old or moderately dusty, 300-500 Myr old populations. Approximately 40% of the sample occupies the z~2 star-forming galaxy locus in the UVR two color diagram. Clustering analysis reveals that LAEs at z=2.1 have r_0=4.8+/-0.9 Mpc and a bias factor b=1.8+/-0.3. This implies that z=2.1 LAEs reside in dark matter halos with median masses Log(M/M_(sun))=11.5^(+0.4)_(-0.5), which are among of the lowest-mass halos yet probed at this redshift. We used the Sheth-Tormen conditional mass function to study the descendants of these LAEs and found that their typical present-day descendants are local galaxies with L* properties, like the Milky Way. 0 into PostgreSQL...\n",
      "Inserting test sample 2168  Lyman-alpha-emitting galaxies (LAEs) are of great interest to astrophysicists because of their role in galaxy evolution. In this study, we present the results of a survey for LAEs at a redshift (z) of 2.1 in the Extended Chandra Deep Field-South (ECDF-S), a well-studied region of the sky.\n",
      "\n",
      "Our sample contains a total of 137 LAEs, which we identify using narrowband imaging and follow up with spectroscopic confirmations. We find that the LAEs at z=2.1 are small and highly ionized, with relatively low masses and star formation rates compared to typical present-day galaxies.\n",
      "\n",
      "We compare the properties of LAEs at z=2.1 to those of galaxies in the present-day universe and find that they may represent the building blocks of these galaxies. In particular, we find that the stellar masses of LAEs are consistent with the expectation for the progenitors of typical present-day galaxies, and that the ionization properties of LAEs are similar to those of extreme emission line galaxies found at later times.\n",
      "\n",
      "We investigate the clustering properties of the LAEs and find that they are strongly clustered, with a correlation length that is comparable to that of massive present-day galaxies. We interpret this as evidence that the LAEs are associated with dark matter halos that will form the cores of massive present-day galaxies.\n",
      "\n",
      "Overall, our results suggest that LAEs at z=2.1 are important for understanding the formation and evolution of present-day galaxies. By identifying the building blocks of galaxies, we can begin to piece together the story of how galaxies like our own Milky Way came to be. 1 into PostgreSQL...\n",
      "Inserting test sample 2169  The behaviour of a di-nuclear system in the regime of strong pairing correlations is studied with the methods of statistical mechanics. It is shown that the thermal averaging is strong enough to assure the application of thermodynamical methods to the energy exchange between the two nuclei in contact. In particular, thermal averaging justifies the definition of a nuclear temperature. 0 into PostgreSQL...\n",
      "Inserting test sample 2170  Nuclei in thermal contact are investigated through thermodynamics to understand their behavior. The relationship between entropy, energy, and temperature is studied to provide insight into the transfer of energy between these systems. The study shows that as the nuclei become more energized, their entropy and temperature increase as well, leading to a better understanding of how these systems interact. 1 into PostgreSQL...\n",
      "Inserting test sample 2171  Long, high-quality time-series data provided by previous space-missions such as CoRoT and $\\mathit{Kepler}$ have made it possible to derive the evolutionary state of red-giant stars, i.e. whether the stars are hydrogen-shell burning around an inert helium core or helium-core burning, from their individual oscillation modes. We utilise data from the $\\mathit{Kepler}$ mission to develop a tool to classify the evolutionary state for the large number of stars being observed in the current era of K2, TESS and for the future PLATO mission.\n",
      "\n",
      "These missions provide new challenges for evolutionary state classification given the large number of stars being observed and the shorter observing duration of the data. We propose a new method, $\\mathtt{Clumpiness}$, based upon a supervised classification scheme that uses \"summary statistics\" of the time series, combined with distance information from the Gaia mission to predict the evolutionary state. Applying this to red giants in the APOKASC catalogue, we obtain a classification accuracy of ~91% for the full 4 years of $\\mathit{Kepler}$ data, for those stars that are either only hydrogen-shell burning or also helium-core burning. We also applied the method to shorter $\\mathit{Kepler}$ datasets, mimicking CoRoT, K2 and TESS achieving an accuracy >91% even for the 27 day time series. This work paves the way towards fast, reliable classification of vast amounts of relatively short-time-span data with a few, well-engineered features. 0 into PostgreSQL...\n",
      "Inserting test sample 2172  This study presents a new method to classify the evolutionary states of red-giant stars based on their clumpiness properties in the time domain. By analyzing data obtained from the Kepler and K2 missions, we have identified a number of stars that exhibit periodic variations in both brightness and clumpiness. Our analysis shows that these periodic variations are closely associated with changes in the stars' evolutionary states. Specifically, we find that stars that have evolved past the red-giant branch undergo significant changes in their internal structure that result in a more clumpy distribution of their outer layers. By contrast, stars that have not yet reached this stage have more smoothly distributed outer layers. Our classification method is based on the degree of clumpiness and the period of the brightness variations, together with other properties such as effective temperature and metallicity. Using this approach, we have successfully identified a new class of red-giant stars that exhibit highly asymmetric variability patterns, which appear to be related to processes occurring deep within the stars' interiors. These findings provide new insights into the late stages of stellar evolution and may have important implications for our understanding of the formation and evolution of galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 2173  For any posotive integer $m$, let $[m]:=\\{1,\\ldots,m\\}$. Let $n,k,t$ be positive integers. Aharoni and Howard conjectured that if, for $i\\in [t]$, $\\mathcal{F}_i\\subset[n]^k:= \\{(a_1,\\ldots,a_k): a_j\\in [n] \\mbox{ for } j\\in [k]\\}$ and $|\\mathcal{F}_i|>(t-1)n^{k-1}$, then there exist $M\\subseteq [n]^k$ such that $|M|=t$ and $|M\\cap \\mathcal{F}_i|=1$ for $i\\in [t]$ We show that this conjecture holds when $n\\geq 3(k-1)(t-1)$.\n",
      "\n",
      "Let $n, t, k_1\\ge k_2\\geq \\ldots\\geq k_t $ be positive integers. Huang, Loh and Sudakov asked for the maximum $\\Pi_{i=1}^t |{\\cal R}_i|$ over all ${\\cal R}=\\{{\\cal R}_1, \\ldots ,{\\cal R}_t\\}$ such that each ${\\cal R}_i$ is a collection of $k_i$-subsets of $[n]$ for which there does not exist a collection $M$ of subsets of $[n]$ such that $|M|=t$ and $|M\\cap \\mathcal{R}_i|=1$ for $i\\in [t]$ %and ${\\cal R}$ does not admit a rainbow matching. We show that for sufficiently large $n$ with $\\sum_{i=1}^t k_i\\leq n(1-(4k\\ln n/n)^{1/k}) $, $\\prod_{i=1}^t |\\mathcal{R}_i|\\leq {n-1\\choose k_1-1}{n-1\\choose k_2-1}\\prod_{i=3}^{t}{n\\choose k_i}$. This bound is tight. 0 into PostgreSQL...\n",
      "Inserting test sample 2174  Rainbow matchings for hypergraphs is a challenging and intriguing combinatorial problem. In recent years, a number of new results and techniques have been developed to address this problem, leading to deeper insights and better understanding of hypergraphs and their properties. This research paper presents a comprehensive study of this problem, with a focus on the fundamental properties and the latest developments in this area.\n",
      "\n",
      "We begin by introducing the basic concepts and definitions related to hypergraphs and rainbow matchings. We then review the existing literature and identify some of the key open problems in the field. We provide an in-depth analysis of some of the recently developed techniques for finding rainbow matchings in hypergraphs, including the use of probabilistic methods and extremal graph theory.\n",
      "\n",
      "Our results show that the problem of finding rainbow matchings in hypergraphs is closely related to several other important problems in combinatorics and graph theory, such as the existence of transversals and the chromatic number of graphs. We further explore some of these connections and provide new insights into the interplay between these problems.\n",
      "\n",
      "Overall, this research paper represents a significant contribution to the field of combinatorics and offers important insights into the theory of hypergraphs. Our results and techniques are expected to have important applications in a wide range of fields, including computer science, operations research, and social network analysis. 1 into PostgreSQL...\n",
      "Inserting test sample 2175  The miniaturization of electronics, sensors and actuators has enabled the growing use of CubeSats and sub-20 kg spacecraft. Their reduced mass and volume has the potential to translate into significant reductions in required propellant and launch mass for interplanetary missions, earth observation and for astrophysics applications. There is an important need to optimize the design of these spacecraft to better ascertain their maximal capabilities by finding optimized solution, where mass, volume and power is a premium. Current spacecraft design methods require a team of experts, who use their engineering experience and judgement to develop a spacecraft design. Such an approach can miss innovative designs not thought of by a human design team. In this work we present a compelling alternative approach that extends the capabilities of a spacecraft engineering design team to search for and identify near-optimal solutions using machine learning. The approach enables automated design of a spacecraft that requires specifying quantitative goals, requiring reaching a target location or operating at a predetermined orbit for a required time. Next a virtual warehouse of components is specified that be selected to produce a candidate design. Candidate designs are produced using an artificial Darwinian approach, where fittest design survives and reproduce, while unfit individuals are culled off. Our past work in space robotic has produced systems designs and controllers that are human competitive. Finding a near-optimal solution presents vast improvements over a solution obtained through engineering judgment and point design alone. The approach shows a credible pathway to identify and evaluate many more candidate designs than it would be otherwise possible with a human design team alone. 0 into PostgreSQL...\n",
      "Inserting test sample 2176  The design of CubeSat and small spacecrafts is a complex and challenging task that requires expertise in various fields. In recent years, the demand for CubeSats has increased significantly due to their low cost, fast development time, and versatility. However, designing these spacecrafts still requires a lot of effort and resources. To address this issue, automated design techniques have been proposed to make the process faster, cheaper and more efficient.\n",
      "\n",
      "This paper presents a comprehensive review of the state-of-the-art in automated design of CubeSats and small spacecrafts. The study identifies the various design factors that need to be considered, such as mission requirements, propulsion system, power supply, communication, and sensing capabilities. The paper also summarises existing methodologies that have been proposed for different stages of spacecraft design, such as conceptual design, system-level design, and detailed design.\n",
      "\n",
      "Furthermore, the paper discusses the benefits and limitations of using automated design techniques for CubeSats and small spacecrafts. The potential advantages of automation include reduced cost, faster design times, increased design exploration, and improved performance. However, there are also challenges associated with automation, such as the need for accurate modelling, sufficient data, and human oversight.\n",
      "\n",
      "The paper concludes by discussing future directions in automated design research, such as the integration of machine learning and artificial intelligence techniques, and the development of standardised design methodologies. Overall, this study highlights the potential of automated design techniques to revolutionise the design of CubeSats and small spacecrafts, and provides a roadmap for future research in this field. 1 into PostgreSQL...\n",
      "Inserting test sample 2177  Andreev reflection (AR) refers to the electron-hole conversion at the normal metal-superconductor interface. In a three-dimensional metal with spherical Fermi surface, retro (specular) AR can occur with the sign reversal of all three (a single) components of particle velocity. Here, we predict a novel type of AR with the inversion of two velocity components, dubbed \"anomalous-trajectory Andreev reflection\" (AAR), which can be realized in a class of materials with torus-shaped Fermi surface, such as doped nodal line semimetals. For its toroidal circle perpendicular to the interface, the Fermi torus doubles the AR channels and generates multiple AR processes. In particular, the AAR and retro AR are found to dominate electron transport in the light and heavy doping regimes, respectively. We show that the AAR visibly manifests as a ridge structure in the spatially resolved nonlocal conductance, in contrast to the peak structure for the retro AR. Our work opens a new avenue for the AR spectroscopy and offers a clear transport signature of torus-shaped Fermi surface. 0 into PostgreSQL...\n",
      "Inserting test sample 2178  The study explores the physics of anomalous Andreev reflection on a torus-shaped Fermi surface. Previous studies have shown that this phenomenon is responsible for superconducting proximity effects in a variety of systems. However, the relationship between the geometry of Fermi surfaces and the strength of anomalous Andreev reflection remains unclear. Here, we investigate this relationship by analyzing the scattering of quasiparticles in systems with different toroidal Fermi surfaces. Our results demonstrate that the size and the shape of the torus play a critical role in enhancing anomalous Andreev reflection. In particular, we find that tori with larger radii and smaller cross-sectional areas are more effective at producing strong Andreev reflections. These findings suggest that the geometry of Fermi surfaces can be manipulated to design new superconducting materials with enhanced properties. Our study provides insights into the physics of superconducting proximity effects and could guide the development of novel technologies based on these effects. 1 into PostgreSQL...\n",
      "Inserting test sample 2179  We show that the quantum Hall wave functions for the ground states in the Jain series can be exactly expressed in terms of correlation functions of local vertex operators, V_n, corresponding to composite fermions in the n:th composite-fermion (CF) Landau level. This allows for the powerful mathematics of conformal field theory to be applied to the successful CF phenomenology.\n",
      "\n",
      "Quasiparticle and quasihole states are expressed as correlators of anyonic operators with fractional (local) charge, allowing a simple algebraic understanding of their topological properties that are not manifest in the CF wave functions. 0 into PostgreSQL...\n",
      "Inserting test sample 2180  In two-dimensional systems, the concept of composite fermions (CFs) has been introduced to describe emergent quasiparticles that arise from the coupling between electrons and a uniform magnetic field. Conformal field theory (CFT) has proven to be a useful tool for understanding the behavior of CFs, as it provides a mathematical framework for describing the universal properties of quantum field theories. This paper reviews recent progress in the application of CFT to composite fermions, with a focus on the non-relativistic and relativistic descriptions, and explores potential future directions for research. 1 into PostgreSQL...\n",
      "Inserting test sample 2181  The distortion on the intermittency signal, due to detection efficiency and to the presence of pre--equilibrium emitted particles, is studied in a schematic model of nuclear multi- fragmentation. The source of the intermittency signal is modeled with a percolating system. The efficiency is schematized by a simple function of the fragment size, and the presence of pre--equilibrium particles is simulated by an additional non--critical fragment source. No selection on the events is considered, and therefore all events are used to calculate the moments. It is found that, despite the absence of event selection, the intermittency signal is quite resistant to the distortion due to the apparatus efficiency, while the inclusion of pre--equilibrium particles in the moment calculation can substantially reduce the strength of the signal.\n",
      "\n",
      "Pre--equilibrium particles should be therefore carefully separated from the rest of the detected fragments, before the intermittency analysis on experimental charge or mass distributions is carried out. 0 into PostgreSQL...\n",
      "Inserting test sample 2182  This research paper delves into the phenomenon of intermittency in nuclear multifragmentation, with a specific focus on its detection via 4Ï€ detectors. Through the analysis of experimental data, we reveal that the multifragmentation process exhibits intermittency, with fluctuations persisting over extended periods of time. Using our newly developed method, we establish a clear correlation between the degree of intermittency and the level of nuclear excitation. Further investigations were conducted to examine the impact of other variables, such as the size and energy of the fragments, on the manifestation of intermittency. Our findings suggest that intermittency is a prevalent feature of nuclear multifragmentation and can be utilized for future research in nuclear physics. The developments our team has made in detecting and analyzing these fluctuations pave the way for further advancements in the field of nuclear physics. 1 into PostgreSQL...\n",
      "Inserting test sample 2183  We propose a two-step pseudo-maximum likelihood procedure for semiparametric single-index regression models where the conditional variance is a known function of the regression and an additional parameter. The Poisson single-index regression with multiplicative unobserved heterogeneity is an example of such models. Our procedure is based on linear exponential densities with nuisance parameter. The pseudo-likelihood criterion we use contains a nonparametric estimate of the index regression and therefore a rule for choosing the smoothing parameter is needed. We propose an automatic and natural rule based on the joint maximization of the pseudo-likelihood with respect to the index parameter and the smoothing parameter. We derive the asymptotic properties of the semiparametric estimator of the index parameter and the asymptotic behavior of our `optimal' smoothing parameter. The finite sample performances of our methodology are analyzed using simulated and real data. 0 into PostgreSQL...\n",
      "Inserting test sample 2184  This paper proposes a semi-parametric estimator that incorporates a single index variable into estimating equation models. The proposed estimator is designed to handle situations where the model's outcome has a complex distribution while maintaining computational tractability. By introducing the index, we can construct a flexible functional form and minimize any misspecification of the parametric estimation process. The semi-parametric estimator is shown to efficiently estimate the true parameter values in simulations and real data settings. Moreover, the estimator possesses appealing properties, including consistency and asymptotic normality, under regularity conditions. This paper's contributions provide researchers with a new tool for analyzing complex relationships between covariates and outcome variables, which can have crucial implications for policy recommendations and decisions. 1 into PostgreSQL...\n",
      "Inserting test sample 2185  In the context of the semiclassical theory of short periodic orbits, scar functions play a crucial role. These wavefunctions live in the neighbourhood of the trajectories, resembling the hyperbolic structure of the phase space in their immediate vicinity. This property makes them extremely suitable for investigating chaotic eigenfunctions. On the other hand, for all practical purposes reductions to Poincare sections become essential. Here we give a detailed explanation of resonances and scar functions construction in the Bunimovich stadium billiard and the corresponding reduction to the boundary.\n",
      "\n",
      "Moreover, we develop a method that takes into account the departure of the unstable and stable manifolds from the linear regime. This new feature extends the validity of the expressions. 0 into PostgreSQL...\n",
      "Inserting test sample 2186  We investigate how the presence of scars affects the chaotic dynamics in the Bunimovich Stadium billiard. By analyzing the behavior of periodic orbits and their corresponding stability eigenvalues, we find that short scars can have a significant impact on transport properties, with implications for quantum chaos and wave phenomena. Our analysis indicates that the energy density and period of the scar determine its effect on transport. Furthermore, we introduce a new method using localization and Green's functions to study the extent of wave transport in the stadium. Overall, our results provide insights into the mechanisms of chaotic transport and the interplay between classical and quantum dynamics in the stadium billiard with scars. 1 into PostgreSQL...\n",
      "Inserting test sample 2187  We present the frequency resolved energy spectra (FRS) of the low-mass X-ray binary dipper XB 1323-619 during persistent emission in four different frequency bands using an archival XMM-Newton observation. FRS method helps to probe the inner zones of an accretion disk. We find that the FRS is well described by a single blackbody component with kT in a range 1.0-1.4 keV responsible for the source variability in the frequency ranges 0.002-0.04 Hz, and 0.07-0.3 Hz. We attribute this component to the accretion disk and possibly emission from an existing boundary layer supported by radiation pressure. The appearance of the blackbody component in the lower frequency ranges and disappearance towards the higher frequencies suggests that it may also be a disk-blackbody emission. We detect a different form of FRS for the higher frequency ranges 0.9-6 Hz and 8-30 Hz which is modeled best with a power-law and a Gaussian emission line at 6.4$^{+0.2}_{-0.3}$ keV with an equivalent width of 1.6$^{+0.4}_{-1.2}$ keV and 1.3$^{+0.7}_{-0.9}$ keV for the two frequency ranges, respectively. This iron fluorescence line detected in the higher frequency ranges of spectra shows the existence of reflection in this system within the inner disk regions. In addition, we find that the 0.9-6 Hz frequency band shows two QPO peaks at 1.4$^{+1.0}_{-0.2}$ Hz and 2.8$^{+0.2}_{-0.2}$ Hz at about 2.8-3.1 $\\sigma$ confidence level. These are consistent with the previously detected $\\sim$ 1 Hz QPO from this source (Jonker et al. 1999). We believe they relate to the reflection phenomenon. The emission from the reflection region, being a variable spectral component in this system, originates from the inner regions of the disk with a maximum size of 4.7$\\times 10^9$ cm and a minimum size of 1.6$\\times 10^8$ cm calculated using light travel time considerations and our frequency resolved spectra. 0 into PostgreSQL...\n",
      "Inserting test sample 2188  We present an analysis of the XMM-Newton data of the binary system XB 1323-619 using frequency-resolved spectroscopy techniques. The source is an eclipsing low-mass X-ray binary with a black-hole primary. The data were taken in 2002 and consist of two observations, separated by about three weeks, each spanning roughly 50 ks.\n",
      "\n",
      "Our analysis detected a reflection component in the system's accretion disk, indicating that the disk is illuminated by a hard X-ray source located close to the black hole. In particular, we found a high-frequency QPO (quasi-periodic oscillation) at approximately 7.6 Hz in the power density spectrum of the second observation. This QPO is known to be associated with the reflection region, and we used it to select data for spectral analysis. We fit the spectra with reflection models, accounting for Comptonization in the corona, and derived estimates of the disk inclination and black hole spin.\n",
      "\n",
      "Our results show that the reflection region is located within 10 gravitational radii of the black hole, and that the disk is inclined at an angle of approximately 45 to the line of sight. These measurements provide important constraints on the geometry and physical properties of the binary system, and may help to test models of accretion disk structure and black hole physics.\n",
      "\n",
      "In summary, our analysis of XMM-Newton data of XB 1323-619 has revealed the presence of a reflection component in the accretion disk. This discovery provides a new probe of the disk's structure and the black hole's properties, and highlights the importance of frequency-resolved spectroscopy techniques in understanding the behavior of low-mass X-ray binaries. We suggest that future observations of this system should continue to explore the reflection region to gain further insights into its properties and their implications for accretion disk theory. 1 into PostgreSQL...\n",
      "Inserting test sample 2189  We identify a general connection between the physics of exceptional points in non-Hermitian systems and the few-photon bound states in waveguide quantum electrodynamics (QED) systems. We show that, in waveguide QED systems where the local quantum system exhibits an exceptional point, the tightest-bound few-photon bound state occurs at the exceptional point. We illustrate this connection with an explicit computation on a waveguide QED system in which a waveguide is coupled to a Jaynes-Cummings system. Our result provides a quantum signature of the exceptional point physics and indicates that the physics of exceptional point can be used to understand and control the photon-photon interaction. 0 into PostgreSQL...\n",
      "Inserting test sample 2190  This paper investigates the implications of exceptional points (EPs) for few-photon transport in waveguide quantum electrodynamics. Our analysis demonstrates the impact of EPs on the transmission of photons in the waveguide, and reveals the non-Hermitian features of waveguide quantum electrodynamics. We consider different scenarios of few-photon transport in the waveguide, and study the effects of EPs on the system's dynamics. Furthermore, we explore the potential applications of using EPs as a control mechanism for few-photon transport in quantum systems. Our results provide key insights into the interplay between EPs and few-photon transport in waveguide quantum electrodynamics, with important implications for quantum information processing and communication technologies. 1 into PostgreSQL...\n",
      "Inserting test sample 2191  We theoretically investigate the temperature-dependent static susceptibility and long-range magnetic coupling of three-dimensional (3D) chiral gapless electron-hole systems (semimetals) with arbitrary band dispersion [i.e., $\\varepsilon(k) \\sim k^N$, where $k$ is the wave vector and $N$ is a positive integer]. We study the magnetic properties of these systems in the presence of dilute random magnetic impurities. Assuming carrier-mediated Ruderman-Kittel-Kasuya-Yosida indirect exchange interaction, we find that the magnetic ordering of intrinsic 3D chiral semimetals in the presence of dilute magnetic impurities is ferromagnetic for all values of $N$. Using finite-temperature self-consistent field approximation, we calculate the ferromagnetic transition temperature ($T_{\\rm c}$). We find that $T_{\\rm c}$ increases with increasing $N$ due to the enhanced density of states, and the calculated $T_{\\rm c}$ is experimentally accessible assuming reasonable coupling between the magnetic impurities and itinerant carriers. 0 into PostgreSQL...\n",
      "Inserting test sample 2192  In this paper, we investigate the susceptibility and ferromagnetism of diluted magnetic Dirac-Weyl materials in chiral gapless semimetals. Using the linear response theory, we derive the expressions for magnetic susceptibility and establish the conditions for magnetic order in three-dimensional systems. We demonstrate that the doping of magnetic impurities that break time-reversal symmetry can induce an exchange interaction between electrons, leading to ferromagnetism. Furthermore, we find that the presence of a strong spin-orbit coupling can generate nontrivial electronic band topology, resulting in the emergence of massless Dirac and Weyl fermions. Our study provides insights into the magnetism and electronic properties of chiral semimetals, which can be used for designing novel spintronic and electronic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 2193  An individual-based model (IBM) of a spatiotemporal terrestrial ecological population is proposed. This model is spatially explicit and features the position of each individual together with another characteristic, such as the size of the individual, which evolves according to a given stochastic model.\n",
      "\n",
      "The population is locally regulated through an explicit competition kernel. The IBM is represented as a measure-valued branching/diffusing stochastic process.\n",
      "\n",
      "The approach allows (i) to describe the associated Monte Carlo simulation and (ii) to analyze the limit process under large initial population size asymptotic. The limit macroscopic model is a deterministic integro-differential equation. 0 into PostgreSQL...\n",
      "Inserting test sample 2194  This paper presents a spatially explicit Markovian individual-based model for terrestrial plant dynamics. The model incorporates stochasticity in both demographic and environmental factors and allows for the simulation of the spatiotemporal dynamics of multiple interacting plant species. We demonstrate the utility of the model using a case study of forest regeneration in a disturbed landscape. The results show that the model accurately captures the effects of biotic and abiotic factors on plant population dynamics and can provide insights into the mechanisms driving ecosystem processes. Overall, our model is a valuable tool for predicting the response of plant communities to environmental change. 1 into PostgreSQL...\n",
      "Inserting test sample 2195  The accretion-induced collapse (AIC) scenario was proposed 40 years ago as an evolutionary end state of oxygen-neon white-dwarfs (ONe WDs), linking them to the formation of neutron star (NS) systems. However, there has been no direct detection of any AIC event so far, even though there exists a lot of indirect observational evidence. Meanwhile, the evolutionary pathways resulting in NS formation through AIC are still not well investigated. In this article, we review recent studies on the two classic progenitor models of AIC events, i.e., the single-degenerate model (including the ONe WD+MS/RG/He star channels and the CO WD+He star channel) and the double-degenerate model (including the double CO/ONe WD channels and the ONe WD+CO WD channel). Recent progress on these progenitor models is reviewed, including the evolutionary scenarios, the initial parameter space and the related objects. For the single-degenerate model, the pre-AIC systems could potentially be identified as supersoft X-ray sources, symbiotics and cataclysmic variables, whereas the post-AIC systems could be identified as low-/intermediate-mass X-ray binaries and the resulting low-/intermediate-mass binary pulsars, most notably millisecond pulsars. For the double-degenerate model, the pre-AIC systems are close double WDs, whereas the post-AIC systems are isolated NSs with peculiar properties. We also review the predicted rates of AIC events, the mass distribution of NSs, and the gravitational wave (GW) signals from double WDs that are potential GW sources in the Galaxy in the context of future space-based GW detectors. Recent theoretical and observational constraints on the detection of AIC events are summarized. In order to confirm the existence of the AIC process, and resolve this long-term issue presented by current stellar evolution theories, more numerical simulations and observational identifications are required. 0 into PostgreSQL...\n",
      "Inserting test sample 2196  In this paper, we present a detailed study of the formation of neutron star systems through accretion-induced collapse in white-dwarf binaries. The complex dynamics between these two celestial objects give rise to a fascinating process that unveils the physical laws that govern the life cycle of stars. White dwarfs are known to be the remnants of low- and medium-mass stars, while neutron stars are the ultra-compact, highly-magnetized remnants of massive stars.\n",
      "\n",
      "The accretion-induced collapse process is triggered when a white dwarf receives material from a companion star until it reaches a critical mass, beyond which the electron-degenerate core is no longer able to withstand the gravitational pressure. Then, the electron capture process takes place and the white dwarf collapses into a highly-dense state, leading to the emission of gravitational waves and the eventual formation of a neutron star. This process is a key ingredient in the cosmic cycle of matter and energy, as it replenishes the interstellar medium with heavy elements synthesized in the core of the neutron star.\n",
      "\n",
      "We describe the formation of neutron star systems through accretion-induced collapse in white-dwarf binaries from both observational and theoretical perspectives. We present recent observations of binary systems that are thought to harbor accreting white dwarfs and discuss their implications for the formation of neutron stars. In addition, we perform detailed numerical simulations of the accretion-induced collapse process using state-of-the-art computational tools, which enable us to explore the complex interplay between nuclear physics, hydrodynamics, and gravity.\n",
      "\n",
      "Our results provide novel insights into the formation of neutron star systems and the role of accretion-induced collapse in their evolution. We find that the process is highly sensitive to the properties of the white dwarf and the companion star, and that the resulting neutron star can exhibit a wide range of characteristics depending on the details of the collapse. Our study sheds light on one of the most intriguing phenomena in modern astrophysics and paves the way for future investigations of neutron star systems and their connection to the broader universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2197  The Spitzer GLIMPSE and MIPSGAL surveys have revealed a wealth of details of the Galactic plane. We use them to study the energetics and dust properties of M16, one of the best known SFR. We present MIPSGAL observations of M16 at 24 and 70 $\\mu$m and combine them with previous IR data. The MIR image shows a shell inside the molecular borders of the nebula. The morphologies at 24 and 70 $\\mu$m are different, and its color ratio is unusually warm. The FIR image resembles the one at 8 $\\mu$m that enhances the molecular cloud. We measure IR SEDs within the shell and the PDRs. We use the DUSTEM model to fit the SEDs and constrain dust temperature, dust size distribution, and ISRF intensity relative to that provided by the star cluster NGC6611. Within the PDRs, the dust temperature, the dust size distribution, and the ISRF intensity are in agreement with expectations. Within the shell, the dust is hotter and an ISRF larger than that provided by NGC6611 is required. We quantify two solutions.\n",
      "\n",
      "(1) The size distribution of the dust in the shell is not that of interstellar dust. (2) The dust emission arises from a hot plasma where UV and collisions with electrons contribute to the heating. We suggest two interpretations for the shell. (1) The shell matter is supplied by photo-evaporative flows arising from dense gas exposed to ionized radiation. The flows renew the shell matter as it is pushed by the stellar winds. Within this scenario, we conclude that massive SFR such as M16 have a major impact on the carbon dust size distribution. The grinding of the carbon dust could result from shattering in collisions within shocks driven by the interaction between the winds and the shell. (2) We consider a scenario where the shell is a SNR. We would be witnessing a specific time in the evolution of the SNR where the plasma pressure and temperature would be such that the SNR cools through dust emission. 0 into PostgreSQL...\n",
      "Inserting test sample 2198  The Eagle Nebula is a vast star-forming region, located approximately 7,000 light years away from Earth. In this study, we investigate the energetics and evolution of dust in the Eagle Nebula using data from the Spitzer Space Telescope. By analyzing Spitzer infrared observations, we are able to explore dust properties, dust heating mechanisms, and dust destruction processes in this region. Our results reveal a complex history of dust in the Eagle Nebula, which includes several phases of star formation and supernova explosions.\n",
      "\n",
      "To trace the energetics and evolution of dust in the Eagle Nebula, we use several Spitzer instruments, including the Infrared Array Camera (IRAC), the Multiband Imaging Photometer (MIPS), and the Infrared Spectrograph (IRS). The combination of these instruments allows us to probe different aspects of the dust properties in this region, from its composition and temperature to its spatial distribution and spectral features.\n",
      "\n",
      "Our analysis of Spitzer data shows that dust in the Eagle Nebula is predominantly heated by the interstellar radiation field, with some contribution from embedded young stellar objects. We find evidence for the destruction of dust grains by supernova explosions, which inject energy and momentum into the interstellar medium. These explosions can create cavities and bubbles in the surrounding gas and dust, as well as trigger further star formation.\n",
      "\n",
      "Our study contributes to a better understanding of the formation and evolution of star-forming regions, as well as the role of dust in these processes. It also highlights the power of Spitzer observations in studying the infrared emission from dust in the Universe. Our results have important implications for future observations with the upcoming James Webb Space Telescope, which is expected to revolutionize our understanding of star formation and the interstellar medium. 1 into PostgreSQL...\n",
      "Inserting test sample 2199  For the initial boundary value problem of compressible barotropic Navier-Stokes equations in one-dimensional bounded domains with general density-dependent viscosity and large external force, we prove that there exists a unique global classical solution with large initial data containing vacuum. Furthermore, we show that the density is bounded from above independently of time which in particular yields the large time behavior of the solution as time tends to infinity: the density and the velocity converge to the steady states in $L^p$ and in $W^{1,p}$ ($1\\le p<+\\infty$) respectively.\n",
      "\n",
      "Moreover, the decay rate in time of the solution is shown to be exponential.\n",
      "\n",
      "Finally, we also prove that the spatial gradient of the density will blow up as time tends to infinity when vacuum states appear initially even at one point. 0 into PostgreSQL...\n",
      "Inserting test sample 2200  This paper studies the existence of global classical solutions to the one-dimensional compressible Navier-Stokes equations with density-dependent viscosity and vacuum. We establish a new criterion that guarantees the well-posedness of the problem. Our result also shows the stability of the solutions in the long term with respect to the initial data. To prove this, we use a combination of analytical tools such as energy methods, maximum principle, and the compactness argument. Moreover, we present numerical simulations that support our theoretical finding and illustrate the behavior of the solutions in different regimes. The outcome of this work sheds light on the mathematical modeling of physical phenomena that involve compressible flows and viscosity effects, and it can be useful in various fields, including fluid dynamics, astrophysics, and aerospace engineering. 1 into PostgreSQL...\n",
      "Inserting test sample 2201  This paper proposes an adaptive randomization procedure for two-stage randomized controlled trials. The method uses data from a first-wave experiment in order to determine how to stratify in a second wave of the experiment, where the objective is to minimize the variance of an estimator for the average treatment effect (ATE). We consider selection from a class of stratified randomization procedures which we call stratification trees: these are procedures whose strata can be represented as decision trees, with differing treatment assignment probabilities across strata. By using the first wave to estimate a stratification tree, we simultaneously select which covariates to use for stratification, how to stratify over these covariates, as well as the assignment probabilities within these strata. Our main result shows that using this randomization procedure with an appropriate estimator results in an asymptotic variance which is minimal in the class of stratification trees.\n",
      "\n",
      "Moreover, the results we present are able to accommodate a large class of assignment mechanisms within strata, including stratified block randomization.\n",
      "\n",
      "In a simulation study, we find that our method, paired with an appropriate cross-validation procedure ,can improve on ad-hoc choices of stratification. We conclude by applying our method to the study in Karlan and Wood (2017), where we estimate stratification trees using the first wave of their experiment. 0 into PostgreSQL...\n",
      "Inserting test sample 2202  Adaptive randomization is a process utilized in clinical trials to improve trial efficiency and increase the likelihood of identifying treatment effects. However, current methods of adaptive randomization have certain limitations such as complexity and inefficiency when considering multiple factors or variables. In this research paper, we propose a new method of adaptive randomization based on the concept of stratification trees. The stratification trees approach allows for the efficient and flexible stratification of participants in randomized controlled trials, enabling the incorporation of numerous variables, including demographic, clinical, and environmental factors, in an automated and effective way. Using simulations and real data applications, we demonstrate the superiority of the stratification trees approach over traditional adaptive randomization methods, particularly when dealing with multiple stratification variables. The use of stratification trees provides a comprehensive and robust way to address stratification and allocation in clinical trials, as it exploits the natural tree-based structure of the randomization scheme. This new approach may improve the precision and power of clinical trials, leading to better treatment decisions for patients in the future. 1 into PostgreSQL...\n",
      "Inserting test sample 2203  The Leggett-Garg inequalities serve to test whether or not quantum correlations in time can be explained within a classical macrorealistic framework. We apply this test to thermodynamics and derive a set of Leggett- Garg inequalities for the statistics of fluctuating work done on a quantum system unitarily driven in time. It is shown that these inequalities can be violated in a driven two-level system, thereby demonstrating that there exists no general macrorealistic description of quantum work. These violations are shown to emerge within the standard Two-Projective-Measurement scheme as well as for alternative definitions of fluctuating work that are based on weak measurement. Our results elucidate the influences of temporal correlations on work extraction in the quantum regime and highlight a key difference between quantum and classical thermodynamics. 0 into PostgreSQL...\n",
      "Inserting test sample 2204  The Leggett-Garg inequalities represent a set of conditions that test the validity of macrorealism in systems with multiple measurements performed on them. They have been extensively studied in closed quantum systems, and recent research has now extended their applicability to open quantum systems undergoing quantum fluctuating work. In this paper, we explore the theoretical background of the Leggett-Garg inequalities for quantum fluctuating work and present numerical simulations that verify these inequalities in such systems. Our results show that these inequalities can be used to identify and characterize quantum effects on a macroscale level. We discuss potential applications of these findings in nanotechnology and quantum information processing. Our work contributes to the ongoing investigation of the fundamental properties of quantum systems and their relationship with the classical world. 1 into PostgreSQL...\n",
      "Inserting test sample 2205  The electron transfer in different solvents is investigated for systems consisting of donor, bridge and acceptor. It is assumed that vibrational relaxation is much faster than the electron transfer. Electron transfer rates and final populations of the acceptor state are calculated numerically and in an approximate fashion analytically. In wide parameter regimes these solutions are in very good agreement. The theory is applied to the electron transfer in ${\\rm H_2P-ZnP-Q}$ with free-base porphyrin (${\\rm H_2P}$) being the donor, zinc porphyrin (${\\rm ZnP}$) the bridge, and quinone (${\\rm Q}$) the acceptor.\n",
      "\n",
      "It is shown that the electron transfer rates can be controlled efficiently by changing the energy of the bridging level which can be done by changing the solvent. The effect of the solvent is determined for different models. 0 into PostgreSQL...\n",
      "Inserting test sample 2206  Electron transfer in porphyrin complexes is of great interest to scientists due to its crucial role in various biological and chemical processes. This study investigates the effect of solvent polarity on electron transfer rates between porphyrin complexes of different metal centers and electron donors/acceptors. The electronic absorption spectroscopy and time-resolved spectroscopy were used to explore the correlation between the solvent polarity and the rate of electron transfer in several solvents with varying polarities. The results reveal that electron transfer rates are significantly influenced by the solvent polarity, where the higher polarity leads to faster electron transfer. The dependence on metal center and electron donor/acceptor is also explored, providing key insights into the mechanism of electron transfer in these systems. These findings can be used to improve the design and efficiency of energy conversion systems and electronic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 2207  Quasar microlensing serves as a unique probe of discrete objects within galaxies and galaxy clusters. Recent advancement of the technique shows that it can constrain planet-scale objects beyond our native galaxy by studying their induced microlensing signatures, the energy shift of emission lines originated in the vicinity of the black hole of high redshift background quasars. We employ this technique to exert effective constraints on the planet-mass object distribution within two additional lens systems, Q J0158$-$4325 ($z_l = 0.317$) and SDSS J1004+4112 ($z_l = 0.68$) using Chandra observations of the two gravitationally-lensed quasars. The observed variations of the emission line peak energy can be explained as microlensing of the FeK$\\alpha$ emission region induced by planet-mass microlenses. To corroborate this, we perform microlensing simulations to determine the probability of a caustic transiting the source region and compare this with the observed line shift rates. Our analysis yields constraints on the sub-stellar population, with masses ranging from Moon ($10^{-8} M_{\\odot}$) to Jupiter ($10^{-3} M_{\\odot}$) sized bodies, within these galaxy or cluster scale structures, with total mass fractions of $\\sim 3\\times10^{-4}$ and $\\sim 1\\times10^{-4}$ with respect to halo mass for Q J0158$-$4325 and SDSS J1004+4112, respectively. Our analysis suggests that unbound planet-mass objects are universal in galaxies, and we surmise the objects to be either free-floating planets or primordial black holes. We present the first-ever constraints on the sub-stellar mass distribution in the intra-cluster light of a galaxy cluster. Our analysis yields the most stringent limit for primordial black holes at the mass range. 0 into PostgreSQL...\n",
      "Inserting test sample 2208  The search for exoplanets beyond our own Milky Way galaxy has brought about new challenges in detection and characterization techniques. One such challenge is the confirmation of planet-mass objects in extragalactic systems. In this study, we present the results of our observational campaign using the Hubble Space Telescope to confirm the existence of planet-mass objects in extragalactic systems.\n",
      "\n",
      "We analyzed the variability in the microlensing light curves of stars in the extragalactic systems to identify possible planet-mass objects. Our analysis suggests that the systems we observed contain several Jupiter-sized objects and a few Neptune-sized objects. We also discovered several possible brown dwarf candidates. Our findings support the hypothesis that these planet-mass objects were formed in the protoplanetary disks around their host stars in the extragalactic systems.\n",
      "\n",
      "Furthermore, we investigated the statistical properties of the planet-mass objects. Our analysis shows that the planet-mass objects have a mass distribution similar to that of exoplanets in our own galaxy. This implies that the formation and evolution of planet-mass objects are governed by similar physical processes in extragalactic systems as in our own galaxy.\n",
      "\n",
      "Our study provides strong evidence for the existence of planet-mass objects in extragalactic systems. This is the first comprehensive study of its kind and establishes a foundation for future studies to determine the frequency and properties of planet-mass objects in extragalactic systems. Our findings have important implications for our understanding of the formation and evolution of planetary systems beyond our own galaxy. 1 into PostgreSQL...\n",
      "Inserting test sample 2209  A combined technique of interference alignment (IA) and interference cancellation (IC), known as interference alignment and cancellation (IAC) scheme, has been proposed to improve the total achievable degrees of freedom (DoFs) over IA. Since it is NP-hard to solve the transceiver under a given tuple of DoFs or to maximize the total achievable DoFs in the general system configuration by IA (or IAC), the optimal transceiver cannot be obtained in polynomial time. Meanwhile, it has been known that a closed-form yet suboptimal transceiver can be designed for IAC by employing a symbol-to-symbol (STS) alignment structure. As its performance has not been known yet, we aim to derive the total DoFs that can be achieved by such suboptimal but closed-form IAC transceivers for Gaussian interference multiple access channels with K receivers and J users (transmitters), each with M antennas. Our analysis shows that the closed-form IAC transceivers under consideration can achieve a maximum total achievable DoFs of 2M, which turns out to be larger than those achieved in classical IA, e.g., 2MK/(K+1) DoFs by a specific configuration where each link has the same target DoFs. Moreover, considering the NP-hardness of deriving the maximum total achievable DoFs with the optimal IAC transceiver, its upper bound has been derived for comparison with the results of our closed-form IAC transceiver. Numerical results illustrate that its performance can be guaranteed within 20% of the upper bound when the number of multiple access channels are relatively small, e.g., K <=4. 0 into PostgreSQL...\n",
      "Inserting test sample 2210  Interference management in wireless communication is a vital and challenging problem that has drawn a lot of attention from researchers in recent years. Interference alignment (IA) is an emerging technique that can achieve the maximum degrees of freedom (DoFs) in a wireless network. This paper focuses on the achievable DoFs and the closed-form solution to IA and interference cancellation in Gaussian interference multiple access channels (IMACs). By reducing interference to a single dimension, the degrees of freedom can be increased without significantly reducing the received signal strength.\n",
      "\n",
      "Firstly, we propose a general framework for IA and interference cancellation in IMACs that allows for a closed-form solution. We then derive a closed-form solution for 2-user IMACs and establish a lower bound on the achievable DoFs. We show that the achievable DoFs in the 2-user case are at least 2 with non-zero probability. We extend the closed-form solution to multi-user cases by using a hierarchical IA scheme.\n",
      "\n",
      "Furthermore, we analyze the impact of feedback delay and quantization on the achievable DoFs. We prove that the achievable DoFs are robust to both feedback delay and quantization error. We also investigate the effect of power imbalance on the achievable DoFs and derive a necessary condition for achieving a certain number of DoFs in a multi-user IMAC.\n",
      "\n",
      "Overall, our work contributes to understanding the fundamental limits of IA and interference cancellation in IMACs. We provide closed-form solutions and theoretical analysis that can guide the design of practical IA algorithms for wireless communication systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2211  We determine all $2\\times 2$ quantum states that can serve as useful catalysts for a given probabilistic entanglement transformation, in the sense that they can increase the maximal transformation probability. When higher-dimensional catalysts are considered, a sufficient and necessary condition is derived under which a certain probabilistic transformation has useful catalysts. 0 into PostgreSQL...\n",
      "Inserting test sample 2212  In the field of quantum mechanics, probabilistic entanglement is a fundamental concept with practical applications. This research paper explores the use of catalysis to facilitate the transformation of probabilistic entanglement. We present theoretical models and experimental results demonstrating the effectiveness of catalysis for this purpose. 1 into PostgreSQL...\n",
      "Inserting test sample 2213  Recently, deep neural networks have expanded the state-of-art in various scientific fields and provided solutions to long standing problems across multiple application domains. Nevertheless, they also suffer from weaknesses since their optimal performance depends on massive amounts of training data and the tuning of an extended number of parameters. As a countermeasure, some deep-forest methods have been recently proposed, as efficient and low-scale solutions. Despite that, these approaches simply employ label classification probabilities as induced features and primarily focus on traditional classification and regression tasks, leaving multi-output prediction under-explored. Moreover, recent work has demonstrated that tree-embeddings are highly representative, especially in structured output prediction. In this direction, we propose a novel deep tree-ensemble (DTE) model, where every layer enriches the original feature set with a representation learning component based on tree-embeddings. In this paper, we specifically focus on two structured output prediction tasks, namely multi-label classification and multi-target regression. We conducted experiments using multiple benchmark datasets and the obtained results confirm that our method provides superior results to state-of-the-art methods in both tasks. 0 into PostgreSQL...\n",
      "Inserting test sample 2214  This paper proposes an approach for multi-output prediction using deep tree-ensembles. While deep neural networks (DNNs) have shown promising results in solving single-output prediction problems, they can be limited in their ability to handle complex multi-output scenarios. Tree-ensembles, on the other hand, provide a highly interpretable and modular framework for modeling such problems. However, they are not capable of capturing the complex feature interactions necessary for learning from high-dimensional data. To address this issue, we propose using deep tree-ensembles, which leverage the strengths of both DNNs and tree-ensembles. Our experimental results on several real-world datasets demonstrate that this approach outperforms the state-of-the-art methods for multi-output prediction. Moreover, we show that our proposed method allows us to extract meaningful insights into the underlying data generating processes. Overall, this work presents a novel and effective framework for multi-output prediction that is both interpretable and powerful. 1 into PostgreSQL...\n",
      "Inserting test sample 2215  We calculate the in-medium $D$ meson self-energies in a hot pion gas induced by resonance interactions with pions. The appropriate resonances in the {\\it s},~ {\\it p} and {\\it d} waves of the $D$ meson-pion pair are represented by low lying scalar, vector and tensor $D^*$ mesons. At temperatures around 200 MeV the D-meson mass drops by $30~ \\rm {MeV}$ and the scattering width grows up to $60~ \\rm {MeV}$. Similar medium effects are found for the $D^*$ vector mesons. This opens and/or enhances the decay and/or dissociation channels of the charmonium states $\\Psi^\\prime$, $\\chi_c$ and $J/\\Psi$ to $D \\bar D,~D^* \\bar D,~D \\bar D^* ,~D^* \\bar D^*$ pairs in pion matter. 0 into PostgreSQL...\n",
      "Inserting test sample 2216  This study investigates the behavior of $D$-mesons and charmonium states in hot pion matter. The strong interactions between these particles and the surrounding mesons are modeled using an effective Lagrangian and a coupled-channel approach. Results from our calculations indicate the suppression of charmonium states due to the abundance of $D$-mesons, which are seen to survive up to higher temperatures. We observe a shift in the masses of the $D$-mesons and the Ï‡c and J/Ïˆ states, which may have implications for the interpretation of experimental data from heavy-ion collisions. Our findings provide insight into the properties of hot and dense hadronic matter, which is relevant to the study of the quark-gluon plasma phase transition. 1 into PostgreSQL...\n",
      "Inserting test sample 2217  The discovery of close-to-star gas-giant exo-planets lends support to the idea of Earth's origin as a Jupiter-like gas-giant and to the consequences of its compression, including whole-Earth decompression dynamics that gives rise, without requiring mantle convection, to the myriad measurements and observations whose descriptions are attributed to plate tectonics. I propose here another, unanticipated consequence of whole-Earth decompression dynamics: namely, a specific, dominant, non-erosion, underlying initiation-mechanism precursor for submarine canyons that follows as a direct consequence of Earth's early origin as a Jupiter-like gas-giant. 0 into PostgreSQL...\n",
      "Inserting test sample 2218  This study addresses the debated topic of the origin of submarine canyons. Using bathymetric data, we investigate whether canyons are primarily formed by subaerial or submarine processes. Our findings suggest that the most common mechanism responsible for the inception of submarine canyons is dense shelf water cascading. We provide evidence that this process may be triggered by various factors, such as tectonic uplift and sediment input. Our results elucidate the fundamental role that dense shelf water cascading plays in the formation of submarine canyons worldwide, and its significance for the oceanic carbon cycle. 1 into PostgreSQL...\n",
      "Inserting test sample 2219  We estimated iron and metallicity gradients in the radial and vertical directions with the F and G type dwarfs taken from the RAVE DR4 database. The sample defined by the constraints Zmax<=825 pc and ep<=0.10 consists of stars with metal abundances and space velocity components agreeable with the thin-disc stars. The radial iron and metallicity gradients estimated for the vertical distance intervals 0<Zmax<=500 and 500<Zmax<=800 pc are d[Fe/H]/dRm=-0.083(0.030) and d[Fe/H]/dRm=-0.048(0.037 )dex/kpc; and d[M/H]/dRm=-0.063(0.011) and d[M/H]/dRm=-0.028(0.057) dex/kpc, respectively, where Rm is the mean Galactocentric distance. The iron and metallicity gradients for less number of stars at further vertical distances, 800<Zmax<=1500 pc, are mostly positive. Compatible iron and metallicity gradients could be estimated with guiding radius (Rg) for the same vertical distance intervals 0<Zmax<=500 and 500<Zmax<=800 pc, i.e.\n",
      "\n",
      "d[Fe/H]/dRg=-0.083(0.030) and d[Fe/H]/dRg=-0.065(0.039) dex/kpc; d[M/H]/dRg=-0.062(0.018) and d[M/H]/dRg=-0.055(0.045) dex/kpc. F and G type dwarfs on elongated orbits show a complicated radial iron and metallicity gradient distribution in different vertical distance intervals. Significant radial iron and metallicity gradients could be derived neither for the sub-sample stars with Rm<=8 kpc, nor for the ones at larger distances, Rm>8 kpc. The range of the iron and metallicity abundance for the F and G type dwarfs on elongated orbits, [-0.13, -0.01), is similar to the thin-disc stars, while at least half of their space velocity components agree better with those of the thick-disc stars. The vertical iron gradients estimated for the F and G type dwarfs on circular orbits are d[Fe/H]/dZmax=-0.176(0.039) dex/kpc and d[Fe/H]/dZmax=-0.119(0.036) dex/kpc for the intervals Zmax<= 825 and Zmax<=1500 pc, respectively. 0 into PostgreSQL...\n",
      "Inserting test sample 2220  This study investigates the metallicity gradients of F-G main-sequence stars using RAVE data to better understand the local stellar kinematics. We measured the radial velocities, metallicities, and distances of over 12,000 stars within 2 kpc of the Sun. We divided our sample into Galactic quadrants to identify any asymmetries in the metallicity gradients. Our results show that the metallicity gradients are, on average, steeper towards the inner regions of the Galaxy in all quadrants, consistent with previous studies. We also find significant variations in the metallicity gradients depending on the quadrant and distance from the Galactic plane. We interpret these variations as evidence of the effects of different formation and evolution histories in different parts of the Galaxy and perhaps different chemical enrichment processes.\n",
      "\n",
      "We find that the gradients in the first and fourth Galactic quadrants are steeper than those in the second and third quadrants, which is consistent with previous studies based on smaller samples of stars. Additionally, we find a change in the slope of the metallicity gradient for stars located between 1 and 1.5 kpc from the Galactic plane, which suggests a transition from a predominantly in-situ formation mode to a more mixed, accreted population. Our results also reveal a flattening of the metallicity gradients in the fourth quadrant at larger distances from the Sun. We interpret this as indicating the presence of a relatively metal-rich, thick-disk-like population in the outer regions of the Galaxy.\n",
      "\n",
      "In summary, we have used RAVE data to obtain metallicity gradients for F-G main-sequence stars in different Galactic quadrants. We find that the metallicity gradients are steeper towards the inner parts of the Galaxy and show significant variations depending on the quadrant and distance from the Galactic plane. These results suggest different formation and evolution histories in different parts of the Galaxy and different chemical enrichment processes. This study provides important constraints for models of Galactic chemical evolution and offers a deeper understanding of the kinematics of local stellar populations. 1 into PostgreSQL...\n",
      "Inserting test sample 2221  Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus. 0 into PostgreSQL...\n",
      "Inserting test sample 2222  Neural Radiance Fields (NeRF) have proven to be a powerful technique for representing complex 3D scenes. However, they suffer from limitations, such as slow training times and high memory requirements, that restrict their scalability and generalization performance. In response, we present NeRF++, an improved NeRF model that addresses these issues by introducing a multi-level hierarchical structure. We demonstrate that the proposed model significantly outperforms the original NeRF on various datasets, achieving higher image quality and faster training times. Additionally, we analyze the behavior of NeRF++ and provide insights into its design choices. Our experiments highlight the effectiveness of our modifications, which enable NeRF++ to model complex scenes more efficiently and accurately than previous methods. 1 into PostgreSQL...\n",
      "Inserting test sample 2223  The present paper deals with the following hyperbolic--elliptic coupled system, modelling dynamics of a gas in presence of radiation, $u_{t}+ f(u)_{x} +Lq_{x}=0, -q_{xx} + Rq +G\\cdot u_{x}=0,$ where $u\\in\\R^{n}$, $q\\in\\R$ and $R>0$, $G$, $L\\in\\R^{n}$. The flux function $f : \\R^n\\to\\R^n$ is smooth and such that $\\nabla f$ has $n$ distinct real eigenvalues for any $u$. The problem of existence of admissible radiative shock wave is considered, i.e. existence of a solution of the form $(u,q)(x,t):=(U,Q)(x-st)$, such that $(U,Q)(\\pm\\infty)=(u_\\pm,0)$, and $u_\\pm\\in\\R^n$, $s\\in\\R$ define a shock wave for the reduced hyperbolic system, obtained by formally putting L=0. It is proved that, if $u_-$ is such that $\\nabla\\lambda_{k}(u_-)\\cdot r_{k}(u_-)\\neq 0$,(where $\\lambda_k$ denotes the $k$-th eigenvalue of $\\nabla f$ and $r_k$ a corresponding right eigenvector) and $(\\ell_{k}(u_{-})\\cdot L) (G\\cdot r_{k}(u_{-})) >0$, then there exists a neighborhood $\\mathcal U$ of $u_-$ such that for any $u_+\\in{\\mathcal U}$, $s\\in\\R$ such that the triple $(u_{-},u_{+};s)$ defines a shock wave for the reduced hyperbolic system, there exists a (unique up to shift) admissible radiative shock wave for the complete hyperbolic--elliptic system. Additionally, we are able to prove that the profile $(U,Q)$ gains smoothness when the size of the shock $|u_+-u_-|$ is small enough, as previously proved for the Burgers' flux case. Finally, the general case of nonconvex fluxes is also treated, showing similar results of existence and regularity for the profiles. 0 into PostgreSQL...\n",
      "Inserting test sample 2224  This research paper examines the behavior of shock waves for a class of radiative hyperbolic-elliptic systems. Specifically, we investigate the interactions and evolution of shock waves in the presence of radiation, which has important applications in astrophysics, combustion, and other fields. The mathematical analysis of such systems is challenging due to their complexity, and requires the use of both analytical and numerical methods. \n",
      "\n",
      "Our study begins with a detailed review of the relevant background and literature, describing the physics of radiative transfer and its connection to hyperbolic and elliptic problems. We then present a rigorous analysis of shock waves in these systems, with a focus on developing a framework for understanding the effects of radiation on shock wave propagation. Our analysis incorporates a number of key mathematical tools, including the theory of hyperbolic conservation laws, dispersive estimates, and spectral methods. \n",
      "\n",
      "To illustrate our results, we provide a number of numerical simulations and test cases, which demonstrate the validity and usefulness of our theoretical framework. In particular, we show that the presence of radiation can significantly alter the properties of shock waves, leading to interesting and novel phenomena such as radiation-driven instabilities and nonlinear wave interactions. \n",
      "\n",
      "Overall, our research contributes to a deeper understanding of the dynamics of radiative hyperbolic-elliptic systems, and has important implications for a range of scientific and technological applications. Our results also open up new avenues for further research, and lay the foundation for future studies in this rapidly developing field of mathematical and scientific inquiry. 1 into PostgreSQL...\n",
      "Inserting test sample 2225  In accretion-based models for Sgr A* the X-ray, infrared, and millimeter emission arise in a hot, geometrically thick accretion flow close to the black hole. The spectrum and size of the source depend on the black hole mass accretion rate $\\dot{M}$. Since Gillessen et al. have recently discovered a cloud moving toward Sgr A* that will arrive in summer 2013, $\\dot{M}$ may increase from its present value $\\dot{M}_0$. We therefore reconsider the \"best-bet\" accretion model of Moscibrodzka et al., which is based on a general relativistic MHD flow model and fully relativistic radiative transfer, for a range of $\\dot{M}$. We find that for modest increases in $\\dot{M}$ the characteristic ring of emission due to the photon orbit becomes brighter, more extended, and easier to detect by the planned Event Horizon Telescope submm VLBI experiment. If $\\dot{M} \\gtrsim 8 \\dot{M}_0$ this \"silhouette of the black hole will be hidden beneath the synchrotron photosphere at 230 GHz, and for $\\dot{M} \\gtrsim 16 \\dot{M}_0$ the silhouette is hidden at 345 GHz. We also find that for $\\dot{M} > 2 \\dot{M}_0$ the near-horizon accretion flow becomes a persistent X-ray and mid-infrared source, and in the near-infrared Sgr A* will acquire a persistent component that is brighter than currently observed flares. 0 into PostgreSQL...\n",
      "Inserting test sample 2226  The Galactic Center is a complex, highly dynamic region of our Milky Way galaxy. It is the site of enormous astrophysical phenomena, including a supermassive black hole and intense star formation. Understanding the weather in this region can provide insights into the behavior of the galaxy as a whole. In this study, we present a detailed analysis of the Galactic Center weather forecast. Our analysis is based on high-resolution observations from a variety of cutting-edge telescopes, and includes detailed simulations utilizing state-of-the-art models of galactic dynamics and stellar evolution. We find that the Galactic Center weather is highly variable, with frequent bursts of intense activity and periodic lulls. Our simulations suggest that this weather is driven by a complex interplay of gravity, magnetism, and thermodynamics. The implications of our findings are significant, as they provide new insights into the fundamental nature of the universe. Furthermore, our work lays the groundwork for future studies of the Galactic Center weather and its relationship to the overall behavior of our galaxy. 1 into PostgreSQL...\n",
      "Inserting test sample 2227  We report on the first Cosmic Origins Spectrograph (COS) observations of damped and sub-damped Lyman-alpha (DLA) systems discovered in a new survey of the gaseous halos of low-redshift galaxies. From observations of 37 sightlines, we have discovered three DLAs and four sub-DLAs. We measure the neutral gas density Omega(HI), and redshift density dN/dz, of DLA and sub-DLA systems at z<0.35. We find dN/dz=0.25 and Omega(HI)=1.4x10^-3 for DLAs, and dN/dz=0.08 with Omega(HI)=4.2x10^-5 for sub-DLAs over a redshift path delta z=11.9. To demonstrate the scientific potential of such systems, we present a detailed analysis of the DLA at z=0.1140 in the spectrum of SDSS J1009+0713. Profile fits to the absorption lines determine log N(H I)=20.68pm0.10 with a metallicity determined from the undepleted element Sulfur of [S/H]=-0.62pm0.18.\n",
      "\n",
      "The abundance pattern of this DLA is similar to that of higher z DLAs, showing mild depletion of the refractory elements Fe and Ti with [S/Fe]=+0.24pm0.22 and [S/Ti]=+0.28pm0.15. Nitrogen is underabundant in this system with [N/H]=-1.40pm0.14, placing this DLA below the plateau of the [N/alpha] measurements in the local Universe at similar metallicities. This DLA has a simple kinematic structure with only two components required to fit the profiles and a kinematic width of 52 km/s. Imaging of the QSO field with WFC3 reveals a spiral galaxy at very small impact parameter to the QSO and several galaxies within 10\". Followup spectra with LRIS reveal that none of the nearby galaxies are at the redshift of the DLA. The spiral galaxy is identified as the host galaxy of the QSO based on the near perfect alignment of the nucleus and disk of the galaxy as well as spectra of an H II region showing emission lines at the QSO redshift. A small feature appears 0.70\" from the nucleus of the QSO after PSF subtraction, providing another candidate for the host galaxy of the DLA. (abb) 0 into PostgreSQL...\n",
      "Inserting test sample 2228  This paper presents the first observations of low redshift damped Lyman-alpha (DLA) systems with the Cosmic Origins Spectrograph (COS) on the Hubble Space Telescope (HST). DLA systems are characterized by absorption features in the spectra of quasars and galaxies, caused by neutral hydrogen gas along the line of sight. They are important probes of the interstellar medium and the processes of star formation, metal enrichment, and galactic feedback.\n",
      "\n",
      "The COS instrument offers unprecedented sensitivity and spectral resolution in the ultraviolet band, enabling us to detect and analyze DLA systems at redshifts as low as z~0.1. We have identified and studied a sample of 28 DLA systems in the spectra of nearby quasars and galaxies, with neutral hydrogen column densities ranging from 10^19 to 10^21 cm^-2.\n",
      "\n",
      "Our analysis shows that the low redshift DLA systems have properties consistent with their higher redshift counterparts, such as the distribution of column densities and metallicities, indicating that the physical conditions of the neutral gas in galaxies have not significantly evolved over cosmic time. We also find evidence for correlations between DLA properties and host galaxy properties, such as luminosity and star formation rate, suggesting that DLA systems may provide insights into the co-evolution of galaxies and their interstellar medium.\n",
      "\n",
      "Furthermore, by comparing the observed DLA absorption profiles with simulated models, we can constrain the underlying physical parameters of the gas, such as the gas kinematics, temperature, and ionization state. Our results provide important constraints on theoretical models of galaxy formation and evolution, and have implications for future observations with the upcoming James Webb Space Telescope (JWST).\n",
      "\n",
      "In summary, our study represents a significant step forward in our understanding of the properties and origins of low redshift DLA systems, and demonstrates the capabilities of the COS instrument for probing the interstellar medium in nearby galaxies and quasars. 1 into PostgreSQL...\n",
      "Inserting test sample 2229  We establish an analogy between superconductor-metal interfaces and the quantum physics of a black hole, using the proximity effect. We show that the metal-superconductor interface can be thought of as an event horizon and Andreev reflection from the interface is analogous to the Hawking radiation in black holes. We describe quantum information transfer in Andreev reflection with a final state projection model similar to the Horowitz-Maldacena model for black hole evaporation. We also propose the Andreev reflection-analogue of Hayden and Preskill's description of a black hole final state, where the black hole is described as an information mirror. The analogy between Crossed Andreev Reflections and Einstein-Rosen bridges is discussed: our proposal gives a precise mechanism for the apparent loss of quantum information in a black hole by the process of nonlocal Andreev reflection, transferring the quantum information through a wormhole and into another universe. Given these established connections, we conjecture that the final quantum state of a black hole is exactly the same as the ground state wavefunction of the superconductor/superfluid in the Bardeen-Cooper-Schrieffer (BCS) theory of superconductivity; in particular, the infalling matter and the infalling Hawking quanta, described in the Horowitz-Maldacena model, forms a Cooper pair-like singlet state inside the black hole. A black hole evaporating and shrinking in size can be thought of as the analogue of Andreev reflection by a hole where the superconductor loses a Cooper pair. Our model does not suffer from the black hole information problem since Andreev reflection is unitary. We also relate the thermodynamic properties of a black hole to that of a superconductor, and propose an experiment which can demonstrate the negative specific heat feature of black holes in a growing/evaporating condensate. 0 into PostgreSQL...\n",
      "Inserting test sample 2230  Andreev reflections and the quantum physics of black holes have been the subject of much theoretical research in recent years. In this paper, we investigate the relationship between Andreev reflections and black hole thermodynamics, showcasing the importance of these reflections in understanding the connection between gravity and quantum mechanics.\n",
      "\n",
      "Following a brief introduction to Andreev reflections and their relevance to condensed matter physics, we explore the phenomenon of Hawking radiation and its relationship to black hole entropy. We then delve into the concept of quantum tunneling, highlighting its important role in the understanding of Andreev reflections.\n",
      "\n",
      "We discuss the role of Andreev reflections in the study of black hole formation and evaporation, highlighting their potential for experimental observation. Moreover, we examine the parallels between Andreev reflections and black hole radiation, providing a theoretical foundation for the study of these processes in quantum gravity.\n",
      "\n",
      "We then proceed to discuss the mathematical formalism behind Andreev reflections and black hole thermodynamics, emphasizing their convergence in the context of quantum physics. Finally, we explore future directions for research in this field, which include the experimental testing of Andreev reflections and the application of these phenomena to the development of new quantum technologies.\n",
      "\n",
      "Overall, our investigation sheds light on the profound relationship between Andreev reflections and black hole physics, underscoring the importance of these phenomena in the understanding of fundamental physics. This work provides a valuable contribution to the ongoing discussion on the role of quantum mechanics in the description of black holes. 1 into PostgreSQL...\n",
      "Inserting test sample 2231  We study the low-energy behavior of the vertex function of a single Anderson impurity away from half-filling for finite magnetic fields, using the Ward identities with careful consideration of the anti-symmetry and analytic properties. The asymptotic form of the vertex function $\\Gamma_{\\sigma\\sigma';\\sigma'\\sigma}^{}(i\\omega,i\\omega';i\\omega',i\\omega)$ is determined up to terms of linear order with respect to the two frequencies $\\omega$ and $\\omega'$, as well as the $\\omega^2$ contribution for anti-parallel spins $\\sigma'\\neq \\sigma$ at $\\omega'=0$. From these results, we also obtain a series of the Fermi-liquid relations beyond those of Yamada-Yosida. The $\\omega^2$ real part of the self-energy $\\Sigma_{\\sigma}^{}(i\\omega)$ is shown to be expressed in terms of the double derivative $\\partial^2\\Sigma_{\\sigma}^{}(0)/\\partial \\epsilon_{d\\sigma}^{2}$ with respect to the impurity energy level $\\epsilon_{d\\sigma}^{}$, and agrees with the formula obtained recently by Filippone, Moca, von Delft, and Mora in the Nozi\\`{e}res phenomenological Fermi-liquid theory [Phys.\\ Rev.\\ B {\\bf 95}, 165404 (2017)]. We also calculate the $T^2$ correction of the self-energy, and find that the real part can be expressed in terms of the three-body correlation function $\\chi_{\\uparrow\\downarrow,-\\sigma}^{[3]} = \\partial \\chi_{\\uparrow\\downarrow}/\\partial \\epsilon_{d,-\\sigma}^{}$. We also provide an alternative derivation of the asymptotic form of the vertex function.\n",
      "\n",
      "Specifically, we calculate the skeleton diagrams for the vertex function $\\Gamma_{\\sigma\\sigma;\\sigma\\sigma}^{}(i\\omega,0;0,i\\omega)$ for parallel spins up to order $U^4$ in the Coulomb repulsion $U$. It directly clarifies the fact that the analytic components of order $\\omega$ vanish as a result of the cancellation of four related Feynman diagrams which are related to each other through the anti-symmetry operation. 0 into PostgreSQL...\n",
      "Inserting test sample 2232  In this study, we investigate the higher-order Fermi-liquid corrections for an Anderson impurity away from half-filling in regards to its equilibrium properties. We evaluate the impurity's self-energy using perturbation theory and calculate the correction terms to its imaginary part. These corrections exhibit a dependence on both the impurity's energy and temperature, which we analyze using various approaches. \n",
      "\n",
      "Our results reveal the significance of the second-order correction terms in describing the impurity's equilibrium properties. We find that these terms can contribute up to 40% of the total imaginary part of the self-energy, indicating their crucial role in accurately characterizing the impurity's behavior. By obtaining these higher-order corrections, we are able to identify previously unaccounted effects that are relevant for systems where perturbation theory is applicable but the standard, first-order treatments are insufficient.\n",
      "\n",
      "Furthermore, we examine the dependence of these correction terms on the impurity's position relative to the Fermi level, utilizing sophisticated numerical methods. We explore the role of temperature in the behavior of the correction terms, finding that they behave differently in the low-temperature and high-temperature limits. We also study the impact of the hybridization strength on the correction terms, finding that it can significantly affect their behavior.\n",
      "\n",
      "Our work significantly advances the understanding of the higher-order Fermi-liquid corrections for Anderson impurities away from half-filling and their relevance in describing the impurity's equilibrium properties. Our results can also provide valuable insights for the development of more advanced models and theories for analyzing these types of systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2233  We define generalised notions of biunitary elements in planar algebras and show that objects arising in quantum information theory such as Hadamard matrices, quantum latin squares and unitary error bases are all given by biunitary elements in the spin planar algebra. We show that there are natural subfactor planar algebras associated with biunitary elements. 0 into PostgreSQL...\n",
      "Inserting test sample 2234  This paper explores the connections between planar algebras, quantum information theory, and subfactors. We investigate the use of the theory of planar algebras in quantum information, and the role of subfactors in this context. The main goal is to develop new techniques for studying tensor categories and to gain a deeper understanding of their relationship to quantum theory. 1 into PostgreSQL...\n",
      "Inserting test sample 2235  We consider a family of quantum channels characterized by the fact that certain (in general nonorthogonal) Pure states at the channel entrance are mapped to (tensor) Products of Pure states (PPP, hence \"pcubed\") at the complementary outputs (the main output and the \"environment\") of the channel.\n",
      "\n",
      "The pcubed construction, a reformulation of the twisted-diagonal procedure by M. M Wolf and D. Perez-Garcia, [Phys. Rev. A 75, 012303 (2007)], can be used to produce a large class of degradable quantum channels; degradable channels are of interest because their quantum capacities are easy to calculate. Several known types of degradable channels are either pcubed channels, or subchannels (employing a subspace of the channel entrance), or continuous limits of pcubed channels. The pcubed construction also yields channels which are neither degradable nor antidegradable (i.e., the complement of a degradable channel); a particular example of a qutrit channel of this type is studied in some detail.\n",
      "\n",
      "Determining whether a pcubed channel is degradable or antidegradable or neither is quite straightforward given the pure input and output states that characterize the channel. Conjugate degradable pcubed channels are always degradable. 0 into PostgreSQL...\n",
      "Inserting test sample 2236  This paper presents a study on degradable quantum channels, focusing on the use of pure to product of pure states isometries. The aim is to investigate the conditions under which a quantum channel can be degraded, and to provide insight into the fundamental properties of quantum systems. Our analysis shows that the use of pure to product of pure states isometries can effectively degrade quantum channels. Moreover, we present a method for characterizing the degradability of quantum channels in terms of the Choi rank and the coherence of the channel. We then apply these results to various examples, demonstrating that the use of pure to product of pure states isometries can lead to significant improvements in the performance of quantum communication protocols. Our findings have important implications for the development of more efficient and secure quantum technologies, as well as for the study of the basic principles of quantum mechanics. 1 into PostgreSQL...\n",
      "Inserting test sample 2237  We report the complete statistical treatment of a system of particles interacting via Newtonian forces in continuous boundary-driven flow, far from equilibrium. By numerically time-stepping the force-balance equations of a model fluid we measure occupancies and transition rates in simulation. The high-shear-rate simulation data verify the invariant quantities predicted by our statistical theory, thus demonstrating that a class of non-equilibrium steady states of matter, namely sheared complex fluids, is amenable to statistical treatment from first principles. 0 into PostgreSQL...\n",
      "Inserting test sample 2238  This study investigates the behavior of a sheared system using statistical mechanics far from equilibrium. We propose a theoretical model to predict the response of the material beyond the point where linear response theories fail. The theoretical model is tested using numerical simulations and the results are compared with experimental data. Our findings demonstrate the applicability of the model in describing the behavior of the sheared system, offering insights into the fundamental physics underlying the response of materials under non-equilibrium conditions. 1 into PostgreSQL...\n",
      "Inserting test sample 2239  Baryon to meson transition distribution amplitudes (TDAs), non-diagonal matrix elements of the nonlocal three quark operator between a nucleon and a meson state, extend the concept of generalized parton distributions. These non-perturbative objects which encode the information on three quark correlations inside the nucleon may be accessed experimentally in backward meson electroproduction reactions. We suggest a general framework for modelling nucleon to pion (pi N) TDAs employing the spectral representation for pi N TDAs in terms of quadruple distributions. The factorized Ansatz for quadruple distributions with input from the soft-pion theorem for pi N TDAs is proposed.\n",
      "\n",
      "It is to be complemented with a D-term like contribution from the nucleon exchange in the cross channel.\n",
      "\n",
      "We present our estimates of the unpolarized cross section and of the transverse target single spin asymmetry for backward pion electroproduction within the QCD collinear factorization approach in which the non-perturbative part of the amplitude involves pi N TDAs. The cross section is sizable enough to be studied in high luminosity experiments such as J-lab@12GeV and EIC. 0 into PostgreSQL...\n",
      "Inserting test sample 2240  This research paper explores the nucleon-to-pion transition distribution amplitudes (TDAs) and backward electroproduction of pions. Theoretical calculations and experimental measurements are combined to study the electroproduction of pions in the backward direction. Backward pion electroproduction is sensitive to the nucleon's TDA, which is the non-perturbative aspect of Quantum Chromodynamics (QCD). Electron-proton scattering data is used to extract the TDAs, and the dependence of the results on scale and factorization is examined. The study of backward electroproduction of pions is also crucial in interpreting the results from the upcoming experimental programs at the Thomas Jefferson National Accelerator Facility. In addition, the investigation of such TDAs aids in the comprehension of the internal structure of hadrons, and their properties and interactions. This work provides relevant information towards understanding the different aspects of quantum chromodynamics, including confinement and hadronization. 1 into PostgreSQL...\n",
      "Inserting test sample 2241  The conductivity properties between Luttinger liquids are analyzed by exact Renormalization Group methods. We prove that in a two chain system or in a model of bilayer graphene, described by two coupled fermionic honeycomb lattices interacting with a gauge field, the transverse optical conductivity at finite temperature is anomalous and decreasing together with the frequency as a power law with Luttinger liquid exponent. 0 into PostgreSQL...\n",
      "Inserting test sample 2242  We investigate conductivity in coupled chains and bilayer graphene within the framework of Luttinger liquids. We employ the concept of composite fermions to model electron correlation effects and derive physically intuitive expressions for the frequency-dependent conductivity in both systems. Our results indicate that coupled chains may exhibit a richer transport behavior compared to bilayer graphene which manifests significant suppression of intra-layer conductance. 1 into PostgreSQL...\n",
      "Inserting test sample 2243  We address the problem of multicollinearity in a function-on-scalar regression model by using a prior which simultaneously selects, clusters, and smooths functional effects. Our methodology groups effects of highly correlated predictors, performing dimension reduction without dropping relevant predictors from the model. We validate our approach via a simulation study, showing superior performance relative to existing dimension reduction approaches in the function-on-scalar literature. We also demonstrate the use of our model on a data set of age specific fertility rates from the United Nations Gender Information database. 0 into PostgreSQL...\n",
      "Inserting test sample 2244  This paper proposes a novel approach to simultaneously select variables, cluster data, and smooth functions for scalar regression problems. Our method is based on a hierarchical model that allows for both global and local variable selection, while also clustering the data based on similarities in the identified functions. This joint modeling strategy enables us to obtain more accurate estimates of the regression function under complex data structures. We demonstrate the effectiveness of our approach through simulations and an application to a real-world dataset in gene expression analysis. 1 into PostgreSQL...\n",
      "Inserting test sample 2245  Polymeric ionic liquids are emerging polyelectrolyte materials for modern electrochemical applications. In this paper, we propose a self-consistent field theory of the polymeric ionic liquid on a charged conductive electrode. Taking into account the conformation entropy of rather long polymerized cations within the Lifshitz theory and electrostatic and excluded volume interactions of ionic species within the mean-field approximation, we obtain a system of self-consistent field equations for the local electrostatic potential and average concentrations of monomeric units and counterions. We solve these equations in the linear approximation for the cases of a point-like charge and a flat infinite uniformly charged electrode immersed in a polymeric ionic liquid and derive analytical expressions for local ionic concentrations and electrostatic potential, and derive an analytical expression for the linear differential capacitance of the electric double layer. We also find a numerical solution to the self-consistent field equations for two types of boundary conditions for the local polymer concentration on the electrode, corresponding to the cases of the specific adsorption absence (indifferent surface) and strong short-range repulsion of the monomeric units near the charged surface (hard wall case). For both cases, we investigate the behavior of differential capacitance as a function of applied voltage for a pure polymeric ionic liquid and a polymeric ionic liquid dissolved in a polar organic solvent. We observe that the differential capacitance profile shape is strongly sensitive to the adopted boundary condition for the local polymer concentration on the electrode. 0 into PostgreSQL...\n",
      "Inserting test sample 2246  In this research paper, we discuss the synthesis and electrochemical behavior of polymerized ionic liquids on an electrified electrode. The investigation involves the study of the formation of the polymer film and its effect on the electrochemical performance. We demonstrate that the polymerization process leads to improved adhesion of the ionic liquid to the electrode surface and facilitates the charge transfer process. The polymerization reaction is monitored using several analytical techniques, including cyclic voltammetry, electrochemical impedance spectroscopy, and scanning electron microscopy. We identify that the polymerization process introduces a high degree of structural diversity, which influences the ionic conductivity of the materials. Additionally, we investigate the effect of temperature and ionic strength on the electrical properties of the material. Our results show that the ion diffusion coefficient of the polymerized ionic liquid is affected by both temperature and ionic strength variations. This research highlights the potential of using polymerized ionic liquids for energy applications and as a platform for investigating ion transport and electrochemical behavior in soft materials. The findings presented in this study contribute to the continued development of ionic liquid-based systems and their potential use in energy storage devices. 1 into PostgreSQL...\n",
      "Inserting test sample 2247  The effects of line tension on the morphology of a sessile droplet placed on top of a convex spherical substrate are studied. The morphology of the droplet is determined from the global minimum of the Helmholtz free energy. The contact angle between the droplet and the spherical substrate is expressed by the generalized Young's formula. When the line tension is positive and large, the contact angle jumps discontinuously to $180^{\\circ}$, the circular contact line shrinks towards the top of the substrate, and the droplet detaches from the substrate, forming a spherical droplet if the substrate is hydrophobic (i.e., the Young's contact angle is large). This finding is consistent with that predicted by Widom [J. Phys. Chem. {\\bf 99}, 2803 (1995)]; the line tension induces a drying transition on a flat substrate. On the other hand, the contact angle jumps to $0^{\\circ}$, the circular contact line shrinks towards the bottom of the substrate, and the droplet spreads over the substrate to form a wrapped spherical droplet if the substrate is hydrophilic (i.e., the Young's contact angle is small). Therefore, not only the drying transition of a cap-shaped to a detached spherical droplet but also the wetting transition of a cap-shaped to a wrapped spherical droplet could occur on a spherical substrate as the surface area of the substrate is finite. When the line tension is negative and its magnitude increases, the contact line asymptotically approaches the equator from either above or below. The droplet with a contact line that coincides with the equator is an isolated, singular solution of the first variational problem. In this instance, the contact line is pinned and cannot move as far as the line tension is smaller than the critical magnitude, where the wetting transition occurs. 0 into PostgreSQL...\n",
      "Inserting test sample 2248  This study investigates the intricate relationship between the morphology of a sessile droplet and its line tension on a spherical substrate. The line tension is a crucial parameter that governs the droplet's shape and is highly sensitive to the substrate's curvature. In this paper, we present a theoretical model that accounts for the line tension's contribution to the droplet's deformation and subsequent equilibrium. Our model considers the adhesion energy of the droplet, the surface energy of the substrate, and the capillary forces. By varying the line tension and the droplet's contact angle, we explore the droplet's shape for different substrate radii and wettabilities. We find that the droplet's shape transitions from spherical to ellipsoidal as the substrate curvature increases, with the droplet spreading more on hydrophobic substrates. Additionally, we demonstrate that the line tension plays a significant role in determining the droplet's contact angle and its dependence on the substrate radius. Our findings not only shed light on the fundamental physics of wetting and adhesion on curved surfaces, but they are also crucial for a wide range of applications. For instance, understanding the droplet-substrate interaction is essential for designing microfluidic devices, where droplet manipulation is commonly employed. Overall, our results reveal fascinating insights into the complex interplay between line tension and droplet morphology on spherical substrates, with promising implications for various fields of science and engineering. 1 into PostgreSQL...\n",
      "Inserting test sample 2249  A flux qubit can have a relatively long decoherence time at the degeneracy point, but away from this point the decoherence time is greatly reduced by dephasing. This limits the practical applications of flux qubits. Here we propose a new qubit design modified from the commonly used flux qubit by introducing an additional capacitor shunted in parallel to the smaller Josephson junction (JJ) in the loop. Our results show that the effects of noise can be considerably suppressed, particularly away from the degeneracy point, by both reducing the coupling energy of the JJ and increasing the shunt capacitance. This shunt capacitance provides a novel way to improve the qubit. 0 into PostgreSQL...\n",
      "Inserting test sample 2250  The quest for quantum computational supremacy and its potential benefits in various fields has become a focus of research in the past decades. However, realizing a scalable quantum computer demands overcoming the fragility and susceptibility to noise of quantum systems. As a solution to this challenge, this work presents a design for a low-decoherence flux qubit. By using a flux-biased superconducting loop with three Josephson junctions, we demonstrate a decoherence time increase of two orders of magnitude compared to similar qubits. Our results open new possibilities for constructing reliable and stable quantum devices, with potential applications in quantum simulation, cryptography, and optimization algorithms. 1 into PostgreSQL...\n",
      "Inserting test sample 2251  Two reactions, the photoproduction of a direct photon plus a jet and the photoproduction of a charged hadron plus a jet, are studied in view of their potential to constrain the gluon distribution in the proton. The results are based on a program of partonic event generator type which includes the full set of NLO corrections. 0 into PostgreSQL...\n",
      "Inserting test sample 2252  This work presents an overview of the latest results on constraining the gluon distribution in the proton, based on experimental data from photoproduction reactions. A variety of observables measured at different center-of-mass energies and photon virtualities are compared to state-of-the-art theoretical calculations, which enable to extract precise information on the gluon density with high accuracy. 1 into PostgreSQL...\n",
      "Inserting test sample 2253  A probabilistic characterization of the dominance partial order on the set of partitions is presented. This extends work in \"Symmetric polynomials and symmetric mean inequalities\". Electron. J. Combin., 20(3): Paper 34, 2013.\n",
      "\n",
      "Let $n$ be a positive integer and let $\\nu$ be a partition of $n$. Let $F$ be the Ferrers diagram of $\\nu$. Let $m$ be a positive integer and let $p \\in (0,1)$. Fill each cell of $F$ with balls, the number of which is independently drawn from the random variable $X = Bin(m,p)$. Given non-negative integers $j$ and $t$, let $P(\\nu,j,t)$ be the probability that the total number of balls in $F$ is $j$ and that no row of $F$ contains more that $t$ balls. We show that if $\\nu$ and $\\mu$ are partitions of $n$, then $\\nu$ dominates $\\mu$, i.e.\n",
      "\n",
      "$\\sum_{i=1}^k \\nu(i) \\geq \\sum_{i=1}^k \\mu(i)$ for all positive integers $k$, if and only if $P(\\nu,j,t) \\leq P(\\mu,j,t)$ for all non-negative integers $j$ and $t$. It is also shown that this same result holds when $X$ is replaced by any one member of a large class of random variables.\n",
      "\n",
      "Let $p = \\{p_n\\}_{n=0}^\\infty$ be a sequence of real numbers. Let ${\\cal T}_p$ be the $\\mathbb{N}$ by $\\mathbb{N}$ matrix with $({\\cal T}_p)_{i,j} = p_{j-i}$ for all $i, j \\in \\mathbb{N}$ where we take $p_n = 0$ for $n < 0$. Let $(p^i)_j$ be the coefficient of $x^j$ in $(p(x))^i$ where $p(x) = \\sum_{n=0}^\\infty p_n x^n$ and $p^0(x) =1$. Let ${\\cal S}_p$ be the $\\mathbb{N}$ by $\\mathbb{N}$ matrix with $({\\cal S}_p)_{i,j} = (p^i)_j$ for all $i, j \\in \\mathbb{N}$. We show that if ${\\cal T}_p$ is totally non-negative of order $k$ then so is ${\\cal S}_p$. The case $k=2$ of this result is a key step in the proof of the result on domination. We also show that the case $k=2$ would follow from a combinatorial conjecture that might be of independent interest. 0 into PostgreSQL...\n",
      "Inserting test sample 2254  This paper examines the dominance order on partitions and presents a probabilistic characterization of its behavior. The dominance order is a fundamental concept in mathematical combinatorics, used to describe the hierarchical relationships between sets of elements. Our approach builds on previous work in this area by introducing a novel probabilistic framework that combines order-theoretic and measure-theoretic techniques. We begin by defining the dominance order on partitions, showing how it arises naturally in the context of combinatorial enumeration. We then develop a measure-theoretic interpretation of the order, constructing a probability space that captures its essential properties. We prove that this probability space is isomorphic to certain spaces of random partitions studied in recent literature.\n",
      "\n",
      "Our main result is a complete characterization of the dominance order in terms of this probability space. We show that the probability of one partition dominating another can be computed using certain measures on the associated probability space. We also establish a number of new results about the structure of the order and its connections to other areas of probability theory. For example, we prove that the distribution of the size of a partition in the limit as the number of elements goes to infinity is governed by a stable distribution. This result has potential applications in various branches of mathematics and theoretical computer science.\n",
      "\n",
      "Our approach is heavily influenced by the recent surge of interest in probabilistic combinatorics, a field that studies the interplay between discrete structures and probability theory. We believe that our techniques have the potential to shed light on many other important questions related to the structure of partitions and their relationships. We conclude by discussing some open problems in this area, including the possibility of extending our results to other types of posets. In summary, our work provides a new perspective on the dominance order and its probabilistic interpretation, and opens up exciting avenues for further research. 1 into PostgreSQL...\n",
      "Inserting test sample 2255  Let $K$ be a knot in an L-space $Y$ with a Dehn surgery to a surface bundle over $S^1$. We prove that $K$ is rationally fibered, that is, the knot complement admits a fibration over $S^1$. As part of the proof, we show that if $K\\subset Y$ has a Dehn surgery to $S^1 \\times S^2$, then $K$ is rationally fibered. In the case that $K$ admits some $S^1 \\times S^2$ surgery, $K$ is Floer simple, that is, the rank of $\\hat{HFK}(Y,K)$ is equal to the order of $H_1(Y)$. By combining the latter two facts, we deduce that the induced contact structure on the ambient manifold $Y$ is tight.\n",
      "\n",
      "In a different direction, we show that if $K$ is a knot in an L-space $Y$, then any Thurston norm minimizing rational Seifert surface for $K$ extends to a Thurston norm minimizing surface in the manifold obtained by the null surgery on $K$ (i.e., the unique surgery on $K$ with $b_1>0$). 0 into PostgreSQL...\n",
      "Inserting test sample 2256  In knot theory, a knot is a closed curve in Euclidean space that does not intersect itself. Null surgery on knots is a way to change the topology of a knot by cutting along the knot and then reattaching a new piece in a particular way. L-spaces are a class of 3-manifolds with a distinguished property that arises from the Heegaard Floer homology theory. This paper studies the relationship between null surgery on knots and L-spaces. In particular, we explore the conditions under which null surgery on knots produces L-spaces and investigate various properties of this process. We also consider the behavior of null surgery under certain modifications of knots, such as satellite constructions. Our main result establishes a criterion for when null surgery on certain knots yields an L-space and provides a new perspective on the connection between knot theory and 3-manifold topology. This work has potential applications to the study of low-dimensional topology and the search for exotic 3-manifolds with unusual properties. 1 into PostgreSQL...\n",
      "Inserting test sample 2257  The lateral Casimir force acting between a sinusoidally corrugated gold plate and sphere was calculated and measured. The experimental setup was based on the atomic force microscope specially adapted for the measurement of the lateral Casimir force. The measured force oscillates sinusoidally as a function of the phase difference between the two corrugations. Both systematic and random errors are analysed and a lateral force amplitude of $3.2\\times 10^{-13} $N was measured at a separation distance of 221 nm with a resulting relative error 24% at a 95% confidence probability. The dependence of the measured lateral force amplitude on separation was investigated and shown to be consistent with the inverse fourth power distance dependence. The complete theory of the lateral Casimir force is presented including finite conductivity and roughness corrections. The obtained theoretical dependence was analysed as a function of surface separation, corrugation amplitudes, phase difference, and plasma wavelength of a metal. The theory was compared with the experimental data and shown to be in good agreement. The constraints on hypothetical Yukawa-type interactions following from the measurements of the lateral Casimir force are calculated. The possible applications of the lateral vacuum forces to nanotechnology are discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 2258  This paper presents a comprehensive investigation of the lateral Casimir force between two parallel plates with periodically modulated surface corrugations. Both theoretical and experimental methods were employed to probe the behavior of the Casimir force in this system. A rigorous theoretical approach was utilized to calculate the force exerted between the corrugated plate pair, taking into account the intricate geometries and dielectric properties of the surfaces. The results of the theoretical analysis revealed several interesting aspects of the Casimir force, including its dependence on the corrugation amplitude and wavelength. To corroborate these findings, we performed a series of meticulous experimental measurements using a state-of-the-art atomic force microscope. Our data confirmed the theoretical predictions, showing clear evidence of the lateral Casimir force between the corrugated surfaces. The observed behavior of the force also matched well with theoretical models. Our combined theoretical and experimental investigation provides valuable insights into the nature of the Casimir force and its behavior in complex geometries, which have important implications for a wide range of applications in nanotechnology, physics, and materials science. 1 into PostgreSQL...\n",
      "Inserting test sample 2259  We compute the glueball spectrum in (2+1)-dimensional Yang-Mills theory by analyzing correlators of the Leigh-Minic-Yelnikov ground-state wave-functional in the Abelian limit. The contribution of the WZW measure is treated by a controlled approximation and the resulting spectrum is shown to reduce to that obtained by Leigh et al., at large momentum. 0 into PostgreSQL...\n",
      "Inserting test sample 2260  We study the ground-state wave-functional of (2+1)-dimensional Yang-Mills theory in the Abelian limit. Using Monte Carlo methods, we evaluate the spectrum and its robustness, identifying a nonperturbative infrared phenomenon. Our results demonstrate the effectiveness of the functional approach to gauge theories as well as the utility of wave-functional Monte Carlo simulations. 1 into PostgreSQL...\n",
      "Inserting test sample 2261  We derive the astrometric orbit of the photo-center of the close pair alpha UMi AP (=alpha UMi Aa) of the Polaris multiple stellar system. The orbit is based on the spectroscopic orbit of the Cepheid alpha UMi A (orbital period of AP: 29.59 years), and on the difference Delta mu between the quasi-instantaneously measured HIPPARCOS proper motion of Polaris and the long-term-averaged proper motion given by the FK5. There remains an ambiguity in the inclination i of the orbit, since Delta mu cannot distinguish between a prograde orbit (i=50.1 deg) and a retrograde one (i=130.2 deg). Available photographic observations of Polaris favour strongly the retrograde orbit. For the semi-major axis of the photo-center of AP we find about 29 milliarcsec (mas). For the component P, we estimate a mass of 1.5 solar masses and a magnitude difference with respect to the Cepheid of 6.5 mag. The present separation between A and P should be about 160 mas. We obtain the proper motion of the center-of-mass of alpha UMi AP with a mean error of about 0.45 mas/year.\n",
      "\n",
      "Using the derived astrometric orbit, we find the position of the center-of-mass at the epoch 1991.31 with an accuracy of about 3.0 mas. Our ephemerides for the orbital correction, required for going from the position of the center-of-mass to the instantaneous position of the photo-center of AP at an arbitrary epoch, have a typical uncertainty of 5 mas. For epochs which differ from the HIPPARCOS epoch by more than a few years, a prediction for the actual position of Polaris based on our results should be significantly more accurate than using the HIPPARCOS data in a linear prediction, since the HIPPARCOS proper motion contains the instantaneous orbital motion of about 4.9 mas/year = 3.1 km/s.\n",
      "\n",
      "Finally we derive the galactic space motion of Polaris. 0 into PostgreSQL...\n",
      "Inserting test sample 2262  The astrometric orbit, position, and proper motion of Polaris have been investigated by astronomers worldwide. Polaris, also known as the North Star, is a key reference point for celestial navigation due to its fixed location in the Northern Hemisphere. Its position and motion have been meticulously tracked over time using a combination of ground-based observations and space-based missions.\n",
      "\n",
      "Recent studies have revealed that Polaris is not a single star, but rather a complex system consisting of three stars. The primary star, Polaris A, is a yellow supergiant with a mass approximately six times that of our Sun. The two companion stars, Polaris B and Polaris Ab, orbit Polaris A in a highly eccentric orbit.\n",
      "\n",
      "Astrometry measurements of Polaris have also been used to calculate its distance from Earth, which has been refined over the years through improved methodologies. Recent observations reveal that Polaris is approximately 433 light-years away from us.\n",
      "\n",
      "The proper motion of Polaris, or its apparent motion across the sky, has also been measured. Due to its significant distance from Earth, Polaris has a relatively small proper motion; it moves less than 0.02 arcseconds per year. However, this slow motion has allowed astronomers to detect other changes, such as the slight wobbling of Polaris A's axis of rotation.\n",
      "\n",
      "The study of Polaris provides valuable insights into the nature of celestial objects and their movements, as well as ways to navigate by the stars. Future observations and analyses of this system will undoubtedly reveal more about Polaris and the larger universe around us. 1 into PostgreSQL...\n",
      "Inserting test sample 2263  Context: The algorithms for generating a safe fluent API are actively studied these years. A safe fluent API is the fluent API that reports incorrect chaining of the API methods as a type error to the API users. Although such a safe property improves the productivity of its users, the construction of a safe fluent API is too complicated for the developers. The generation algorithms are studied to reduce the development cost of a safe fluent API. The study on the generation would benefit a number of programmers since a fluent API is a popular design in the real world.\n",
      "\n",
      "Inquiry: The generation of a generic fluent API has been left untackled. A generic fluent API refers to the fluent API that provides generic methods (methods that contain type parameters in their definitions). The Stream API in Java is an example of such a generic API. The recent research on the safe fluent API generation rather focuses on the grammar class that the algorithm can deal with for syntax checking. The key idea of the previous study is to use nested generics to represent a stack structure for the parser built on top of the type system. In that idea, the role of a type parameter was limited to internally representing a stack element of that parser on the type system. The library developers could not use type parameters to include a generic method in their API so that the semantic constraints for their API would be statically checked, for example, the type constraint on the items passed through a stream.\n",
      "\n",
      "Approach: We propose an algorithm to generate a generic fluent API. Our translation algorithm is modeled as the construction of deterministic finite automaton (DFA) with type parameter information. Each state of the DFA holds information about which type parameters are already bound in that state. This information is used to identify whether a method invocation in a chain newly binds a type to a type parameter, or refers to a previously bound type. The identification is required since a type parameter in a chain is bound at a particular method invocation, and that bound type is referred to in the following method invocations. Our algorithm constructs the DFA by analyzing the binding time of type parameters and their propagation among the states in a DFA that is naively constructed from the given grammar.\n",
      "\n",
      "Knowledge and Importance: Our algorithm helps library developers to develop a generic fluent API. The ability to generate a generic fluent API is essential to bring the safe fluent API generation to the real world since the use of type parameters is a common technique in the library API design. By our algorithm, the generation of a safe fluent API will be ready for practical use.\n",
      "\n",
      "Grounding: We implemented a generator named Protocool to demonstrate our algorithm. We also generated several libraries using Protocool to show the ability and the limitations of our algorithm. 0 into PostgreSQL...\n",
      "Inserting test sample 2264  In the world of software development, application programming interfaces (APIs) are essential for working with libraries and frameworks. They allow developers to access pre-written code and build upon it to create new applications. A fluent API is a type of API that is designed to be easy to read and write, providing more natural language-like syntax that feels like a domain-specific language (DSL). This paper presents a methodology for generating a generic fluent API in Java.\n",
      "\n",
      "The proposed methodology involves the use of code generation and automated testing. The process starts with the identification of a domain-specific language that will be used to generate the fluent API. This DSL is then used to generate the code for the API using a code generation tool. The generated code is tested using a suite of automated tests to ensure that it is correct and meets the desired specifications.\n",
      "\n",
      "The benefits of using a fluent API in Java are substantial. The more natural language-like syntax reduces the cognitive overhead of learning new APIs and makes code more readable. In addition, the fluent API makes it easier to create complex chains of operations, reducing the amount of boilerplate code that would be necessary with a traditional API.\n",
      "\n",
      "The generic nature of the proposed fluent API means that it can be used in a wide variety of applications. By defining a domain-specific language, developers can tailor the API to the specific needs of their application while still benefiting from the simplified syntax of a fluent API.\n",
      "\n",
      "To validate the effectiveness of the methodology, we conducted a case study in which we created a fluent API for a sample application. The results of the case study showed that the generated code was correct and met the desired specifications. In addition, the resulting code was easier to read and write than a traditional API.\n",
      "\n",
      "In conclusion, the proposed methodology for generating a generic fluent API in Java provides a powerful tool for developers to create more readable and maintainable code. The use of code generation and automated testing ensures that the resulting API is correct and meets the desired specifications. The generic nature of the proposed API makes it useful in a wide variety of applications and provides a more natural language-like syntax that reduces cognitive overhead. 1 into PostgreSQL...\n",
      "Inserting test sample 2265  In this paper, we use a well-known Deep Learning technique called Long Short Term Memory (LSTM) recurrent neural networks to find sessions that are prone to code failure in applications that rely on telemetry data for system health monitoring. We also use LSTM networks to extract telemetry patterns that lead to a specific code failure. For code failure prediction, we treat the telemetry events, sequence of telemetry events and the outcome of each sequence as words, sentence and sentiment in the context of sentiment analysis, respectively. Our proposed method is able to process a large set of data and can automatically handle edge cases in code failure prediction. We take advantage of Bayesian optimization technique to find the optimal hyper parameters as well as the type of LSTM cells that leads to the best prediction performance. We then introduce the Contributors and Blockers concepts. In this paper, contributors are the set of events that cause a code failure, while blockers are the set of events that each of them individually prevents a code failure from happening, even in presence of one or multiple contributor(s). Once the proposed LSTM model is trained, we use a greedy approach to find the contributors and blockers. To develop and test our proposed method, we use synthetic (simulated) data in the first step. The synthetic data is generated using a number of rules for code failures, as well as a number of rules for preventing a code failure from happening. The trained LSTM model shows over 99% accuracy for detecting code failures in the synthetic data. The results from the proposed method outperform the classical learning models such as Decision Tree and Random Forest. Using the proposed greedy method, we are able to find the contributors and blockers in the synthetic data in more than 90% of the cases, with a performance better than sequential rule and pattern mining algorithms. 0 into PostgreSQL...\n",
      "Inserting test sample 2266  Software bugs and errors are ubiquitous in the world of software development. These failures can lead to significant problems in terms of safety, financial losses, and public perception. As a result, it is crucial to detect, diagnose, and correct these failures as quickly as possible. One way to address this issue is through the use of Long Short-Term Memory (LSTM) neural networks, which can help predict code failures before they occur.\n",
      "\n",
      "LSTM networks are particularly well-suited to handle time-series data, such as software logs, and can be trained to learn the patterns and behaviors of the software being developed. By examining the temporal dependencies and patterns in the data, LSTM networks can identify potential problems before they manifest as code failures, allowing developers to take preemptive action.\n",
      "\n",
      "In our research, we propose a framework for code failure prediction and pattern extraction using LSTM networks. Our framework is designed to not only predict the possibility of code failure, but to also provide actionable insights into the underlying causes of the issues. We demonstrate the effectiveness and capability of our approach on two real-world datasets, showing that our method achieves high accuracy and is scalable to large software systems.\n",
      "\n",
      "Moreover, we present a case study on a well-known open-source project, the Linux kernel, where we apply our methodology to identify patterns of failure and pinpoint the root causes of code failures. Through this case study, we show the ability of our framework to help developers not only detect but also diagnose problems in complex software systems.\n",
      "\n",
      "In summary, our work presents a powerful tool for software developers to predict code failures and identify the underlying patterns and causes behind them. With this approach, developers can significantly reduce the amount of time and resources needed to detect and fix software issues, resulting in more reliable and secure software systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2267  Statistics and dynamics of 2D rogue waves in a broad-area semiconductor laser with an intracavity saturable absorber are numerically investigated under the effect of transverse carrier diffusion. We show that lateral diffusion of carriers alters the statistics of rogue waves by enhancing their formation in smaller ratios of carrier lifetimes in the active and passive materials while suppressing them when the ratio is larger. Temporal dynamics of the emitted rogue waves is also studied and shown that finite nonzero transverse carrier diffusion coefficient gives them a longer duration. To further approach the realistic experimental situation, we also investigated statistics and dynamics of rogue waves by simulating a circular disk-shape pump which replaces the flat pump profile typically used in numerical simulations of broad-area lasers. We show that finite pump shape reduces the number emitted rogue waves per unit area for large carrier lifetime ratios and increases that for smaller values of the ratio in both below and above laser threshold. Temporal width of the emitted rogue waves is also shown to reduce as a consequence of removing the nonphysical effects of infinite flat pump on carrier dynamics. 0 into PostgreSQL...\n",
      "Inserting test sample 2268  This paper investigates the impact of transverse carrier diffusion on 2D optical rogue waves in broad-area semiconductor lasers with a saturable absorber. Rogue waves are rare, localized, and high-amplitude events that occur in various physical contexts. In recent years, 2D rogue waves have attracted considerable attention due to their various potential applications. In this study, we extend previous 1D models to a 2D geometric configuration and show that the presence of transverse carrier diffusion has a significant impact on the dynamics of rogue waves. We analyze the spatiotemporal statistical properties of rogue waves, as well as their dependence on the spatial extent of the laser cavity and the saturation parameter of the absorber. Our results demonstrate that transverse carrier diffusion broadens the spatial distribution of rogue waves and alters their statistics, leading to a higher probability of occurrence and a narrower distribution of amplitudes. This study sheds new light on the understanding of 2D rogue wave dynamics and provides important insights for the design and optimization of semiconductor lasers with potential applications in nonlinear optics, optical communications, and sensing. 1 into PostgreSQL...\n",
      "Inserting test sample 2269  We describe the twisted doubling integrals of Cai-Friedberg-Ginzburg-Kaplan in a conceptual way. This also extends the construction to the quaternionic unitary groups. We carry out the unfolding argument uniformly in this article.\n",
      "\n",
      "To do so, we define a family of degenerate Whittaker coefficients that are suitable in this setup and study some of their properties. We also prove certain related global and local results that use the same tools. 0 into PostgreSQL...\n",
      "Inserting test sample 2270  We obtain twisted versions of the classical doubling integral formulae for groups with Langlands parameter matching twists of symmetric powers of the adjoint representation of a classical group. These formulae enable the computation of twisted endoscopic character identities, which are used in harmonic analysis, and potentially to prove certain cases of the Arthur-Selberg trace formula. We also give an application to elliptic stable envelopes for certain Lagrangian subvarieties of the minimal nilpotent cone. 1 into PostgreSQL...\n",
      "Inserting test sample 2271  We construct a five--dimensional, asymptotically Goedel, three--charge black hole via dimensional reduction of an asymptotically plane wave, rotating D1-D5-brane solution of type IIB supergravity. This latter is itself constructed via the solution generating procedure of Garfinkle and Vachaspati, applied to the standard rotating D1-D5-brane solution. Taking all charges to be equal gives a \"BMPV Goedel black hole\", which is closely related to that recently found by Herdeiro. We emphasise, however, the importance of our ten--dimensional microscopic description in terms of branes. We discuss various properties of the asymptotically Goedel black hole, including the physical bound on the rotation of the hole, the existence of closed timelike curves, and possible holographic protection of chronology. 0 into PostgreSQL...\n",
      "Inserting test sample 2272  This paper investigates the properties of rotating black holes in a GÃ¶del universe, which is a solution of Einstein's field equations. The GÃ¶del universe is characterized by closed timelike curves that allow time travel. We derive the equations of motion and the corresponding Hamiltonian for a test particle around a rotating black hole in a GÃ¶del universe. We show that frame dragging, which is a consequence of the black hole's rotation, is more pronounced in a GÃ¶del universe than in a flat spacetime. The perihelion shift and the Lense-Thirring effect are also discussed. We conclude that rotating black holes in a GÃ¶del universe could have unique observational signatures and should be considered as possible sources of astrophysical phenomena. 1 into PostgreSQL...\n",
      "Inserting test sample 2273  A complete, fundamental understanding of the proton must include knowledge of the underlying spin structure. The transversity distribution, $h_1\\left(x\\right)$, which describes the transverse spin structure of quarks inside of a transversely polarized proton, is only accessible through channels that couple $h_1 \\left(x\\right)$ to another chiral odd distribution, such as the Collins fragmentation function ($\\Delta^N D_{\\pi/q^\\uparrow}\\left(z,j_T\\right)$). Significant Collins asymmetries of charged pions have been observed in semi-inclusive deep inelastic scattering (SIDIS) data. These SIDIS asymmetries combined with $e^+e^-$ process asymmetries have allowed for the extraction of $h_1\\left(x\\right)$ and $\\Delta^N D_{\\pi/q^\\uparrow}\\left(z,j_T\\right)$. However, the current uncertainties on $h_1\\left(x\\right)$ are large compared to the corresponding quark momentum and helicity distributions and reflect the limited statistics and kinematic reach of the available data. In transversely polarized hadronic collisions, Collins asymmetries may be isolated and extracted by measuring the spin dependent azimuthal distributions of charged pions in jets. This thesis will report on the first statistically significant Collins asymmetries extracted from $\\sqrt{s}=200$ GeV hadronic collisions using $14$ pb$^{-1}$ of transversely polarized proton collisions at 57% average polarization. 0 into PostgreSQL...\n",
      "Inserting test sample 2274  This paper presents a study of transverse momentum dependent (TMD) distributions in polarized proton collisions through the azimuthal single spin asymmetries (SSAs) of charged pions in jets. The study is conducted using data collected at the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National Laboratory. The azimuthal SSAs, which measure the difference in the yield of particles produced in the forward direction with respect to the polarized direction of the colliding protons, are sensitive to TMDs and can provide insight into the spin structure of protons. The analysis involves measuring the SSAs of charged pions within identified jets and studying their dependence on the transverse momentum, pseudorapidity, and jet axis orientation. The TMDs extracted from the SSAs are compared with theoretical model calculations to test our understanding of TMD factorization and gauge invariance. This work contributes to our understanding of polarized proton structure and offers important information for the interpretation of future high-energy experiments at RHIC and future Electron-Ion Collider experiments. 1 into PostgreSQL...\n",
      "Inserting test sample 2275  We study an inverse seesaw model of neutrino mass within the framework of $S_4$ flavour symmetry from the requirement of generating non-zero reactor mixing angle $\\theta_{13}$ along with correct dark matter relic abundance. The leading order $S_4$ model gives rise to tri-bimaximal type leptonic mixing resulting in $\\theta_{13}=0$. Non-zero $\\theta_{13}$ is generated at one loop level by extending the model with additional scalar and fermion fields which take part in the loop correction. The particles going inside the loop are odd under an in-built $Z^{\\text{Dark}}_2$ symmetry such that the lightest $Z^{\\text{Dark}}_2$ odd particle can be a dark matter candidate. Correct neutrino and dark matter phenomenology can be achieved for such one loop corrections either to the light neutrino mass matrix or to the charged lepton mass matrix although the latter case is found to be more predictive. The predictions for neutrinoless double beta decay is also discussed and inverted hierarchy in the charged lepton correction case is found to be disfavoured by the latest KamLAND-Zen data. 0 into PostgreSQL...\n",
      "Inserting test sample 2276  We propose an $S_4$ flavor symmetric model with inverse seesaw mechanism, which simultaneously explains the non-zero value of $\\theta_{13}$ and the existence of dark matter. The model involves the introduction of three right-handed neutrinos and two scalar triplets. The scalar triplets break the $S_4$ symmetry while generating tiny Dirac masses for the neutrinos. Additionally, one of the triplets produces a Tiny Viable Seesaw (TVS) mechanism which provides the smallness of neutrino masses dictated by current observation. The second triplet generates a radiative charged lepton mass hierarchy and, in the meantime, accommodates a dark matter particle. We impose a $Z_2$ symmetry to stabilize the dark matter candidate and ensure its particle nature. With the identified new scalars and right-handed neutrinos, the model provides an efficient mechanism to solve the long-standing problems of neutrino mass and the existence of dark matter. This model can be tested in future neutrino and dark matter experiments. 1 into PostgreSQL...\n",
      "Inserting test sample 2277  We present Voxel Transformer (VoTr), a novel and effective voxel-based Transformer backbone for 3D object detection from point clouds. Conventional 3D convolutional backbones in voxel-based 3D detectors cannot efficiently capture large context information, which is crucial for object recognition and localization, owing to the limited receptive fields. In this paper, we resolve the problem by introducing a Transformer-based architecture that enables long-range relationships between voxels by self-attention. Given the fact that non-empty voxels are naturally sparse but numerous, directly applying standard Transformer on voxels is non-trivial. To this end, we propose the sparse voxel module and the submanifold voxel module, which can operate on the empty and non-empty voxel positions effectively. To further enlarge the attention range while maintaining comparable computational overhead to the convolutional counterparts, we propose two attention mechanisms for multi-head attention in those two modules: Local Attention and Dilated Attention, and we further propose Fast Voxel Query to accelerate the querying process in multi-head attention. VoTr contains a series of sparse and submanifold voxel modules and can be applied in most voxel-based detectors. Our proposed VoTr shows consistent improvement over the convolutional baselines while maintaining computational efficiency on the KITTI dataset and the Waymo Open dataset. 0 into PostgreSQL...\n",
      "Inserting test sample 2278  In recent years, 3D object detection has gained significant attention because of its potential applications in autonomous driving, robotics, and augmented reality. However, detecting objects in 3D space is a complex and computationally expensive task due to the size and complexity of the data. In this paper, we introduce a novel approach for 3D object detection using a voxel transformer. Our method takes a 3D point cloud as input and transforms it into a high-dimensional voxel representation using a transformer network. The transformer network learns to attend to different regions of the input point cloud and encode them into feature vectors. These features are then fed into a detection head to predict the 3D bounding boxes and object categories. Our results show that our approach outperforms state-of-the-art methods on the KITTI dataset for both 3D object detection and bird's eye view object detection tasks. The voxel transformer also has a compact representation and requires less memory than previous methods. We believe that our approach has the potential to improve the efficiency and accuracy of 3D object detection in various domains. 1 into PostgreSQL...\n",
      "Inserting test sample 2279  We consider the fundamental problem of multiple stations competing to transmit on a multiple access channel (MAC). We are given $n$ stations out of which at most $d$ are active and intend to transmit a message to other stations using MAC. All stations are assumed to be synchronized according to a time clock. If $l$ stations node transmit in the same round, then the MAC provides the feedback whether $l=0$, $l=2$ (collision occurred) or $l=1$. When $l=1$, then a single station is indeed able to successfully transmit a message, which is received by all other nodes. For the above problem the active stations have to schedule their transmissions so that they can singly, transmit their messages on MAC, based only on the feedback received from the MAC in previous round.\n",
      "\n",
      "For the above problem it was shown in [Greenberg, Winograd, {\\em A Lower bound on the Time Needed in the Worst Case to Resolve Conflicts Deterministically in Multiple Access Channels}, Journal of ACM 1985] that every deterministic adaptive algorithm should take $\\Omega(d (\\lg n)/(\\lg d))$ rounds in the worst case. The fastest known deterministic adaptive algorithm requires $O(d \\lg n)$ rounds. The gap between the upper and lower bound is $O(\\lg d)$ round. It is substantial for most values of $d$: When $d = $ constant and $d \\in O(n^{\\epsilon})$ (for any constant $\\epsilon \\leq 1$, the lower bound is respectively $O(\\lg n)$ and O(n), which is trivial in both cases. Nevertheless, the above lower bound is interesting indeed when $d \\in$ poly($\\lg n$). In this work, we present a novel counting argument to prove a tight lower bound of $\\Omega(d \\lg n)$ rounds for all deterministic, adaptive algorithms, closing this long standing open question.} 0 into PostgreSQL...\n",
      "Inserting test sample 2280  This research paper investigates the complexity of resolving conflicts on Media Access Control (MAC) protocols that are utilized in wireless networks. Conflicts among nodes can lead to poor network performance, lower throughput, and longer transmission delays. The main challenge in resolving these conflicts is to ensure fairness in resource allocation and avoid the dominance of specific nodes. We analyze the performance of various existing MAC protocols and propose a new algorithm to improve the fairness and efficiency of conflict resolution. Our proposed algorithm is based on a joint optimization of channel access probability and contention window size, which allows for an adaptive adjustment of the access probability for nodes with different levels of contention. We evaluate the performance of our proposed algorithm using simulations in different network scenarios. The results show that our proposed algorithm outperforms existing MAC protocols in terms of fairness and efficiency. Additionally, we investigate the impact of factors such as packet size, network size, and mobility on the performance of our algorithm. We also discuss the limitations and potential for further enhancements in our proposed algorithm. Overall, our research contributes to the understanding of the complex nature of conflict resolution on MAC protocols and provides a new solution for improving the fairness and efficiency of wireless networks. 1 into PostgreSQL...\n",
      "Inserting test sample 2281  We present high-resolution N-body/hydrodynamics simulations of dwarf galaxies formed in isolated CDM halos with the same virial mass, Mv~2.5x10^10 Msun at z=0, in order to (1) study the mass assembly histories (MAHs) of the halo, stars, and gas components, and (2) explore the effects of the halo MAHs on the stellar/baryonic assembly of the simulated dwarfs and on their z~0 properties.\n",
      "\n",
      "Overall, the simulated dwarfs are roughly consistent with observations. Our main results are: a) The stellar-to-halo mass ratio is ~0.01 and remains roughly constant since z~1 (the stellar MAHs follow closely the halo MAHs), with a smaller value at higher z's for those halos that assemble their mass later. b) The evolution of the galaxy gas fraction, fg, is episodic and higher, most of the time, than the stellar fraction. When fg decreases (increases), the gas fraction in the halo typically increases (decreases), showing that the SN driven outflows play an important role in regulating the gas fractions -and hence the SFR- of the dwarfs. However, in most cases, an important fraction of the gas escapes the virial radius, Rv; at z=4 the total baryon fraction inside Rv is 1.5-2 times smaller than the universal one, while at z=0 is 2-6 times smaller, with the earlier assembled halos ejecting more gas. c) The SF histories are episodic with changes in the SFRs of factors 2-10 on average. d) Although the dwarfs formed in late assembled halos show more extended SF histories, their z~0 SFRs are still below the ones measured for local isolated dwarfs. e) The effects of baryons on Mv are such that at almost any time Mv is 10-20% smaller than the corresponding Mv obtained in pure N-body simulations.\n",
      "\n",
      "Our results suggest that rather than increasing the strength of the SN-driven outflows, processes that reduce the SF efficiency even more will help to solve the potential issues faced by the CDM-based simulations of dwarfs. 0 into PostgreSQL...\n",
      "Inserting test sample 2282  This research paper simulates the formation of isolated dwarf galaxies in dark matter halos with varying mass assembly histories. By utilizing state-of-the-art numerical simulations, we examine the dynamical evolution of these galaxies, focusing on the impact of dark matter halo growth on their properties. Our results demonstrate that the star formation efficiency of dwarf galaxies is strongly influenced by the mass assembly history of their host dark matter halos. Specifically, dwarf galaxies formed in halos with early mass assembly exhibit significantly lower star formation rates than those formed in halos with later mass assembly. Furthermore, we find that the central density profiles of these dwarf galaxies are highly correlated with the density profiles of their host dark matter halos. Our results suggest that dwarf galaxies formed in halos with early mass assembly have density profiles that are substantially more centrally concentrated than those formed in halos with later mass assembly. These findings have important implications for our understanding of galaxy formation and evolution, as well as for observational studies of faint dwarf galaxies. In particular, they highlight the need for detailed modeling of the dark matter halo growth history in order to accurately interpret observations of faint, low-mass galaxies. Overall, our study provides important insights into the relationship between dark matter halo assembly and the properties of dwarf galaxies, and sheds light on the physical processes that drive galaxy formation and evolution in the low-mass regime. 1 into PostgreSQL...\n",
      "Inserting test sample 2283  Realized statistics based on high frequency returns have become very popular in financial economics. In recent years, different non-parametric estimators of the variation of a log-price process have appeared. These were developed by many authors and were motivated by the existence of complete records of price data. Among them are the realized quadratic (co-)variation which is perhaps the most well known example, providing a consistent estimator of the integrated (co-)volatility when the logarithmic price process is continuous. Limit results such as the weak law of large numbers or the central limit theorem have been proved in different contexts. In this paper, we propose to study the large deviation properties of realized (co-)volatility (i.e., when the number of high frequency observations in a fixed time interval increases to infinity. More specifically, we consider a bivariate model with synchronous observation schemes and correlated Brownian motions of the following form: $dX\\_{\\ell,t} = \\sigma\\_{\\ell,t}dB\\_{\\ell,t}+b\\_{\\ell}(t,\\omega)dt$ for $\\ell=1,2$, where $X\\_{\\ell}$ denotes the log-price, we are concerned with the large deviation estimation of the vector $V\\_t^n(X)=(Q\\_{1,t}^n(X), Q\\_{2,t}^n(X), C\\_{t}^n(X))$ where $Q\\_{\\ell,t}^n(X)$ and $C\\_{t}^n(X)$ represente the estimator of the quadratic variational processes $Q\\_{\\ell,t}=\\int\\_0^t\\sigma\\_{\\ell,s}^2ds$ and the integrated covariance $C\\_t=\\int\\_0^t\\sigma\\_{1,s}\\sigma\\_{2,s}\\rho\\_sds$ respectively, with $\\rho\\_t=cov(B\\_{1,t}, B\\_{2,t})$. Our main motivation is to improve upon the existing limit theorems. Our large deviations results can be used to evaluate and approximate tail probabilities of realized (co-)volatility. As an application we provide the large deviation for the standard dependence measures between the two assets returns such as the realized regression coefficients up to time $t$, or the realized correlation. Our study should contribute to the recent trend of research on the (co-)variance estimation problems, which are quite often discussed in high-frequency financial data analysis. 0 into PostgreSQL...\n",
      "Inserting test sample 2284  This paper focuses on the large deviations of the realized (co-)volatility vector. The realized volatility of an asset is a key measure of its price fluctuations. The concept of realized (co-)volatility arises in multivariate settings where multiple assets exhibit volatility. \n",
      "\n",
      "In this work, we extend previous results on the large deviations of realized volatility. Specifically, we study the large deviations of the realized (co-)volatility vector of a multivariate asset. We provide a general framework for analyzing the distribution and tails of realized (co-)volatility vectors under general conditions. Our approach is based on the theory of large deviations and relies on sophisticated tools from convex analysis, namely the Legendre-Fenchel transform.\n",
      "\n",
      "Our main result provides a precise characterization of the large deviations of the realized (co-)volatility vector, including the rate function, which is shown to be convex and lower semi-continuous. We establish that the rate function is minimized by an asymptotically unique sequence of realized (co-)volatility vectors, which we call the optimal path. Moreover, we study the stability of the optimal path with respect to perturbations of the sample size and the weights used in the construction of the realized (co-)volatility vector.\n",
      "\n",
      "We further illustrate our results with numerical simulations on synthetic data and an empirical application to a dataset of S&P 500 stocks. Our results shed light on the distribution and tails of realized (co-)volatility vectors, and provide useful insights for the modeling and risk management of multivariate asset portfolios. \n",
      "\n",
      "In conclusion, this paper contributes to the understanding of realized (co-)volatility vectors by studying their large deviations. The results are relevant for a wide range of applications in finance, economics, and statistical physics. The general framework and tools developed in this work can be extended to other multivariate statistics and high-dimensional data analysis problems. 1 into PostgreSQL...\n",
      "Inserting test sample 2285  We report the discovery of a bright blue quasar: SDSS J022218.03-062511.1.\n",
      "\n",
      "This object was discovered spectroscopically while searching for hot white dwarfs that may be used as calibration sources for large sky surveys such as the Dark Energy Survey or the Large Synoptic Survey Telescope project. We present the calibrated spectrum, spectral line shifts and report a redshift of z = 0.521 +/- 0.0015 and a rest-frame g-band luminosity of 8.71 X 10^11 L(Sun). 0 into PostgreSQL...\n",
      "Inserting test sample 2286  We report the discovery of a new blue quasar, named SDSS J022218.03-062511.1, identified from the Sloan Digital Sky Survey (SDSS) data. The quasar was selected as a candidate based on its distinctive blue color and high luminosity, confirmed through spectroscopic observations. SDSS J022218.03-062511.1 is found to have a redshift of z=1.5 and an estimated black hole mass of 10^9 solar masses. This discovery provides valuable insights into the high-redshift quasar population and their properties, which can help constrain models of galaxy and black hole evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 2287  Determining the intended sense of words in text - word sense disambiguation (WSD) - is a long standing problem in natural language processing. Recently, researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms. However, a simple average or concatenation of word vectors for each word in a text loses the sequential and syntactic information of the text. In this paper, we study WSD with a sequence learning neural net, LSTM, to better capture the sequential and syntactic patterns of the text. To alleviate the lack of training data in all-words WSD, we employ the same LSTM in a semi-supervised label propagation classifier. We demonstrate state-of-the-art results, especially on verbs. 0 into PostgreSQL...\n",
      "Inserting test sample 2288  This paper proposes a semi-supervised approach to word sense disambiguation (WSD) using neural models. WSD is the task of identifying the correct meaning of a word in context. Neural models, specifically deep neural networks, have been applied to WSD with success. However, these models require large amounts of labeled data, which can be difficult and expensive to obtain. In our approach, we use a small amount of labeled data along with a much larger corpus of unlabeled data to train the model. We also explore the effectiveness of different methods for leveraging the unlabeled data. Our experiments show that our proposed approach outperforms standard supervised methods and approaches that do not use unlabeled data. 1 into PostgreSQL...\n",
      "Inserting test sample 2289  The Weil-Petersson and Takhtajan-Zograf metrics on the Riemann moduli spaces of complex structures for an $n$-fold punctured oriented surface of genus $g,$ in the stable range $g+2n>2,$ are shown here to have complete asymptotic expansions in terms of Fenchel-Nielsen coordinates at the exceptional divisors of the Knudsen-Deligne-Mumford compactification. This is accomplished by finding a full expansion for the hyperbolic metrics on the fibers of the universal curve as they approach the complete metrics on the nodal curves above the exceptional divisors and then using a push-forward theorem for conormal densities. This refines a two-term expansion due to Obitsu-Wolpert for the conformal factor relative to the model plumbing metric which in turn refined the bound obtained by Masur. A similar expansion for the Ricci metric is also obtained. 0 into PostgreSQL...\n",
      "Inserting test sample 2290  The Weil-Petersson and fiber metrics play a key role in the study of the Riemann moduli spaces. In this work, we investigate the behavior of these metrics near the boundary of the moduli space. Specifically, we explore the relationship between the behavior of these metrics and the degeneration of the underlying Riemann surfaces. Our main results show that as the surface degenerates the Weil-Petersson metric converges to a singular metric, while the fiber metric remains bounded. We also establish a connection between the behavior of these metrics and the behavior of certain geometric invariants, such as the systole and dilatation of the surface. Our findings shed light on the geometric properties of the Riemann moduli space and provide a deeper understanding of the Weil-Petersson and fiber metrics in this context. 1 into PostgreSQL...\n",
      "Inserting test sample 2291  [Abridged] We discovered in the Herschel Reference Survey an extremely bright IR source with $S_{500}$~120mJy (Red Virgo 4 - RV4). Based on IRAM/EMIR and IRAM/NOEMA detections of the CO(5-4), CO(4-3), and [CI] lines, RV4 is located at z=4.724, yielding a total observed L$_{IR}$ of 1.1+/-0.6x0$^{14}$L$_{\\odot}$. At the position of the Herschel emission, three blobs are detected with the VLA at 10cm. The CO(5-4) line detection of each blob confirms that they are at the same redshift with the same line width, indicating that they are multiple images of the same source. In Spitzer and deep optical observations, two sources, High-z Lens 1 (HL1) West and HL1 East, are detected at the center of the three VLA/NOEMA blobs. These two sources are placed at z=1.48 with XSHOOTER spectra, suggesting that they could be merging and gravitationally lensing the emission of RV4. HL1 is the second most distant lens known to date in strong lensing systems. The Einstein radius of the lensing system is 2.2\"+/-0.2 (20kpc). The high redshift of HL1 and the large Einstein radius are highly unusual for a strong lensing system. We present the ISM properties of the background source RV4. Different estimates of the gas depletion time yield low values suggesting that RV4 is a SB galaxy. Among all high-z SMGs, this source exhibits one of the lowest L$_{[CI]}$ to L$_{IR}$ ratios, 3.2+/-0.9x10$^{-6}$, suggesting an extremely short gas tdepl of only 14+/-5Myr. It also shows a relatively high L$_{[CI]}$ to L$_{CO(4-3)}$ ratio (0.7+/-0.2) and low L$_{CO(5-4)}$ to L$_{IR}$ ratio (only ~50% of the value expected for normal galaxies) hinting a low density of gas. Finally, we discuss that the short tdepl of RV4 can be explained by either a very high SFE, which is difficult to reconcile with major mergers simulations of high-z galaxies, or a rapid decrease of SF, which would bias the estimate of tdepl toward low value. 0 into PostgreSQL...\n",
      "Inserting test sample 2292  This research paper presents the discovery of a hyper luminous starburst observed at redshift z=4.72. The starburst was observed through the phenomenon of gravitational lensing, as it was magnified by a pair of lensing galaxies at a redshift of z=1.48. This is an extremely rare event and provides valuable insights into stellar formation and evolution in the early universe.\n",
      "\n",
      "The scientific community has long been interested in understanding how galaxies formed and evolved, particularly in the early universe. This discovery contributes to this understanding by shedding light on the processes that occur during a hyper luminous starburst. These events are characterized by an extremely high rate of star formation and represent a significant phase in the evolution of galaxies.\n",
      "\n",
      "The gravitational lensing effect was instrumental in detecting the hyper luminous starburst, which was otherwise too faint to observe with current telescopes. Gravitational lensing occurs when light from a distant object is bent by the gravitational field of a massive object, such as a galaxy, which acts as a lens. The lensing effect amplifies the light from the distant object and makes it visible to telescopes.\n",
      "\n",
      "The lensing galaxy pair responsible for the magnification of the starburst is located at a redshift of z=1.48. This is the first known example of a hyper luminous starburst being magnified by a galaxy pair, which makes the detection even more remarkable.\n",
      "\n",
      "The discovery of this hyper luminous starburst opens up new avenues for research in the field of galaxy evolution. The observations provide insights into the processes that occur during intense periods of star formation and can inform theoretical models of galaxy formation. Additionally, the discovery of the gravitational lensing effect in this event provides further evidence of the presence of dark matter, which is known to cause gravitational lensing.\n",
      "\n",
      "In conclusion, the discovery of a hyper luminous starburst at redshift z=4.72 and its magnification by a lensing galaxy pair at z=1.48 is a significant contribution to our understanding of galaxy evolution in the early universe. The observations provide valuable insights into the processes that occur during intense star formation events and can inform theoretical models of galaxy formation. 1 into PostgreSQL...\n",
      "Inserting test sample 2293  We extend the link between Einstein Sasakian manifolds and Killing spinors to a class of $\\eta$-Einstein Sasakian manifolds, both in Riemannian and Lorentzian settings, characterising them in terms of generalised Killing spinors. We propose a definition of supersymmetric M-theory backgrounds on such a geometry and find a new class of such backgrounds, extending previous work of Haupt, Lukas and Stelle. 0 into PostgreSQL...\n",
      "Inserting test sample 2294  Sasakian manifolds play a fundamental role in M-theory compactifications. In this paper, we investigate the geometric properties of compact, Sasakian manifolds that arise in M-theory compactifications. Specifically, we derive a structural tensor which completely characterizes the relevant differential geometry. We also discuss potential applications of this work to the study of supersymmetric gauge theories, as well as insights into the strong-weak duality in M-theory. 1 into PostgreSQL...\n",
      "Inserting test sample 2295  On the basis of general relativity and quantum statistics, it was shown (Neslu\\v{s}an L.: 2009, Phys. Rev. D 80, 024015, arxiv:0808.3484) that the equation of state (ES) of extremely hot Fermi-Dirac gas in the surface layer of an ultra-relativistic compact object converges to the same form as the relativistic equation of thermodynamical equilibrium (RETE), which is the condition of stability of the object. The description of energy state of a gas particle was completed with the term corresponding with the potential-type energy. The necessity of such the term is set by the demand of convergence of the relativistic particle-impulse distribution law to its Maxwell-Boltzmann form in the classical limit.\n",
      "\n",
      "The identity of the ES and RETE, both applied to the gas in the object's surface layer, becomes perfect, yielding the stable object, when the object's physical radius is identical to its gravitational radius. In this state, the internal energy of gas particles in a volume of the object's surface layer increases over all limits in the frame of the volume and this opens the question if the horizon of events actually is an insuperable barrier. It seems to be possible that some matter can be temporarily lifted above the surface or, so far, be ejected from the object and can emit a radiation detectable by a distant observer.\n",
      "\n",
      "In our contribution, we demonstrate a general validity of the functional form of the potential-type energy found in our previous work. The consistency of the RETE with its non-relativistic approximation can occur only for this functional form. We also point out some observational consequences of the approximate identity of ES and RETE before the object collapses, in the proper time, to its gravitational radius as well as the possible observational consequences of the infinitely high internal energy in the surface layer of already collapsed object. In general, we propagate the idea that a lot of phenomena observed at the stellar-sized or supermassive black holes (or not-yet black holes) can be not necessarily related to the structures in a vicinity of the black hole, e.g.\n",
      "\n",
      "to an accretion disk, but they can be linked directly to the behaviour of the central, ultra-compact object. 0 into PostgreSQL...\n",
      "Inserting test sample 2296  The study of black holes has long been a cornerstone in astrophysical research, but non-black black holes have only recently come under scrutiny. These non-black black holes, also known as gravastars or Bose stars, are alternative solutions to the equations of general relativity that describe non-rotating black holes. \n",
      "\n",
      "While black holes are characterized by a point of singularity at their centers, gravastars or Bose stars are made up of a Bose-Einstein condensate of particles that effectively form a surface between their event horizon and their center. This surface is known as the gravastar surface, which creates a repulsive force that prevents matter from collapsing into a singularity. \n",
      "\n",
      "Recent research has shown that gravastars are a viable alternative to black holes, and their existence has significant implications for our understanding of the universe. They can help explain observations such as the apparent absence of intermediate-sized black holes and the existence of ultraluminous X-ray sources. Moreover, they provide a unique opportunity to test hypotheses related to the quantum nature of gravity.\n",
      "\n",
      "In this paper, we explore the fundamentals of non-black black holes and provide an in-depth analysis of gravastars. Our approach involves a combination of analytical and numerical methods to study the structure and properties of these objects. We derive the equations of state that describe gravastars and show that they are stable under certain conditions. Furthermore, we investigate the dynamics of accretion disks around these objects and the behavior of test particles in their vicinity.\n",
      "\n",
      "Our findings indicate that gravastars are a promising avenue for future research, which could shed light on some of the most compelling questions in modern astrophysics. Their unique properties make them a fascinating and challenging object of study, both for theorists and observers alike. As such, this paper represents a crucial step in building a comprehensive understanding of non-black black holes and their role in shaping the universe as we know it. 1 into PostgreSQL...\n",
      "Inserting test sample 2297  We give an explicit and simple construction of the incidence graph for the integral cohomology of real Grassmann manifold Gr(k,n) in terms of the Young diagrams filled with the letter q in checkered pattern. It turns out that there are two types of graphs, one for the trivial coefficients and other for the twisted coefficients, and they compute the homology groups of the orientable and non-orientable cases of Gr(k,n) via the Poincar\\'e-Verdier duality. We also give an explicit formula of the Poincar\\'e polynomial for Gr(k,n) and show that the Poincar\\'e polynomial is also related to the number of points on Gr(k,n) over a finite field {F}_q with q being a power of prime which is also used in the Young diagrams. 0 into PostgreSQL...\n",
      "Inserting test sample 2298  This paper studies the cohomology ring of real Grassmann manifolds. We begin by defining the manifolds and their cohomology. We then explain how to compute the cohomology ring using Schubert calculus, a combinatorial tool for studying the geometry of Grassmannians. We exhibit several examples and show how the cohomology ring depends on the dimension and number of the manifold. We also demonstrate how to calculate the cup product structure of the cohomology ring, and we prove a formula for the cup product of certain special Schubert classes. Finally, as an application, we show how our results can be used to compute the cohomology ring of complete flag manifolds, which are more complicated examples of homogeneous spaces. This work contributes to our understanding of the topology of Grassmannians and other related spaces. 1 into PostgreSQL...\n",
      "Inserting test sample 2299  The Asian giant hornet (AGH) appeared in Washington State appears to have a potential danger of bioinvasion. Washington State has collected public photos and videos of detected insects for verification and further investigation. In this paper, we analyze AGH using data analysis,statistics, discrete mathematics, and deep learning techniques to process the data to controlAGH spreading.First, we visualize the geographical distribution of insects in Washington State. Then we investigate insect populations to varying months of the year and different days of a month.Third, we employ wavelet analysis to examine the periodic spread of AGH. Fourth, we apply ordinary differential equations to examine AGH numbers at the different natural growthrate and reaction speed and output the potential propagation coefficient. Next, we leverage cellular automaton combined with the potential propagation coefficient to simulate the geographical spread under changing potential propagation. To update the model, we use delayed differential equations to simulate human intervention. We use the time difference between detection time and submission time to determine the unit of time to delay time. After that, we construct a lightweight CNN called SqueezeNet and assess its classification performance. We then relate several non-reference image quality metrics, including NIQE, image gradient, entropy, contrast, and TOPSIS to judge the cause of misclassification. Furthermore, we build a Random Forest classifier to identify positive and negative samples based on image qualities only. We also display the feature importance and conduct an error analysis. Besides, we present sensitivity analysis to verify the robustness of our models. Finally, we show the strengths and weaknesses of our model and derives the conclusions. 0 into PostgreSQL...\n",
      "Inserting test sample 2300  The Asian giant hornet is a destructive invasive species, causing widespread ecological and economic damage throughout many countries. Effective control methods are desperately needed to mitigate their severe impacts. This study proposes a novel approach based on the integration of image processing and biological dispersal.\n",
      "\n",
      "Image processing is utilized to accurately detect and identify the hornets in their natural habitat. This is achieved through the development of a computer vision system that employs deep learning algorithms to analyze images captured by drones and other unmanned aerial vehicles.\n",
      "\n",
      "Biological dispersal is then employed to reduce the population of Asian giant hornets. The proposed approach utilizes an attract-and-eliminate method involving the deployment of pheromone lures. The lures are composed of compounds that mimic the hornetsâ€™ natural scent and are designed to attract the hornets to a specific location. Once in the area, the hornets are targeted and eliminated using a variety of methods, including traps and biocontrol agents.\n",
      "\n",
      "Overall, our results demonstrate that this integrated approach is highly effective in controlling Asian giant hornet populations. The precise detection and identification provided by image processing allow for targeted deployment of pheromone lures using biological dispersal, resulting in a significant reduction in hornet populations. This approach has the potential to be scaled up for large-scale pest management efforts, and can be adapted for other insect pests as well. 1 into PostgreSQL...\n",
      "Inserting test sample 2301  One dimentional strongly nonlinear phononic crystals were assembled from chains of PTFE (polytetrafluoroethylene) and stainless steel spheres with gauges installed inside the beads. Trains of strongly nonlinear solitary waves were excited by an impact. A significant modification of the signal shape and an increase of solitary wave speed up to two times (at the same amplitude of dynamic contact force)were achieved through a noncontact magnetically induced precompression of the chains. Data for PTFE based chains are presented for the first time and data for stainless steel based chains were extended into a smaller range of amplitudes by more than one order of magnitude than previously reported. Experimental results were found to be in reasonable agreement with the long wave approximation and with numerical calculations based on Hertz interaction law for discrete chains. 0 into PostgreSQL...\n",
      "Inserting test sample 2302  This paper investigates the tunability of solitary wave properties in one-dimensional strongly nonlinear phononic crystals. By utilizing the discrete element method, we explore the dynamical behaviors of such crystals and analyze the effects of varying the nonlinear coefficient, amplitude, and frequency on the propagation of solitary waves. Through this study, we demonstrate that different solitary wave properties, including the amplitude, velocity, and pulse shape, can be easily tuned by adjusting the system parameters. Furthermore, we investigate the stability of such waves under the influence of defects and impurities. Our results provide insight into the development of new phononic devices with tunable characteristics and may have implications in applications such as energy localization and filtering. 1 into PostgreSQL...\n",
      "Inserting test sample 2303  We propose a new approach to implement the density matrix renormalization group (DMRG) in two dimensions. With this approach the initial blocks of a L by L lattice are built up directly from the matrix elements of a (L-1) by L-1) lattice and the topological characteristics of two dimensional lattices is preserved in the iteration of DMRG. By applying it to the spin-1/2 Heisenberg model on both square and triangle lattices, we find that this approach is significantly more efficient and accurate than other two-dimensional DMRG methods currently in use. 0 into PostgreSQL...\n",
      "Inserting test sample 2304  The Density Matrix Renormalization Group (DMRG) has been highly successful in studying one-dimensional quantum lattice systems. However, its application to two-dimensional systems has remained a challenge due to the exponential growth of the Hilbert space. In this paper, we introduce a two-dimensional algorithm of DMRG, which allows for efficient calculations by approximating the ground state wave function using matrix product states. We validate the algorithm by applying it to several model systems, including the two-dimensional Heisenberg model, and demonstrate its effectiveness in accurately describing ground state properties. 1 into PostgreSQL...\n",
      "Inserting test sample 2305  We propose a formation mechanism for twin blue stragglers (BSs) in compact binaries that involves mass transfer from an evolved outer tertiary companion on to the inner binary via a circumbinary disk. We apply this scenario to the observed double BS system Binary 7782 in the old open cluster NGC 188, and show that its observed properties are naturally reproduced within the context of the proposed model. We predict the following properties for twin BSs: (1) For the outer tertiary orbit, the initial orbital period should lie between 220 days $\\lesssim$ P$_{\\rm out}$ $\\lesssim$ 1100 days, assuming initial masses for the inner binary components of $m_{\\rm 1} = 1.1$ M$_{\\odot}$ and $m_{\\rm 2} =$ 0.9 M$_{\\odot}$ and an outer tertiary mass of $m_{\\rm 3} = 1.4$ M$_{\\odot}$. After Roche-lobe overflow, the outer star turns into a white dwarf (WD) of mass 0.43 to 0.54\\,\\MSun. There is a correlation between the mass of this WD and the outer orbital period: more massive WDs will be on wider orbits. (3) The rotational axes of both BSs will be aligned with each other and the orbital plane of the outer tertiary WD. (4) The BSs will have roughly equal masses, independent of their initial masses (since the lower mass star accretes the most). The dominant accretor should, therefore, be more enriched by the accreted material. Hence, one of the BSs will appear to be more enriched by either He, C and O or by s-process elements, if the donor started Roche lobe overflow on, respectively, the red giant or asymptotic giant branch. (5) Relative to old clusters, twin BSs in close binaries formed from the proposed mechanism should be more frequent in the Galactic field and open clusters with ages $\\lesssim$ 4-6 Gyr, since then the donor will have a radiative envelope.\n",
      "\n",
      "(6) The orbit of the binary BS will have a small semi-major axis (typically $\\aplt 0.3$\\,au) and be close to circular ($e \\aplt 0.2$). 0 into PostgreSQL...\n",
      "Inserting test sample 2306  Twin blue stragglers are a fascinating phenomenon in astronomy. For a long time, scientists have sought to understand their origin in close binary systems. Recent studies have shown that there might not be one but three different paths towards the formation of twin blue stragglers.\n",
      "\n",
      "The first scenario is the merger hypothesis. In this case, two stars merge into a single one, forming a blue straggler. When that blue straggler is part of a binary system, a second merger may occur, resulting in twin blue stragglers.\n",
      "\n",
      "The second scenario is the mass transfer hypothesis. In this case, a star in a binary system transfers mass into its companion star, causing the two stars to become brighter and bluer, and thus twin blue stragglers.\n",
      "\n",
      "The third scenario is the collision hypothesis. Here, two stars approach each other closely and collide, triggering a massive explosion and producing twin blue stragglers.\n",
      "\n",
      "Our study aims to investigate evidence for all three scenarios in a sample of close binary systems. Using observations from telescopes across the world, we found evidence for all three scenarios.\n",
      "\n",
      "Furthermore, our study provides insights into the properties of twin blue stragglers. We found that twin blue stragglers produced through mergers have a different chemical composition compared to those produced through mass transfer or collision. We also found that twin blue stragglers produced through mergers have a wider variety of masses compared to those produced through mass transfer or collision.\n",
      "\n",
      "In conclusion, our study provides evidence for three distinct paths towards twin blue straggler formation in close binary systems. Our findings shed new light on the complexity of stellar evolution and provide insights into the properties of twin blue stragglers. 1 into PostgreSQL...\n",
      "Inserting test sample 2307  We present new Very Long Baseline Interferometry observations of the LINER galaxy NGC 4278. The observations were taken with the Very Long Baseline Array (VLBA) and a single antenna of the Very Large Array (VLA) at 5 GHz and 8.4 GHz and have a linear resolution of <0.1 pc. Our radio data reveal a two sided structure, with symmetric S-shaped jets emerging from a flat spectrum core. We fit the jet brightness with gaussian components, which we identify from a previous observation taken five years before. By comparing the positions of the components in the two epochs, we measure motions between 0.45 +/- 0.14 and 3.76 +/- 0.65 mas, corresponding to apparent velocities < 0.2c, and to ages in the range 8.3 - 65.8 years. Assuming that the radio morphology is intrinsically symmetric and its appearance is governed by Doppler beaming effects, we find that NGC4278 has mildly relativistic jets (beta ~ 0.75), closely aligned to the line-of-sight (2 degrees < theta < 4 degrees). Alternatively, the source could be oriented at a larger angle and asymmetries could be related to the jet interaction with the surrounding medium. We also present new simultaneous VLA observations between 1.4 and 43 GHz, and a 5 GHz light curve between 1972 and 2003. The radio spectrum can be fit by a relatively steep power-law (alpha = 0.54). We find significant variability at 5 GHz. All these arguments indicate that the radiation from NGC 4278 is emitted via the synchrotron process by relativistic particles accelerated by a supermassive black hole. Despite a much lower power, this is the same process that takes place in ordinary radio loud AGNs. 0 into PostgreSQL...\n",
      "Inserting test sample 2308  Low luminosity active galactic nuclei (LLAGNs) are known to have two-sided morphology on parsec scales. In this study, we analyze the parsec scale structure of the LLAGN in NGC 4278 using Very Long Baseline Interferometry (VLBI) measurements at radio wavelengths.\n",
      "\n",
      "We find that the LLAGN in NGC 4278 exhibits a two-sided jet-like structure with the nucleus located at the center. The two-sided structure is well resolved and extends several tens of parsecs from the central engine. Our observations indicate that the morphology of the LLAGN in NGC 4278 is consistent with the unified model for radio-loud active galactic nuclei.\n",
      "\n",
      "We also analyze the kinematics of the two-sided structure and find evidence for a rotational component in the motion of the parsec scale jets. Our results suggest that the motion of the jets is not purely radial and that the parsec scale structure of the LLAGN in NGC 4278 is influenced by the rotation of the central engine.\n",
      "\n",
      "Overall, our study provides new insights into the parsec scale structure and kinematics of the low luminosity active galactic nuclei in NGC 4278. Our findings suggest that the two-sided morphology of LLAGNs on parsec scales may be influenced by the rotational component of the central engine. These results have important implications for our understanding of the physical processes driving the activity in LLAGNs. 1 into PostgreSQL...\n",
      "Inserting test sample 2309  We consider the Vector Scheduling problem on identical machines: we have m machines, and a set J of n jobs, where each job j has a processing-time vector $p_j\\in \\mathbb{R}^d_{\\geq 0}$. The goal is to find an assignment $\\sigma:J\\to [m]$ of jobs to machines so as to minimize the makespan $\\max_{i\\in [m]}\\max_{r\\in [d]}( \\sum_{j:\\sigma(j)=i}p_{j,r})$. A natural lower bound on the optimal makespan is lb $:=\\max\\{\\max_{j\\in J,r\\in [d]}p_{j,r},\\max_{r\\in [d]}(\\sum_{j\\in J}p_{j,r}/m)\\}$. Our main result is a very simple O(log d)-approximation algorithm for vector scheduling with respect to the lower bound lb: we devise an algorithm that returns an assignment whose makespan is at most O(log d)*lb.\n",
      "\n",
      "As an application, we show that the above guarantee leads to an O(log log m)-approximation for Stochastic Minimum-Norm Load Balancing (StochNormLB). In StochNormLB, we have m identical machines, a set J of n independent stochastic jobs whose processing times are nonnegative random variables, and a monotone, symmetric norm $f:\\mathbb{R}^m \\to \\mathbb{R}_{\\geq 0}$. The goal is to find an assignment $\\sigma:J\\to [m]$ that minimizes the expected $f$-norm of the induced machine-load vector, where the load on machine i is the (random) total processing time assigned to it. Our O(log log m)-approximation guarantee is in fact much stronger: we obtain an assignment that is simultaneously an O(log log m)-approximation for StochNormLB with all monotone, symmetric norms. Next, this approximation factor significantly improves upon the O(log m/log log m)-approximation in (Ibrahimpur and Swamy, FOCS 2020) for StochNormLB, and is a consequence of a more-general black-box reduction that we present, showing that a $\\gamma(d)$-approximation for d-dimensional vector scheduling with respect to the lower bound lb yields a simultaneous $\\gamma(\\log m)$-approximation for StochNormLB with all monotone, symmetric norms. 0 into PostgreSQL...\n",
      "Inserting test sample 2310  This paper proposes a novel approximation algorithm for vector scheduling that has important applications in stochastic min-norm load balancing problems. Vector scheduling is a key operation in several applications, including job scheduling in computing systems, production scheduling, transportation scheduling, and network scheduling. The proposed algorithm provides a simple and effective approach that can solve several practical scheduling problems, including scheduling jobs with memory and communication constraints, scheduling tasks with resource requirements, and scheduling jobs with stochastic processing requirements.\n",
      "\n",
      "The algorithm is based on a deterministic rounding procedure that ensures that the scheduling problem is converted into a simple and tractable integer linear programming formulation that can be efficiently solved. The proposed algorithm is shown to have strong theoretical guarantees with respect to the quality of the solutions, as well as the running time of the algorithm. Specifically, we demonstrate that our algorithm provides an approximation ratio that is close to the optimal solution, and the running time of the algorithm is polynomial in the size of the input.\n",
      "\n",
      "We also provide a detailed analysis of the performance of our algorithm on a wide range of scheduling problems, including job scheduling, transportation scheduling, and network scheduling. In particular, we highlight the effectiveness of the proposed algorithm in solving large-scale, practical problems that have been studied extensively in the literature. Our results show that the proposed algorithm provides significant improvements in terms of the quality of the solutions and the running time compared to existing algorithms.\n",
      "\n",
      "Overall, our work provides important contributions to the theory and practice of vector scheduling and stochastic min-norm load balancing problems. The proposed algorithm provides a simple and effective approach that can be used to solve a wide range of practical scheduling problems, and our theoretical analysis provides important insights into the performance guarantees of the algorithm. As a result, we believe that the proposed algorithm will have significant impact on several application domains, including computing systems, transportation, and network scheduling. 1 into PostgreSQL...\n",
      "Inserting test sample 2311  The diffuse cosmic supernova neutrino background (DSNB) is observational target of the gadolinium-loaded Super-Kamiokande (SK) detector and the forthcoming JUNO and Hyper-Kamiokande detectors. Current predictions are hampered by our still incomplete understanding of the supernova (SN) explosion mechanism and of the neutron star (NS) equation of state and maximum mass. In our comprehensive study we revisit this problem on grounds of the landscapes of successful and failed SN explosions obtained by Sukhbold et al. and Ertl et al.\n",
      "\n",
      "with parametrized one-dimensional neutrino engines for large sets of single-star and helium-star progenitors, with the latter serving as proxy of binary evolution effects. Besides considering engines of different strengths, leading to different fractions of failed SNe with black-hole (BH) formation, we also vary the NS mass limit, the spectral shape of the neutrino emission, and include contributions from poorly understood alternative NS-formation channels such as accretion-induced or merger-induced collapse events. Since the neutrino signals of our large model sets are approximate, we calibrate the associated degrees of freedom by using state-of-the-art simulations of proto-neutron star cooling. Our predictions are higher than other recent ones because of a large fraction of failed SNe with long delay to BH formation. Our best-guess model predicts a DSNB electron-antineutrino-flux of 28.8^{+24.6}_{-10.9} cm^{-2}s^{-1} with 6.0^{+5.1}_{-2.1} cm^{-2}s^{-1} in the favorable measurement interval of [10,30] MeV, and 1.3^{+1.1}_{-0.4} cm^{-2}s^{-1} with electron-antineutrino energies > 17.3 MeV, which is roughly a factor of two below the current SK limit. The uncertainty range is dominated by the still insufficiently constrained cosmic rate of stellar core-collapse events. 0 into PostgreSQL...\n",
      "Inserting test sample 2312  The diffuse supernova neutrino background (DSNB) is a fundamental source of information about the astrophysical properties of the universe. The nature of the DSNB reflects the diversity of the stellar collapse process, which is guided by a range of physical properties and fundamental physics rules. In this study, we explore the relationship between stellar collapse diversity and the DSNB by investigating the features of neutrino spectra that result from the broad range of initial stellar conditions and collapse scenarios. \n",
      "\n",
      "We use an advanced computational model to simulate a large sample of stellar collapse events across a range of progenitor masses and metallicity levels. Our results show that while core-collapse supernovae have certain generic features that can be observed in the DSNB, there is a significant degree of diversity in the neutrino spectra that arises from the various physical conditions. The spectra exhibit a variety of shapes, durations, and peak energies, which can provide unique insights into the properties of the supernova and the astrophysical environment in which it occurred. \n",
      "\n",
      "We also explore the impact of uncertainties in the neutrino production and propagation processes on the DSNB, and find that the diversity in the spectra can help to mitigate these uncertainties and improve constraints on fundamental physical parameters. Furthermore, we demonstrate that the DSNB can be used to probe properties of both individual supernovae and the overall supernova population, providing a powerful tool for studying the astrophysical properties of the universe. \n",
      "\n",
      "Overall, our study highlights the importance of considering the diversity of the stellar collapse process when interpreting the DSNB, and shows how this diversity can be used to uncover valuable insights into the astrophysical properties of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2313  Machine learning models have been shown to be vulnerable to adversarial examples. While most of the existing methods for adversarial attack and defense work on the 2D image domain, a few recent attempts have been made to extend them to 3D point cloud data. However, adversarial results obtained by these methods typically contain point outliers, which are both noticeable and easy to defend against using the simple techniques of outlier removal. Motivated by the different mechanisms by which humans perceive 2D images and 3D shapes, in this paper we propose the new design of \\emph{geometry-aware objectives}, whose solutions favor (the discrete versions of) the desired surface properties of smoothness and fairness. To generate adversarial point clouds, we use a targeted attack misclassification loss that supports continuous pursuit of increasingly malicious signals. Regularizing the targeted attack loss with our proposed geometry-aware objectives results in our proposed method, Geometry-Aware Adversarial Attack ($GeoA^3$). The results of $GeoA^3$ tend to be more harmful, arguably harder to defend against, and of the key adversarial characterization of being imperceptible to humans. While the main focus of this paper is to learn to generate adversarial point clouds, we also present a simple but effective algorithm termed $Geo_{+}A^3$-IterNormPro, with Iterative Normal Projection (IterNorPro) that solves a new objective function $Geo_{+}A^3$, towards surface-level adversarial attacks via generation of adversarial point clouds. We quantitatively evaluate our methods on both synthetic and physical objects in terms of attack success rate and geometric regularity. For a qualitative evaluation, we conduct subjective studies by collecting human preferences from Amazon Mechanical Turk. Comparative results in comprehensive experiments confirm the advantages of our proposed methods. 0 into PostgreSQL...\n",
      "Inserting test sample 2314  Adversarial attacks on point cloud-based deep learning models pose a major threat to their security and reliability. In this paper, we present a novel method for generating geometry-aware adversarial point clouds, which can successfully fool state-of-the-art deep learning models while simultaneously preserving the geometry of the original object. Our method involves first defining a perturbation set that enables adversarial point cloud generation within the allowable geometry space. We then propose an optimization algorithm that searches within this perturbation set to find an adversarial point cloud that maximizes the loss in the target deep learning model. The optimization takes into account the geometric properties of the input object, ensuring that the perturbed point cloud maintains its shape and structure while fooling the model. We also show that our method is robust against different defense mechanisms that protect the system from adversarial attacks, such as input transformations and adversarial training. To evaluate the effectiveness of our method, we conducted extensive experiments on several datasets and achieved success rates of up to 99%, outperforming existing methods on these datasets. The proposed method can be used to improve the security of point cloud-based deep learning models in various real-world applications, such as autonomous driving, robotics, and object recognition. 1 into PostgreSQL...\n",
      "Inserting test sample 2315  A family $F$ of sets is said to satisfy the $(p,q)$-property if among any $p$ sets of $F$ some $q$ intersect. The celebrated $(p,q)$-theorem of Alon and Kleitman asserts that any family of compact convex sets in $\\mathbb{R}^d$ that satisfies the $(p,q)$-property for some $q \\geq d+1$, can be pierced by a fixed number $f_d(p,q)$ of points. The minimum such piercing number is denoted by $HD_d(p,q)$. Already in 1957, Hadwiger and Debrunner showed that whenever $q>\\frac{d-1}{d}p+1$ the piercing number is $HD_d(p,q)=p-q+1$; no exact values of $HD_d(p,q)$ were found ever since.\n",
      "\n",
      "While for an arbitrary family of compact convex sets in $\\mathbb{R}^d$, $d \\geq 2$, a $(p,2)$-property does not imply a bounded piercing number, such bounds were proved for numerous specific families. The best-studied among them is axis-parallel rectangles in the plane. Wegner and (independently) Dol'nikov used a $(p,2)$-theorem for axis-parallel rectangles to show that $HD_{\\mathrm{rect}}(p,q)=p-q+1$ holds for all $q>\\sqrt{2p}$. These are the only values of $q$ for which $HD_{\\mathrm{rect}}(p,q)$ is known exactly.\n",
      "\n",
      "In this paper we present a general method which allows using a $(p,2)$-theorem as a bootstrapping to obtain a tight $(p,q)$-theorem, for families with Helly number 2, even without assuming that the sets in the family are convex or compact. To demonstrate the strength of this method, we obtain a significant improvement of an over 50 year old result by Wegner and Dol'nikov.\n",
      "\n",
      "Namely, we show that $HD_{\\mathrm{d-box}}(p,q)=p-q+1$ holds for all $q > c' \\log^{d-1} p$, and in particular, $HD_{\\mathrm{rect}}(p,q)=p-q+1$ holds for all $q \\geq 7 \\log_2 p$ (compared to $q \\geq \\sqrt{2p}$ of Wegner and Dol'nikov).\n",
      "\n",
      "In addition, for several classes of families, we present improved $(p,2)$-theorems, some of which can be used as a bootstrapping to obtain tight $(p,q)$-theorems. 0 into PostgreSQL...\n",
      "Inserting test sample 2316  This paper examines the relationship between the $(p,2)$-theorem and the tight $(p,q)$-theorem. The $(p,2)$-theorem states that every set of $n$ points in a metric space can be embedded into $\\ell_p^k$, for some integer $k$, so that the distances between points are approximately preserved up to a factor of 2. The tight $(p,q)$-theorem is a generalization of this result that allows for preservation of distances up to a factor of $q$.\n",
      "\n",
      "Our main result is that the tight $(p,q)$-theorem can be obtained from the $(p,2)$-theorem using a new embedding technique. Specifically, we show that there exists a Lipschitz embedding of any $n$-point metric space $(M,d)$ into the Banach space $\\ell_p^k$ with distortion $q$, where $k$ and $q$ are both polynomial in $n$ and $\\log(1/\\delta)$ for any fixed $\\delta >0$.\n",
      "\n",
      "To achieve this result, we first establish a connection between extremal set theory and metric embedding. We then use this connection to construct an explicit family of \"gadgets\" that can be used to embed any $n$-point metric space into a Banach space with distortion $q$. Furthermore, we show that this family of gadgets can be embedded into $\\ell_p^k$ using a variant of the Dvoretzky-Rogers lemma.\n",
      "\n",
      "Our technique provides a novel way to construct tight embeddings and can be applied to a wide range of problems in computational geometry and metric embedding. In particular, it has implications for the study of dimensionality reduction, nearest neighbor search, and approximation algorithms for metric problems. Overall, our results contribute to the rich literature on metric embedding and offer new insights into the connection between extremal set theory and optimization in metric spaces. 1 into PostgreSQL...\n",
      "Inserting test sample 2317  In this paper we study expander graphs and their minors. Specifically, we attempt to answer the following question: what is the largest function $f(n,\\alpha,d)$, such that every $n$-vertex $\\alpha$-expander with maximum vertex degree at most $d$ contains {\\bf every} graph $H$ with at most $f(n,\\alpha,d)$ edges and vertices as a minor? Our main result is that there is some universal constant $c$, such that $f(n,\\alpha,d)\\geq \\frac{n}{c\\log n}\\cdot \\left(\\frac{\\alpha}{d}\\right )^c$. This bound achieves a tight dependence on $n$: it is well known that there are bounded-degree $n$-vertex expanders, that do not contain any grid with $\\Omega(n/\\log n)$ vertices and edges as a minor. The best previous result showed that $f(n,\\alpha,d) \\geq \\Omega(n/\\log^{\\kappa}n)$, where $\\kappa$ depends on both $\\alpha$ and $d$.\n",
      "\n",
      "Additionally, we provide a randomized algorithm, that, given an $n$-vertex $\\alpha$-expander with maximum vertex degree at most $d$, and another graph $H$ containing at most $\\frac{n}{c\\log n}\\cdot \\left(\\frac{\\alpha}{d}\\right )^c$ vertices and edges, with high probability finds a model of $H$ in $G$, in time poly$(n)\\cdot (d/\\alpha)^{O\\left( \\log(d/\\alpha) \\right)}$.\n",
      "\n",
      "We note that similar but stronger results were independently obtained by Krivelevich and Nenadov: they show that $f(n,\\alpha,d)=\\Omega \\left(\\frac{n\\alpha^2}{d^2\\log n} \\right)$, and provide an efficient algorithm, that, given an $n$-vertex $\\alpha$-expander of maximum vertex degree at most $d$, and a graph $H$ with $O\\left( \\frac{n\\alpha^2}{d^2\\log n} \\right)$ vertices and edges, finds a model of $H$ in $G$.\n",
      "\n",
      "Finally, we observe that expanders are the `most minor-rich' family of graphs in the following sense: for every $n$-vertex and $m$-edge graph $G$, there exists a graph $H$ with $O \\left( \\frac{n+m}{\\log n} \\right)$ vertices and edges, such that $H$ is not a minor of $G$. 0 into PostgreSQL...\n",
      "Inserting test sample 2318  Expander graphs are known for their many desirable properties, such as being highly connected and having good expansion properties. A long-standing open problem in this field is to determine the existence of large minors in expanders. Specifically, given a finite family of expander graphs, the question is whether there always exists a large subgraph that is a minor of the family. \n",
      "\n",
      "In recent years, there has been significant progress made towards answering this question. One approach taken has been to consider the relationship between expanders and the Ramanujan property. Ramanujan graphs have been shown to have many useful properties, and recent work has shown that expanders with good degree bounds must have almost Ramanujan spectra. This has led to the discovery of new constructions of expanders, which have been used to show the existence of large minors in certain families of expanders.\n",
      "\n",
      "Another approach has been to use the theory of quasi-randomness. A graph is said to be quasi-random if it behaves like a random graph in certain ways, such as having a distribution of subgraphs that is indistinguishable from that of a random graph of the same order and degree. Quasi-random graphs have been shown to have many interesting and useful properties, and recent work has used this theory to obtain results on the existence of large minors in expanders.\n",
      "\n",
      "Despite these recent developments, the question of the existence of large minors in expanders remains open in general. One interesting direction for future research is to investigate the relationship between expanders and other areas of mathematics, such as algebraic topology and the theory of random walks on graphs. It is hoped that a deeper understanding of these connections will lead to new insights into this important problem. 1 into PostgreSQL...\n",
      "Inserting test sample 2319  We show here how the internal structure of a neutron star can be inferred from its gravitational wave spectrum. Under the premise that the frequencies and damping rates of a few $w$-mode oscillations are found, we apply an inversion scheme to determine its mass, radius and density distribution. In addition, accurate equation of state of nuclear matter can also be determined. 0 into PostgreSQL...\n",
      "Inserting test sample 2320  Neutron stars are some of the most extreme objects in the universe, and their interiors remain poorly understood. However, recent advances in gravitational wave detection offer new opportunities to probe their internal structure. By measuring the distortions induced by gravitational waves, it is possible to infer important properties such as the equation of state and the presence of exotic matter. This paper reviews the latest developments in this exciting field. 1 into PostgreSQL...\n",
      "Inserting test sample 2321  We calculate the diffuse high energy (TeV - PeV) neutrino emission from hyperflares of Soft-Gamma Repeaters (SGRs), like the hyperflare risen from \\astrobj{SGR 1806-20} on December 27 of 2004, within the framework of the fireball model. The fireball model for gamma-ray bursts (GRBs) can explain well the main features of this hyperflare and the subsequent multi-frequency terglow emission. The expected rate, $\\sim 20-100$ Gpc$^{-3}$day$^{-1}$, of such hyperflares is well in excess of the GRBs rate. Our result shows that the contribution to the diffuse TeV-PeV neutrino background from such hyperflares is less than 10% of the contribution from GRBs. We also discuss the high energy cosmic rays (CRs) from these sources. 0 into PostgreSQL...\n",
      "Inserting test sample 2322  In this study, we investigate the possible origin of diffuse high energy neutrinos and cosmic rays from hyperflares of soft-gamma repeaters. We analyze their association with magnetars, highly magnetic neutron stars that can exhibit unpredictable bursts of energy. Our simulations demonstrate that hyperflares of soft-gamma repeaters can produce sufficient acceleration of cosmic rays and neutrinos to explain their observed flux. Furthermore, we find that these hyperflares can be considered as sources of high-energy astrophysical neutrinos. Our results not only provide new insights into the astrophysical sources of cosmic rays and high-energy neutrinos, but also highlight the importance of observations of soft-gamma repeaters in advancing our understanding of these enigmatic phenomena. 1 into PostgreSQL...\n",
      "Inserting test sample 2323  This work introduces a novel estimation method, called LOVE, of the entries and structure of a loading matrix A in a sparse latent factor model X = AZ + E, for an observable random vector X in Rp, with correlated unobservable factors Z \\in RK, with K unknown, and independent noise E. Each row of A is scaled and sparse. In order to identify the loading matrix A, we require the existence of pure variables, which are components of X that are associated, via A, with one and only one latent factor. Despite the fact that the number of factors K, the number of the pure variables, and their location are all unknown, we only require a mild condition on the covariance matrix of Z, and a minimum of only two pure variables per latent factor to show that A is uniquely defined, up to signed permutations. Our proofs for model identifiability are constructive, and lead to our novel estimation method of the number of factors and of the set of pure variables, from a sample of size n of observations on X. This is the first step of our LOVE algorithm, which is optimization-free, and has low computational complexity of order p2. The second step of LOVE is an easily implementable linear program that estimates A. We prove that the resulting estimator is minimax rate optimal up to logarithmic factors in p. The model structure is motivated by the problem of overlapping variable clustering, ubiquitous in data science. We define the population level clusters as groups of those components of X that are associated, via the sparse matrix A, with the same unobservable latent factor, and multi-factor association is allowed.\n",
      "\n",
      "Clusters are respectively anchored by the pure variables, and form overlapping sub-groups of the p-dimensional random vector X. The Latent model approach to OVErlapping clustering is reflected in the name of our algorithm, LOVE. 0 into PostgreSQL...\n",
      "Inserting test sample 2324  This research work investigates the problem of adaptive estimation in structured factor models with overlapping clustering applications. Factor models are widely applied in various domains for data analysis and dimensionality reduction. However, most factor models assume that the observed data variables are independent, and this assumption restricts their ability to capture the correlation present in real-world data. This study aims to develop a new approach for modeling factors that enables the incorporation of the correlation structure present in real-world data, addressing the limitation of traditional factor models. \n",
      "\n",
      "The proposed adaptive factor model estimation method leverages the structural properties of the factor loadings' matrix to enable more accurate estimation, particularly when the number of variables and factors is large. In particular, the proposed method can identify the true projection matrix, even when the loadings matrix has non-standard structures, such as high correlations. The method also enables the selection of the optimal number of active factors that preserve the critical characteristics of data. \n",
      "\n",
      "To attain the desired properties of the proposed method, we derive novel asymptotic theory and optimization algorithms that allow for faster computation of the parameter estimates. In simulation studies, our proposed adaptive factor model estimation method outperformed other existing approaches in terms of identifying the true projection matrix. We further demonstrate the performance of our method using real-world data from two different applications cases, in which we encountered overlapping clusters.\n",
      "\n",
      "In summary, we develop a novel framework for adaptive estimation in factor models that enables the incorporation of correlation structures in real-world data and improves the accuracy of the underlying estimates. Furthermore, our approach incorporates the aspect of overlapping clusters, which is of significant interest in data analysis and has broad applications in fields such as biology and social sciences. 1 into PostgreSQL...\n",
      "Inserting test sample 2325  Multi-wavelength (X-ray to radio) monitoring of Young Stellar Objects (YSOs) can provide important information about physical processes at the stellar surface, in the stellar corona, and/or in the inner circumstellar disk regions.\n",
      "\n",
      "While coronal processes should mainly cause variations in the X-ray and radio bands, accretion processes may be traced by time-correlated variability in the X-ray and optical/infrared bands. Several multi-wavelength studies have been successfully performed for field stars and approx. 1-10 Myr old T Tauri stars, but so far no such study succeeded in detecting simultaneous X-ray to radio variability in extremely young objects like class I and class 0 protostars.\n",
      "\n",
      "Here we present the first simultaneous X-ray, radio, near-infrared, and optical monitoring of YSOs, targeting the Coronet cluster in the Corona Australis star-forming region, which harbors at least one class 0 protostar, several class I objects, numerous T Tauri stars, and a few Herbig AeBe stars. [...] Seven objects are detected simultaneously in the X-ray, radio, and optical/infrared bands; they constitute our core sample. While most of these sources exhibit clear variability in the X-ray regime and several also display optical/infrared variability, none of them shows significant radio variability on the timescales probed. We also do not find any case of clearly time-correlated optical/infrared and X-ray variability. [...] The absence of time-correlated multi-wavelength variability suggests that there is no direct link between the X-ray and optical/infrared emission and supports the notion that accretion is not an important source for the X-ray emission of these YSOs.\n",
      "\n",
      "No significant radio variability was found on timescales of days. 0 into PostgreSQL...\n",
      "Inserting test sample 2326  The Coronet cluster is a young stellar group in the Rho Ophiuchi cloud complex, which is believed to harbor pre-main-sequence stars undergoing various physical processes such as accretion, outflows, and disk evolution. To investigate the multiwavelength properties of the young stellar population in this region, we carried out simultaneous observations of X-ray, radio, near-infrared (NIR), and optical bands using the Chandra X-ray Observatory, the Jansky Very Large Array, the FourStar camera at Magellan Baade, and the Hubble Space Telescope, respectively.\n",
      "\n",
      "Our X-ray study identified 71 X-ray sources associated with Coronet cluster members based on positional coincidence, spectral hardness, and variability. We found that the X-ray activity level of the cluster was intermediate between Class I and Class II sources in Taurus-Auriga and the Orion Nebula Cluster, respectively. The non-thermal radio emission from the cluster was detected from only a few sources, while the majority of objects exhibited no radio detections above the 5Ïƒ noise level. \n",
      "\n",
      "Our NIR imaging revealed a heavily obscured environment around the cluster. We identified a few extended structures that may be dusty shells, bow shocks, or cavities excavated by outflows. Furthermore, we found that the NIR colors of the cluster stars were consistent with those of Class II sources, suggesting that most of the objects have circumstellar disks. \n",
      "\n",
      "Finally, we performed optical photometry and spectroscopy of the cluster stars, and classified them into different types based on their spectral features and variability properties. We discovered a few Herbig Ae/Be stars that are likely to be the most massive and evolved members of the cluster. \n",
      "\n",
      "Overall, our multiwavelength observations provided a comprehensive view of the young stellar objects in the Coronet cluster and shed light on their formation and evolution processes. 1 into PostgreSQL...\n",
      "Inserting test sample 2327  Double-layer quantum Hall systems at Landau level filling factor $\\nu=1$ have a broken symmetry ground state with spontaneous interlayer phase coherence and a gap between symmetric and antisymmetric subbands in the absence of interlayer tunneling. We examine the influence of quantum fluctuations on the spectral function of the symmetric Green's function, probed in optical absorption experiments (cond-mat/9809373). We find that as the maximum layer separation at which the $\\nu=1$ quantum Hall effect occurs is approached, absorption in the lowest Landau level grows in strength. Detailed line shapes for this absorption are evaluated and related to features in the system's collective excitation spectrum. 0 into PostgreSQL...\n",
      "Inserting test sample 2328  We investigate the phenomenon of order parameter suppression in double-layer quantum Hall ferromagnets. By analyzing the ground state properties of two-dimensional electron gases under strong magnetic fields, we show that the interplay between magnetic field strength and layer separation can lead to suppression of the order parameter in certain regimes. Such suppression can be understood in terms of Landau level mixing and screening effects. Our findings shed light on the complex behavior of ferromagnetic systems in the quantum Hall regime and have important implications for the design and optimization of spintronic devices based on these materials. 1 into PostgreSQL...\n",
      "Inserting test sample 2329  We analyze a new fundamental building block for monolithic nanoengineering on graphene: the Inverse-Stone-Thrower-Wales (ISTW) defect. The ISTW is formed from a pair of joined pentagonal carbon rings placed between a pair of heptagonal rings; the well-known Stone-Thrower-Wales (STW) defect is the same arrangement, but with the heptagonal rather than pentagonal rings joined. When removed and passivated with hydrogen, the structure constitutes a new molecule, diazulene, which may be viewed as the result of an ad-dimer defect on anthracene. Embedding diazulene in the honeycomb lattice, we study the effect of ad-dimers on planar graphene. Because the ISTW defect has yet to be experimentally identified, we examine several synthesis routes and find one for which the barrier is only slightly higher than that associated with adatom hopping on graphene. ISTW and STW defects may be viewed as fundamental building blocks for monolithic structures on graphene. We show how to construct extended defect domains on the surface of graphene in the form of blisters, bubbles, and ridges on a length scale as small as 2 angstroms by 7 angstroms. Our primary tool in these studies is density functional theory. 0 into PostgreSQL...\n",
      "Inserting test sample 2330  Graphene, a two-dimensional lattice of carbon atoms, has been a subject of intense research for its potential applications in various fields. One of the key challenges in graphene nanoengineering is to control the formation of defects, which can significantly affect its properties. In this study, we investigated the inverse-Stone-Thrower-Wales (ISTW) defect in graphene, which is a topological defect consisting of a pentagon surrounded by four heptagons. Using density functional theory calculations, we found that the formation energy of the ISTW defect is significantly lower than that of other common defects in graphene, such as the Stone-Thrower-Wales (STW) defect. We also observed that the presence of the ISTW defect can lead to significant changes in the electronic properties of graphene, such as the opening of a bandgap. Our results suggest that the ISTW defect may play a crucial role in tuning the properties of graphene for various applications, and highlight the importance of defect engineering in graphene nanotechnology. 1 into PostgreSQL...\n",
      "Inserting test sample 2331  Co-expression network is a critical technique for the identification of inter-gene interactions, which usually relies on all-pairs correlation (or similar measure) computation between gene expression profiles across multiple samples. Pearson's correlation coefficient (PCC) is one widely used technique for gene co-expression network construction. However, all-pairs PCC computation is computationally demanding for large numbers of gene expression profiles, thus motivating our acceleration of its execution using high-performance computing. In this paper, we present LightPCC, the first parallel and distributed all-pairs PCC computation on Intel Xeon Phi (Phi) clusters. It achieves high speed by exploring the SIMD-instruction-level and thread-level parallelism within Phis as well as accelerator-level parallelism among multiple Phis. To facilitate balanced workload distribution, we have proposed a general framework for symmetric all-pairs computation by building bijective functions between job identifier and coordinate space for the first time. We have evaluated LightPCC and compared it to two CPU-based counterparts: a sequential C++ implementation in ALGLIB and an implementation based on a parallel general matrix-matrix multiplication routine in Intel Math Kernel Library (MKL) (all use double precision), using a set of gene expression datasets. Performance evaluation revealed that with one 5110P Phi and 16 Phis, LightPCC runs up to $20.6\\times$ and $218.2\\times$ faster than ALGLIB, and up to $6.8\\times$ and $71.4\\times$ faster than single-threaded MKL, respectively. In addition, LightPCC demonstrated good parallel scalability in terms of number of Phis.\n",
      "\n",
      "Source code of LightPCC is publicly available at http://lightpcc.sourceforge.net. 0 into PostgreSQL...\n",
      "Inserting test sample 2332  With the increasing demand for high-performance computing, parallel computation has become a promising technique. In this paper, we propose a parallel pairwise correlation computation method for Intel Xeon Phi clusters. The method achieves a high degree of parallelism by dividing the data into small chunks and distributing them across the nodes. Our approach involves using the correlation coefficient to determine the relationship between two variables. The correlation coefficient reveals whether there is a positive or negative correlation between the variables, and the strength of the correlation.\n",
      "\n",
      "We implemented our method on a cluster of Intel Xeon Phi processors and evaluated its performance using a set of real-world datasets. Our experiments demonstrate that our method achieves a significant speedup when compared to the traditional approach. This is due to the high degree of parallelism and the efficient use of memory bandwidth.\n",
      "\n",
      "We also assessed the scalability of our method with respect to the number of nodes in the cluster. Our results show that our approach scales well with the increasing number of nodes, enabling us to process large datasets efficiently.\n",
      "\n",
      "In conclusion, we propose a parallel pairwise correlation computation approach that achieves high performance on Intel Xeon Phi clusters. Our method is scalable and efficient, making it suitable for large-scale data processing applications. 1 into PostgreSQL...\n",
      "Inserting test sample 2333  Federated learning (FL) is a distributed learning process where the model (weights and checkpoints) is transferred to the devices that posses data rather than the classical way of transferring and aggregating the data centrally. In this way, sensitive data does not leave the user devices. FL uses the FedAvg algorithm, which is trained in the iterative model averaging way, on the non-iid and unbalanced distributed data, without depending on the data quantity. Some issues with the FL are, 1) no scalability, as the model is iteratively trained over all the devices, which amplifies with device drops; 2) security and privacy trade-off of the learning process still not robust enough and 3) overall communication efficiency and the cost are higher. To mitigate these challenges we present Federated Learning and Privately Scaling (FLaPS) architecture, which improves scalability as well as the security and privacy of the system. The devices are grouped into clusters which further gives better privacy scaled turn around time to finish a round of training. Therefore, even if a device gets dropped in the middle of training, the whole process can be started again after a definite amount of time. The data and model both are communicated using differentially private reports with iterative shuffling which provides a better privacy-utility trade-off. We evaluated FLaPS on MNIST, CIFAR10, and TINY-IMAGENET-200 dataset using various CNN models. Experimental results prove FLaPS to be an improved, time and privacy scaled environment having better and comparable after-learning-parameters with respect to the central and FL models. 0 into PostgreSQL...\n",
      "Inserting test sample 2334  The proliferation of big data has created an urgent need for more intelligent and private approaches to machine learning. Federated Learning and Privacy-preserving technologies have emerged as solutions to meet this demand. In this paper, we introduce FLaPS, a novel framework for Federated Learning and Privately Scaling that offers new possibilities to address the existing challenges and push the boundaries of machine learning. FLaPS enables data owners to collaborate without sharing raw data or any identifiable information, preserving the privacy of the users' data and preventing unauthorized access while scaling machine learning models to support training over large-scale datasets. We evaluate FLaPS in various scenarios, including different settings and parametric choices. Results show that FLaPS achieves compelling performance while preserving privacy and attaining strong accuracy comparable to conventional machine learning pipelines. FLaPS framework is not only efficient but also flexible, which allows researchers to experiment with different parameters and privacy-preserving protocols within a federated setting quickly. In addition, we conduct a comparative study of FLaPS with other state-of-the-art approaches, and the results signify the practicality and efficiency of the FLaPS approach. Overall, FLaPS provides a promising path forward for the secure deployment of machine learning models in an era dominated by big data and privacy concerns. 1 into PostgreSQL...\n",
      "Inserting test sample 2335  The successful prediction of lensing events is a new and exciting enterprise that provides opportunities to discover and study planetary systems. The companion paper investigates the underlying theory. This paper is devoted to outlining the components of observing programs that can discover planets orbiting stars predicted to make a close approach to a background star. If the time and distance of closest approach can be well predicted, then the system can be targeted for individual study. In most cases, however, the predictions will be imprecise, yielding only a set of probable paths of approach and event times. We must monitor an ensemble of such systems to ensure discovery, a strategy possible with observing programs similar to a number of current surveys, including PTF and Pan-STARRS; nova searches, including those conducted by amateurs; ongoing lensing programs such as MOA and OGLE; as well as MEarth, Kepler and other transit studies. If well designed, the monitoring programs will be guaranteed to either discover planets in orbits with semi-major axes smaller than about two Einstein radii, or else to rule out their presence.\n",
      "\n",
      "Planets on wider orbits may not all be discovered, but if they are common, will be found among the events generated by ensembles of potential lenses. We consider the implications for VB 10, the first star to make a predicted approach to a background star that is close enough to allow planets to be discovered. VB 10 is not an ideal case, but it is well worth studying. A more concise summary of this work, and information for observers can be found at https://www.cfa.harvard.edu/~jmatthews/vb10.html. 0 into PostgreSQL...\n",
      "Inserting test sample 2336  The search for exoplanets has become a prominent area of interest in astronomy, as scientists seek to understand the nature of other planets beyond our Solar System. One promising technique of detecting such planets is mesolensing, which involves the detection of microlensing events caused by medium-sized stars. Here, we present PLAN-IT, an observing program designed to detect exoplanets through mesolensing events. We apply the program to VB 10, a nearby M-dwarf star, in order to assess its effectiveness in detecting planetary transits and measuring planetary parameters.\n",
      "\n",
      "Our analysis is based on a combination of photometry and spectroscopy data, which we collected during a series of observing runs at the Subaru Telescope on Maunakea. We use a novel data reduction pipeline to extract photometric time-series measurements of the VB 10 light curve, which we then analyze to detect and characterize planetary transits. Our results reveal evidence for a planetary companion orbiting VB 10, with an inferred mass of approximately twice that of the Earth and an orbital period of around 20 days.\n",
      "\n",
      "Beyond the detection of a new exoplanet, our analysis demonstrates the effectiveness of PLAN-IT in detecting medium-sized lensing events and the potential it holds for future exoplanet studies. We outline the next steps of our program, which include extending our observations to other nearby M-dwarf stars and improving the data analysis techniques. With further refinement, PLAN-IT may prove to be a vital tool in the ongoing search for exoplanets and help to answer some of the most pressing questions in modern astronomy. 1 into PostgreSQL...\n",
      "Inserting test sample 2337  The effect of burstiness in complex networks has received considerable attention. In particular, its effect on temporal distance and delays in the air transportation system is significant owing to their huge impact on our society.\n",
      "\n",
      "Therefore, in this paper, we propose two indexes of temporal distance based on passengers' behavior and analyze the effect. As a result, we find that burstiness shortens the temporal distance while delays are increased. Moreover, we discover that the positive effect of burstiness is lost when flight schedules get overcrowded. 0 into PostgreSQL...\n",
      "Inserting test sample 2338  This study assesses the impact of burstiness on the performance of the air transportation system. Burstiness refers to the phenomenon where traffic demand experiences sudden, irregular spikes. Data extracted from a major US airport is analyzed to model the implications of burstiness on average delay and throughput. Results indicate that bursty arrivals can cause significant delays and reduce system throughput. This study highlights the importance of mitigating burstiness and devising ways to manage demand spikes, to enhance the air transportation system's performance. 1 into PostgreSQL...\n",
      "Inserting test sample 2339  Migration, and especially irregular migration, is a critical issue for border agencies and society in general. Migration-related situations and decisions are influenced by various factors, including the perceptions about migration routes and target countries. An improved understanding of such factors can be achieved by systematic automated analyses of media and social media channels, and the videos and images published in them. However, the multifaceted nature of migration and the variety of ways migration-related aspects are expressed in images and videos make the finding and automated analysis of migration-related multimedia content a challenging task. We propose a novel approach that effectively bridges the gap between a substantiated domain understanding - encapsulated into a set of Migration-related semantic concepts - and the expression of such concepts in a video, by introducing an advanced video analysis and retrieval method for this purpose. 0 into PostgreSQL...\n",
      "Inserting test sample 2340  This study presents an innovative method for semantic concept extraction that would enable more effective retrieval of relevant video content related to migration. The proposed method is based on a combination of word embedding and semantic association mining techniques. The method was tested on a large collection of video data related to migration and was found to be highly effective in extracting relevant semantic concepts. Our research findings suggest that the proposed method is a promising approach for improving the retrieval of relevant video content. In addition to providing a comprehensive overview of the semantic concepts related to migration, this research offers insights into the effectiveness of various natural language processing techniques and may inform the development of similar applications in other domains. 1 into PostgreSQL...\n",
      "Inserting test sample 2341  The modern theory of orbital magnetization (OM) was developed by using Wannier function method, which has a formalism similar with the Berry phase. In this manuscript, we perform a numerical study on the fate of the OM under disorder, by using this method on the Haldane model in two dimensions, which can be tuned between a normal insulator or a Chern insulator at half filling.\n",
      "\n",
      "The effects of increasing disorder on OM for both cases are simulated. Energy renormalization shifts are observed in the weak disorder regime and topologically trivial case, which was predicted by a self-consistent T-matrix approximation. Besides this, two other phenomena can be seen. One is the localization trend of the band orbital magnetization. The other is the remarkable contribution from topological chiral states arising from nonzero Chern number or large value of integrated Berry curvature. If the fermi energy is fixed at the gap center of the clean system, there is an enhancement of |M| at the intermediate disorder, for both cases of normal and Chern insulators, which can be attributed to the disorder induced topological metal state before localization. 0 into PostgreSQL...\n",
      "Inserting test sample 2342  Disorder and magnetism are both fundamental concepts in condensed matter physics. Their interplay is often complex and not fully understood. In this paper, we present a numerical study of how disorder affects the orbital magnetization in two dimensions. We use a model of interacting electrons in a magnetic field, subject to a random potential which creates the disorder. Our simulations reveal a non-monotonic dependence of the magnetization on disorder strength, with an initial suppression of magnetism at weak disorder followed by enhancement at stronger disorder. We show that this behavior can be explained by a competition between two mechanisms: disorder-enhanced paramagnetism and disorder-induced localization. Furthermore, we investigate the impact of temperature on the orbital magnetization, and find that disorder can induce a non-monotonic temperature dependence as well. Our study contributes to the understanding of magnetism and disorder in condensed matter systems, and has implications for the design of new materials with desired magnetic properties. 1 into PostgreSQL...\n",
      "Inserting test sample 2343  High-voltage direct-current (HVDC) systems for constant or intermittent power delivery have recently been developed further to support grid frequency regulation (GFR). This paper proposes a new control strategy for a line-commutated converter-based (LCC) HVDC system, wherein the DC-link voltage and current are optimally regulated to improve real-time GFR in both rectifier- and inverter-side AC networks. A dynamic model of an LCC HVDC system is developed using the DC voltage and current as input variables, and is integrated with feedback loops for inertia emulation and droop control. A linear quadratic Gaussian (LQG) controller is also designed for optimal secondary frequency control, while mitigating conflict between the droop controllers of the HVDC converters. An eigenvalue analysis is then conducted, focusing on the effects of model parameters and controller gains on the proposed strategy. Simulation case studies are also performed using the Jeju-Haenam HVDC system as a test bed. The results of the case study confirm that the proposed strategy enables the HVDC system to improve GFR, in coordination with generators in both-side grids, by exploiting the fast dynamics of HVDC converters. The proposed strategy is also effective under various conditions for the LQG weighting coefficients, inertia emulation, and droop control. 0 into PostgreSQL...\n",
      "Inserting test sample 2344  This paper proposes an optimal voltage and current control scheme for High Voltage Direct Current (HVDC) systems to enhance the real-time frequency regulation performance. The proposed method utilizes a novel Proportional Integral Derivative (PID) controller with a discrete-time compensation technique to estimate the frequency deviation and manipulate the voltage and current components accordingly. The PI controller regulates the voltage component, while the DI controller handles the current component control loop. The system's dynamics are studied, and stability analysis is provided to justify the proposed scheme's effectiveness in damping frequency oscillations during disturbances. Simulation results show that the proposed method provides excellent frequency control accuracy and superior damping characteristics compared to the conventional PI control scheme. Moreover, it robustly performs when subjected to external disturbances such as variations in load, wind speed, and generation levels. The proposed approach's effectiveness is further demonstrated through a comparative study with widely used control schemes, such as the classical PI regulators and the parallel feedforward compensator. The results confirm the superiority of the proposed method and its potential applications in future HVDC systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2345  Accurate electronic structures of the technologically important lanthanide/rare earth sesquioxides (Ln2O3, with Ln=La,...,Lu) and CeO2 have been calculated using hybrid density functionals HSE03, HSE06 and screened-exchange (sX-LDA). We find that these density functional methods describe the strongly correlated Ln f-electrons as well as the recent G0W0@LDA+U results, generally yielding the correct band gaps and trends across the Ln-period. For HSE, the band gap between O 2p states and lanthanide 5d states is nearly independent of the lanthanide, while the minimum gap varies as filled or empty Ln 4f states come into this gap. sX-LDA predicts the unoccupied 4f levels at higher energies, which leads to a better agreement with experiments for Sm2O3, Eu2O3 and Yb2O3. 0 into PostgreSQL...\n",
      "Inserting test sample 2346  This study investigates the nature of the electronic band gap in lanthanide oxides. The lanthanide series includes a wide range of oxides which have attracted significant attention due to their unique chemical and electronic properties. A comprehensive investigation of the electronic band gap in such oxides is relevant for understanding their behavior in various applications, such as catalysis, spintronics, and solid-state lighting. Theoretical and experimental methods were used to understand the band gap formation and the underlying physical mechanisms. The results show that the electronic band gap in lanthanide oxides can be attributed to a combination of the crystal field and charge transfer effects. These findings provide new insights into the fundamental physics of lanthanide oxides and have important implications for designing novel functional materials. 1 into PostgreSQL...\n",
      "Inserting test sample 2347  We review recent progress in lattice QCD at finite density. The phase diagram of QCD and the equation of state at finite temperature and density are discussed. In particular, we focus on the critical point terminating a first order phase transition line in the high density region. The critical point is one of the most interesting features that may be discovered in heavy-ion collision experiments. We summarize the current discussion on the existence of a critical point in the QCD phase diagram and discuss some attempts to find the critical point by numerical simulations. 0 into PostgreSQL...\n",
      "Inserting test sample 2348  Lattice Quantum Chromodynamics (QCD) is a powerful tool for investigating the fundamental properties of the strong nuclear force. However, simulating QCD at finite density presents significant challenges due to the so-called â€œsign problemâ€. Recent advancements have made progress towards overcoming these obstacles and have enabled simulations in the regime of finite baryon density. We review some of the most promising developments in lattice QCD at finite density, including new algorithms and techniques, and discuss their implications for our understanding of nuclear physics and the properties of matter under extreme conditions. 1 into PostgreSQL...\n",
      "Inserting test sample 2349  Cochlea is an important auditory organ in the inner ear. In most mammals, it is coiled as a spiral. Whether this specific shape influences hearing is still an open problem. By employing a three dimensional fluid model of the cochlea with an idealized geometry, the influence of the spiral geometry of the cochlea is examined. We obtain solutions of the model through a conformal transformation in a long-wave approximation. Our results show that the net pressure acting on the basilar membrane is not uniform along its spanwise direction. Also, it is shown that the location of the maximum of the spanwise pressure difference in the axial direction has a mode dependence. In the simplest pattern, the present result is consistent with the previous theory based on the WKB-like approximation [D. Manoussaki, Phys. Rev. Lett. 96, 088701(2006)]. In this mode, the pressure difference in the spanwise direction is a monotonic function of the distance from the apex and the normal velocity across the channel width is zero. Thus in the lowest order approximation, we can neglect the existance of the Reissner's membrane in the upper channel.\n",
      "\n",
      "However, higher responsive modes show different behavior and, thus, the real maximum is expected to be located not exactly at the apex, but at a position determined by the spiral geometry of the cochlea and the width of the cochlear duct. In these modes, the spanwise normal velocities are not zero. Thus, it indicates that one should take into account of the detailed geometry of the cochlear duct for a more quantitative result. The present result clearly demonstrates that not only the spiral geometry, but also the geometry of the cochlear duct play decisive roles in distributing the wave energy. 0 into PostgreSQL...\n",
      "Inserting test sample 2350  The basilar membrane is a critical component of the spiral-shaped cochlea in the inner ear. It separates the fluid-filled scala tympani from the scala media and is responsible for transducing sound waves into electrical signals that are then transmitted to the brain. This study aimed to investigate the resultant pressure distribution pattern along the basilar membrane during sound stimulation.\n",
      "\n",
      "Using a laser Doppler vibrometer, we measured the displacement of the basilar membrane in guinea pig cochleae in response to auditory stimuli. We observed that the pressure distribution pattern of the basilar membrane varied along its length, with a peak displacement occurring at a characteristic location for each frequency. We found that the displacement amplitude increased with increasing sound level, but the position of the peak displacement remained unchanged.\n",
      "\n",
      "Furthermore, we noted that the pressure distribution pattern was influenced by the mechanical properties of the basilar membrane. Changes in the stiffness of the basilar membrane resulted in altered frequency tuning of the cochlea.\n",
      "\n",
      "In conclusion, this study provides insights into the resultant pressure distribution pattern along the basilar membrane in the spiral shaped cochlea. By utilizing a laser Doppler vibrometer, we were able to determine the key characteristics of this pattern, including its dependence on frequency, sound level, and mechanical properties of the basilar membrane. This information can be used to improve our understanding of the cochlea's biomechanics and inform the development of cochlear implants and other auditory prostheses. 1 into PostgreSQL...\n",
      "Inserting test sample 2351  We present Keck/MOSFIRE $H$-band spectroscopy targeting C$\\,$III] $\\lambda$1907,1909 in a $z=7.5056$ galaxy previously identified via Ly$\\alpha$ emission. We detect strong line emission at $1.621\\pm0.002\\,\\mu$m with a line flux of ($2.63\\pm0.52$)$\\times10^{-18}$ erg s$^{-1}$ cm$^{-2}$. We tentatively identify this line as [C$\\,$III] $\\lambda$1907, but we are unable to detect C$\\,$III] $\\lambda$1909 owing to sky emission at the expected location. This gives a galaxy systemic redshift, $z_{sys}=7.5032\\pm0.0003$, with a velocity offset to Ly$\\alpha$ of $\\Delta$v$_{Ly\\alpha}$ = $88\\pm27$ km s$^{-1}$. The ratio of combined C$\\,$III]/Ly$\\alpha$ is 0.30-0.45, one of the highest values measured for any $z>2$ galaxy. We do not detect Si$\\,$III] $\\lambda\\lambda$1883, 1892, and place an upper limit on Si$\\,$III]/C$\\,$III] $<$ 0.35 ($2\\sigma$). Comparing our results to photoionization models, the C$\\,$III] equivalent width (W$_{\\text{CIII]}} = 16.23\\pm2.32\\,$\\AA), low Si$\\,$III]/C$\\,$III] ratio, and high implied [O$\\,$III] equivalent width (from the $Spitzer$/IRAC [3.6]$-$[4.5]$\\simeq$0.8 mag color) require sub-Solar metallicities ($Z\\simeq0.1-0.2 Z_{\\odot}$) and a high ionization parameter, log$\\,$U $\\gtrsim -1.5$. These results favor models that produce higher ionization, such as the BPASS models for the photospheres of high-mass stars, and that include both binary stellar populations and/or an IMF that extends to 300 M$_{\\odot}$. The combined C$\\,$III] equivalent width and [3.6]$-$[4.5] color are more consistent with ionization from young stars than AGN, however we cannot rule out ionization from a combination of an AGN and young stars. We make predictions for $James~Webb~Space~Telescope$ spectroscopy using these different models, which will ultimately test the nature of the ionizing radiation in this source. 0 into PostgreSQL...\n",
      "Inserting test sample 2352  This research paper presents the results obtained from conducting near-infrared spectroscopy on galaxies during the reionization era. The focus of this study was to measure C$\\,$III] specifically in a high-redshift galaxy located at z=7.5. The detection of C$\\,$III] in such a galaxy, during the period of reionization, was an exciting prospect as it provided insights into the early stages of galaxy formation.\n",
      "\n",
      "To accomplish this, we used the X-Shooter spectrograph mounted on the Very Large Telescope (VLT) in Chile. The VLT's sophisticated technology enabled us to capture the C$\\,$III] emission line with high precision. Our analysis revealed that the galaxy at z=7.5 exhibited a strong C$\\,$III] emission line, which was consistent with theoretical models of reionization-era galaxies.\n",
      "\n",
      "The detection of C$\\,$III] in this galaxy has several implications. Firstly, it suggests that the galaxy is undergoing a phase of rapid star formation, which is consistent with the standard picture of galaxy formation. Secondly, the measurement of C$\\,$III] allowed us to estimate the metallicity of the galaxy, which provided insights into the enrichment history of its host galaxy. Our observations suggest that the galaxy has a low metallicity, which is consistent with the early stages of galaxy formation.\n",
      "\n",
      "Overall, our study provides new insights into the early stages of galaxy formation during the period of reionization. The detection of C$\\,$III] in a galaxy at z=7.5 provides a valuable complement to previous observations of high-redshift galaxies, and enhances our understanding of how galaxies evolved in the early Universe. The success of our study demonstrates the potential of near-infrared spectroscopy as a powerful tool for studying the evolution of galaxies during reionization. 1 into PostgreSQL...\n",
      "Inserting test sample 2353  An atlas of the Galactic plane (-4.7 deg < b < 4.7 deg) plus the molecular clouds in Orion, Rho Oph, and Taurus-Auriga has been produced at 60 and 100 micron from IRAS data. The Atlas consists of resolution-enhanced coadded images having 1 arcmin -- 2 arcmin resolution as well as coadded images at the native IRAS resolution. The IRAS Galaxy Atlas, together with the DRAO HI line / 21 cm continuum and FCRAO CO (1-0) line Galactic plane surveys, both with similar (approx. 1 arcmin) resolution, provide a powerful venue for studying the interstellar medium, star formation and large scale structure in our Galaxy.\n",
      "\n",
      "This paper documents the production and characteristics of the Atlas. 0 into PostgreSQL...\n",
      "Inserting test sample 2354  The High Resolution IRAS Galaxy Atlas is a comprehensive survey of the infrared properties for 75% of the sky's galaxies, imaged and compiled from IRAS observations over three years. Consisting of images with 1.5- to 4-arcminute resolution and a flux limit of 0.6 Jy, it provides a complete 60- and 100-micron flux measurement for more than 11,000 galaxies. The Atlas includes detailed source lists and clear representations of diffuse infrared emission regions. It also contains a wealth of information relating to the physical characteristics of galaxies, such as dust temperature and luminosity. The Atlas is a vast resource for future researchers, enabling more insight into the far-infrared properties of galaxies and the processes that shape them. 1 into PostgreSQL...\n",
      "Inserting test sample 2355  We report the results of a spectroscopic search for Lyman-alpha emission from gamma-ray burst host galaxies. Based on the well-defined TOUGH sample of 69 X-ray selected Swift GRBs, we have targeted the hosts of a subsample of 20 GRBs known from afterglow spectroscopy to be in the redshift range 1.8-4.5. We detect Lya emission from 7 out of the 20 hosts, with the typical limiting 3sigma line flux being 8E-18 erg/cm2/s, corresponding to a Lya luminosity of 6E41 erg/s at z=3. The Lya luminosities for the 7 hosts in which we detect Lya emission are in the range (0.6-2.3)E42 erg/s corresponding to star-formation rates of 0.6-2.1 Msun/yr (not corrected for extinction). The rest-frame Lya equivalent widths (EWs) for the 7 hosts are in the range 9-40A. For 6 of the 13 hosts for which Lya is not detected we place fairly strong 3sigma upper limits on the EW (<20A), while for others the EW is either unconstrained or has a less constraining upper limit. We find that the distribution of Lya EWs is inconsistent with being drawn from the Lya EW distribution of bright Lyman break galaxies at the 98.3% level, in the sense that the TOUGH hosts on average have larger EWs than bright LBGs. We can exclude an early indication, based on a smaller, heterogeneous sample of pre-Swift GRB hosts, that all GRB hosts are Lya emitters. We find that the TOUGH hosts on average have lower EWs than the pre-Swift GRB hosts, but the two samples are only inconsistent at the 92% level. The velocity centroid of the Lya line is redshifted by 200-700 km/s with respect to the systemic velocity, similar to what is seen for LBGs, possibly indicating star-formation driven outflows from the host galaxies. There seems to be a trend between the Lya EW and the optical to X-ray spectral index of the afterglow (beta_OX), hinting that dust plays a role in the observed strength and even presence of Lya emission. [ABRIDGED] 0 into PostgreSQL...\n",
      "Inserting test sample 2356  The Optically Unbiased GRB Host (TOUGH) survey is a comprehensive study of Gamma Ray Burst (GRB) host galaxies. In this paper, we present the fourth installment of the TOUGH survey, which focuses on Lyman-alpha emitters (LAEs). We use deep imaging with the Hyper Suprime-Cam on the Subaru Telescope to search for LAEs in the fields of 16 GRBs at redshifts ranging from 2.3 to 4.6. For comparison, we also observe blank fields in the same redshift range. \n",
      "\n",
      "We identify 22 LAEs associated with 12 GRBs. The Lyman-alpha luminosities of these galaxies range from 5 x 10^42 to 4 x 10^43 erg/s, placing them in the so-called \"bright LAE\" category. We find that the LAEs associated with GRBs tend to be located in the outskirts of their host galaxies, consistent with previous studies. \n",
      "\n",
      "We also compare the number density of LAEs in the fields of GRBs to that of blank fields. We find that the LAE overdensity is higher in the GRB fields, confirming that GRBs are preferentially associated with star-forming regions. However, the overdensity is not uniform across all GRBs, suggesting that the GRB environment is not a homogeneous population. \n",
      "\n",
      "Finally, we investigate the relationship between LAEs and dark matter halos. We stack the LAE images at the positions of the GRBs and measure the projected dark matter density profiles. We find that the LAEs tend to reside in halos with mass Mhalo ~ 10^11 Msun, consistent with previous studies of LAEs in blank fields. \n",
      "\n",
      "Our results provide valuable insights into the properties of LAEs associated with GRBs and their host galaxies. The TOUGH survey continues to be a powerful tool for probing the galaxy populations at high redshifts and their relationship with GRBs. 1 into PostgreSQL...\n",
      "Inserting test sample 2357  This paper considers a time-inconsistent stopping problem in which the inconsistency arises from non-constant time preference rates. We show that the smooth pasting principle, the main approach that has been used to construct explicit solutions for conventional time-consistent optimal stopping problems, may fail under time-inconsistency. Specifically, we prove that the smooth pasting principle solves a time-inconsistent problem within the intra-personal game theoretic framework if and only if a certain inequality on the model primitives is satisfied. We show that the violation of this inequality can happen even for very simple non-exponential discount functions. Moreover, we demonstrate that the stopping problem does not admit any intra-personal equilibrium whenever the smooth pasting principle fails. The \"negative\" results in this paper caution blindly extending the classical approaches for time-consistent stopping problems to their time-inconsistent counterparts. 0 into PostgreSQL...\n",
      "Inserting test sample 2358  This paper investigates the failure of the smooth pasting principle and the nonexistence of equilibrium stopping rules under time-inconsistency. We study a general discrete-time model where agents face time-inconsistency problems, and the underlying asset prices are subject to jumps. We derive the necessary and sufficient conditions for the existence of continuous-time smooth pasting solutions for the problem of an agent maximizing the expected discounted utility of his/her wealth. We further prove that, in the absence of continuous-time smooth pasting solutions, the equilibrium stopping rule may not exist. Simulations show that our results have significant implications for the pricing and hedging of derivatives, and can improve our understanding of the market's reactions to central bank announcements. Our findings deepen the understanding of the theory in this area and provide new insights into the behavior of agents in time-inconsistent models with jumps. 1 into PostgreSQL...\n",
      "Inserting test sample 2359  The heaviness of the glueball mass scale is suggested as the source of the OZI rule at low energy. The $J/\\psi \\to \\rho\\pi$ decay \"anomaly\" implies the vector glueball $O$ has mass $m_O \\approx m_{J/\\psi}$. Such a heavy mass is supported by other glueball studies. Glueball-meson matrix elements turn out to be not suppressed at all at the 1 GeV scale, and a simple and intuitive picture emerges which is consistent with the Gell-Mann-Okubo mass formula as well as the measured sign of $\\phi$-$\\omega$ mixing. The suppression of glueball mediated $\\bar q_iq_i \\longleftrightarrow \\bar q_jq_j$ transitions and the cancellation mechanism in two-step meson rescatterings are viewed as related by duality. Extensions to the $2^{++}$, $3^{--}$ meson sectors, and failure for $0^{\\pm +}$ mesons are also discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 2360  The OZI rule, which constrains certain decay processes in strong interactions, is believed to be rooted in the massiveness of glueballs â€“ hypothetical particles composed entirely of gluons. In this paper, we explore this connection in depth, employing a theoretical framework based on QCD sum rules. We calculate the masses of the lightest scalar and tensor glueballs and investigate their properties. Our results support the notion that the OZI rule arises from the large mass of these particles, which suppresses the decay of resonances containing strange quarks. We also examine the dependence of the glueball masses on the strange quark mass. We find that the predicted mass spectrum is consistent with experimental data, lending further support to our hypothesis. 1 into PostgreSQL...\n",
      "Inserting test sample 2361  We study the effects of the beaming-induced luminosity function on statistics of observed GRBs, assuming the cosmological scenario. We select and divide the BATSE 4B data into 588 long bursts (T$_{90}>2.5$ sec) and 149 short bursts (T$_{90}<2.5$ sec), and compare the statistics calculated in each subgroup. The $<V/V_{\\rm max}>$ of the long bursts is $ 0.2901\\pm 0.0113$, and that of the short bursts is $0.4178\\pm 0.0239$, which is a Euclidean value. For luminosity function models, we consider a cylindrical-beam and a conic-beam. We take into account the spatial distribution of GRB sources as well. A broad luminosity function is naturally produced when one introduces beaming of GRBs. We calculate the maximum detectable redshift of GRBs, $z_{\\rm max}$. The estimated $z_{\\rm max}$ for the cylindrical-beam case is as high as $\\sim 14$ for the long bursts and $\\sim 3$ for the short bursts. The large $z_{\\rm max}$ value for the short bursts is rather surprising in that the $<V/V_{\\rm max}>$ for this subgroup is close to the so-called Euclidean value, 0.5. We calculate the fraction of bursts whose redshifts are larger than a certain redshift $z'$, i.e. $f_{\\rm > z'}$. When we take $z'=3.42$ and apply the luminosity function derived for the cylindrical-beam, the expected $f_{\\rm > z'}$ is $\\sim 75 %$ for long bursts. When we increase the opening angle of the conic beam to $\\Delta \\theta =3^\\circ.0$, $f_{\\rm > z'}$ decreases to $\\sim 20 %$ at $ {\\rm z'=3.42}$. We conclude that the beaming-induced luminosity functions are compatible with the redshift distribution of observed GRBs and that the apparent Euclidean value of $<V/V_{\\rm max}>$ may not be due to the Euclidean space distribution but to the luminosity distribution. 0 into PostgreSQL...\n",
      "Inserting test sample 2362  Gamma-ray bursts (GRBs) are sudden and intense bursts of gamma-ray radiation observed in the cosmos, and they are believed to occur as a result of various astrophysical phenomena such as supernovae, neutron star mergers, and other energetic events. The statistics of GRBs provide important insights into the origins and evolution of these events, with luminosity being a key parameter. Here we investigate the effects of luminosity functions induced by relativistic beaming on the statistics of cosmological GRBs.\n",
      "\n",
      "Relativistic beaming occurs when an emitting source moves at a velocity close to the speed of light, causing the radiation to be preferentially emitted in the direction of motion. This causes the observed luminosity to depend on the orientation of the observer relative to the emitting source, leading to a non-uniform luminosity function. We use numerical simulations to investigate the effects of this beaming on the observed statistics of GRBs.\n",
      "\n",
      "Our simulations show that relativistic beaming can significantly affect the observed luminosity distribution of cosmological GRBs, leading to a bimodal distribution where many low-luminosity and a few high-luminosity bursts are observed. We also find that the observed duration distribution is affected by beaming, with longer-duration GRBs being preferentially observed from sources with lower intrinsic luminosities.\n",
      "\n",
      "Our results have important implications for understanding the physical mechanisms underlying GRBs and their cosmological evolution. They suggest that beaming should be taken into account when interpreting observational data, and provide a potential explanation for the observed bimodal luminosity function. Further studies are needed to fully characterize the effects of beaming on the statistics of GRBs, and to determine its role in shaping the observed properties of these enigmatic astrophysical events. 1 into PostgreSQL...\n",
      "Inserting test sample 2363  There are two major routes to address the ubiquitous family of inverse problems appearing in signal and image processing, such as denoising or deblurring. A first route relies on Bayesian modeling, where prior probabilities are used to embody models of both the distribution of the unknown variables and their statistical dependence with the observed data. The estimation process typically relies on the minimization of an expected loss (e.g. minimum mean squared error, or MMSE). The second route has received much attention in the context of sparse regularization and compressive sensing: it consists in designing (often convex) optimization problems involving the sum of a data fidelity term and a penalty term promoting certain types of unknowns (e.g., sparsity, promoted through an 1 norm). Well known relations between these two approaches have lead to some widely spread misconceptions. In particular, while the so-called Maximum A Posterori (MAP) estimate with a Gaussian noise model does lead to an optimization problem with a quadratic data-fidelity term, we disprove through explicit examples the common belief that the converse would be true. It has already been shown [7, 9] that for denoising in the presence of additive Gaussian noise, for any prior probability on the unknowns, MMSE estimation can be expressed as a penalized least squares problem, with the apparent characteristics of a MAP estimation problem with Gaussian noise and a (generally) different prior on the unknowns. In other words, the variational approach is rich enough to build all possible MMSE estimators associated to additive Gaussian noise via a well chosen penalty. We generalize these results beyond Gaussian denoising and characterize noise models for which the same phenomenon occurs. In particular, we prove that with (a variant of) Poisson noise and any prior probability on the unknowns, MMSE estimation can again be expressed as the solution of a penalized least squares optimization problem. For additive scalar denois-ing the phenomenon holds if and only if the noise distribution is log-concave. In particular, Laplacian denoising can (perhaps surprisingly) be expressed as the solution of a penalized least squares problem. In the multivariate case, the same phenomenon occurs when the noise model belongs to a particular subset of the exponential family. For multivariate additive denoising, the phenomenon holds if and only if the noise is white and Gaussian. 0 into PostgreSQL...\n",
      "Inserting test sample 2364  Bayesian estimation is a powerful technique for estimating the parameters of statistical models. It has proven to be particularly useful in settings where the amount of available data is limited or where there are strong prior beliefs about the unknown parameters. In this paper, we explore the relationship between Bayesian estimation and proximity operators.\n",
      "\n",
      "Proximity operators are a type of nonlinear operator that can be used to solve a variety of optimization problems. These operators have been shown to be closely related to Bayesian estimation, particularly in the context of regularization. In fact, one can view many popular regularization techniques as using proximity operators to enforce constraints on the estimated parameters. \n",
      "\n",
      "In this paper, we highlight the connections between Bayesian estimation and proximity operators by examining several specific examples. We show that Bayesian estimation can be viewed as a type of proximal mapping, where the prior distribution is used as a regularizer. This approach allows us to impose structure on the estimated parameters in a way that is consistent with our prior beliefs.\n",
      "\n",
      "We also investigate the role of proximity operators in the context of empirical Bayes methods. Empirical Bayes is a technique for estimating hyperparameters in hierarchical models, where parameters are estimated in a Bayesian framework. We show that proximity operators can be used to derive closed-form expressions for the hyperparameters in many cases, which can be more efficient than traditional methods such as maximum likelihood estimation.\n",
      "\n",
      "Finally, we examine the use of proximity operators in nonparametric Bayesian models. Nonparametric models are useful in situations where the underlying distribution is unknown or where the data has a complex structure. We show that proximity operators can be used to develop efficient algorithms for inference in these models, allowing us to make accurate predictions and make informed decisions.\n",
      "\n",
      "Overall, our analysis highlights the use of proximity operators in Bayesian estimation and illustrates their usefulness in a variety of settings. We believe that our results will be of interest to researchers in statistics, machine learning, and related fields who are interested in developing efficient and accurate inference methods. 1 into PostgreSQL...\n",
      "Inserting test sample 2365  Electric Vehicles (EV) impact urban networks both when driving (e.g., noise and pollution reduction) and charging. For the electrical grid, the flexibility of EV charging makes it a significant actor in \"Demand Response\" mechanisms.\n",
      "\n",
      "Therefore, there is a need to design incentive mechanisms to foster customer engagement. A congestion game approach is adopted to evaluate the performance of such electrical transportation system with multiple classes of vehicles: EV and Gasoline Vehicles. Both temporal and energy operating costs are considered.\n",
      "\n",
      "The latter is nonseparable as it depends on the global charging need of all EV, which is scheduled in time by a centralized aggregator in function of nonflexible consumption at charging location. Thus, driving and charging decisions are coupled. An adaptation of Beckmann's method proves the existence of a Wardrop Equilibrium (WE) in the considered nonseparable congestion game; this WE is unique when the charging unit price is an increasing function of the global charging need. A condition on the nonflexible load is given to guarantee the monotonicity of this function. This condition is tested on real consumption data in France and in Texas, USA. Optimal tolls are used to control this electrical transportation system and then computed in order to minimize an environmental cost on a simple network topology. 0 into PostgreSQL...\n",
      "Inserting test sample 2366  The adoption of electric vehicles in urban areas is a crucial step towards reducing transportation-related carbon emissions. However, electric vehicle penetration in cities is impeded by a number of barriers, including range anxiety and the lack of infrastructure. Incentive-based mechanisms have been proposed as a means to increase the number of electric vehicles in cities. Traditional incentive designs rely on monetary rewards for driving electric vehicles but neglect the impact of charging infrastructure availability on electric vehicle adoption. This paper proposes a new approach to incentive design that couples charging and driving incentives to enhance the overall effectiveness of incentives for electric vehicle adoption in urban networks. The proposed approach considers both the availability of charging infrastructure and the payoff of driving electric vehicles, which are jointly optimized to encourage electric vehicle deployment. Through a case study in a city network, the proposed design demonstrated significant success in increasing the number of electric vehicles on the road while considering the charging limitations and driving patterns of individuals. The results suggest that the new design approach could help cities reduce carbon emissions and promote sustainable transport. 1 into PostgreSQL...\n",
      "Inserting test sample 2367  In the current study, mechanical characteristics of graphene oxide (GO) as a promising substitute of graphene are systematically studied through molecular dynamics simulation. For this purpose, several GO samples having different concentrations of epoxide and hydroxyl functional groups are considered. The results reveal that increasing the epoxide coverage causes a noticeable deterioration in the mechanical characteristics of GO systems. This change is correlated with the increase of the formation of ripples in the structure upon increasing the epoxide coverage. Moreover, investigating the bond lengths in the system, it is concluded that the higher epoxide percentage leads to an increase in the length of single and hybrid resonance bonds leading to an overall deterioration of the mechanical properties of GO samples. Additionally, our results demonstrate that high concentration of functional groups can lead to a negative Poisson ratio. Increasing the amount of hydroxyl groups shows the same declining effect on the Young modulus. In a graphene system containing both epoxide and hydroxyl groups, it is deduced that a higher percentage of the former can result in a higher residual strain because of the formation of more ripples within the system. 0 into PostgreSQL...\n",
      "Inserting test sample 2368  Graphene oxide (GO) has garnered significant attention due to its exceptional mechanical properties, such as high strength and stiffness. Recent studies have shown that the presence of functional groups on the GO surface significantly impacts its mechanical behavior. The diverse range of functionalization techniques used to modify graphene oxide include oxidation, reduction, and chemical modification. These techniques affect the nature and density of functional groups present on the GO surface, resulting in changes in its mechanical properties. This review provides an overview of recent progress in the field of GO functionalization and its impact on mechanical properties. The strength, stiffness, and fracture toughness of GO are heavily reliant on the degree and nature of functionalization. Additionally, the influence of functional group placement on the fracture patterns and failure mechanisms is discussed in detail. The effect of functionalization on the interfacial properties of GO-based composite materials is also highlighted. This review underscores the importance of understanding how functional groups affect GO mechanical properties in order to design and engineer advanced materials and devices. 1 into PostgreSQL...\n",
      "Inserting test sample 2369  A model of natural inflation with an effectively trans-Planckian decay constant can be easily achieved by the \"phase locking\" mechanism while keeping field values in the effective field theory within the Planck scale. We give detailed description of \"phase locked\" inflation based on this mechanism. We also construct supersymmetric natural inflation based on this mechanism and show that the model is consistent with low scale supersymmetry. We also investigate couplings of the inflaton with the minimal supersymmetric standard model to achieve an appropriate reheating process. Interestingly, in a certain class of models, we find that the inflation scale is related to the mass of the right-handed neutrino in a consistent way with the seesaw mechanism. 0 into PostgreSQL...\n",
      "Inserting test sample 2370  Inflationary cosmology provides a plausible explanation of many of the observed features of the universe, such as its isotropy and flatness, and the origin of cosmic microwave background fluctuations. Here we investigate the dynamics of â€œphase-lockedâ€ inflation, in which the inflaton field oscillates coherently along a complex path. We show that this scenario can lead to naturally flat potentials on super-Planckian scales, and it can hence provide a novel implementation of large-field inflaton models. We also show that, in simple models, phase-locked inflation can be consistent with current cosmological observations and avoids the need for fine-tuning of parameters. Therefore, it represents an interesting alternative to standard natural inflation. 1 into PostgreSQL...\n",
      "Inserting test sample 2371  We show that a useful connection exists between spontaneous parametric downconversion (SPDC) and sum frequency generation in nonlinear optical waveguides with arbitrary scattering loss, while the same does not hold true for SPDC and difference frequency generation. This result deepens the relationship between quantum and classical second-order nonlinear optical processes in waveguides, and identifies the most accurate characterization of their quantum performance in the presence of loss based solely on classical measurements. 0 into PostgreSQL...\n",
      "Inserting test sample 2372  In this study, we investigate the impact of scattering losses on the interface between classical and quantum processes in second-order nonlinear waveguides. We employ a nonperturbative approach which allows for the rigorous treatment of the waveguide behavior and the development of a quantitative description of the underlying physics. By analyzing our results, we find that scattering losses can significantly affect the interface between classical and quantum processes, particularly in the regime of low powers. 1 into PostgreSQL...\n",
      "Inserting test sample 2373  We study a simple model for the metallic stripes found in $La_{1.6-x}Nd_{0.4}Sr_xCuO_4$: two chain Hubbard ladder embedded in a static antiferromagnetic environments. We consider two cases: a ``topological stripe'', for which the phase of the Neel order parameter shifts by $\\pi$ across the ladder, and a ``non-topological stripe'', for which there is no phase shift across the ladder. We perform one-loop renormalization group calculations to determine the low energy properties. We compare the results with those of the isolated ladder and show that for small doping superconductivity is enhanced in the topological stripe, and suppressed in the non-topological one. In the topological stripe, the superconducting order parameter is a mixture of a spin singlet component with zero momentum and a spin triplet component with momentum $\\pi$. We argue that this mixture is generic, and is due to the presence of a new term in the quantum Ginzburg-Landau action. Some consequences of this mixing are discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 2374  The discovery of high-temperature superconductivity in metallic stripes embedded in antiferromagnetic materials has become a subject of intense research. This paper reports on a detailed investigation of the fundamental properties of such a system using a combination of experimental techniques and theoretical modeling. Specifically, we focus on the dependence of the superconductivity on the geometrical and magnetic properties of the stripe, as well as on the strength of the superconducting pairing interactions. Our results reveal a complex interplay between the magnetic order of the antiferromagnet and the formation of superconducting states in the stripe, which is reflected in a rich phase diagram as a function of temperature, magnetic field, and doping. Overall, these findings provide important insights into the underlying mechanisms of high-temperature superconductivity in complex materials and have direct implications for the design of novel electronic devices based on this phenomenon. 1 into PostgreSQL...\n",
      "Inserting test sample 2375  Unbiased CLTR requires click propensities to compensate for the difference between user clicks and true relevance of search results via IPS. Current propensity estimation methods assume that user click behavior follows the PBM and estimate click propensities based on this assumption. However, in reality, user clicks often follow the CM, where users scan search results from top to bottom and where each next click depends on the previous one. In this cascade scenario, PBM-based estimates of propensities are not accurate, which, in turn, hurts CLTR performance. In this paper, we propose a propensity estimation method for the cascade scenario, called CM-IPS. We show that CM-IPS keeps CLTR performance close to the full-information performance in case the user clicks follow the CM, while PBM-based CLTR has a significant gap towards the full-information. The opposite is true if the user clicks follow PBM instead of the CM. Finally, we suggest a way to select between CM- and PBM-based propensity estimation methods based on historical user clicks. 0 into PostgreSQL...\n",
      "Inserting test sample 2376  Counterfactual learning to rank is an important problem in the field of information retrieval. In this paper, we propose a novel approach to estimating propensity scores for counterfactual learning to rank based on the cascade model. Specifically, we first generate a set of candidate documents using the cascade model, which captures the relevance and attractiveness of documents to users. We then estimate the propensity score for each candidate document based on the cascade model and use it to generate counterfactual outcomes. Our experiments on a benchmark dataset demonstrate that the proposed approach outperforms several state-of-the-art methods in terms of both effectiveness and efficiency. Moreover, we show that the cascade model can be used to guide the exploration of the document space, leading to further improvements in learning to rank performance. These promising results suggest that our approach has potential for practical applications in the field of information retrieval. 1 into PostgreSQL...\n",
      "Inserting test sample 2377  The existence of the QCD critical point at non-zero baryon density is not only of great interest for experimental physics but also a challenge for the theory. Any hint of the existence of the first order phase transition and, particularly, its critical point will be valuable towards a full understanding of the QCD phase diagram. We use lattice simulation based on the canonical ensemble method to explore the finite baryon density and finite temperature region and look for the QCD critical point. As a benchmark, we run simulations for the four degenerate flavor QCD where we observe a clear signal of the expected first order phase transition. In the two flavor case, we do not see any signal for temperatures as low as $0.83 \\rm{T_c}$. Although our real world contains two light quarks and one heavier quark, three degenerate flavor case shares a lot of similar phase structures as the QCD. We scan the phase diagram using clover fermions with $m_\\pi \\approx 700{MeV}$ on $6^3\\times4$ lattices.\n",
      "\n",
      "The baryon chemical potential is measured as we increase the baryon number and we see the characteristic \"S-shape\" that signals the first order phase transition. We determine the phase boundaries by Maxwell construction and report our preliminary results for the location of critical point for the present lattice. 0 into PostgreSQL...\n",
      "Inserting test sample 2378  The search for the critical point of quantum chromodynamics (QCD) continues to be an active area of research in high-energy physics. In this study, we investigate the QCD critical point through the use of the canonical ensemble method. Specifically, we analyze the behavior of the fluctuation of conserved charges such as baryon number and electric charge near the critical point using the canonical partition function. We also examine the critical behavior of the system by exploring the scaling properties of various observables in the vicinity of the critical point. Our research highlights the importance of studying the QCD critical point as it provides valuable insight into the properties of strongly interacting matter under extreme conditions. The canonical ensemble method allows for a detailed exploration of the critical behavior of the QCD system, ultimately leading to a better understanding of the nature of the phase transition. These findings contribute to the ongoing pursuit of a fundamental understanding of the complex interactions among fundamental particles, offering the potential to improve our understanding of the early universe and the behavior of matter in extreme astrophysical environments. 1 into PostgreSQL...\n",
      "Inserting test sample 2379  Ordinarily, quiver varieties are constructed as moduli spaces of quiver representations in the category of vector spaces. It is also natural to consider quiver representations in a richer category, namely that of vector bundles on some complex variety equipped with a fixed sheaf that twists the morphisms. Representations of A-type quivers in this twisted category --- known in the literature as \"holomorphic chains\" --- have practical use in questions concerning the topology of the moduli space of Higgs bundles. In that problem, the variety is a Riemann surface of genus at least 2, and the twist is its canonical line bundle. We extend the treatment of twisted A-type quiver representations to any genus using the Hitchin stability condition induced by Higgs bundles and computing their deformation theory. We then focus in particular on so-called \"argyle quivers\", where the rank labelling alternates between 1 and integers $r_i\\geq1$. We give explicit geometric identifications of moduli spaces of twisted representations of argyle quivers on $\\mathbb{P}^1$ using invariant theory for a non-reductive action via Euclidean reduction on polynomials. This leads to a stratification of the moduli space by change of bundle type, which we identify with \"collision manifolds\" of invariant zeroes of polynomials. We also relate the present work to Bradlow-Daskalopoulos stability and Thaddeus' pullback maps to stable tuples. We apply our results to computing $\\mathbb{Q}$-Betti numbers of low-rank twisted Higgs bundle moduli spaces on $\\mathbb{P}^1$, where the Higgs fields take values in an arbitrary ample line bundle. Our results agree with conjectural Poincar\\'e series arising from the ADHM recursion formula. 0 into PostgreSQL...\n",
      "Inserting test sample 2380  In this paper, we study the relationship between twisted argyle quivers and Higgs bundles. Argyle quivers were introduced as a combinatorial language to describe quiver representations. Twisted argyle quivers, which arise from twisting the edges of quivers, provide a natural generalization of argyle quivers. They have been shown to encode a rich combinatorial structure and have found applications in various areas of mathematics, including knot theory and algebraic geometry. \n",
      "\n",
      "Higgs bundles, on the other hand, are vector bundles equipped with a Higgs field, which is a section of the endomorphism bundle of the vector bundle. They have been studied extensively in the context of algebraic geometry and mathematical physics, particularly in the study of moduli spaces of vector bundles. \n",
      "\n",
      "We establish a correspondence between twisted argyle quivers and certain Higgs bundles on an algebraic curve. Our main result is a characterization of the moduli space of stable Higgs bundles of a fixed rank and degree in terms of twisted argyle quivers. We show that this correspondence is compatible with the natural action of the braid group on twisted argyle quivers, and that it can be used to compute certain topological invariants of Higgs bundles. \n",
      "\n",
      "As an application of our results, we establish a new connection between Higgs bundles and the moduli space of flat connections on the complement of a link in the three-sphere. This leads to a new characterization of the topological type of certain flat connections in terms of a twisted argyle quiver associated to the link. We also discuss several directions for future research, including the study of higher rank Higgs bundles and the generalization of our results to higher-dimensional algebraic varieties. 1 into PostgreSQL...\n",
      "Inserting test sample 2381  We solve the equations of motion and find the Lorentz transformation associated with a kink in superconducting cosmic string. The kink velocity does not depend on its amplitude. The kink amplitude cannot be arbitrary but it varies within definite range and determines the explicit form of the relevant Lorentz transformation. 0 into PostgreSQL...\n",
      "Inserting test sample 2382  We present an exact solution for a kink configuration on a superconducting cosmic string, which involves a change in the field configuration. Our results show that the kink can be created and sustained with appropriate superconducting properties, with potential implications for the observation of cosmic strings. 1 into PostgreSQL...\n",
      "Inserting test sample 2383  Barium stars show enhanced abundances of the slow neutron capture (s-process) heavy elements, and for this reason they are suitable objects for the study of s-process elements. The aim of this work is to quantify the contributions of the s-, r- and p-processes for the total abundance of heavy elements from abundances derived for a sample of 26 barium stars. The abundance ratios between these processes and neutron exposures were studied. The abundances of the sample stars were compared to those of normal stars thus identifying the fraction relative to the s-process main component. The fittings of the sigmaN curves (neutron capture cross section times abundance, plotted against atomic mass number) for the sample stars suggest that the material from the companion asymptotic giant branch star had approximately the solar isotopic composition as concerns fractions of abundances relative to the s-process main component.\n",
      "\n",
      "The abundance ratios of heavy elements, hs, ls and s and the computed neutron exposure are similar to those of post-AGB stars. For some sample stars, an exponential neutron exposure fits well the observed data, whereas for others, a single neutron exposure provides a better fit. The comparison between barium and AGB stars supports the hypothesis of binarity for the barium star formation. Abundances of r-elements that are part of the s-process path in barium stars are usually higher than those in normal stars,and for this reason, barium stars seemed to be also enriched in r-elements, although in a lower degree than s-elements. No dependence on luminosity classes was found in the abundance ratios behaviour among the dwarfs and giants of the sample barium stars. 0 into PostgreSQL...\n",
      "Inserting test sample 2384  Barium stars are known to have an overabundance of neutron-capture elements, which are thought to be produced by various neutron capture mechanisms. In this study, we analyze the contributions of the s-, r-, and p-processes in the production of heavy elements in 26 barium stars. Using high-resolution spectroscopy and stellar atmosphere models, we derived the abundances of various elements and determined the contributions of each neutron capture process. Our results show that the s-process is the dominant contributor to the abundance of barium and other heavy elements in these stars. The r-process also contributes significantly to the production of heavy elements, although its contribution is generally lower than that of the s-process. Additionally, we found that the p-process has a negligible contribution to the production of heavy elements in barium stars. \n",
      "\n",
      "Our findings provide important insights into the nucleosynthesis processes that occur in these stars. By understanding the relative contributions of each neutron capture process, we can gain a better understanding of the conditions under which these elements are produced. This knowledge can also aid in the interpretation of observational data from these stars and the determination of their ages and evolutionary histories.\n",
      "\n",
      "In conclusion, our analysis of 26 barium stars highlights the important role of the s-, r-, and p-processes in the production of heavy elements. Our findings emphasize the need for further study of these processes in order to fully understand the nucleosynthesis of heavy elements in stars. 1 into PostgreSQL...\n",
      "Inserting test sample 2385  Coronal mass ejections (CMEs) are large-scale explosions of the coronal magnetic field. It is believed that magnetic reconnection significantly builds up the core structure of CMEs, a magnetic flux rope, during the eruption.\n",
      "\n",
      "However, the quantitative evolution of the flux rope, particularly its toroidal flux, is still unclear. In this paper, we study the evolution of the toroidal flux of the CME flux rope for four events. The toroidal flux is estimated as the magnetic flux in the footpoint region of the flux rope, which is identified by a method that simultaneously takes the coronal dimming and the hook of the flare ribbon into account. We find that the toroidal flux of the CME flux rope for all four events shows a two-phase evolution: a rapid increasing phase followed by a decreasing phase. We further compare the evolution of the toroidal flux with that of the Geostationary Operational Environmental Satellites soft X-ray flux and find that they are basically synchronous in time, except that the peak of the former is somewhat delayed. The results suggest that the toroidal flux of the CME flux rope may be first quickly built up by the reconnection mainly taking place in the sheared overlying field and then reduced by the reconnection among the twisted field lines within the flux rope, as enlightened by a recent 3D magnetohydrodynamic simulation of CMEs. 0 into PostgreSQL...\n",
      "Inserting test sample 2386  Coronal mass ejections (CMEs) are one of the most powerful events in the solar system. They are characterized by a twisted configuration of magnetic fields, known as a flux rope. The evolution of toroidal flux in these structures is essential in understanding the dynamics of CMEs. In this study, we analyze the toroidal flux of CME flux ropes during eruption using a numerical model. We found that the toroidal flux in the flux ropes increases significantly before the eruption and then decreases rapidly after the CME is triggered. The eruption is caused by the reconfiguration of the magnetic fields, which leads to the release of energy stored in the flux ropes. Our results provide new insights into the pre-eruption dynamics and the driving mechanism of CMEs. We also compare our results with observational data and find that our model is consistent with the observed behavior of CMEs. Finally, we discuss the implications of our findings for space weather forecasting and the mitigation of the adverse effects of CMEs on technological systems. Our study highlights the importance of understanding the evolution of toroidal flux in CME flux ropes for improving our ability to predict and mitigate the effects of space weather events. 1 into PostgreSQL...\n",
      "Inserting test sample 2387  Earbuds are subjected to constant use and scenarios that may degrade sound quality. Indeed, a common fate of earbuds is being forgotten in pockets and faced with a laundry cycle (LC). Manufacturers' accounts of the extent to which LCs affect earbud sound quality are vague at best, leaving users to their own devices in assessing the damage caused. This paper offers a systematic, empirical approach to measure the effects of laundering earbuds on sound quality. Three earbud pairs were subjected to LCs spaced 24 hours apart. After each LC, a professional microphone as well as a mid-market smartphone were used to record i) a test tone ii) a frequency sweep and iii) a music signal played through the earbuds. We deployed mixed effects models and found significant degradation in terms of RMS noise loudness, Total Harmonic Distortion (THD), as well as measures of change in the frequency responses of the earbuds. All transducers showed degradation already after the first cycle, and no transducers produced a measurable signal after the sixth LC. The degradation effects were detectable in both, the professional microphone as well as the smartphone recordings. We hope that the present work is a first step in establishing a practical, and ecologically valid method for everyday users to assess the degree of degradation of their personal earbuds. 0 into PostgreSQL...\n",
      "Inserting test sample 2388  This study investigates the effects of water immersion on the audio quality of earbuds. The experiment was conducted by submerging earbuds in water for varying lengths of time then evaluating their sound quality using subjective and objective methods. Results showed that there was a significant decrease in audio quality after exposure to water. Specifically, audio played through submerged earbuds had lower fidelity and experienced distortion. It was also observed that immersion time had a profound effect on audio degradation. Longer immersion times resulted in greater audio quality loss. Microscopic examination of the earbuds revealed that water infiltrated internal components, leading to corrosion and damage to the microspeakers. This study provides a better understanding of how water affects earbud audio quality and suggests possible ways to mitigate the degradation effects. It also has important implications for design and engineering of earbuds for water-resistant or waterproof use. The results of this study could have significant impacts on the audio industry and consumers, particularly those who use earbuds during water sports or other activities involving water immersion. 1 into PostgreSQL...\n",
      "Inserting test sample 2389  The liquid argon time projection chamber (LArTPC) detector technology has an excellent capability to measure properties of low-energy neutrinos produced by the sun and supernovae and to look for exotic physics at very low energies. In order to achieve those physics goals, it is crucial to identify and reconstruct signals in the waveforms recorded on each TPC wire. In this paper, we report on a novel algorithm based on a one-dimensional convolutional neural network (CNN) to look for the region-of-interest (ROI) in raw waveforms. We test this algorithm using data from the ArgoNeuT experiment in conjunction with an improved noise mitigation procedure and a more realistic data-driven noise model for simulated events. This deep-learning ROI finder shows promising performance in extracting small signals and gives an efficiency approximately twice that of the traditional algorithm in the low energy region of $\\sim$0.03-0.1 MeV. This method offers great potential to explore low-energy physics using LArTPCs. 0 into PostgreSQL...\n",
      "Inserting test sample 2390  The Liquid Argon Time Projection Chamber (LArTPC) is an essential component of modern neutrino experiments, as it allows for three-dimensional imaging of neutrino interactions. However, its large data volume presents challenges for data acquisition and processing. In this paper, we present a deep-learning based raw waveform region-of-interest (ROI) finder for the LArTPC, which significantly reduces data volume while preserving important features of the raw waveform. Specifically, our ROI finder uses a convolutional neural network (CNN) to identify regions of the waveform most likely to contain ionization signals from neutrino interactions. The CNN is trained on simulated waveforms and then applied to real data, achieving high accuracy in identifying ROIs. We also explore the impact of network depth and training parameters on ROI finder performance. Our results demonstrate the effectiveness of deep learning approaches in tackling challenges in neutrino experiment data processing. 1 into PostgreSQL...\n",
      "Inserting test sample 2391  We present an algorithm for the classification of triples of lattice polytopes with a given mixed volume $m$ in dimension 3. It is known that the classification can be reduced to the enumeration of so-called irreducible triples, the number of which is finite for fixed $m$. Following this algorithm, we enumerate all irreducible triples of normalized mixed volume up to 4 that are inclusion-maximal. This produces a classification of generic trivariate sparse polynomial systems with up to 4 solutions in the complex torus, up to monomial changes of variables. By a recent result of Esterov, this leads to a description of all generic trivariate sparse polynomial systems that are solvable by radicals. 0 into PostgreSQL...\n",
      "Inserting test sample 2392  In this paper, we focus on the classification of triples of lattice polytopes with a given mixed volume. Given a triple of lattice polytopes with fixed mixed volume, we investigate the property of being \"good\", which is equivalent to having certain numerical invariants. We prove that this property is invariant under a natural group of transformations, and use this to classify all triples of lattice polytopes with fixed mixed volume. We also identify a special class of triples, called \"unimodular\", which have particularly nice properties and are linked to the theory of toric varieties. Our results have applications in algebraic geometry, combinatorics, and mathematical physics. 1 into PostgreSQL...\n",
      "Inserting test sample 2393  We study physics potential of Very Long Base-Line (VLBL) Neutrino-Oscillation Experiments with the High Intensity Proton Accelerator (HIPA), which will be completed by the year 2007 in Tokai-village, Japan, as a joint project of KEK and JAERI (Japan Atomic Energy Research Institute). The HIPA 50 GeV proton beam will deliver neutrino beams of a few GeV range with the intensity about two orders of magnitude higher than the present KEK beam for K2K experiment. As a sequel to the proposed HIPA-to-Super-Kamiokande experiment, we study impacts of experiments with a 100 kton-level detector and the base-line length of a few-thousand km. The pulsed narrow-band nu_mu beams (NBB) allow us to measure the nu_mu to nu_e transition probability and the nu_mu survival probability through counting experiments at large water-Cerenkov detector. We study sensitivity of such experiments to the neutrino mass hierarchy, the mass-squared differences, the three angles, and one CP phase of the three-generation lepton-flavor-mixing matrix. We find that experiments at a distance between 1,000 and 2,000 km can determine the sign of the larger mass-squared difference (m_3^2-m_1^2) if the mixing between nu_e and nu_3 (the heaviest-or-lightest neutrino) is not too small; 2|U_{e3}|^2(1-|U_{e3}|^2) gsim 0.03. The CP phase can be constrained if the |U_{e3}| element is sufficiently large, 2|U_{e3}|^2(1-|U_{e3}|^2) gsim 0.06, and if the smaller mass-squared difference (m_2^2-m_1^2) and the U_{e2} element are in the prefered range of the large-mixing-angle solution of the solar-neutrino deficit. The magunitude | m_3^2-m_1^2| and the matrix element U_{mu 3} can be precisely measured, but we find little sensitivity to m_2^2-m_1^2 and the matrix element U_{e2}. 0 into PostgreSQL...\n",
      "Inserting test sample 2394  The KEK-JAERI High Intensity Proton Accelerator has opened up new possibilities for the study of very long baseline neutrino oscillation experiments. These experiments seek to understand the behavior of neutrinos over long distances, which could shed light on some of the most fundamental questions in physics. The prospects for these experiments are exciting, and the KEK-JAERI accelerator's capabilities may be key to unlocking their potential.\n",
      "\n",
      "The KEK-JAERI accelerator's high intensity proton beams allow for the production of a large number of neutrinos, which can then be studied as they travel over very long distances. This is achieved by observing the neutrinos at two or more distant locations and comparing their properties. Very long baseline neutrino experiments have already yielded important insights into the nature of neutrinos, such as the discovery that they undergo oscillations, or changes in flavor, as they travel.\n",
      "\n",
      "The KEK-JAERI accelerator's unique capabilities could help researchers uncover even more about neutrino oscillations. For example, it may be possible to use the accelerator to produce beams of different types of neutrinos, which could be studied separately and compared. This could help clarify some of the outstanding questions about neutrino behavior, such as their masses and mixing angles.\n",
      "\n",
      "In addition to its potential for advancing our fundamental understanding of physics, very long baseline neutrino experiments with the KEK-JAERI accelerator could also have practical applications. For example, studying neutrinos that have passed through the Earth's mantle could provide insights into the Earth's structure and composition.\n",
      "\n",
      "Overall, the prospects for very long baseline neutrino oscillation experiments with the KEK-JAERI High Intensity Proton Accelerator are promising. The accelerator's unique capabilities could lead to important new insights into neutrino behavior and its connections to fundamental physics. 1 into PostgreSQL...\n",
      "Inserting test sample 2395  Evaluating the degree of partisan districting (Gerrymandering) in a statistical framework typically requires an ensemble of districting plans which are drawn from a prescribed probability distribution that adheres to a realistic and non-partisan criteria. In this article we introduce novel non-reversible Markov chain Monte-Carlo (MCMC) methods for the sampling of such districting plans which have improved mixing properties in comparison to previously used (reversible) MCMC algorithms. In doing so we extend the current framework for construction of non-reversible Markov chains on discrete sampling spaces by considering a generalization of skew detailed balance. We provide a detailed description of the proposed algorithms and evaluate their performance in numerical experiments. 0 into PostgreSQL...\n",
      "Inserting test sample 2396  In political districting, fair representation can be ensured by constructing districting maps that are free from gerrymandering. Markov chain Monte Carlo (MCMC) methods are commonly used for sampling from complex distributions, but they are known to suffer from reversibility which can impact convergence. This paper proposes a non-reversible MCMC algorithm for sampling of districting maps that guarantees faster mixing and better convergence properties. Our experiments on simulated data and real-world redistricting plans demonstrate that the proposed algorithm provides significantly improved sampling efficiency compared to classical MCMC algorithms. We believe that this work will contribute to a more robust and dependable approach to fair political districting. 1 into PostgreSQL...\n",
      "Inserting test sample 2397  Functional principal components (FPC's) provide the most important and most extensively used tool for dimension reduction and inference for functional data. The selection of the number, d, of the FPC's to be used in a specific procedure has attracted a fair amount of attention, and a number of reasonably effective approaches exist. Intuitively, they assume that the functional data can be sufficiently well approximated by a projection onto a finite-dimensional subspace, and the error resulting from such an approximation does not impact the conclusions. This has been shown to be a very effective approach, but it is desirable to understand the behavior of many inferential procedures by considering the projections on subspaces spanned by an increasing number of the FPC's. Such an approach reflects more fully the infinite-dimensional nature of functional data, and allows to derive procedures which are fairly insensitive to the selection of d. This is accomplished by considering limits as d tends to infinity with the sample size.\n",
      "\n",
      "We propose a specific framework in which we let d tend to infinity by deriving a normal approximation for the two-parameter partial sum process of the scores \\xi_{i,j} of the i-th function with respect to the j-th FPC. Our approximation can be used to derive statistics that use segments of observations and segments of the FPC's. We apply our general results to derive two inferential procedures for the mean function: a change-point test and a two-sample test. In addition to the asymptotic theory, the tests are assessed through a small simulation study and a data example. 0 into PostgreSQL...\n",
      "Inserting test sample 2398  Functional data analysis (FDA) is a statistical methodology used to analyze data sets of functions. It is useful for exploring changes in data over time or across continuous variables such as spatial coordinates. One key component of FDA is the decomposition of data into a small number of projection components. Increasing the number of projections can significantly enhance the accuracy of the analysis. In this paper, we propose a novel method for FDA that leverages increasing numbers of projections to improve the analysis.\n",
      "\n",
      "Our approach builds on the established continuous path framework, which allows for modeling complex, non-linear dynamics in the behavior of the functions. Specifically, we extend this framework by introducing new projection matrices that enable us to capture additional information about the data. We explore the performance of this method on simulated and real-world data sets and find that increasing the number of projections can significantly improve accuracy.\n",
      "\n",
      "Furthermore, we demonstrate the usefulness of this approach in a case study of gene expression data. Our results show that our method is able to accurately identify important functional changes in the gene expression data and identify key genetic pathways that are driving these changes.\n",
      "\n",
      "Overall, our approach represents a significant advancement in functional data analysis, enabling researchers to more accurately model and analyze complex data sets. By leveraging increasing numbers of projections, we are able to capture more information about the data and make more accurate predictions about its behavior. 1 into PostgreSQL...\n",
      "Inserting test sample 2399  Time-resolved photoluminescence spectra after nonresonant excitation show a distinct 1s resonance, independent of the existence of bound excitons. A microscopic analysis identifies excitonic and electron-hole plasma contributions. For low temperatures and low densities the excitonic emission is extremely sensitive to even minute optically active exciton populations making it possible to extract a phase diagram for incoherent excitonic populations. 0 into PostgreSQL...\n",
      "Inserting test sample 2400  The interplay between excitons and plasma in semiconductor quantum wells (QWs) is investigated through the study of excitonic photoluminescence. Through experimental data and modeling, it was found that plasma luminescence dominates at high excitation intensities, while excitonic luminescence dominates at low excitation intensities. This study sheds light on the complex mechanisms at play in QWs and opens up possibilities for advanced optoelectronic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 2401  We present a detailed analysis of an eruptive event that occurred on early 2019 March 8 in active region AR 12734, to which we refer as the International Women's day event. The event under study is intriguing in several aspects: 1) low-coronal eruptive signatures come in ''pairs'' (a double-peak flare, two coronal dimmings, and two EUV waves); 2) although the event is characterized by a complete chain of eruptive signatures, the corresponding coronagraphic signatures are weak; 3) although the source region of the eruption is located close to the center of the solar disc and the eruption is thus presumably Earth-directed, heliospheric signatures are very weak with little Earth-impact.\n",
      "\n",
      "We analyze a number of multi-spacecraft and multi-instrument (both remote-sensing and in situ) observations, including Soft X-ray, (extreme-) ultraviolet (E)UV), radio and white-light emission, as well as plasma, magnetic field and particle measurements. We employ 3D NLFF modeling to investigate the coronal magnetic field configuration in and around the active region, the GCS model to make a 3D reconstruction of the CME geometry and the 3D MHD numerical model EUHFORIA to model the background state of the heliosphere. Our results indicate two subsequent eruptions of two systems of sheared and twisted magnetic fields, which merge already in the upper corona and start to evolve further out as a single entity. The large-scale magnetic field significantly influences both, the early and the interplanetary evolution of the structure.\n",
      "\n",
      "During the first eruption the stability of the overlying field was disrupted which enabled the second eruption. We find that during the propagation in the interplanetary space the large-scale magnetic field, i.e. , the location of heliospheric current sheet between the AR and the Earth likely influences propagation and the evolution of the erupted structure(s). 0 into PostgreSQL...\n",
      "Inserting test sample 2402  The occurrence of a two-step solar flare on March 8, 2019, coinciding with the International Women's Day event, is analyzed in this study. The event was characterized by multiple eruptive signatures, including an M-class flare and an associated coronal mass ejection (CME). The first step of the flare was found to be impulsive and followed by a gradual phase that resulted in a double-peaked X-ray emission. Additionally, the CME was associated with a type II radio burst and a narrow CME core structure. The eruption source region was located at the solar limb, with the flare ribbon elongated along a northeast-southwest direction.\n",
      "\n",
      "Despite the event's complexity, it posed no significant threat to the Earth's environment, with the CME missing the Earth's orbit and producing only small geomagnetic disturbances. The analysis of this event provides important insights into the mechanisms of complex eruptive solar flares and sheds light on their predictability.\n",
      "\n",
      "The observational results were obtained using multi-wavelength observations, including X-ray data from the Geostationary Operational Environmental Satellite (GOES), white-light observations from the Large Angle and Spectrometric Coronagraph (LASCO) onboard the Solar and Heliospheric Observatory (SOHO), and radio observations from the NanÃ§ay Radioheliograph. The data were analyzed using advanced data processing techniques, including the differentiation of occulted and unocculted emissions and the separation of CME core and flank structures.\n",
      "\n",
      "Overall, this study contributes to our understanding of the dynamic solar activity and its potential impact on the Earth's environment. The results demonstrate the importance of monitoring the Sun's activity and its effects on our planet, especially during special events such as the International Women's Day, to ensure the safety and well-being of our society. 1 into PostgreSQL...\n",
      "Inserting test sample 2403  The tri-boson production is one of the key processes for the study of quartic gauge couplings. Next-to-leading order (NLO) corrections are mandatory to reduce theoretical uncertainties. In this study, the most up-to-date predictions including NLO QCD and NLO EW corrections to the total cross section and distributions of the WWZ production at the LHC are presented. We show that the QCD correction is about 100% and the EW correction is of a few percent at the total cross section level. The EW correction however becomes significant in the high energy regime of the gauge boson transverse momentum distributions. 0 into PostgreSQL...\n",
      "Inserting test sample 2404  The production of NLO WWZ, or vector boson scattering, at the Large Hadron Collider (LHC) is an intriguing phenomenon that has gained significant attention in recent years. This process plays a critical role in the study of weak interactions, electroweak symmetry breaking and triple gauge couplings, making it a promising avenue for further research. This paper provides a comprehensive investigation of NLO WWZ production through Monte Carlo simulations and theoretical calculations. Our results suggest that by analyzing this process with high precision measurements, we can establish more informative theoretical models and gain a better understanding of fundamental particle physics. 1 into PostgreSQL...\n",
      "Inserting test sample 2405  In the $SO(5)\\times U(1)$ gauge-Higgs unification (GHU), Kaluza-Klein (KK) excited states of charged and neutral vector bosons, $W^{(1)}$, $W_R^{(1)}$, $Z^{(1)}$, $\\gamma^{(1)}$ and $Z_R^{(1)}$, can be observed as $W'$ and $Z'$ signals in collider experiments. In this paper we evaluate the decay rates of the $W'$ and $Z'$, and $s$-channel cross sections mediated by $W'$ and $Z'$ bosons with final states involving the standard model (SM) fermion pair ($\\ell \\nu$, $\\ell\\bar{\\ell}$, $q\\bar{q}'$), $WH$, $ZH$, $WW$ and $WZ$. $W'$ and $Z'$ resonances appear around 6.0 TeV (8.5 TeV) for $\\theta_H=0.115$ (0.0737) where $\\theta_H$ is the Aharonov-Bohm phase in the fifth dimension in GHU. For decay rates we find $\\Gamma(W'\\to WH) \\simeq \\Gamma(W'\\to WZ)$ ($W'=W^{(1)}$, $W_R^{(1)}$), $\\Gamma(W^{(1)}\\to WH,WZ) \\sim\\Gamma(W_R^{(1)}\\to WH,WZ)$, $\\Gamma(Z^{(1)}\\to ZH) \\simeq\\sum_{Z'=Z^{(1)},\\gamma^{(1)}}\\Gamma(Z'\\to WW)$, and $\\Gamma(Z_R^{(1)}\\to ZH) \\simeq \\Gamma(Z_R^{(1)}\\to WW)$. $W'$ and $Z'$ signals of GHU can be best found at the LHC experiment in the processes $pp\\to W' (Z')+X$ followed by $W'\\to t\\bar{b}$, $WH$, and $Z'\\to e^+ e^-$, $\\mu^+\\mu^-, ZH$. For the lighter $Z'$ ($\\theta_H = 0.115$) case, with forthcoming 30 fb$^{-1}$ data of the 13 TeV LHC experiment we expect about ten $\\mu^+\\mu^-$ events for the invariant mass range 3 to 7 TeV, though the number of the events becomes much smaller when $\\theta_H=0.0737$. In the process with $WZ$ in the final state, it is confirmed that the unitarity is preserved, provided that all KK excited intermediate states are taken into account.\n",
      "\n",
      "Deviation of the $WWZ$ coupling from the SM is very tiny. Exotic partners $t_T^{(1)}$ and $b_Y^{(1)}$ of the top and bottom quarks with electric charge $+5/3$ and $-4/3$ have mass $M_{t_T^{(1)},b_Y^{(1)}}\\simeq 4-5$ TeV, becoming the lightest non-SM particles in GHU which can be singly produced in collider experiments. 0 into PostgreSQL...\n",
      "Inserting test sample 2406  The gauge-Higgs unification (GHU) is a theoretical framework that aims to unify the electroweak symmetry breaking and the unification of gauge groups. In this paper, we study the collider signals of $W'$ and $Z'$ bosons in the context of GHU. The production and decay of these heavy bosons are affected by the presence of additional Kaluza-Klein (KK) excitations of the Standard Model (SM) gauge bosons, which arise as a consequence of the compactification of an extra dimension.\n",
      "\n",
      "We consider a simplified GHU model where the SM gauge group is embedded into a larger gauge group $G$, which is broken down to the SM gauge group through the Hosotani mechanism. In this model, the extra dimension is compactified on a circle with radius $R$. The KK excitations of the gauge bosons can be parameterized in terms of a mass scale $M$, which is related to the size of the extra dimension.\n",
      "\n",
      "We analyze the production and decay of $W'$ and $Z'$ bosons at the Large Hadron Collider (LHC) using the Monte Carlo event generator PYTHIA. We focus on the decays into SM gauge bosons and top quarks, which are the dominant decay modes. We study the dependence of the signal cross sections on the mass scale $M$ and the radius $R$.\n",
      "\n",
      "Our results show that the production cross sections of $W'$ and $Z'$ bosons can be significantly enhanced by the presence of KK excitations. The decay modes into top quarks are more dominant for higher mass scales $M$. The invariant mass distributions of SM gauge boson pairs and top quark pairs provide a promising channel for discovery. The reach of the LHC for $W'$ and $Z'$ bosons in the GHU scenario is found to be up to several TeV in mass scale.\n",
      "\n",
      "In conclusion, our study demonstrates that collider signals of $W'$ and $Z'$ bosons can provide a unique signature of the GHU scenario. The search for these signatures can shed light on the unification of gauge and Higgs sectors and the nature of the extra dimension. Our findings may also have implications for other models with extra dimensions and extended gauge groups. 1 into PostgreSQL...\n",
      "Inserting test sample 2407  We attack the problem of learning face models for public faces from weakly-labelled images collected from web through querying a name. The data is very noisy even after face detection, with several irrelevant faces corresponding to other people. We propose a novel method, Face Association through Model Evolution (FAME), that is able to prune the data in an iterative way, for the face models associated to a name to evolve. The idea is based on capturing discriminativeness and representativeness of each instance and eliminating the outliers. The final models are used to classify faces on novel datasets with possibly different characteristics. On benchmark datasets, our results are comparable to or better than state-of-the-art studies for the task of face identification. 0 into PostgreSQL...\n",
      "Inserting test sample 2408  FAME, or Face Association through Model Evolution, is a system that aims to improve the accuracy of facial recognition technology. By using an evolutionary algorithm, FAME can iteratively refine its face recognition model through comparison with a collection of annotated face images. This approach improves the generalization of the model and reduces the effect of variations in pose, expression, and lighting. In our experiments, we demonstrate that FAME outperforms state-of-the-art face recognition systems, achieving an accuracy of 99.2% on the popular Labeled Faces in the Wild dataset. Besides recognizing faces in static images, FAME's model evolution can also be extended to other computer vision tasks such as face tracking and video-based recognition. 1 into PostgreSQL...\n",
      "Inserting test sample 2409  Recently, it has been discovered that systems of Active Brownian particles (APB) at high density organise their velocities into coherent domains showing large spatial structures in the velocity field. Such a collective behavior occurs spontaneously, i.e. is not caused by any specific interparticle force favoring the alignment of the velocities. This phenomenon was investigated in the absence of thermal noise and in the overdamped regime where inertial forces could be neglected. In this work, we demonstrate through numerical simulations and theoretical analysis that the velocity alignment is a robust property of ABP and persists even in the presence of inertial forces and thermal fluctuations. We also show that a single dimensionless parameter, such as the P\\'eclet number customarily employed in the description of self-propelled particles, is not sufficient to fully characterize such a phenomenon neither in the regimes of large viscosity nor small mass. Indeed, the size of the velocity domains, measured through the correlation length of the spatial velocity correlation, remains constant when the swim velocity increases while decreases as the rotational diffusion becomes larger. We find that the spatial velocity correlation depends on the inertia but, contrary to common belief, are non-symmetrically affected by mass and inverse viscosity variations. We conclude that in self-propelled systems, at variance with passive systems, variations of the inertial time (mass over solvent viscosity) and mass act as independent control parameters. Finally, we highlight the non-thermal nature of the spatial velocity correlations that are fairly insensitive both to solvent and active temperatures. 0 into PostgreSQL...\n",
      "Inserting test sample 2410  This research paper investigates the spatial velocity correlations in inertial systems of Active Brownian Particles (ABP). ABPs are self-propelled particles that exhibit both Brownian motion and active motion. The active motion arises due to the intrinsic energy of ABPs, which is converted into directed motion. The Brownian motion arises due to the stochastic interaction of ABPs with the solvent molecules. Through computer simulations, we explore the velocity correlations of ABPs in inertial systems and compare them with those in non-inertial systems. We find that the velocity correlations in inertial systems depend on the particle's swimming speed, while in non-inertial systems, they depend on the particle's rotational diffusion coefficient. Furthermore, the correlations in inertial systems are anisotropic and depend on the angle between the relative positions and velocities of the particles. We also investigate the effects of the hydrodynamic interactions among ABPs in these systems. Our results show that the correlations can be enhanced or suppressed by the hydrodynamic interactions, depending on their strengths and the swimming speed of the ABPs. These findings have implications for understanding the collective behavior of ABPs in real-world systems, such as bacterial suspensions and self-propelled colloidal particles, and can potentially lead to the development of new technologies based on active matter. 1 into PostgreSQL...\n",
      "Inserting test sample 2411  We investigate the notion of symplectic divisorial compactification for symplectic 4-manifolds with either convex or concave type boundary. This is motivated by the notion of compactifying divisors for open algebraic surfaces.\n",
      "\n",
      "We give a sufficient and necessary criterion, which is simple and also works in higher dimensions, to determine whether an arbitrarily small concave/convex neighborhood exist for an $\\omega$-orthogonal symplectic divisor (a symplectic plumbing). If deformation of symplectic form is allowed, we show that a symplectic divisor has either a concave or convex neighborhood whenever the symplectic form is exact on the boundary of its plumbing. As an application, we classify symplectic compactifying divisors having finite boundary fundamental group. We also obtain a finiteness result of fillings when the boundary can be capped by a symplectic divisor with finite boundary fundamental group. 0 into PostgreSQL...\n",
      "Inserting test sample 2412  This paper examines the symplectic divisorial capping in dimension 4. We discuss the existence and uniqueness of symplectic structures on compact 4-manifolds with boundary. Specifically, we focus on the divisorial capping problem, which involves capping off symplectic divisors with symplectic handlebodies. We analyze the behavior of these cappings with respect to variations in the underlying symplectic structures and establish criteria that guarantee the existence and uniqueness of solutions to the divisorial capping problem in dimension 4. We also investigate the relationship between divisorial capping and symplectic embedding, and provide examples to demonstrate the application of our results. Our findings contribute to the ongoing study of symplectic topology, and have implications for other areas of mathematics and mathematical physics, such as mirror symmetry and gauge theory. 1 into PostgreSQL...\n",
      "Inserting test sample 2413  The notion of a compact object immune to the horizon problem and comprising an anisotropic inhomogeneous fluid with a specific radial pressure behavior, i.e. the gravastar, is extended by introducing an electrically charged component. Einstein-Maxwell field equations are solved in the asymptotically de Sitter interior where a source of the electric field is coupled to the fluid energy density. Two different solutions which satisfy the dominant energy condition are given: one is the delta-shell model for which the analysis is carried out within Israel's thin shell formalism, the other approach - the continuous profile model - is solved numerically and the interior solutions have been (smoothly) joined with the Reissner-Nordstrom exterior. The effect of electric charge is considered, and the equation of state, the speed of sound and the surface redshift are calculated for both models. 0 into PostgreSQL...\n",
      "Inserting test sample 2414  This research paper examines the consequences of endowing gravastars with an electric charge. Gravastars are hypothetical objects which could replace black holes as an end-state for the gravitational collapse of massive stars. We study whether the electric charge can stabilize the gravastar configuration against collapse and investigate the effect of the charge on the stability of the gravastar interior. Using analytical and numerical methods, we obtain a new family of electrically charged gravastar solutions and perform an extensive study of their properties. Our results show that the electric charge influences the mass-radius relation and the stability of the gravastar configurations. We also analyze the behavior of the surface redshift and investigate the effects of the charge on the gravitational wave emission. Our study sheds light on the potential role of electric charges in the structure and stability of compact objects in astrophysics. 1 into PostgreSQL...\n",
      "Inserting test sample 2415  Project AMIGA (Absorption Maps In the Gas of Andromeda) is a large ultraviolet Hubble Space Telescope program, which has assembled a sample of 43 QSOs that pierce the circumgalactic medium (CGM) of Andromeda (M31) from R=25 to 569 kpc (25 of them probing gas from 25 kpc to about the virial radius-Rvir = 300 kpc-of M31). Our large sample provides an unparalleled look at the physical conditions and distribution of metals in the CGM of a single galaxy using ions that probe a wide range of gas phases (Si II, Si III, Si IV, C II, C IV, and O VI, the latter being from the Far Ultraviolet Spectroscopic Explorer). We find that Si III and O VI have near unity covering factor maintained all the way out to 1.2Rvir and 1.9Rvir, respectively. We show that Si III is the dominant ion over Si II and Si IV at any R. While we do not find that the properties of the CGM of M31 depend strongly on the azimuth, we show that they change remarkably around 0.3-0.5Rvir, conveying that the inner regions of the CGM of M31 are more dynamic and have more complicated multi-phase gas-structures than at R>0.5Rvir. We estimate the metal mass of the CGM within Rvir as probed by Si II, Si III, and Si IV is 2x10^7 Msun and by O VI is >8x10^7 Msun, while the baryon mass of the 10^4-10^5.5 K gas is ~4x10^10 (Z/0.3 Zsun)^(-1) Msun within Rvir. We show that different zoom-in cosmological simulations of L* galaxies better reproduce the column density profile of O VI with R than Si III or the other studied ions. We find that observations of the M31 CGM and zoom-in simulations of L* galaxies have both lower ions showing higher column density dispersion and dependence on R than higher ions, indicating that the higher ionization structures are larger and/or more broadly distributed. 0 into PostgreSQL...\n",
      "Inserting test sample 2416  The study of the circumgalactic medium (CGM) is critical for our understanding of the evolution of galaxies. Project AMIGA aims to probe the CGM of Andromeda, our nearest galactic neighbor, through deep, high-resolution spectroscopy. The unprecedented sensitivity and resolution of our observations provide a detailed look at the complex physical processes shaping the gas around Andromeda. \n",
      "\n",
      "Our analysis reveals a multi-phase CGM with a range of densities, temperatures, and ionization states. We find evidence of gas accretion onto Andromeda, as well as gas outflows driven by feedback from star formation and supernovae. The outflowing material is highly enriched in heavy elements produced by previous generations of stars. We also detect absorption features indicative of gas inflow, possibly from a neighboring system or a diffuse intergalactic medium. \n",
      "\n",
      "By modeling the observed absorption profiles, we estimate the physical properties of the CGM, including its kinematics, metallicity, and ionization state. Our results suggest that the CGM is largely ionized, with a warm phase extending to distances beyond the stellar disk of Andromeda. The ionization is likely driven by a combination of photoionization from the galaxy's own stars and heating from feedback processes. \n",
      "\n",
      "In addition to providing new insights into the CGM of Andromeda, our results have broader implications for galaxy evolution and the intergalactic medium. The CGM represents a reservoir of gas that can fuel future star formation, and its physical properties are intimately linked to the formation and evolution of galaxies. Our study demonstrates the power of deep, high-resolution spectroscopy as a tool for probing the CGM, and highlights the potential for future observations of this critical component of galactic ecosystems. 1 into PostgreSQL...\n",
      "Inserting test sample 2417  We infer dark matter properties from gamma ray residuals extracted using eight different interstellar emission scenarios proposed by the Fermi-LAT Collaboration to explain the Galactic Center gamma ray excess. Adopting the most plausible simplified ansatz, we assume that the dark matter particle is a Majorana fermion interacting with standard fermions via a scalar mediator.\n",
      "\n",
      "Using this theoretical hypothesis and the Fermi residuals we calculate Bayesian evidences, including Fermi-LAT exclusion limits from 15 dwarf spheroidal galaxies as well. Our Bayes factors single out four of the Fermi scenarios as compatible with the simplified dark matter model. In the most preferred scenario the dark matter (mediator) mass is in the 100-500 (1-200) GeV range and its annihilation is dominated by top quark final state. Less preferred but still plausible is annihilation into b\\bar{b} and tau^+tau^- final states with an order of magnitude lower dark matter mass. Our conclusion is that the properties of dark matter extracted from gamma ray data are highly sensitive to the modeling of the interstellar emission. 0 into PostgreSQL...\n",
      "Inserting test sample 2418  The study presents an analysis of gamma ray interstellar emissions models to derive the properties of dark matter. Dark matter is one of the biggest mysteries in physics that cannot be directly observed but is believed to influence the formation of galaxies and large-scale structures in the universe. The gamma ray interstellar emissions are generated by cosmic ray interactions with interstellar gas and radiation fields, providing a promising signal for indirect detection of dark matter. The presented models have considered different dark matter self-annihilation channels and distributions, and compared the predicted gamma ray spectra with observations from satellite-based instruments such as Fermi-LAT. The results suggest a preference for dark matter particles with masses between 30 and 100 GeV, and annihilation channels such as those into quarks or W-bosons. Furthermore, the study investigates the implications of different assumptions on the interstellar gas and radiation fields, highlighting the importance of accurately modeling the astrophysical background. 1 into PostgreSQL...\n",
      "Inserting test sample 2419  We show that there exist infinitely many particular choices of parameters for which the three-term recurrence relations governing the expansions of the solutions of the general Heun equation in terms of the Gauss hypergeometric functions become two-term. In these cases the coefficients are explicitly written in terms of the gamma functions. 0 into PostgreSQL...\n",
      "Inserting test sample 2420  We study the connection between the general Heun function and hypergeometric expansions, with a specific focus on their two-term recurrence relations. Our analysis reveals intriguing links with harmonic sums, modular forms, and quantum field theory. The results hold broad implications for diverse fields, including condensed matter physics, number theory, and computational mathematics. 1 into PostgreSQL...\n",
      "Inserting test sample 2421  We show how the switching-on of an electron transport through a system of two parallel quantum dots embedded in a short quantum wire in a photon cavity can trigger coupled Rabi and collective electron-photon oscillations. We select the initial state of the system to be an eigenstate of the closed system containing two Coulomb interacting electrons with possibly few photons of a single cavity mode. The many-level quantum dots are described by a continuous potential. The Coulomb interaction and the para- and dia-magnetic electron-photon interactions are treated by exact diagonalization in a truncated Fock-space. To identify the collective modes the results are compared for an open and a closed system with respect to the coupling to external electron reservoirs, or leads. We demonstrate that the vacuum Rabi oscillations can be seen in transport quantities as the current in and out of the system. 0 into PostgreSQL...\n",
      "Inserting test sample 2422  This study investigates the interplay of photon cavity resonance and electron transport dynamics in quantum dot systems. We show that the strong coupling between excitons and the cavity mode leads to unconventional oscillatory behavior in the light emission spectra. Specifically, we observe novel collective Rabi oscillations and coupled electron-photon oscillations, both of which arise from the interplay between fast electronic hopping and slow cavity mode relaxation times. The emergence of these coherent oscillations is accompanied by a periodic modulation of the cavity emission linewidth, providing direct evidence of the hybridization of electronic and photonic degrees of freedom. Our results demonstrate that coherent light-matter interaction at the nanoscale can manifest in highly orchestrated dynamical regimes, motivating the design of novel quantum devices for optical and quantum information processing technologies. 1 into PostgreSQL...\n",
      "Inserting test sample 2423  The generalized pseudospectral method is employed for the accurate calculation of eigenvalues, densities and expectation values for the spiked harmonic oscillators. This allows \\emph{nonuniform} and \\emph{optimal} spatial discretization of the corresponding single-particle radial Schr\\\"odinger equation satisfying the Dirichlet boundary conditions leading to the standard diagonalization of the symmetric matrices. The present results for a large range of potential parameters are in excellent agreement with those from the other accurate methods available in the literature. The ground and excited states (both low as well as high angular momentum states) are obtained with equal ease and accuracy. Some new states including the higher excited states are reported here for the first time. This offers a simple, accurate and efficient method for the treatment of these and a wide variety of other singular potentials of physical and chemical interest in quantum mechanics. 0 into PostgreSQL...\n",
      "Inserting test sample 2424  This paper proposes a computational method for solving the dynamics of spiked harmonic oscillators based on a generalized pseudospectral method. This approach is particularly useful for systems that exhibit complex behavior or nonlinearity. The method involves applying a weighted residual approach to obtain the approximate solution to the differential equations governing the system. The proposed method is compared with existing numerical methods including finite difference time domain and finite element approaches, and it is shown to provide accurate solutions with reduced computational cost. The method is applied to various examples of spiked harmonic oscillators, including systems with time-varying stiffness, damping, or forcing functions to demonstrate its effectiveness. Overall, this method can serve as a useful tool for understanding and simulating the behavior of spiked harmonic oscillators in a wide range of applications. 1 into PostgreSQL...\n",
      "Inserting test sample 2425  Weak gravitational lensing by large-scale structure affects the determination of the cosmological deceleration parameter $q_0$. We find that the lensing induced dispersions on truly standard candles are $0.04$ and $0.02$ mag at redshift $z=1$ and $z=0.5$, respectively, in a COBE-normalized cold dark matter universe with $\\Omega_0=0.40$, $\\Lambda_0=0.6$, $H=65$km/s/Mpc and $\\sigma_8=0.79$. It is shown that one would observe $q_0=-0.44^{+0.17}_{-0.05}$ and $q_0=-0.45^{+0.10}_{-0.03}$ (the errorbars are $2\\sigma$ limits) with standard candles with zero intrinsic dispersion at redshift $z=1$ and $z=0.5$, respectively, compared to the truth of $q_0=-0.40$ in this case, i.e., a 10\\% error in $q_0$ will be made. A standard COBE normalized $\\Omega_0=1$ CDM model would produce three times as much variance and a mixed (hot and cold) dark matter model would lead to an intermediate result. One unique signature of this dispersion effect is its non Gaussianity. Although the lensing induced dispersion at lower redshift is still significantly smaller than the currently best observed (total) dispersion of $0.12$ mag in a sample of type Ia supernovae, selected with the multicolor light curve shape method, it becomes significant at higher redshift. We show that there is an optimal redshift, in the range $z\\sim 0.5-2.0$ depending on the amplitude of the intrinsic dispersion of the standard candles, at which $q_0$ can be most accurately determined. 0 into PostgreSQL...\n",
      "Inserting test sample 2426  The determination of the deceleration parameter $q_0$ is of great importance in the study of the universe's expansion. Weak gravitational lensing has been shown to be a useful tool in measuring $q_0$ by directly observing the influence of large-scale structure on the cosmic microwave background (CMB). In this paper, we investigate the effects of weak gravitational lensing from large-scale structure on the accuracy of $q_0$ determination. We analyze the influence of lensing on different observational probes, including CMB, galaxy number counts, and galaxy weak lensing shear. We also examine the impact of survey properties, such as sky coverage and depth, on the determination of $q_0$. Our results demonstrate that weak gravitational lensing can have a significant effect on the inferred value of $q_0$, leading to biases and uncertainties in its measurement. We find that CMB and galaxy weak lensing shear are less affected by lensing than galaxy number counts. We also show that survey specifications, such as sky coverage and depth, play a crucial role in mitigating the impact of lensing on $q_0$ determination. Our work provides a valuable contribution to the field of cosmology by shedding light on the impact of weak gravitational lensing on our understanding of the universe's expansion. 1 into PostgreSQL...\n",
      "Inserting test sample 2427  We use a sample built on the SDSS DR7 catalogue and the bulge-disc decomposition of Simard et al. (2011) to study how the bulge and disc components contribute to the parent galaxy's star formation activity, by determining its position in the star formation rate (SFR) - stellar mass (M$_{\\star}$) plane at 0.02$<z<$0.1. We use the bulge and disc colours as proxy for their SFRs. We study the mean galaxy bulge-total mass ratio (B/T) as a function of the residual from the MS ($\\Delta_{MS}$) and find that the B/T-$\\Delta_{MS}$ relation exhibits a parabola-like shape with the peak of the MS corresponding to the lowest B/Ts at any stellar mass. The lower and upper envelop of the MS are populated by galaxies with similar B/T, velocity dispersion and concentration ($R_{90}/R_{50}$) values. Bulges above the MS are characterised by blue colours or, when red, by a high level of dust obscuration, thus indicating that in both cases they are actively star forming.\n",
      "\n",
      "When on the MS or below it, bulges are mostly red and dead. At stellar masses above $10^{10.5} $M$_{\\odot}$, bulges on the MS or in the green valley tend to be significantly redder than their counterparts in the quiescence region, despite similar levels of dust obscuration. The disc color anti-correlates at any mass with the distance from the MS, getting redder when approaching the MS lower envelope and the quiescence region. We conclude that the position of a galaxy in the LogSFR-LogM$_{\\star}$ plane depends on the star formation activity of its components: above the MS both bulge and disk are actively star forming. The nuclear activity is the first to be suppressed, moving the galaxies on the MS. Once the disk stops forming stars as well, the galaxy moves below the MS and eventually to the quiescence region. This is confirmed by a large fraction ($\\sim45\\%$) of passive galaxies with a secure two component morphology. 0 into PostgreSQL...\n",
      "Inserting test sample 2428  This research paper investigates the relationship between galaxy structure and star formation activity in the local Universe by studying bulges and disks. Extensive observational evidence points to the central regions of spiral galaxies containing bulges, which are characterized by higher surface brightness, velocity dispersion and an older stellar population compared to the disk regions. We examine the role played by such bulges in regulating star formation activity in galaxies, by considering the connection between their properties and the star formation activity of their host galaxies. Our analysis is based on the largest available sample of nearby galaxies, which includes both early- and late-type systems, with a total of more than 2000 galaxies. We construct a morphologically classified sample of bulge-dominated, disk-dominated and intermediate galaxies by performing a detailed photometric decomposition of two-dimensional image data from multiple telescopes. The star formation properties of isolated and non-isolated galaxies in this sample were studied by using the H-alpha indicator from a number of spectroscopic surveys. Our results suggest a clear correlation between the structural properties of galaxies and their star formation activity. Specifically, we find a strong decrease in H-alpha emission in central regions of bulge-dominated galaxies, which indicates that bulges are capable of inhibiting star formation in their host galaxies. We also show that the fraction of disk-dominated galaxies with high star formation activity increases with decreasing bulge-to-disk ratio. Moreover, we find that the transition from bulge-dominated to disk-dominated systems is associated with a sharp increase in star formation activity. Our study supports a scenario in which the properties of a galaxy are strongly influenced by the structural components it is composed of, making this a promising avenue for further exploration in the study of galaxy properties and evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 2429  We report on a successful attempt to extract the cross-section for the high-energy scattering of colour dipoles of fixed transverse size off protons using electroproduction and photoproduction total cross-section data, subject to the constraint provided by the ratio of the overall photon dissociation cross-section to the total cross-section. 0 into PostgreSQL...\n",
      "Inserting test sample 2430  We present a method to extract the dipole cross-section from photo- and electro-production total cross-section data using dispersion relations. The results for the proton at low energies show good agreement with theoretical predictions and established experimental data, providing new insights into the structure of nucleons. 1 into PostgreSQL...\n",
      "Inserting test sample 2431  The origin of rapid neutron capture (r-process) nuclei remains one of the longest standing mysteries in nuclear astrophysics. Core collapse supernovae (SNe) and neutron star binary mergers are likely r-process sites, but little evidence yet exists for their in situ formation in such environments. Motivated by the advent of sensitive new or planned X-ray telescopes such as the Nuclear Spectroscopic Telescope Array (NuSTAR) and the Large Observatory for X-ray Timing (LOFT), we revisit the prospects for the detection of X-ray decay lines from r-process nuclei in young or nearby supernova remnants. For all remnants planned to be observed by NuSTAR (and several others), we conclude that r-process nuclei are detectable only if the remnant possesses a large overabundance O > 1e3 relative to the average yield per SN. Prospects are better for the next Galactic SN (assumed age of 3 years and distance of 10 kpc), for which an average r-process yield is detectable via the 10.7(9.2) keV line complexes of Os194 by LOFT at 6 sigma (5 sigma) confidence; the 27.3 keV line complex of Sb125 is detectable by NuSTAR at 2 sigma for O > 2. We also consider X-rays lines from the remnants of Galactic magnetars, motivated by the much higher r-process yields of the magneto-rotationally driven SNe predicted to birth magnetars. The ~ 3.6-3.9 keV lines of Sn126 are potentially detectable in the remnants of the magnetars 1E1547.0-5408 and 1E2259+586 by LOFT for an assumed r-process yield predicted by recent simulations. The (non-)detection of these lines can thus probe whether magnetars are indeed born with millisecond periods. Finally, we consider a blind survey of the Galactic plane with LOFT for r-process lines from the most recent binary neutron star merger remnant, concluding that a detection is unlikely without additional information on the merger location. 0 into PostgreSQL...\n",
      "Inserting test sample 2432  Supernova remnants (SNRs) play a crucial role in understanding the origins of heavy elements through nucleosynthesis and the formation of magnetars, highly magnetized neutron stars. In particular, the observation and analysis of X-ray decay lines emitted by heavy nuclei in SNRs can provide valuable insight into these processes. \n",
      "\n",
      "The r-process, which produces half of the elements heavier than iron, remains enigmatic. However, it is believed that the explosive environments of SNRs, such as those resulting from core-collapse supernovae or neutron star mergers, may be the site of this process. By studying the abundance and distribution of heavy elements in SNRs, we can gain a better understanding of the r-process and its origins.\n",
      "\n",
      "Furthermore, the presence of magnetars in SNRs allows for the possibility of studying their formation and evolution. While the exact birth periods of magnetars are still uncertain, a number of models have been proposed to explain their formation. By studying the properties of X-ray decay lines emitted from heavy nuclei in SNRs, we can potentially constrain the birth periods of magnetars and provide deeper insight into their formation mechanisms.\n",
      "\n",
      "In this paper, we present a detailed analysis of X-ray decay lines from heavy nuclei in several SNRs. We compare our results to theoretical models of r-process nucleosynthesis and magnetar formation, and discuss the implications for our understanding of these processes. Our findings suggest that the X-ray decay lines are consistent with the presence of heavy elements produced by the r-process, supporting the hypothesis that SNRs are important sites of this process. Furthermore, we find evidence that the birth periods of magnetars may be shorter than previously thought, and suggest further observations to test this hypothesis.\n",
      "\n",
      "Overall, our study demonstrates the potential of X-ray decay lines as a powerful probe of the r-process and the formation of magnetars in SNRs. Further investigation in this area promises to deepen our understanding of these fundamental processes in astrophysics. 1 into PostgreSQL...\n",
      "Inserting test sample 2433  We have investigated the Se isotope effect in layered bismuth chalcogenide (BiCh2-based) superconductor LaO0.6F0.4Bi(S,Se)2 with 76Se and 80Se. For all examined samples, the Se concentration, which is linked to the superconducting properties, is successfully controlled within x = 1.09-1.14 in LaO0.6F0.4BiS2-xSex. From the magnetization and electrical resistivity measurements, changes in Tc are not observed for the LaO0.6F0.4Bi(S,Se)2 samples with 76Se and 80Se isotopes. Our results suggest that pairing in the BiCh2-based superconductors is not mediated by phonons, and unconventional superconductivity states may emerge in the BiCh2 layers of LaO0.6F0.4Bi(S,Se)2. 0 into PostgreSQL...\n",
      "Inserting test sample 2434  In this study, we investigate the superconducting properties of layered bismuth chalcogenide superconductor LaO0.6F0.4Bi(S,Se)2 in order to detect unconventional superconductivity. Using Se isotope effect, we have identified a clear indication of unconventional superconductivity in the material. Our results suggest that the superconducting state in LaO0.6F0.4Bi(S,Se)2 arises from an unconventional mechanism, possibly due to the presence of spin-triplet pairing. These findings add to the growing body of evidence for unconventional superconductivity in materials previously thought to exhibit only conventional superconductivity. Our study underscores the importance of investigating unconventional superconducting properties in order to better understand the fundamental behavior of these materials. 1 into PostgreSQL...\n",
      "Inserting test sample 2435  We predict a two-dimensional (2D) antiferromagnetic (AFM) boron (designated as M-boron) by using ab initio evolutionary methodology. M-boron is entirely composed of B20 clusters in a hexagonal arrangement. Most strikingly, the highest valence band of M-boron is isolated, strongly localized, and quite flat, which induces spin polarization on each cap of the B20 cluster. This flat band originates from the unpaired electrons of the capping atoms, and is responsible for magnetism. M-boron is thermodynamically metastable and is the first cluster-based 2D magnetic material in the elemental boron system. 0 into PostgreSQL...\n",
      "Inserting test sample 2436  Two-dimensional materials such as graphene have revolutionized the field of materials science due to their exceptional physical properties. In this study, we explore the possibility of a two-dimensional magnetic boron material. By utilizing first-principles calculations, we demonstrate that a monolayer of boron atoms can exhibit ferromagnetic behavior under specific conditions. Our results suggest that this material could have potential applications in spintronics and magnetic storage devices. Additionally, we predict unique electronic and magnetic properties that could open up new avenues for further research in the field of two-dimensional magnetic materials. 1 into PostgreSQL...\n",
      "Inserting test sample 2437  Near magnitudes of Dirac particle mass-ratios, mixing hierarchy-quantities and electroweak charges against the background of highly successful flavor-universal one-generation EW theory is a puzzle in need of diverse inclusive research. In this paper I study the problem in proper terms of lepton and quark Deviation-from-Mass-Degeneracy (DMD) hierarchies at tree EW approximation. As primary are considered not discrete flavor symmetry but rather the deviations from mass-degeneracy-symmetry without inventing exact particular symmetry. Empirically suggested benchmark flavor patterns (zero approximation) and deviations from benchmarks caused by emergence of a small related to EW charges parameter are considered two sources of realistic flavor quantities. Physically interesting mass and mixing flavor quantities are obtained as solutions of linear and quadratic DMD-hierarchy equation-pairs with complementary patterns of quark and lepton DMD-hierarchies. Dual relations between DMD-quantities of quarks and charged leptons (Dirac particles), on the one hand, and neutrinos (likely Majorana particles), on the other hand, are inferences. Considered in the literature approximate quark-neutrino mixing angle complementarity appears naturally from the violation of benchmark patterns by the emergent small parameter. 0 into PostgreSQL...\n",
      "Inserting test sample 2438  This paper discusses the mass-degeneracy hierarchies of quarks and leptons in light of complementary deviations from the hierarchy. The Standard Model of particle physics has helped describe particles and their interactions accurately. However, discrepancies in experiments suggest that there may be physics beyond this model. The mass of particles, in particular, is a subject of interest. In this work, we examine the deviation from mass-degeneracy hierarchy, a necessary outcome of the Standard Model, in both the quark and lepton sectors. We show that the deviations in the masses of the two sectors have complementary behavior. Our findings indicate that the deviation trends in the quark and lepton sectors point to the possibility of physics beyond the Standard Model, such as the coupling between the two sectors. This study has implications for the development of new theories to accommodate such anomalies, leading to a deeper understanding of the fundamental interactions among particles. 1 into PostgreSQL...\n",
      "Inserting test sample 2439  We present results of a programme of multi-epoch, intra-night optical monitoring of a sample of non-blazar type AGN, which includes seven radio-quiet QSOs (RQQs) and an equal number of radio-loud, lobe-dominated quasars (LDQs), covering a redshift range from about 0.2 to 2.0. These two sets of optically bright and intrinsically luminous QSOs are well matched in the redshift-optical luminosity ($z - M_B$) plane. Our CCD monitoring covered a total of 61 nights with an average of 6.1 hours of densely sampled monitoring of just a single QSO per night, thereby achieving a typical detection threshold of $\\sim 1$% variation over the night. Unambiguous detection of intra-night variability(INOV) amplitudes in the range 1$-$3% on day-like or shorter time scales were thus made for both RQQs and LDQs. Based on these clear detections of INOV, we estimate duty cycles of 17% and 9% for RQQs and LDQs, respectively; inclusion of the two cases of probable variations of LDQs would raise the duty cycle to 15% for LDQs. The similarity in the duty cycle and amplitude of INOV for the RQQs and LDQs suggests, firstly, that the radio loudness alone does not guarantee an enhanced INOV in QSOs and,secondly, that as in LDQs, relativistic jets may also be present in RQQs. We argue that, as compared to BL Lacs, the conspicuously milder, rarer and possibly slower INOV of RQQs and LDQs, can in fact be readily understood in terms of their having optical synchrotron jets which are modestly misaligned from us,but are otherwise intrinsically as relativistic and active as the jets in BL Lacs. 0 into PostgreSQL...\n",
      "Inserting test sample 2440  This research investigates the intranight optical variability of radio-quiet and radio lobe dominated quasars. Quasars are known to exhibit rapid and unpredictable variability in their optical emission, and this study focuses on the variability of two quasar types with distinct characteristics: radio-quiet and radio lobe dominated. In order to investigate their variability, we utilized photometric observations obtained through a 1.2m telescope. Our sample included 10 radio-quiet and 10 radio lobe dominated quasars, each observed on six different nights. \n",
      "\n",
      "Our analysis revealed that the majority of our sample exhibited intranight optical variability with peak-to-peak amplitudes of âˆ¼0.1 mag. The variability was found to be more prominent in the radio-quiet quasars than in the radio lobe dominated quasars. In addition, we discovered a correlation between the amplitude of the optical variability and the redshift of the quasars, with higher redshift quasars exhibiting larger amplitude variability. This correlation hints at the possibility of intrinsic, physical causes for the observed variability.\n",
      "\n",
      "Furthermore, we investigated whether the optical variability might be related to the radio properties of the quasars. Our results showed that the radio properties of the quasars did not strongly correlate with their optical variability, indicating that the observed variability may be due to intrinsic properties of the quasars themselves. However, additional studies are necessary to fully understand the underlying causes of the observed variability.\n",
      "\n",
      "Overall, our study provides valuable insights into the intranight optical variability of radio-quiet and radio lobe dominated quasars, and highlights the importance of further research to fully comprehend the complexity of the quasar phenomenon. 1 into PostgreSQL...\n",
      "Inserting test sample 2441  We set out to evaluate the potential of the Colombian Andes for millimeter-wave astronomical observations. Previous studies for astronomical site testing in this region have suggested that nighttime humidity and cloud cover conditions make most sites unsuitable for professional visible-light observations. Millimeter observations can be done during the day, but require that the precipitable water vapor column above a site stays below $\\sim$10 mm.\n",
      "\n",
      "Due to a lack of direct radiometric or radiosonde measurements, we present a method for correlating climate data from weather stations to sites with a low precipitable water vapor column. We use unsupervised learning techniques to low-dimensionally embed climate data (precipitation, rain days, relative humidity, and sunshine duration) in order to group together stations with similar long-term climate behavior. The data were taken over a period of 30 years by 2046 weather stations across the Colombian territory. We find 6 regions with unusually dry, clear-sky conditions, ranging in elevations from 2200 to 3800 masl. We evaluate the suitability of each region using a quality index derived from a Bayesian probabilistic analysis of the station type and elevation distributions. Two of these regions show a high probability of having an exceptionally low precipitable water vapor column. We compared our results with global precipitable water vapor maps and find a plausible geographical correlation with regions with low water vapor columns ($\\sim10$ mm) at an accuracy of $\\sim20$ km. Our methods can be applied to similar datasets taken in other countries as a first step toward astronomical site evaluation. 0 into PostgreSQL...\n",
      "Inserting test sample 2442  This paper presents a novel approach to determine the suitability of radio astronomical sites in the Colombian Andes by using low-dimensional embeddings of climate data. Such embeddings allow for simplified analysis and visualization of complex data sets. We utilize a combination of principal component analysis and autoencoder networks to project the multi-dimensional climate data onto a lower-dimensional space while retaining relevant statistical information.\n",
      "\n",
      "Our methodology involves the collection and preprocessing of meteorological data from weather stations in the Colombian Andes. We select a range of physical parameters, including temperature, humidity, and wind speed, to construct a feature set that characterizes the climate in the region. These features are then projected onto a lower-dimensional space using our hybrid approach. The resulting embedding provides a succinct representation of the climatic variability across different sites.\n",
      "\n",
      "We validate our approach by comparing the low-dimensional embeddings with existing site testing methods. Our results show that the embeddings provide a more comprehensive and intuitive way to understand the climate conditions at each site. Moreover, they enable us to identify previously unseen patterns and correlations between different climatic features.\n",
      "\n",
      "In conclusion, our proposed approach offers a new perspective on radio astronomical site testing in the Colombian Andes. The low-dimensional embeddings of climate data provide an efficient and effective means to assess the suitability of potential sites. We believe our methodology will be of interest to researchers and practitioners in the field of radio astronomy and climate science. 1 into PostgreSQL...\n",
      "Inserting test sample 2443  The recent measurement of the Lamb shift in muonic hydrogen allows for the most precise extraction of the charge radius of the proton which is currently in conflict with other determinations based on $e-p$ scattering and hydrogen spectroscopy. This discrepancy could be the result of some new muon-specific force with O(1-100) MeV force carrier---in this paper we concentrate on vector mediators. Such an explanation faces challenges from the constraints imposed by the $g-2$ of the muon and electron as well as precision spectroscopy of muonic atoms. In this work we complement the family of constraints by calculating the contribution of hypothetical forces to the muonium hyperfine structure. We also compute the two-loop contribution to the electron parity violating amplitude due to a muon loop, which is sensitive to the muon axial-vector coupling.\n",
      "\n",
      "Overall, we find that the combination of low-energy constraints favors the mass of the mediator to be below 10 MeV, and that a certain degree of tuning is required between vector and axial-vector couplings of new vector particles to muons in order to satisfy constraints from muon $g-2$. However, we also observe that in the absence of a consistent standard model embedding, high energy weak-charged processes accompanied by the emission of new vector particles are strongly enhanced by $(E/m_V)^2$, with $E$ a characteristic energy scale and $m_V$ the mass of the mediator. In particular, leptonic $W$ decays impose the strongest constraints on such models completely disfavoring the remainder of the parameter space. 0 into PostgreSQL...\n",
      "Inserting test sample 2444  The existence of dark matter remains one of the biggest mysteries in physics today. While direct evidence is yet to be found, indirect and theoretical evidence suggest that dark matter could interact with particles other than those in the standard model. In this paper, we investigate the possibility of dark matter interacting with muons, focusing on muon-specific dark forces. These interactions could provide a link between the standard model and dark matter, offering valuable insights into the nature of dark matter.\n",
      "\n",
      "We examine the constraints on muon-specific dark forces using experimental and observational data. We summarize the current experimental limits on the muon-specific dark force coupling constant, based on measurements from muon beam experiments and rare meson decays. We also consider cosmological constraints based on observations of cosmic microwave background radiation and large-scale structure. These limits provide an upper bound on the strength of the muon-specific dark force.\n",
      "\n",
      "Our calculations show that the constraints on the muon-specific dark force are still relatively weak, offering considerable room for exploration in the future. We discuss potential experimental avenues for further exploration, including measurements of rare meson decay rates, precision muon spectroscopy, and searches for dark matter annihilation signals in astrophysical observations. Such experimental results will provide critical insights into the nature of dark matter and its interactions with the standard model. 1 into PostgreSQL...\n",
      "Inserting test sample 2445  Metric space magnitude, an active field of research in algebraic topology, is a scalar quantity that summarizes the effective number of distinct points that live in a general metric space. The {\\em weighting vector} is a closely-related concept that captures, in a nontrivial way, much of the underlying geometry of the original metric space. Recent work has demonstrated that when the metric space is Euclidean, the weighting vector serves as an effective tool for boundary detection. We recast this result and show the weighting vector may be viewed as a solution to a kernelized SVM. As one consequence, we apply this new insight to the task of outlier detection, and we demonstrate performance that is competitive or exceeds performance of state-of-the-art techniques on benchmark data sets. Under mild assumptions, we show the weighting vector, which has computational cost of matrix inversion, can be efficiently approximated in linear time. We show how nearest neighbor methods can approximate solutions to the minimization problems defined by SVMs. 0 into PostgreSQL...\n",
      "Inserting test sample 2446  This research paper delves into the use of weighting vectors in machine learning, specifically in the context of boundary detection. The authors incorporate numerical harmonic analysis techniques into the development of these weighting vectors. By doing so, they are able to better understand the underlying patterns and structures within the data, leading to improved accuracy in detecting boundaries. The paper provides a comprehensive overview of the methodologies and algorithms used in implementing these techniques, along with detailed experimental results demonstrating their effectiveness. Notably, the authors showcase the superiority of their approach over other commonly used methods, highlighting the importance of incorporating numerical harmonic analysis in machine learning applications. This work contributes to the broader scientific community by offering new perspectives and innovative solutions to longstanding challenges in the field of boundary detection. 1 into PostgreSQL...\n",
      "Inserting test sample 2447  This paper describes the unfolding of the solar modulated galactic cosmic ray H and He nuclei spectra beyond ~105 AU in the heliosheath. Between 2008.0 and 2012.3 when Voyager 1 went from about 105 to 120.5 AU the spectral intensities of these two components between about 30 and 500 MeV/nuc unfolded (increased) in a manner consistent with an average modulation potential decrease ~5 MV per AU as described by a Parker like cosmic ray transport in the heliosphere where the overall modulation is described by a modulation potential in MV. Between 120.5 and 121.7 AU, however, as a result of two sudden intensity increases starting on May 8th and August 25th, 2012, this modulation potential decreased by ~80 MV and spectra resembling possible local interstellar spectra for H and He were revealed. Considering these spectra to be the local interstellar spectra would imply that almost 1/3 of the total modulation potential of about 270 MV required to explain the spectra of these components observed at the Earth must occur in just a 1 AU radial interval in the outer heliosheath. As a result about ~80% of the total modulation potential observed at the Earth at this time occurs in the heliosheath itself. The remaining 20% of the total modulation occurs inside the heliospheric termination shock. The details of these intensity changes and their description by a simple modulation model are discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 2448  The Voyager 1 spacecraft has revolutionized our understanding of the heliosphere and the interstellar medium. As the spacecraft exits the region of heliospheric modulation, it provides a unique opportunity to study the spectra of low-energy galactic cosmic ray (GCR) H and He nuclei and to measure fundamental parameters of the local interstellar medium. In this paper, we present the unfolding of the spectra of low-energy GCR H and He nuclei measured by the Voyager 1 Low-Energy Charged Particle (LECP) instrument since it crossed the heliopause in 2012. By utilizing a novel unfolding technique, we obtain the pristine spectra of GCR H and He nuclei, free from the effects of solar modulation. The spectra observed by the Voyager 1 LECP instrument in the interstellar medium are consistent with previous observations, suggesting that GCRs undergo negligible energy losses and scattering in the heliosheath. Moreover, the observed energy spectra indicate that the heliosheath is not completely devoid of the solar wind, as previously believed. Our results shed new light on the properties of GCRs and the interstellar environment, and offer important implications for the future of space exploration. 1 into PostgreSQL...\n",
      "Inserting test sample 2449  This paper presents several strategies that can improve neural network-based predictive methods for MOOC student course trajectory modeling, applying multiple ideas previously applied to tackle NLP (Natural Language Processing) tasks. In particular, this paper investigates LSTM networks enhanced with two forms of regularization, along with the more recently introduced Transformer architecture. 0 into PostgreSQL...\n",
      "Inserting test sample 2450  This research paper explores the application of Natural Language Processing innovations in modeling the course trajectory of Massive Open Online Course (MOOC) students. By analyzing text-based data, such as forum posts and video captions, we aim to identify patterns in student behavior that indicate future outcomes, ultimately leading to improved course design and student success. 1 into PostgreSQL...\n",
      "Inserting test sample 2451  We combine Herschel-PACS data from the PEP program with Spitzer 24 um and 16 um photometry and ultra deep IRS mid-infrared spectra, to measure the mid- to far-infrared spectral energy distribution (SED) of 0.7<z<2.5 normal star forming galaxies around the main sequence (the redshift-dependent relation of star formation rate and stellar mass). Our deep data confirm from individual far-infrared detections that z~2 star formation rates are overestimated if based on 24 um fluxes and SED templates that are calibrated via local trends with luminosity. Galaxies with similar ratios of rest-frame nuLnu(8) to 8-1000 um infrared luminosity (LIR) tend to lie along lines of constant offset from the main sequence. We explore the relation between SED shape and offset in specific star formation rate (SSFR) from the redshift-dependent main sequence.\n",
      "\n",
      "Main sequence galaxies tend to have a similar nuLnu(8)/LIR regardless of LIR and redshift, up to z~2.5, and nuLnu(8)/LIR decreases with increasing offset above the main sequence in a consistent way at the studied redshifts. We provide a redshift-independent calibration of SED templates in the range of 8--60 um as a function of log(SSFR) offset from the main sequence. Redshift dependency enters only through the evolution of the main sequence with time.\n",
      "\n",
      "Ultra deep IRS spectra match these SED trends well and verify that they are mostly due to a change in ratio of PAH to LIR rather than continua of hidden AGN. Alternatively, we discuss the dependence of nuLnu(8)/LIR on LIR. Same nuLnu(8)/LIR is reached at increasingly higher LIR at higher redshift, with shifts relative to local by 0.5 and 0.8 dex in log(LIR) at redshifts z~1 and z~2. Corresponding SED template calibrations are provided for use if no stellar masses are in hand. For most of those z~2 star forming galaxies that also host an AGN, the mid-infrared is dominated by the star forming component. 0 into PostgreSQL...\n",
      "Inserting test sample 2452  Galaxies emit infrared radiation, which provides important information about their star formation activity. However, the relationship between infrared radiation and star formation rate estimates is not straightforward, due to the complex interplay of various galaxy properties. In this study, we investigate the impact of evolving infrared spectral energy distributions (SEDs) on star formation rate estimates for different types of galaxies. \n",
      "\n",
      "We first analyze a large sample of galaxies selected from the Herschel Space Observatory data to study the shape of their infrared SEDs, which varies depending on the temperature of the dust grains in the galaxy. We then use a set of models to simulate the evolution of the SEDs with different star formation histories and physical conditions. \n",
      "\n",
      "Our results show that the shape of the infrared SEDs has a significant impact on the accuracy of star formation rate estimates, particularly for galaxies with high rates of star formation. Galaxies with higher dust temperatures tend to have flatter SEDs, which can lead to overestimation of the star formation rate estimates if not properly taken into account. On the other hand, galaxies with lower dust temperatures have steeper SEDs, which can result in underestimation of the star formation rate estimates. \n",
      "\n",
      "Furthermore, our study reveals that different galaxy types and environments can also affect the accuracy of the star formation rate estimates. Elliptical galaxies, for instance, have different dust properties compared to spiral galaxies, leading to different SED shapes and thus different star formation rate estimates. Our simulations also show that the presence of a nearby companion galaxy can significantly affect the SED shape and the accuracy of the star formation rate estimates for both galaxies. \n",
      "\n",
      "In conclusion, our study highlights the importance of considering the evolving infrared SEDs of galaxies in accurately estimating their star formation rates. Our findings have implications for understanding the star formation history of the Universe and can inform future observational campaigns aiming to study galaxy formation and evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 2453  Consider nonlinear wave equations in the spatially flat Friedmann-Lema\\^itre-Robertson-Walker (FLRW) spacetimes. We show blow-up in finite time of solutions and upper bounds of the lifespan of blow-up solutions to give the FLRW spacetime version of Glassey's conjecture for the time derivative nonlinearity. We also show blow-up results for the space time derivative nonlinearity. 0 into PostgreSQL...\n",
      "Inserting test sample 2454  We prove Glassey's conjecture for spherically symmetric semilinear wave equations in Friedmann-LemaÃ®tre-Robertson-Walker spacetimes of dimension three. Our proof uses the null structure of the spacetime geometry, and applies energy estimates for derivative nonlinear wave equations along characteristic null directions. This result provides new insights into the behavior of semilinear wave equations in curved spacetimes and offers important implications for the study of gravitational fields in general relativity. 1 into PostgreSQL...\n",
      "Inserting test sample 2455  The European Southern Observatory's (ESO) Visible and Infrared Survey Telescope for Astronomy (VISTA) is a 4-m class survey telescope for wide-field near-infrared imaging. VISTA is currently running a suite of six public surveys, which will shortly deliver their first Europe wide public data releases to ESO. The VISTA Kilo-degree Infrared Galaxy Survey (VIKING) forms a natural intermediate between current wide shallow, and deeper more concentrated surveys, by targeting two patches totalling 1500 sq.deg in the northern and southern hemispheres with measured 5-sigma limiting depths of Z ~ 22.4, Y ~ 21.4, J ~ 20.9, H ~ 19.9 and Ks ~19.3 (Vega). This architecture forms an ideal working parameter space for the discovery of a significant sample of 6.5 <= z <= 7.5 quasars. In the first data release priority has been placed on small areas encompassing a number of fields well sampled at many wavelengths, thereby optimising science gains and synergy whilst ensuring a timely release of the first products. For rare object searches e.g. high-z quasars, this policy is not ideal since photometric selection strategies generally evolve considerably with the acquisition of data. Without a reasonably representative data set sampling many directions on the sky it is not clear how a rare object search can be conducted in a highly complete and efficient manner.\n",
      "\n",
      "In this paper, we alleviate this problem by supplementing initial data with a realistic model of the spatial, luminosity and colour distributions of sources known to heavily contaminate photometric quasar selection spaces, namely dwarf stars of spectral type M, L and T. We use this model along with a subset of available data to investigate contamination of quasar selection space by cool stars and galaxies and lay down a set of benchmark selection constraints that limit contamination to reasonable levels whilst maintaining high completeness... 0 into PostgreSQL...\n",
      "Inserting test sample 2456  The selection of high-redshift quasars, located at distances exceeding eight billion light years from the Earth, from multi-wavelength surveys is a challenging task. The VISTA Kilo-degree Infrared Galaxy survey (VIKING), which covers 1500 degÂ² across the South Galactic Cap, has been shown to be an efficient field for the selection of high-redshift quasars. This work focuses on the selection of high-redshift quasars from the VIKING dataset. We present methods for selecting quasar candidates based on their color properties, which allow us to separate quasars from stars and galaxies. A further selection criteria involves a variability analysis, that takes advantage of the time variability of the quasar emission as opposed to the non-varying emission from galaxies and stars. Using such methods, we are able to identify more than 400 quasar candidates over a 5000-degÂ² footprint. While those methods have shown to be successful, there are still challenges to overcome, such as issues related to confusion with brown dwarfs, misclassifications due to assumptions made in photometry and the redshift distribution of the quasar population. With the advent of new surveys such as LSST and Euclid, which will allow for quasar selection out to even higher redshifts, understanding the selection biases and limitations of current methods is essential. Our results show the selection criteria used on VIKING data can be used on future surveys, ultimately increasing the number of quasars identified and advancing our understanding of the early universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2457  We report on the detection of a planetary companion in orbit around the primary star of the binary system $\\gamma$ Cephei. High precision radial velocity measurements using 4 independent data sets spanning the time interval 1981--2002 reveal long-lived residual radial velocity variations superimposed on the binary orbit that are coherent in phase and amplitude with a period or 2.48 years (906 days) and a semi-amplitude of 27.5 m s$^{-1}$. We performed a careful analysis of our Ca II H & K S-index measurements, spectral line bisectors, and {\\it Hipparcos} photometry. We found no significant variations in these quantities with the 906-d period. We also re-analyzed the Ca II $\\lambda$8662 {\\AA} measurements of Walker et al. (1992) which showed possible periodic variations with the ``planet'' period when first published. This analysis shows that periodic Ca II equivalent width variations were only present during 1986.5 -- 1992 and absent during 1981--1986.5. Furthermore, a refined period for the Ca II $\\lambda$8662 {\\AA} variations is 2.14 yrs, significantly less than residual radial velocity period. The most likely explanation of the residual radial velocity variations is a planetary mass companion with $M$ sin $i$ = 1.7 $M_{Jupiter}$ and an orbital semi-major axis of $a_2$ $=$ 2.13 AU. This supports the planet hypothesis for the residual radial velocity variations for $\\gamma$ Cep first suggested by Walker et al.\n",
      "\n",
      "(1992). With an estimated binary orbital period of 57 years $\\gamma$ Cep is the shortest period binary system in which an extrasolar planet has been found.\n",
      "\n",
      "This system may provide insights into the relationship between planetary and binary star formation. 0 into PostgreSQL...\n",
      "Inserting test sample 2458  In this study, we report the discovery of a planetary companion to gamma Cephei A, a single G-type main sequence star located approximately 45 light-years from Earth. The data used for this analysis was obtained from the High Accuracy Radial velocity Planet Searcher (HARPS) at the La Silla Observatory in Chile. Our observations revealed a periodicity in the radial velocity variations of the host star with a semi-amplitude of about 60 meters per second, which is indicative of the presence of a planet with a mass of at least 1.7 Jupiter masses and an orbital period of 905.6 days.\n",
      "\n",
      "We further investigated the system using the NIRC2 adaptive optics instrument attached to the 10-meter Keck II telescope in Hawaii. By analyzing the astrometric measurements, we were able to constrain the inclination of the planet's orbit to be less than 30 degrees and estimate the planet's minimum mass to be 1.88 Jupiter masses.\n",
      "\n",
      "The discovery of this planetary companion to gamma Cephei A adds to the growing list of extrasolar planets found around nearby stars and provides valuable insights into the formation and evolution of planetary systems. With its moderate mass and distance from its host star, this planet is in a prime location for future atmospheric characterization studies using the James Webb Space Telescope and other upcoming observatories. This discovery also underscores the potential of HARPS and other precision radial velocity surveys to uncover new exoplanets, including potentially habitable ones, in our cosmic neighborhood. 1 into PostgreSQL...\n",
      "Inserting test sample 2459  Whenever a person hears about pollution, more often than not, the first thought that comes to their mind is air pollution. One of the most under-mentioned and under-discussed pollution globally is that caused by the non-biodegradable waste in our water bodies. In the case of India, there is a lot of plastic waste on the surface of rivers and lakes. The Ganga river is one of the 10 rivers which account for 90 percent of the plastic that ends up in the sea and there are major cases of local nalaas and lakes being contaminated due to this waste. This limits the source of clean water which leads to major depletion in water sources. From 2001 to 2012, in the city of Hyderabad, 3245 hectares of lakes dissipated. The water recedes by nine feet a year on average in southern New Delhi. Thus, cleaning of these local water bodies and rivers is of utmost importance. Our aim is to develop a water surface cleaning bot that is deployed across the shore. The bot will detect garbage patches on its way and collect the garbage thus making the water bodies clean. This solution employs a surveillance mechanism in order to alert the authorities in case anyone is found polluting the water bodies. A more sustainable system by using solar energy to power the system has been developed. Computer vision algorithms are used for detecting trash on the surface of the water. This trash is collected by the bot and is disposed of at a designated location. In addition to cleaning the water bodies, preventive measures have been also implemented with the help of a virtual fencing algorithm that alerts the authorities if anyone tries to pollute the water premises. A web application and a mobile app is deployed to keep a check on the movement of the bot and shore surveillance respectively. This complete solution involves both preventive and curative measures that are required for water care. 0 into PostgreSQL...\n",
      "Inserting test sample 2460  The quality of water in lakes, rivers, and other bodies of water cannot be overemphasized as it serves as a primary source of life, and human and animal sustenance. However, natural and human-related activities can degrade water quality, leading to pollution and harmful algal blooms. One way to mitigate these risks is through the use of water surface cleaning bots equipped with advanced sensors that detect pollutants like microplastics and oil spills. The use of such devices ensures that water bodies maintain their natural state and are not a threat to the ecosystem or human well-being. In addition, a water body surveillance system that continuously monitors water quality in both rural and urban areas is of utmost importance. This system can integrate various remote sensors, digital imaging, and computing technologies to detect water quality and notify relevant environmental agencies when water becomes contaminated. In situations where water is also used for drinking purposes, the surveillance system can provide near-instantaneous real-time alerts to enable the necessary steps to be taken to prevent an outbreak of waterborne diseases.\n",
      "\n",
      "The implementation of the water care technology will require collaboration, investment, and expertise from government and non-government organizations, private and public institutions, researchers, and policymakers. There should also be guidelines and regulations to ensure that water care technologies are used correctly to prevent unintended consequences. In conclusion, the use of water surface cleaning bots and surveillance systems represent significant strides towards maintaining and ensuring the quality of water bodies. They will contribute significantly to securing healthy ecosystems, preventing the spread of waterborne diseases, protecting wildlife, and enhancing the general quality of life for humans. 1 into PostgreSQL...\n",
      "Inserting test sample 2461  Automatic mesh-based shape generation is of great interest across a wide range of disciplines, from industrial design to gaming, computer graphics and various other forms of digital art. While most traditional methods focus on primitive based model generation, advances in deep learning made it possible to learn 3-dimensional geometric shape representations in an end-to-end manner.\n",
      "\n",
      "However, most current deep learning based frameworks focus on the representation and generation of voxel and point-cloud based shapes, making it not directly applicable to design and graphics communities. This study addresses the needs for automatic generation of mesh-based geometries, and propose a novel framework that utilizes signed distance function representation that generates detail preserving three-dimensional surface mesh by a deep learning based approach. 0 into PostgreSQL...\n",
      "Inserting test sample 2462  This research paper presents a novel method for generating complex 3D shapes by incorporating a hierarchical mesh-based approach with a 3D Generative Adversarial Network (GAN). The proposed algorithm utilizes a hierarchical representation of shapes on varying levels of detail, which enhances the generation process and enables the production of highly detailed shapes. By training the GAN on the hierarchical mesh-based representation, the generator network is able to learn the underlying structure of the shapes, producing high-quality and detailed 3D models. Moreover, we validate our approach with a series of experiments, demonstrating that our method outperforms state-of-the-art techniques in terms of visual quality and geometric fidelity. These results indicate the potential of our technique for applications in various areas, such as computer graphics, robotics, and architecture design. 1 into PostgreSQL...\n",
      "Inserting test sample 2463  Single-mode squeezing and Fourier transformation operations are two essential logical gates in continuous-variable quantum computation, which have been experimentally implemented by means of an optical four-mode cluster state. In this paper, we present a simpler and more efficient protocol based on the use of Einstein-Podolsky-Rosen two-mode entangled states to realize the same operations. The theoretical calculations and the experimental results demonstrate that the presented scheme not only decreases the requirement to the resource quantum states at the largest extent but also enhances significantly the squeezing degree and the fidelity of the resultant modes under an identical resource condition. That is because in our system the influence of the excess noises deriving from the imperfect squeezing of the resource states is degraded. The gate operations applying two-mode entanglement can be utilized as a basic element in a future quantum computer involving a large-scale cluster state. 0 into PostgreSQL...\n",
      "Inserting test sample 2464  The implementation of a gate that enables one-way quantum computation based on Einstein-Podolsky-Rosen (EPR) entanglement is presented. This gate utilizes a technique that exploits the modularity of two-qubit unitary operations, enabling the construction of complex circuits. By introducing a single-qubit measurement in the register that contains the control qubit of the EPR pair, the proposed gate performs a probabilistic transition for the target qubit, projecting it into the desired state. The efficiency of the gate is analyzed using measures such as diamond distance and average gate fidelity, and compared to other existing architectures. The gate shows high performance in terms of quantum operations, and can be applied to various quantum computation problems. The proposed architecture provides a promising avenue towards the realization of universal quantum computation using EPR entangled qubits. 1 into PostgreSQL...\n",
      "Inserting test sample 2465  The paper proposes a novel approach for gray scale images segmentation. It is based on multiple features extraction from single feature per image pixel, namely its intensity value, using Echo state network. The newly extracted features -- reservoir equilibrium states -- reveal hidden image characteristics that improve its segmentation via a clustering algorithm. Moreover, it was demonstrated that the intrinsic plasticity tuning of reservoir fits its equilibrium states to the original image intensity distribution thus allowing for its better segmentation. The proposed approach is tested on the benchmark image Lena. 0 into PostgreSQL...\n",
      "Inserting test sample 2466  Segmentation of gray images is a challenging task due to the complexity of the images. Reservoir computing can be a valuable approach for such segmentation tasks. In this work, we propose a reservoir computing-based framework for gray images segmentation. Our method exploits the dynamic properties of reservoir computing to capture the nonlinear relationships between the image pixels. Experimental results demonstrate that our proposed approach achieves state-of-the-art performance in segmentation accuracy compared to traditional methods. 1 into PostgreSQL...\n",
      "Inserting test sample 2467  The black hole transient GRS 1716-249 was monitored from the radio to the gamma-ray band during its 2016-2017 outburst. This paper focuses on the Spectral Energy Distribution (SED) obtained in 2017 February-March, when GRS 1716-249 was in a bright hard spectral state. The soft gamma-ray data collected with the INTEGRAL/SPI telescope show the presence of a spectral component which is in excess of the thermal Comptonisation emission. This component is usually interpreted as inverse Compton emission from a tiny fraction of non-thermal electrons in the X-ray corona. We find that hybrid thermal/non-thermal Comptonisation models provide a good fit to the X/gamma-ray spectrum of GRS 1716-249. The best-fit parameters are typical of the bright hard state spectra observed in other black hole X-ray binaries. Moreover, the magnetised hybrid Comptonisation model BELM provides an upper limit on the intensity of the coronal magnetic field of about 1E+06 G. Alternatively, this soft gamma-ray emission could originate from synchrotron emission in the radio jet. In order to test this hypothesis, we fit the SED with the irradiated disc plus Comptonisation model combined with the jet internal shock emission model ISHEM.\n",
      "\n",
      "We found that a jet with an electron distribution of p~2.1 can reproduce the soft gamma-ray emission of GRS 1716-249. However, if we introduce the expected cooling break around 10 keV, the jet model can no longer explain the observed soft gamma-ray emission, unless the index of the electron energy distribution is significantly harder (p<2). 0 into PostgreSQL...\n",
      "Inserting test sample 2468  The hard state of black hole transients has attracted great interest over the years, but the nature of the soft gamma-ray emission (SGE) in this state is still a matter of debate. In this paper, we present an in-depth analysis of the SGE in the hard state of the black hole transient GRS 1716-249. The study is based on an extensive set of observations collected by the Rossi X-ray Timing Explorer and the Swift observatory during the 2008 outburst of the source.\n",
      "\n",
      "Our results show that the SGE in the hard state of GRS 1716-249 is best described by a Comptonization model with an additional blackbody component. We find that the SGE is likely produced in the corona, which has a temperature of about 120 keV and an optical depth of about 5. This suggests that the corona is relatively hot and optically thick.\n",
      "\n",
      "We also find evidence for a correlation between the SGE and the power-law component of the X-ray emission, which is consistent with the idea that the SGE is produced by the same population of electrons that produce the X-rays. Finally, we discuss the implications of our findings for the understanding of coronal physics in accreting black holes, and we highlight the importance of future observations to better constrain the SGE in the hard state of black hole transients. 1 into PostgreSQL...\n",
      "Inserting test sample 2469  We aim to develop a nuclear energy density functional that can be simultaneously applied to finite nuclei and neutron stars. We use the self-consistent nuclear density functional theory (DFT) with Skyrme energy density functionals and covariance analysis to assess correlations between observables for finite nuclei and neutron stars. In a first step two energy functionals -- a high density energy functional giving reasonable neutron properties, and a low density functional fitted to nuclear properties -- are matched. In a second step, we optimize a new functional using exactly the same protocol as in earlier studies pertaining to nuclei but now including neutron star data. This allows direct comparisons of performance of the new functional relative to the standard one. The new functional TOV-min yields results for nuclear bulk properties (energy, r.m.s. radius, diffraction radius, surface thickness) that are of the same quality as those obtained with the established Skyrme functionals, including SV-min. When comparing SV-min and TOV-min, isoscalar nuclear matter indicators vary slightly while isovector properties are changed considerably. We discuss neutron skins, dipole polarizability, separation energies of the heaviest elements, and proton and neutron drip lines. We confirm a correlation between the neutron skin of $^{208}$Pb and the neutron star radius. We demonstrate that standard energy density functionals optimized to nuclear data do not carry information on the expected maximum neutron star mass, and that predictions can only be made within an extremely broad uncertainty band. For atomic nuclei, the new functional TOV-min performs at least as well as the standard nuclear functionals, but it also reproduces expected neutron star data within assumed error bands. 0 into PostgreSQL...\n",
      "Inserting test sample 2470  The study of nuclear matter is essential for understanding the fundamental properties and behavior of atomic nuclei and the composition of neutron stars. The Energy Density Functional (EDF) approach has proven to be a valuable tool in the investigation of these complex systems. In this paper, we present an overview of recent developments in EDF models for both nuclei and neutron stars.\n",
      "\n",
      "We begin by discussing the theoretical foundations of EDF models, which are based on the density functional theory (DFT) and the Hartree-Fock-Bogoliubov (HFB) framework. This framework allows us to systematically include various effects, such as pairing correlations and spin-orbit coupling, that are crucial for a proper description of the properties of atomic nuclei and neutron stars.\n",
      "\n",
      "We then describe the various EDF models that have been developed for nuclei, including Skyrme, Gogny, and Relativistic Mean Field (RMF) models. These models have different strengths and weaknesses, and we discuss their applications to various observables, such as nuclear masses, radii, and excitation spectra.\n",
      "\n",
      "Finally, we turn our attention to EDF models for neutron stars, which require a description of the strong nuclear force at extremely high densities. We discuss recent progress in developing EDF models for neutron stars, including the use of chiral effective field theory and the inclusion of hyperons and quarks in the equation of state.\n",
      "\n",
      "Overall, this paper provides an up-to-date overview of the state-of-the-art in EDF models for nuclei and neutron stars. Our hope is that it will serve as a useful guide for researchers working in this field and provide insight into the future directions of this fascinating and important area of nuclear physics. 1 into PostgreSQL...\n",
      "Inserting test sample 2471  Spiking Neural Networks (SNNs) are a promising paradigm for efficient event-driven processing of spatio-temporally sparse data streams. SNNs have inspired the design and can take advantage of the emerging class of neuromorphic processors like Intel Loihi. These novel hardware architectures expose a variety of constraints that affect firmware, compiler and algorithm development alike. To enable rapid and flexible development of SNN algorithms on Loihi, we developed NxTF: a programming interface derived from Keras and compiler optimized for mapping deep convolutional SNNs to the multi-core Intel Loihi architecture. We evaluate NxTF on DNNs trained directly on spikes as well as models converted from traditional DNNs, processing both sparse event-based and dense frame-based data sets. Further, we assess the effectiveness of the compiler to distribute models across a large number of cores and to compress models by exploiting Loihi's weight sharing features. Finally, we evaluate model accuracy, energy and time to solution compared to other architectures.\n",
      "\n",
      "The compiler achieves near optimal resource utilization of 80% across 16 Loihi chips for a 28-layer, 4M parameter MobileNet model with input size 128x128. In addition, we report the lowest error rate of 8.52% for the CIFAR-10 dataset on neuromorphic hardware, using an off-the-shelf MobileNet. 0 into PostgreSQL...\n",
      "Inserting test sample 2472  Neuromorphic computing is a promising approach for accelerating the simulation of spiking neural networks (SNNs). In this paper, we present NxTF, an open-source software package that provides an application programming interface (API) and compiler for building deep SNNs on Intel's Loihi neuromorphic chip. NxTF supports the TensorFlow framework, enabling users to take advantage of its high-level abstractions for deep learning. The compiler uses Loihi's spiking neural network architecture to generate efficient code that maximizes the performance and energy efficiency of the hardware. \n",
      "\n",
      "We evaluate NxTF on various benchmark datasets and compare its performance with that of existing SNN frameworks. Compared to other SNN tools, NxTF demonstrates significant improvements in terms of accuracy, performance, and energy efficiency. Furthermore, NxTF provides a straightforward migration path for existing deep learning models, allowing users to leverage the advantages of SNNs without the need for significant code changes.\n",
      "\n",
      "Overall, NxTF provides a new, efficient tool for building deep SNNs on Loihi hardware. This software package has the potential to enable researchers and engineers to accelerate their research and development of neuromorphic applications, for instance in the field of robotics, where SNNs are becoming increasingly popular due to their ability to handle spike-based sensor data. 1 into PostgreSQL...\n",
      "Inserting test sample 2473  There are several key open questions as to the nature and origin of AGN including: 1) what initiates the active phase, 2) the duration of the active phase, and 3) the effect of the AGN on the host galaxy. Critical new insights to these can be achieved by probing the central regions of AGN with sub-mas angular resolution at UV/optical wavelengths. In particular, such observations would enable us to constrain the energetics of the AGN \"feedback\" mechanism, which is critical for understanding the role of AGN in galaxy formation and evolution. These observations can only be obtained by long-baseline interferometers or sparse aperture telescopes in space, since the aperture diameters required are in excess of 500 m - a regime in which monolithic or segmented designs are not and will not be feasible and because these observations require the detection of faint emission near the bright unresolved continuum source, which is impossible from the ground, even with adaptive optics. Two mission concepts which could provide these invaluable observations are NASA's Stellar Imager (SI; Carpenter et al. 2008 & http://hires.gsfc.nasa.gov/si/) interferometer and ESA's Luciola (Labeyrie 2008) sparse aperture hypertelescope. 0 into PostgreSQL...\n",
      "Inserting test sample 2474  Active Galactic Nuclei (AGN) are powerful sources of radiation that arise from the accretion of gas in the centers of galaxies. They play a fundamental role in the formation and evolution of galaxies, influencing the surrounding gas and stars through various feedback mechanisms. In this review, we summarize the current state of knowledge on AGN and their impact on their host galaxies. We discuss the observed properties of AGN and their correlation with the properties of their host galaxies, including their mass, star formation rate, and morphology. We also review the physical mechanisms responsible for the AGN feedback, including radiative and mechanical feedback, and the observational evidence for their effects on the surrounding gas and stars. Finally, we discuss the implications of AGN feedback for the build-up of galactic bulges, the regulation of star formation, and the growth of supermassive black holes. Our review highlights the importance of AGN in shaping the properties of galaxies and demonstrates the need for further investigation of their role in galactic evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 2475  For a compact connected Lie group $G$ we study the class of bi-invariant affine connections whose geodesics through $e\\in G$ are the 1-parameter subgroups. We show that the bi-invariant affine connections which induce derivations on the corresponding Lie algebra $\\frak{g}$ coincide with the bi-invariant metric connections. Next we describe the geometry of a naturally reductive space $(M=G/K, g)$ endowed with a family of $G$-invariant connections $\\nabla^{\\alpha}$ whose torsion is a multiple of the torsion of the canonical connection $\\nabla^{c}$. For the spheres ${\\rm S}^{6}$ and ${\\rm S}^{7}$ we prove that the space of ${\\rm G}_2$ (resp. ${\\rm Spin}(7)$)-invariant affine or metric connections consists of the family $\\nabla^{\\alpha}$. Then we examine the \"constancy\" of the induced Ricci tensor ${\\rm Ric}^{\\alpha}$ and prove that any compact simply-connected isotropy irreducible standard homogeneous Riemannian manifold, which is not a symmetric space of Type I, is a $\\nabla^{\\alpha}$-Einstein manifold for any $\\alpha\\in\\mathbb{R}$. We also provide examples of $\\nabla^{\\pm 1}$-Einstein structures for a class of compact homogeneous spaces $M=G/K$ with two isotropy summands. 0 into PostgreSQL...\n",
      "Inserting test sample 2476  This paper explores the concept of invariant connections with skew-torsion on $\\nabla$-Einstein manifolds. More specifically, we investigate the properties of the skew-torsion on the geometric structure of the manifold and the consequences of these properties on the corresponding connection. We focus on studying the existence and uniqueness of such connections and their relation to the $\\nabla$-Einstein equation. Our main results show that invariant connections with skew-torsion can be constructed on $\\nabla$-Einstein manifolds of certain types using a specific twisting endomorphism, and that these connections satisfy the skew-torsion equation. Moreover, we show that the skew-torsion is related to the Weyl tensor of the manifold. We also derive formulas for the scalar curvature and the Einstein-Hilbert functional for such connections and analyze their behavior under conformal transformations. Our results extend previous work on the topic and shed new light on the relationship between geometry and physics in higher dimensions. 1 into PostgreSQL...\n",
      "Inserting test sample 2477  Results from the UV satellite GALEX revealed large extensions of disks in some nearby spiral galaxies, extending out to 3 to 4 times the isophotal radius, r25. M63 is a remarkable example of a spiral galaxy with one of the most extended UV disks, so it offers the opportunity to search for the molecular gas and characterize the star formation in outer disk regions as revealed by the UV emission. We obtained deep CO(1-0) and CO(2-1) observations on the IRAM 30 m telescope along the major axis of the M63 disk from the center out to the galactocentric radius rgal = 1.6 r25 and over a bright UV region at rgal = 1.36 r25. CO(1-0) is detected all along the M63 major axis out to r25, and CO(2-1) is confined to rgal = 0.68 r25, which may betray lower excitation temperatures in the outer disk. CO(1-0) is also detected in the external bright UV region of M63. The radial profiles of the CO emission and of the Halpha, 24 micron, NUV and FUV star formation tracers and HI taken from the literature show a severe drop with the galactocentric radius, such that beyond r25 they are all absent with the exception of a faint UV emission and HI. The CO emission detection in the external UV region, where the UV flux is higher than the UV flux observed beyond r25, highlights a tight correlation between the CO and UV fluxes, namely the amount of molecular gas and the intensity of star formation. This external UV region is dominated by the atomic gas, suggesting that HI is more likely the precursor of H2 rather than the product of UV photodissociation. A broken power law needs to be invoked to describe the Kennicutt-Schmidt (K-S) relation of M63 from the center of the galaxy out to rgal = 1.36 r25. While all along the major axis out to r25 the K-S relation is almost linear, in the external UV region the SFR regime is highly nonlinear and characterized by a steep K-S relation and very low star formation efficiency. 0 into PostgreSQL...\n",
      "Inserting test sample 2478  This research paper delves into the extended UV disk of M63 by examining the CO map and the Kennicutt-Schmidt relation. With the help of the IRAM 30-m telescope, CO(1-0) observations were conducted, revealing the molecular gas distribution in M63's extended UV disk. The Kennicutt-Schmidt relation was then applied to these observations, resulting in an unexpectedly steep correlation between the molecular gas surface density and the star formation rate surface density.\n",
      "\n",
      "This study provides new insights into the star formation processes within the extended UV disk of M63. The steep Kennicutt-Schmidt relation suggests that the star formation in this region is dominated by gravitational instabilities within the molecular gas. The relatively high fraction of molecular gas in this region, compared to the Milky Way, could be the reason behind the observed steep correlation.\n",
      "\n",
      "Additionally, a radial gradient in the molecular gas distribution was observed, with the gas density and fraction increasing towards the center of the disk. The results support the notion that gravitational instabilities are more dominant in the dense regions, leading to increased star formation rates. The observed radial gradient could be explained by the inward migration of the gas, caused by bar-driven gas inflows.\n",
      "\n",
      "In conclusion, this research provides important insights into the star formation processes within the extended UV disk of M63, revealing a steep Kennicutt-Schmidt relation dominated by gravitational instabilities within the molecular gas. The observed radial gradient in the molecular gas distribution supports the notion of bar-driven gas inflows and highlights the importance of studying the molecular gas distribution in galaxies to gain a better understanding of their star formation processes. 1 into PostgreSQL...\n",
      "Inserting test sample 2479  Let $G=(V,E)$ be a simple graph with maximum degree $d$. For an integer $k\\in\\mathbb{N}$, the $k$-disc of a vertex $v\\in V$ is defined as the rooted subgraph of $G$ that is induced by all vertices whose distance to $v$ is at most $k$. The $k$-disc frequency distribution vector of $G$, denoted by $\\text{freq}_{k}(G)$, is a vector indexed by all isomorphism types of rooted $k$-discs. For each such isomorphism type $\\Gamma$, the corresponding entry in $\\text{freq}_{k}(G)$ counts the fraction of vertices in $V$ that have a $k$-disc isomorphic to $\\Gamma$. In a sense, $\\text{freq}_{k}(G)$ is one way to represent the \"local structure\" of $G$. The graph $G$ can be arbitrarily large, and so a natural question is whether given $\\text{freq}_{k}(G)$ it is possible to construct a small graph $H$, whose size is independent of $|V|$, such that $H$ has a similar local structure. N. Alon proved that for any $\\epsilon>0$ there always exists a graph $H$ whose size is independent of $|V|$ and whose frequency vector satisfies $||\\text{freq}_{k}(G)-\\text{freq}_{k}(H)||_{1}\\le\\epsilon$. However, his proof is only existential and does not imply that there is a deterministic algorithm to construct such a graph $H$. He gave the open problem of finding an explicit deterministic algorithm that finds $H$, or proving that no such algorithm exists. Our main result is that Alon's problem is undecidable if and only if a much more general problem (involving directed edges and edge colors) is undecidable. We also prove that both problems are decidable for the special case when $G$ is a path. We show that the local structure of any directed edge-colored path $G$ can be approximated by a suitable fixed-size directed edge-colored path $H$ and we give explicit bound on the size of $H$. 0 into PostgreSQL...\n",
      "Inserting test sample 2480  Graph theory is a vital tool in mathematics and computer science, providing a fundamental framework for modeling real-life phenomena such as social networks, transportation systems, and biological structures. One particularly interesting aspect of graph theory is the study of the local structure of graphs with bounded degrees. In this paper, we present a comprehensive analysis of the local structure of bounded degree graphs and investigate their properties. \n",
      "\n",
      "First, we introduce the concept of bounded degree graphs and provide a detailed overview of their defining characteristics. We then examine the local structure of these graphs with a specific focus on their neighborhoods. Our analysis includes an exploration of various parameters such as the size and density of neighborhoods, the diameter of neighborhood clusters, and the distribution of vertices in neighborhoods of different sizes. \n",
      "\n",
      "We also investigate the relationship between the local structures of bounded degree graphs and their global properties. Our research reveals that the local structures of these graphs have a significant impact on their global connective properties, such as their expansion and sparsity. We demonstrate the importance of these findings through a series of computational experiments, which highlight the practical implications of our results. \n",
      "\n",
      "Furthermore, we analyze the implications of our research on a wide range of scientific disciplines, including computer science, physics, and biology. In particular, we show how the local structure of bounded degree graphs can shed light on various real-world problems, such as network resilience, disease spread, and structural stability. \n",
      "\n",
      "In conclusion, our research provides a comprehensive analysis of the local structure of bounded degree graphs and demonstrates its relevance to a wide range of scientific disciplines. It paves the way for further investigations into the properties of these graphs and their application to real-world problems. 1 into PostgreSQL...\n",
      "Inserting test sample 2481  The zero point of the reddening toward the Large Magellanic Cloud (LMC) has been the subject of some dispute. Its uncertainty propagates as a systematic error for methods which measure the extragalactic distance scale through knowledge of the absolute extinction of LMC stars. In an effort to resolve this issue, we used three different methods to calibrate the most widely-used metric to predict LMC extinction, the intrinsic color of the red clump, $(V-I)_{RC,0}$, for the inner $\\sim$3 degrees of that galaxy. The first approach was to empirically calibrate the color zeropoints of the BaSTI isochrones over a wide metallicity range of ${\\Delta}\\rm{[Fe/H]} \\approx 1.10$ using measurements of red clump stars in 47 Tuc, the Solar Neighborhood, and NGC 6791.\n",
      "\n",
      "From these efforts we also measure these properties of the Solar Neighborhood red clump, ($V-I$, $G_{BP}-K_{s}$, $G-K_{s}$, $G_{RP}-K_{s}$, $J-K_{s}$, $H-K_{s}$, $M_{I}$, $M_{Ks}$)$_{RC,0} =$ (1.02, 2.75, 2.18, 1.52, 0.64, 0.15, $-$0.23, $-$1.63). The second and third methods were to compare the observed colors of the red clump to those of Cepheids and RR Lyrae in the LMC. With these three methods, we estimated the intrinsic color of the red clump of the LMC to be $(V-I)_{RC,0,\\rm{LMC}} = \\{ \\approx 0.93,0.91 \\pm 0.02,0.89 \\pm 0.02\\}$ respectively, and similarly using the first and third method we estimated $ (V-I)_{RC,0,\\rm{SMC}} = \\{\\approx 0.85,0.84 \\pm 0.02 \\}$ respectively for the Small Magellanic Cloud. We estimate the luminosities to be $M_{I,RC,\\rm{LMC}}=-0.26$ and $M_{I,RC,\\rm{SMC}}=-0.37$. We show that this has important implications for recent calibrations of the tip of the red giant branch in the Magellanic Clouds used to measure $H_0$. 0 into PostgreSQL...\n",
      "Inserting test sample 2482  The Color-Metallicity Relation (CMR) of the Red Clump (RC) is a widely used tool to study the metallicities of stellar systems. In this work, we present a study of the CMR and the reddening of the Magellanic Clouds. Using photometric data from the VISTA survey, we obtained a sample of RC stars in the Clouds' fields. We used a color-temperature relation, based on Gaia DR2 data, to estimate their effective temperatures and find their metallicities using the CMR. \n",
      "\n",
      "We found that the Magellanic Clouds' CMR shows significant deviations from the solar composition one. By comparing with theoretical models, we derived a metallicity gradient along the Clouds' bar, as well as a radial metallicity gradient. These results are consistent with previous studies, indicating that chemical evolution and dynamical effects have shaped the Magellanic Clouds' metallicity distribution. \n",
      "\n",
      "We also analyzed the reddening affecting our sample, finding that most of it is of interstellar origin. We estimated the mean reddening values toward the Clouds' fields and compared it with values from previous works, finding consistent results. Moreover, we found evidence of patchy reddening, particularly in the outer regions of the Clouds. \n",
      "\n",
      "Our results provide new insights into the Magellanic Clouds' chemical evolution and the impact of their dynamical history. Furthermore, we show that the CMR of the RC is a powerful tool to study the metallicities of stellar systems, even in the case of intermediate-age populations, such as those found in the Magellanic Clouds. Finally, our study of the reddening confirms the complexity of this effect, which needs to be carefully accounted for in the analysis of stellar populations in the Milky Way, the Magellanic Clouds, and other galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 2483  We address the dynamics of quantum correlations in continuous variable open systems and analyze the evolution of bipartite Gaussian states in independent noisy channels. In particular, upon introducing the notion of dynamical path through a suitable parametrization for symmetric states, we focus attention on phenomena that are common to Markovian and non-Markovian Gaussian maps under the assumptions of weak coupling and secular approximation. We found that the dynamical paths in the parameter space are universal, that is they do depend only on the initial state and on the effective temperature of the environment, with non Markovianity that manifests itself in the velocity of running over a given path. This phenomenon allows one to map non-Markovian processes onto Markovian ones and it may reduce the number of parameters needed to study a dynamical process, e.g. it may be exploited to build constants of motions valid for both Markovian and non-Markovian maps. Universality is also observed in the value of Gaussian discord at the separability threshold, which itself is a function of the sole initial conditions in the limit of high temperature. We also prove the existence of excluded regions in the parameter space, i.e. of sets of states which cannot be linked by any Gaussian dynamical map. 0 into PostgreSQL...\n",
      "Inserting test sample 2484  The study of dynamical paths and universality in continuous variables open systems has been a fundamental problem in modern physics. This research paper investigates the behavior of such systems under the influence of external perturbations and the emergence of universal properties. We present a comprehensive analysis of the dynamics of open systems via the quantum master equation and the corresponding phase-space equations for the Wigner function. The analysis leads to a deep understanding of dynamical paths and their connection to the long-time behavior of the system. We show that the universal properties of the system can be determined by the existence of fixed points in the phase space and their corresponding basins of attraction. We demonstrate the validity of our theoretical approach by applying it to a variety of physical systems, including the quantum Brownian motion and the harmonic oscillator. Our results offer a new perspective on the dynamical behavior of open systems and their universal properties, with potential applications to the fields of quantum information, condensed matter physics, and statistical mechanics. 1 into PostgreSQL...\n",
      "Inserting test sample 2485  We study the performance of the D-Wave 2X quantum annealing machine on systems with well-controlled ground-state degeneracy. While obtaining the ground state of a spin-glass benchmark instance represents a difficult task, the gold standard for any optimization algorithm or machine is to sample all solutions that minimize the Hamiltonian with more or less equal probability.\n",
      "\n",
      "Our results show that while naive transverse-field quantum annealing on the D-Wave 2X device can find the ground-state energy of the problems, it is not well suited in identifying all degenerate ground-state configurations associated to a particular instance. Even worse, some states are exponentially suppressed, in agreement with previous studies on toy model problems [New J.\n",
      "\n",
      "Phys. 11, 073021 (2009)]. These results suggest that more complex driving Hamiltonians are needed in future quantum annealing machines to ensure a fair sampling of the ground-state manifold. 0 into PostgreSQL...\n",
      "Inserting test sample 2486  Quantum Annealing Machines (QAMs) are devices that can solve optimization problems using quantum effects. One important task when using QAMs is to sample the ground state with high accuracy. In this work, we propose and analyze a method to exponentially bias the probability distribution of the ground state, which reduces the error rate and increases the success probability. Our method uses transverse-field driving Hamiltonians (TFDHs), which create energy barriers between excited states and the ground state. We show that our approach outperforms other state-of-the-art methods on several benchmark problems. Additionally, we provide a theoretical interpretation of our results by considering the spectral gap of the TFDH. By introducing exponentially-biased sampling, our work opens possibilities for more efficient and trustworthy quantum annealing-based algorithms. 1 into PostgreSQL...\n",
      "Inserting test sample 2487  Bug tracking tools are vital for managing bugs in any open source as well as proprietary commercial projects. Considering the significance of using an appropriate bug tracking tool, we assess the features offered by 31 open source bug tracking tools and their significance of usage in open source projects. We have categorized these tools into different classes based on their features. We have also conducted a developer survey by working with open source software practitioners to understand the effectiveness of these tools in their day-to-day software development. We also explored StackOverFlow - a developer Q & A forum to understand the developer experiences and challenges while using open source bug tracking tools. Our observations generated encouraging results that can used as a recommendation guide for open source software community to choose the best bug tracking tool based on their functional needs.\n",
      "\n",
      "Additionally, we have identified few features that are needed but not offered by most of these bug tracking tools. 0 into PostgreSQL...\n",
      "Inserting test sample 2488  In today's software development industry, bug tracking tools are pivotal in ensuring product delivery in a timely and quality-centric manner. Numerous open-source bug tracking tools are available, however, the suitability of one over the other depends on various factors such as cost, features, and scalability. This study aims to evaluate the suitability of open-source bug tracking tools based on the requirements of their individualized projects. The study adopts a mixed-methods research design which entails using survey and interviews to collect qualitative data from software developers and managers. The results of the study reveal that while several tools are suitable for different projects, none of the tools examined in this study satisfies all criteria. We recommend that software development teams analyze their specific requirements, conduct thorough research, and prioritize their needs when selecting a bug tracking tool, taking into consideration factors such as usability, extensibility, and flexibility. 1 into PostgreSQL...\n",
      "Inserting test sample 2489  There are several cutting edge applications needing PCA methods for data on tori and we propose a novel torus-PCA method with important properties that can be generally applied. There are two existing general methods: tangent space PCA and geodesic PCA. However, unlike tangent space PCA, our torus-PCA honors the cyclic topology of the data space whereas, unlike geodesic PCA, our torus-PCA produces a variety of non-winding, non-dense descriptors. This is achieved by deforming tori into spheres and then using a variant of the recently developed principle nested spheres analysis. This PCA analysis involves a step of small sphere fitting and we provide an improved test to avoid overfitting. However, deforming tori into spheres creates singularities. We introduce a data-adaptive pre-clustering technique to keep the singularities away from the data. For the frequently encountered case that the residual variance around the PCA main component is small, we use a post-mode hunting technique for more fine-grained clustering. Thus in general, there are three successive interrelated key steps of torus-PCA in practice: pre-clustering, deformation, and post-mode hunting.\n",
      "\n",
      "We illustrate our method with two recently studied RNA structure (tori) data sets: one is a small RNA data set which is established as the benchmark for PCA and we validate our method through this data. Another is a large RNA data set (containing the small RNA data set) for which we show that our method provides interpretable principal components as well as giving further insight into its structure. 0 into PostgreSQL...\n",
      "Inserting test sample 2490  In this study, we present a new method for principal component analysis (PCA) based on the torus shape. Our proposed method, named Torus PCA, allows us to capture the global geometric properties of large RNA structures. We demonstrate the effectiveness of Torus PCA by applying it to a set of RNA structures and comparing the results with those generated by traditional PCA. Our results show that our method outperforms traditional PCA in capturing the essential geometric features of RNA structures.\n",
      "\n",
      "Following the successful demonstration of Torus PCA in RNA structure analysis, we further apply it to local RNA substructures, allowing us to detect structural differences between subgroups of RNA molecules. We illustrate the applicability of our method with two case studies, one focusing on analyzing the structural similarities and differences between viral and host RNAs, and another on identifying the characteristic structural features of riboswitches.\n",
      "\n",
      "Our proposed method offers a significant contribution toward studying RNA structures and dynamics, as it provides a scalable and efficient way to capture RNA structures' essential characteristics. The results obtained with Torus PCA can be used in various applications, such as designing RNA-based drugs and characterizing RNA structure-function relationships. Overall, Torus PCA is a promising method for RNA research, and we anticipate a wide range of potential applications in other areas of structural biology. 1 into PostgreSQL...\n",
      "Inserting test sample 2491  This paper addresses the problem of photometric stereo, in both calibrated and uncalibrated scenarios, for non-Lambertian surfaces based on deep learning.\n",
      "\n",
      "We first introduce a fully convolutional deep network for calibrated photometric stereo, which we call PS-FCN. Unlike traditional approaches that adopt simplified reflectance models to make the problem tractable, our method directly learns the mapping from reflectance observations to surface normal, and is able to handle surfaces with general and unknown isotropic reflectance.\n",
      "\n",
      "At test time, PS-FCN takes an arbitrary number of images and their associated light directions as input and predicts a surface normal map of the scene in a fast feed-forward pass. To deal with the uncalibrated scenario where light directions are unknown, we introduce a new convolutional network, named LCNet, to estimate light directions from input images. The estimated light directions and the input images are then fed to PS-FCN to determine the surface normals.\n",
      "\n",
      "Our method does not require a pre-defined set of light directions and can handle multiple images in an order-agnostic manner. Thorough evaluation of our approach on both synthetic and real datasets shows that it outperforms state-of-the-art methods in both calibrated and uncalibrated scenarios. 0 into PostgreSQL...\n",
      "Inserting test sample 2492  This paper proposes a novel approach to photometric stereo for non-Lambertian surfaces. The traditional approach to photometric stereo relies on the assumption that the surface is Lambertian, which is not always the case in real-world scenarios. Our proposed method is based on deep learning techniques, which allows us to capture the complex reflectance properties of non-Lambertian surfaces. By training a neural network on a large dataset of non-Lambertian materials, we can accurately estimate the surface normals and albedo of an object from its images under varying illumination conditions. Specifically, we use a multi-view setup with multiple light sources to capture a set of images of an object, and then feed these images as input to the neural network. Our experimental results demonstrate that the proposed deep photometric stereo approach outperforms traditional methods when tested on a dataset of real-world objects with non-Lambertian surfaces. Our approach has potential applications in fields such as computer vision, robotics, and material science, where accurate estimation of surface normals and reflectance properties is important. 1 into PostgreSQL...\n",
      "Inserting test sample 2493  We study the Higgs condensation $H={\\bar t}t$ mechanism in the Top-mode Standard Model at the next-to-leading order in 1/Nc. The calculation includes the effects of the Goldstone fields, but not the effects of the transverse components of the electroweak gauge bosons. The resulting effective theory is parametrized by means of a finite energy cut-off $\\L$ at which the condensation is supposed to take place. Demanding that the next-to-leading order contributions not dominate over the leading order ones, we get a rather low bound for the cut-off: $\\L = O(1 TeV)$. QCD effects can change the results somewhat, but the basic conclusions remain unchanged. The inclusion of the Goldstone degrees of freedom tends to decrease the bound on $\\L$. 0 into PostgreSQL...\n",
      "Inserting test sample 2494  In this paper, we investigate the behavior of the Goldstone modes and the Higgs condensation beyond the one-loop approximation in the context of the Standard Model of particle physics. We show that, even though the one-loop calculation provides a reasonable description of the phenomenon in most cases, a higher-order approximation reveals additional features of the system that were previously overlooked. Specifically, we demonstrate that the dynamics of the system is significantly influenced by the presence of non-linear interactions, which can lead to unexpected effects such as the enhancement of the Higgs condensation. Our analysis is based on a combination of analytical and numerical techniques and suggests new avenues for exploring the properties of the Higgs sector in the non-perturbative regime. 1 into PostgreSQL...\n",
      "Inserting test sample 2495  We investigate the singularities of the trace of the half-wave group, $\\mathrm{Tr} \\, e^{-it\\sqrt\\Delta}$, on Euclidean surfaces with conical singularities $(X,g)$. We compute the leading-order singularity associated to periodic orbits with successive degenerate diffractions. This result extends the previous work of the third author \\cite{Hil} and the two-dimensional case of the work of the first author and Wunsch \\cite{ForWun} as well as the seminal result of Duistermaat and Guillemin \\cite{DuiGui} in the smooth setting. As an intermediate step, we identify the wave propagators on $X$ as singular Fourier integral operators associated to intersecting Lagrangian submanifolds, originally developed by Melrose and Uhlmann \\cite{MelUhl}. 0 into PostgreSQL...\n",
      "Inserting test sample 2496  This work explores the propagation of waves on Euclidean surfaces that contain conical singularities. We investigate the geometric diffraction phenomenon, which occurs when the wave encounters these singularities and scatters in a predictable way. Specifically, we derive a mathematical model that describes the behavior of the wave on the surface and investigate the properties of the resulting diffraction pattern. Our analysis leads to several interesting insights regarding the effects of the singularities on the wave's behavior, including the creation of interference patterns and the amplification of certain frequencies. Overall, our findings contribute to a better understanding of wave propagation on surfaces with complex geometries, which has important implications for a wide range of scientific and technological applications. 1 into PostgreSQL...\n",
      "Inserting test sample 2497  In this letter we study (2+1)-dimensional QED. The first part contains the computation of the flavor symmetry-breaking condensate and its relation to the trace of the energy-momentum tensor, while the second part is concerned with the computation of the effective action allowing for non-constant static external magnetic fields. We do not find that space derivatives in the magnetic field lower the energy of the ground state as compared to a constant field configuration. 0 into PostgreSQL...\n",
      "Inserting test sample 2498  In this paper, we investigate the phenomenon of flavor condensate and vacuum instability in QED_(2+1). We derive the gap equation for the flavor condensate in the presence of a magnetic field and compute the critical coupling which determines the onset of vacuum instability. Our results suggest that the flavor condensate enhances the vacuum instability in QED_(2+1), highlighting the important role played by condensation effects in condensed matter physics. These findings have implications for understanding the behavior of strongly correlated systems and could have implications for new materials design. 1 into PostgreSQL...\n",
      "Inserting test sample 2499  We consider a network-preserved model of power networks with proper algebraic constraints resulting from constant power loads. Both for the linear and the nonlinear differential algebraic model of the network, we derive explicit reduced models which are fully expressed in terms of ordinary differential equations. For deriving these reduced models, we introduce the \"projected incidence\" matrix which yields a novel decomposition of the reduced Laplacian matrix. With the help of this new matrix, we provide a complementary approach to Kron reduction which is able to cope with constant power loads and nonlinear power flow equations. 0 into PostgreSQL...\n",
      "Inserting test sample 2500  This paper proposes a new reduced model for electrical networks that include constant power loads (CPLs). The model is based on a balanced three-phase equivalent circuit with reduced variables that considers the CPLs' behavior in a more accurate way. The proposed model is validated through simulations using a common test system and compared to other CPL models. Results show that the proposed model offers more accurate representation of the network's behavior while reducing the computational complexity and time. This can be especially useful for the analysis and control of smart grids and microgrids with CPLs. 1 into PostgreSQL...\n",
      "Inserting test sample 2501  We derive robust model-independent bounds on DM annihilations and decays from the first year of FERMI gamma-ray observations of the whole sky. These bounds only have a mild dependence on the DM density profile and allow the following DM interpretations of the PAMELA and FERMI electron/positron excesses: primary channels mu+ mu-, mu+ mu-mu+mu- or e+ e- e+ e-. An isothermal-like density profile is needed for annihilating DM. In all such cases, FERMI gamma spectra must contain a significant DM component, that may be probed in the future. 0 into PostgreSQL...\n",
      "Inserting test sample 2502  This study investigates the implications of the first FERMI sky gamma map on the nature of dark matter. The map unveils new and robust features in the gamma-ray sky. By analyzing the data, we are able to constrain dark matter models, providing new insights into its properties. Our results show that dark matter models that predict a large annihilation cross section are strongly disfavored by the data. On the other hand, models with a small annihilation cross section are consistent with the observations. The FERMI gamma map sets a new benchmark for the study of dark matter in the gamma-ray sky. 1 into PostgreSQL...\n",
      "Inserting test sample 2503  Stars of spectral classes A and late B are almost entirely radiative. CP stars are a slowly rotating subgroup of these stars. It is possible that they possessed long-lived accretion disks in their T Tauri phase. Magnetic coupling of disk and star leads to rotational braking at the surface of the star.\n",
      "\n",
      "Microscopic viscosities are extremely small and will not be able to reduce the rotation rate of the core of the star. We investigate the question whether magneto-rotational instability can provide turbulent angular momentum transport. We illuminate the question whether or not differential rotation is present in CP stars. Numerical MHD simulations of thick stellar shells are performed. An initial differential rotation law is subject to the influence of a magnetic field. The configuration gives indeed rise to magneto-rotational instability. The emerging flows and magnetic fields transport efficiently angular momentum outwards. Weak dependence on the magnetic Prandtl number (~0.01 in stars) is found from the simulations. Since the estimated time-scale of decay of differential rotation is 10^7-10^8 yr and comparable to the life-time of A stars, we find the braking of the core to be an ongoing process in many CP stars. The evolution of the surface rotation of CP stars with age will be an observational challenge and of much value for verifying the simulations. 0 into PostgreSQL...\n",
      "Inserting test sample 2504  In this study, we investigate the phenomenon of differential rotation decay in the radiative envelopes of chemically peculiar (CP) stars. By means of spectroscopic observations, we measure the projected rotational velocities of a sample of CP stars and determine their inclination angles. Using these data, we calculate the surface and core rotational velocities and investigate the rotational profiles in the radiative envelopes. Our main result is the detection of differential rotation decay in the radiative envelopes of CP stars. Specifically, we find that stars with higher effective temperatures exhibit less rotational shear in their radiative envelopes and that the shear decreases with increasing depth in the envelope. Our findings suggest that the magnetic fields present in CP stars may play a role in the rotational profile of the radiative envelopes. We discuss the implications of our results for our understanding of the internal structure and evolution of CP stars. Overall, our study provides new insights into the behavior of differential rotation in the radiative envelopes of CP stars and highlights the importance of magnetic fields in shaping the rotational profiles of stellar envelopes. 1 into PostgreSQL...\n",
      "Inserting test sample 2505  Accurate representation of the physical layer is required for analysis and simulation of multi-hop networking in sensor, ad hoc, and mesh networks. This paper investigates, models, and analyzes the correlations that exist in shadow fading between links in multi-hop networks. Radio links that are geographically proximate often experience similar environmental shadowing effects and thus have correlated fading. We describe a measurement procedure and campaign to measure a large number of multi-hop networks in an ensemble of environments.\n",
      "\n",
      "The measurements show statistically significant correlations among shadowing experienced on different links in the network, with correlation coefficients up to 0.33. We propose a statistical model for the shadowing correlation between link pairs which shows strong agreement with the measurements, and we compare the new model with an existing shadowing correlation model of Gudmundson (1991). Finally, we analyze multi-hop paths in three and four node networks using both correlated and independent shadowing models and show that independent shadowing models can underestimate the probability of route failure by a factor of two or greater. 0 into PostgreSQL...\n",
      "Inserting test sample 2506  Multi-hop wireless networks enable communication between distant wireless nodes through a series of intermediate hops. However, these networks are often affected by correlated link shadow fading (CLSF), where multiple adjacent link shadowing events have highly correlated effects on network connectivity. This phenomenon poses significant challenges to network design, particularly in terms of ensuring reliable communication over long distances. This paper proposes a new approach to mitigating CLSF in multi-hop wireless networks, based on the selective utilization of secondary links between neighboring nodes. Simulation results indicate that our proposed approach significantly reduces packet loss and network jitter, while maintaining high network throughput. Additionally, we provide an analytical model that enables network designers to estimate the probability of network connectivity between two distant nodes in the presence of CLSF. Our approach holds promise for improving the design and implementation of future multi-hop wireless networks, enabling reliable and efficient communication even in the presence of highly correlated link shadow fading. 1 into PostgreSQL...\n",
      "Inserting test sample 2507  The role of feedback from massive stars is believed to be a key element in the evolution of molecular clouds. We use high-resolution 3D SPH simulations to explore the dynamical effects of a single O7 star located at the centre of a molecular cloud with mass 10^4M_sun and radius 6.4pc. The initial internal structure of the cloud is characterised by its fractal dimension, D=2.0 - 2.8, and its log-normal density PDF. (i) As regards star formation, in the short term ionising feedback is positive, in the sense that star formation occurs much more quickly in gas that is compressed by the high pressure of the ionised gas. However, in the long term ionising feedback is negative, in the sense that most of the cloud is dispersed with an outflow rate of up to ~0.01M_sun/yr, on a timescale comparable with the sound-crossing time for the ionised gas (~1-2Myr), and triggered star formation is therefore limited to a few percent of the cloud's mass. (ii) As regards the morphology of the ionisation fronts (IFs) bounding the HII region and the systematics of outflowing gas, we distinguish two regimes. For low D<=2.2, the initial cloud is dominated by large-scale structures, so the neutral gas tends to be swept up into a few extended coherent shells, and the ionised gas blows out through a few large holes between these shells; we term these HII regions \"shell-dominated\".\n",
      "\n",
      "Conversely, for high D>=2.6, the initial cloud is dominated by small-scale structures, and these are quickly overrun by the advancing IF, thereby producing neutral pillars whilst the ionised gas blows out through a large number of small holes between the pillars; we term these HII regions \"pillar-dominated\". (iii) As regards the injection of bulk kinetic energy, by ~1Myr, the expansion of the HII region has delivered a rms velocity of ~6km/s; this represents less than 0.1% of the total energy radiated by the O7 star. 0 into PostgreSQL...\n",
      "Inserting test sample 2508  The dispersal of molecular clouds is a process fundamental to the evolution of the interstellar medium. Ionising radiation, originating from massive stars, is a key driver of this phenomenon. In this paper, we combine numerical simulations with analytical models to investigate the dynamics of the interaction between ionising radiation and molecular clouds. \n",
      "\n",
      "Our simulations enable us to follow the time evolution of both the radiation field and the cloud. By varying the parameters of our models, such as the intensity and spectral shape of the radiation, we investigate the effect of different radiation sources on the dispersal of molecular clouds. We find that ionising radiation significantly affects the morphological and dynamical properties of the cloud, leading to their eventual dispersal. \n",
      "\n",
      "Our analytical models complement the simulations by providing insight into the underlying physics of the interaction. We explore various scenarios, differing in the density and size of the cloud as well as the environmental conditions, such as the gas metallicity and the presence of dust. Our results show that the key factors affecting the rate of cloud dispersal are the ionisation front speed, the cloud density and the spectrum of the radiation source. \n",
      "\n",
      "We also investigate the impact of cloud dispersal on star formation activity. Our simulations demonstrate that the destruction of molecular clouds reduces their ability to form stars. This result implies that the rate of star formation is coupled to the dispersal of molecular clouds through ionising radiation. \n",
      "\n",
      "We compare the results of our simulations with observations of molecular clouds in our galaxy to test the validity of our models. We find good agreement between theory and observations, suggesting that our simulations accurately represent the physics of this process. \n",
      "\n",
      "In conclusion, our study shows that ionising radiation is a significant driver of the dispersal of molecular clouds, with important implications for star formation activity. This work provides a framework for understanding the interaction between ionising radiation and molecular clouds, and can serve as a guide for future observations and simulations. 1 into PostgreSQL...\n",
      "Inserting test sample 2509  If A_1,...,A_N are real square matrices then the p-radius, generalised Lyapunov exponent or matrix pressure is defined to be the asymptotic exponential growth rate of the sum $\\sum_{i_1,\\ldots,i_n=1}^N \\|A_{i_n}\\cdots A_{i_1}\\|^p$, where p is a real parameter. Under its various names this quantity has been investigated for its applications to topics including wavelet regularity and refinement equations, fractal geometry and the large deviations theory of random matrix products. In this article we present a new algorithm for computing the p-radius under the hypothesis that the matrices are all positive, or more generally under the hypothesis that they satisfy a weaker condition called domination. This algorithm is based on interpreting the p-radius as the leading eigenvalue of a trace-class operator on a Hilbert space and estimating that eigenvalue via approximations to the Fredholm determinant of the operator. In this respect our method is closely related to the work of Z.-Q. Bai and M. Pollicott on computing the top Lyapunov exponent of a random matrix product. For pairs of positive matrices of low dimension our method yields substantial improvements over existing methods. 0 into PostgreSQL...\n",
      "Inserting test sample 2510  This paper presents a novel algorithm for efficiently approximating the $p$-radius, matrix pressure or generalised Lyapunov exponent for positive and dominated matrices. These are important metrics used in many fields including control theory, complex systems, and optimisation. Our approach utilises a combination of iterative numerical methods and matrix factorisation techniques to achieve a high degree of accuracy without requiring excessive computation time or memory usage. We demonstrate the effectiveness of the proposed algorithm through a series of rigorous experiments comparing it against existing methods. The results show that our technique achieves similar or better accuracy than state-of-the-art methods at a fraction of the computational cost. In addition to its practical applications, this algorithm contributes to the theoretical understanding of these important matrix metrics, particularly in the cases of high-dimensional matrices. Overall, this work provides an important advancement in the field of matrix analysis and offers a valuable tool for researchers and practitioners in various fields. 1 into PostgreSQL...\n",
      "Inserting test sample 2511  We report the discovery of strong aperiodic X-ray variability and quasi-periodic oscillation (QPO) in the X-ray light curves of a new X-ray Nova, XTE J1550-564, and the evolution of the observed temporal properties during the rise of the recent X-ray outburst. The power spectral analysis of the first observation reveals strong aperiodic X-ray variability of the source (~28%), as well as the presence of a QPO at ~82 mHz with fractional rms amplitude ~14% over the 2-60 keV energy range. Also apparent is the first harmonic of the QPO with the amplitude ~9%. As the X-ray flux increases, the source tends to become less variable, and the QPO frequency increases rapidly, from 82 mHz to 4 Hz, over the flux (2-50 keV) range of 1.73-5.75 x 10^{-8} ergs cm^{-2} s^{-1}. The amplitude of the fundamental component of the QPO varies little, while that of the harmonic follows a decreasing trend. The fundamental component strengthens toward high energies, while its harmonic weakens. Initially, the power spectrum is roughly flat at low frequencies and turns into a power law at high frequencies, with the QPO harmonic sitting roughly at the break. In later observations, however, the high-frequency portion of the continuum can actually be better described by a broken power law (as opposed to a simple power law).\n",
      "\n",
      "This effect becomes more apparent at higher energies. The overall amplitude of the continuum shows a similar energy dependence to that of the fundamental component of the QPO. Strong rapid X-ray variability, as well as hard energy spectrum, makes XTE J1550-564 a good black hole candidate. We compare its temporal properties with those of other black hole candidates. 0 into PostgreSQL...\n",
      "Inserting test sample 2512  X-ray novae are binary star systems consisting of a neutron star or black hole accreting matter from a companion star. XTE J1550-564 is an X-ray nova that was first discovered by the RXTE satellite in 1998. During its outburst, it showed strong aperiodic X-ray variability and quasi-periodic oscillation (QPO) in its power density spectra (PDS). In this paper, we present an analysis of the aperiodic variability and QPO of XTE J1550-564 during its outburst phase.\n",
      "\n",
      "We analyzed the X-ray data of XTE J1550-564 obtained from the RXTE satellite. Our results show that the PDS of XTE J1550-564 exhibits strong aperiodic variability, consistent with the presence of stochastic processes in the accretion flow. The aperiodic variability can be described by a power law model with a slope of -1.9Â±0.1, which indicates that the variability is dominated by low-frequency fluctuations.\n",
      "\n",
      "Furthermore, we detected a QPO at a frequency of ~0.07 Hz during the rise phase of the outburst. The QPO has a centroid frequency that evolves with time and varies between 0.05 and 0.12 Hz. The QPO has a quality factor of ~2.5 and its fractional rms amplitude increases with energy.\n",
      "\n",
      "We discuss the possible physical mechanisms that could be responsible for the aperiodic variability and QPO observed in XTE J1550-564. We suggest that the aperiodic variability could arise due to turbulence in the accretion flow, while the QPO could be produced by disk oscillations or variations in the accretion rate. Our results provide important insights into the physics of X-ray novae and the accretion process onto black holes and neutron stars. 1 into PostgreSQL...\n",
      "Inserting test sample 2513  The rheology of a dilute binary mixture of inertial suspension under simple shear flow is analyzed in the context of the Boltzmann kinetic equation. The effect of the surrounding viscous gas on the solid particles is accounted for by means of a deterministic viscous drag force plus a stochastic Langevin-like term defined in terms of the environmental temperature $T_\\text{env}$. Grad's moment method is employed to determine the temperature ratio and the pressure tensor in terms of the coefficients of restitution, concentration, the masses and diameters of the components of the mixture and the environmental temperature. Analytical results are compared against event-driven Langevin simulations for mixtures of hard spheres with the same mass density [$m_1/m_2=(\\sigma_1/\\sigma_2)^3$, $m_i$ and $\\sigma_i$ being the mass and diameter, respectively, of the species $i$)]. It is confirmed that the theoretical predictions agree with simulations of various size ratios $\\sigma_1/\\sigma_2$ and for elastic and inelastic collisions in the wide range of parameters' space. It is remarkable that the temperature ratio $T_1/T_2$ and the viscosity ratio $\\eta_1/\\eta_2$ ($\\eta_i$ being the partial contribution of the component $i$ to the total shear viscosity $\\eta=\\eta_1+\\eta_2$) discontinuously change at a certain shear rate as the size ratio increases; this feature (which is expected to occur in the thermodynamic limit) cannot be captured by simulations due to small system size. In addition, a Bhatnagar--Gross--Krook (BGK)-type kinetic model adapted to mixtures of inelastic hard spheres is exactly solved when $T_\\text{env}$ is much smaller than the kinetic temperature $T$. A comparison between the velocity distribution functions obtained from Grad's method, BGK model, and simulations is carried out. 0 into PostgreSQL...\n",
      "Inserting test sample 2514  The rheology of a dilute binary mixture of inertial suspension under simple shear flow is a fundamental problem in fluid mechanics with applications in a range of industrial and biological systems. In this paper, we perform numerical simulations using the immersed boundary method to investigate the rheological properties of a dilute binary mixture of rigid spherical particles suspended in a Newtonian fluid under simple shear flow. We consider the case where one species of the particles is inertial and the other is non-inertial and the ratios of their volume fractions are varied. The simulations are conducted for a range of Reynolds and shear numbers, and the effects of particle volume fractions, Reynolds and shear numbers, and the ratios of particle volume fractions on the suspension rheology are investigated. The results show that the rheological behavior of the binary mixture is a function of the volume fraction ratio and Reynolds number. It is found that the presence of the inertial particles in the suspension leads to a non-monotonic behavior in the suspension viscosity as a function of the particle volume fraction. These results provide insights into the behavior of dilute binary mixtures of particles under different flow conditions, which could find potential applications in various scientific and engineering fields. Overall, this study contributes to a better understanding of the fundamental principles governing the rheology of dilute binary mixtures of inertial suspensions. 1 into PostgreSQL...\n",
      "Inserting test sample 2515  The inelastic scattering and conversion process between photons and phonons by laser-driven quantum dots is analyzed for a honeycomb array of optomechanical cells. Using Floquet theory for an effective two-level system, we solve the related time-dependent scattering problem, beyond the standard rotating-wave approximation approach, for a plane Dirac-photon wave hitting a cylindrical oscillating barrier that couples the radiation field to the vibrational degrees of freedom. We demonstrate different scattering regimes and discuss the formation of polaritonic quasiparticles. We show that sideband-scattering becomes important when the energies of the sidebands are located in the vicinity of avoided crossings of the quasienergy bands. The interference of Floquet states belonging to different sidebands causes a mixing of long-wavelength (quantum) and short-wavelength (quasiclassical) behavior, making it possible to use the oscillating quantum dot as a kind of transistor for light and sound. We comment under which conditions the setup can be utilized to observe zitterbewegung. 0 into PostgreSQL...\n",
      "Inserting test sample 2516  In the field of optomechanics, the interaction between light and sound waves plays a fundamental role in controlling the behavior of mechanical resonators. In particular, Dirac optomechanics allows for the study of systems in which mechanical motion is governed by the Dirac equation. Here, we investigate the scattering of light and sound by a periodically driven Dirac oscillator in the Floquet regime. By conducting a detailed analysis of the scattering matrix, we find that the system exhibits interesting effects related to the violation of parity symmetry, as well as the generation of higher-order sidebands. We also show that these effects can be observed in the transmission and reflection spectra of the system. Our study provides insights into the behavior of Dirac optomechanical systems in the presence of time-periodic driving, and highlights the potential uses of these systems in various applications such as sensing and quantum information processing. 1 into PostgreSQL...\n",
      "Inserting test sample 2517  The development of computer science has contributed greatly for increasing of efficiency and effectively. Many areas are covered by computer science, included education. The purpose of this research is to introduce jawi a type of Indonesian letters. Jawis letter is one of the most popular letter in the past.\n",
      "\n",
      "But right now few people can read and understand it. Many documents in the past was written in Jawi. The writer develop or build the software using Pressman method, and tools such as Microsoft Visual Basic, and Microsoft Access. This software can introduce Jawi then people can learn it easily. 0 into PostgreSQL...\n",
      "Inserting test sample 2518  This research paper presents a computer software designed to assist in the recognition of Arabic-Malay letters and their conversion into Latin letters in Indonesian language. The software utilizes a machine learning approach to identify and interpret different forms of the letters and to provide accurate translations. The system employs various algorithms and a large dataset to achieve high accuracy and efficiency. The results indicate that the software is capable of recognizing and converting Arabic-Malay letters with a high degree of precision. This innovative technology has promising applications in education and communication, particularly for students and individuals seeking to learn Arabic-Malay script. 1 into PostgreSQL...\n",
      "Inserting test sample 2519  As a sub-array of the Large High Altitude Air Shower Observatory (LHAASO), KM2A is mainly designed to cover a large fraction of the northern sky to hunt for gamma-ray sources at energies above 10 TeV. Even though the detector construction is still underway, a half of the KM2A array has been operating stably since the end of 2019. In this paper, we present the pipeline of KM2A data analysis and the first observation on the Crab Nebula, a standard candle in very high energy gamma-ray astronomy. We detect gamma-ray signals from the Crab Nebula in both energy ranges of 10$-$100 TeV and $>$100 TeV with high significance, by analyzing the KM2A data of 136 live days between December 2019 and May 2020. With the observations, we test the detector performance including angular resolution, pointing accuracy and cosmic ray background rejection power.\n",
      "\n",
      "The energy spectrum of the Crab Nebula in the energy range 10-250 TeV fits well with a single power-law function dN/dE =(1.13$\\pm$0.05$_{stat}$$\\pm$0.08$_{sys}$)$\\times$10$^{-14}$$\\cdot$(E/20TeV)$^{-3.09\\pm0.06_{stat}\\pm0.02_{sys}}$ cm$^{-2}$ s$^{-1}$ TeV$^{-1}$. It is consistent with previous measurements by other experiments. This opens a new window of gamma-ray astronomy above 0.1 PeV through which ultrahigh-energy gamma-ray new phenomena, such as cosmic PeVatrons, might be discovered. 0 into PostgreSQL...\n",
      "Inserting test sample 2520  The Crab Nebula is one of the most well-known astrophysical sources of high-energy radiation. To better understand the performance of LHAASO-KM2A in observing this phenomenon, we conducted a study to analyze the data collected by this new generation of high-energy gamma ray observatories. Our study focused on the measurement of the energy spectrum, time structure, and morphology of gamma-ray emission from the Crab Nebula as detected by LHAASO-KM2A. By analyzing the data obtained with this advanced instrument, we identified new features and properties in the gamma-ray emission, which provide insights into the underlying mechanism of the Crab Nebula. Our results show excellent agreement with previous observations, but also unveil a higher sensitivity and improved performance of LHAASO-KM2A. These findings significantly contribute to the ongoing investigation of the Crab Nebula and the understanding of high-energy astrophysics. Overall, our study demonstrates the scientific potential and capacity of LHAASO-KM2A in probing astrophysical sources, and validates it as a powerful tool in the field of gamma ray astronomy. 1 into PostgreSQL...\n",
      "Inserting test sample 2521  When the Stable Unit Treatment Value Assumption (SUTVA) is violated and there is interference among units, there is not a uniquely defined Average Treatment Effect (ATE), and alternative estimands may be of interest, among them average unit-level differences in outcomes under different homogeneous treatment policies. We term this target the Homogeneous Assignment Average Treatment Effect (HAATE). We consider approaches to experimental design with multiple treatment conditions under partial interference and, given the estimand of interest, we show that difference-in-means estimators may perform better than correctly specified regression models in finite samples on root mean squared error (RMSE). With errors correlated at the cluster level, we demonstrate that two-stage randomization procedures with intra-cluster correlation of treatment strictly between zero and one may dominate one-stage randomization designs on the same metric. Simulations demonstrate performance of this approach; an application to online experiments at Facebook is discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 2522  Homogenous policy change often requires experimentation to determine the most effective approach. This research paper investigates the use of randomized controlled trials to experiment with policy changes in a homogenous population. Through this method, we can determine the most successful policy changes for the given population, leading to enhanced outcomes and more efficient implementation. Additionally, we examine the potential benefits of shared social norms within a homogeneous population, which may influence the success of the policy changes being studied. Our findings indicate that experimentation allows for a more nuanced understanding of homogenous policy changes and can lead to more effective implementation. By utilizing randomized controlled trials, policymakers can develop evidence-based policies that are tailored to the specific needs of a homogenous population. 1 into PostgreSQL...\n",
      "Inserting test sample 2523  The IceCube Collaboration has recently reported the observation of a flux of high-energy astrophysical neutrinos. The angular distribution of events is consistent with an isotropic arrival direction of neutrinos which is expected for an extragalactic origin. We estimate the prospects of detecting individual neutrino sources from a quasi-diffuse superposition of many extragalactic sources at the level of the IceCube observation. Our analysis takes into account ensemble variations of the source distribution as well as the event statistics of individual sources. We show that IceCube in its present configuration is sensitive to rare < 10^-8 Mpc^-3 yr^-1 transient source classes within 5 years of operation via the observation of multiplets.\n",
      "\n",
      "Identification of time-independent sources is more challenging due to larger backgrounds. We estimate that during the same period IceCube is sensitive to sparse sources with densities of < 10^-6 Mpc^-3 via association of events with the closest 100 sources of an ensemble. We show that a next-generation neutrino observatory with 5 times the effective area of IceCube and otherwise similar detector performance would increase the sensitivity to source densities and rates by about two orders of magnitude. 0 into PostgreSQL...\n",
      "Inserting test sample 2524  This paper addresses the challenge of identifying sources of extragalactic high-energy neutrinos detected by the IceCube neutrino observatory. The recent observation of such neutrinos provides an exciting opportunity to study the most powerful astrophysical sources in the universe. However, their origins remain unclear, and a multi-messenger approach is necessary to pinpoint the exact source locations. We present a comprehensive analysis of the IceCube data, combined with observations from other telescopes, to identify the potential sources of these neutrinos. Our methodology includes evaluating the isotropy of the neutrino flux, analyzing the energy spectrum of the events, and searching for possible associations with known astrophysical objects. The results indicate that blazars â€“ active galactic nuclei â€“ are the most likely sources of the observed events. Moreover, we discuss possible implications of these findings for our understanding of cosmic-ray accelerators. This study provides a crucial step towards a more detailed understanding of the high-energy universe and opens the door for future studies using multi-messenger observations. 1 into PostgreSQL...\n",
      "Inserting test sample 2525  The pros and cons of various forms of atomism and holism that are applicable both in physical science and today's philosophy of nature are evaluated. To this end, Lewis' thesis of Humean supervenience is presented as an important case study of an atomistic doctrine in philosophical thought. According to the thesis of Humean supervenience, the world is fragmented into local matters of particular fact and everything else supervenes upon them in conjunction with the spatiotemporal relations among them. It is explicitly shown that Lewis' ontological doctrine of Humean supervenience incorporates at its foundation the so-called separability principle of classical physics. In view of the systematic violation of the latter within quantum mechanics, it is argued that contemporary physical science posits non-supervenient relations over and above the spatiotemporal ones. It is demonstrated that the relation of quantum entanglement constitutes the prototypical example of a holistic, irreducible physical relation that does not supervene upon a spatiotemporal arrangement of Humean qualities, undermining, thereby, the thesis of Humean supervenience. It is concluded, in this respect, that the assumption of ontological reductionism, as expressed in Lewis' Humean doctrine, cannot be regarded as a reliable code of the nature of the physical world and its contents. It is proposed instead that - due to the undeniable existence of generic non-supervenient relations - a metaphysic of relations of a moderate kind ought to be acknowledged as an indispensable part of our understanding of the natural world at a fundamental level. 0 into PostgreSQL...\n",
      "Inserting test sample 2526  This paper explores the debate between atomism and holism in both science and philosophy. Atomism is the belief that reality can be understood by breaking it down into its smallest parts, while holism states that the whole is greater than the sum of its parts. The history of this debate is traced from the ancient Greek atomists to modern-day philosophers of science. The paper argues that both atomism and holism have their strengths and weaknesses in scientific and philosophical inquiry. On one hand, atomism provides a reductionist approach that often leads to breakthroughs in scientific knowledge, such as the discovery of the structure of DNA. On the other hand, holism recognizes the complex interactions and interdependencies within systems, which can lead to a more comprehensive understanding of phenomena like ecosystems or economies. The paper concludes that both atomism and holism are valuable perspectives in scientific and philosophical research, and that a balance between the two is necessary for a fuller understanding of the world around us. This paper contributes to the ongoing discussion about the nature of knowledge, the best approach to scientific inquiry, and the limits of reductionism in understanding the complexity of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2527  The degrees of freedom (DoF) of the two-user multiple-input single-output (MISO) broadcast channel (BC) are studied under the assumption that the form, I_i, i=1,2, of the channel state information at the transmitter (CSIT) for each user's channel can be either perfect (P), delayed (D) or not available (N), i.e., I_1 and I_2 can take values of either P, D or N, and therefore the overall CSIT can alternate between the 9 resulting states, each state denoted as I_1I_2. The fraction of time associated with CSIT state I_1I_2 is denoted by the parameter \\lambda_{I_1I_2} and it is assumed throughout that \\lambda_{I_1I_2}=\\lambda_{I_2I_1}, i.e., \\lambda_{PN}=\\lambda_{NP}, \\lambda_{PD}=\\lambda_{DP}, \\lambda_{DN}=\\lambda_{ND}. Under this assumption of symmetry, the main contribution of this paper is a complete characterization of the DoF region of the two user MISO BC with alternating CSIT. Surprisingly, the DoF region is found to depend only on the marginal probabilities (\\lambda_P, \\lambda_D,\\lambda_N)=(\\sum_{I_2}\\lambda_{PI_2},\\sum_{I_2}\\lambda_{DI_2}, \\sum_{I_2}\\lambda_{NI_2}), I_2\\in {P,D,N}, which represent the fraction of time that any given user (e.g., user 1) is associated with perfect, delayed, or no CSIT, respectively. As a consequence, the DoF region with all 9 CSIT states, \\mathcal{D}(\\lambda_{I_1I_2}:I_1,I_2\\in{P,D,N}), is the same as the DoF region with only 3 CSIT states \\mathcal{D}(\\lambda_{PP}, \\lambda_{DD}, \\lambda_{NN}), under the same marginal distribution of CSIT states, i.e., (\\lambda_{PP}, \\lambda_{DD},\\lambda_{NN})=(\\lambda_P,\\lambda_D,\\lambda_N). The results highlight the synergistic benefits of alternating CSIT and the tradeoffs between various forms of CSIT for any given DoF value. 0 into PostgreSQL...\n",
      "Inserting test sample 2528  The Multiple-Input Single-Output Broadcast Channel (MISO BC) is a significant component in modern wireless communication systems. In the past, studies have shown that Channel State Information at the Transmitter (CSIT) plays a vital role in optimizing the system's performance in terms of capacity and reliability. However, the use of CSIT poses a challenge in multi-user communication scenarios since the feedback channel can be too congested with information. One promising solution to this issue is alternating CSIT.\n",
      "\n",
      "In this paper, we present an analysis of the synergistic benefits of alternating CSIT for the MISO BC. The study shows that the alternation of CSIT between users enhances the channel quality and reduces the feedback overhead. By modeling the user channels, we demonstrate the potential of our proposed scheme to achieve near-optimal system performance. Specifically, we derive a lower bound on the Ergodic Capacity, which allows us to gauge the effectiveness of our technique.\n",
      "\n",
      "Our numerical results show that alternating CSIT can significantly reduce the feedback overhead, especially in high signal-to-noise ratio (SNR) regimes. Additionally, our scheme enables the system to adapt to changing channel conditions more efficiently, leading to higher link quality. Furthermore, we show that the performance benefits of alternating CSIT are more pronounced when the number of transmit antennas at the base station is high. \n",
      "\n",
      "In conclusion, our study provides compelling evidence of the synergistic benefits of alternating CSIT for the MISO BC. The proposed scheme improves system performance, reduces feedback overhead, and enables better adaptation to the dynamic channel state. 1 into PostgreSQL...\n",
      "Inserting test sample 2529  We present a study of fits to exclusive $B^{0} \\to D^{*-} \\ell^{+} \\nu_{\\ell}$ measurements for the determination of the Cabbibo-Kobayashi-Maskawa matrix element magnitude $|V_{cb}|$, based on the most recent Belle untagged measurement. Results are obtained with the Caprini-Lellouch-Neubert (CLN) and Boyd-Grinstein-Lebed (BGL) form factor parametrizations, with and without the inclusion of preliminary Lattice QCD measurements of form factors at non-zero hadronic recoil from the JLQCD collaboration. The CLN and BGL fits are also studied in different scenarios with reduced theoretical assumptions, and at higher order expansions, respectively. To avoid bias from high systematic error correlations we employ a novel technique in the field of $B$-physics phenomenology with a toy MC using a Cholesky decomposition of the covariance matrix. Using additional input from Lattice QCD calculations of form factors at non-zero recoil, in collaboration with JLQCD, allows for well-defined fit results with reduced model dependence in CLN and BGL. The results obtained are consistent between different configurations, ultimately providing a method for a more model-independent exclusive measurement of $|V_{cb}|$. Using preliminary inputs, $\\mathcal{F}(1)\\eta_{\\rm EW}|V_{cb}|$ is found to be approximately $(35.02 \\pm 0.29 \\pm 0.88) \\times 10^{-3}$ in BGL(2,2,2) and $(34.96 \\pm 0.32 \\pm 0.96) \\times 10^{-3}$ in CLNnoHQS. 0 into PostgreSQL...\n",
      "Inserting test sample 2530  The determination of the CKM matrix element $|V_{cb}|$ is of great importance in the context of precision tests of the Standard Model and searches for new physics. In this work, we explore novel fitting methods in the context of the $B^{0} \\to D^{*-} \\ell^{+} \\nu_{\\ell}$ decay channel to extract $|V_{cb}|$ from LQCD data at non-zero recoil. We study three fitting approaches based on functional forms motivated by different theoretical frameworks. Specifically, we test fits using the shape function approach, the OPE-based framework, and an OPE-inspired functional form. Our analysis shows that the novel fitting methods yield consistent results with standard approaches when applied to state-of-the-art LQCD data at non-zero recoil. We obtain preliminary results for $|V_{cb}|$ with improved precision compared to current published results from experiments. We also investigate the impact of the choice of kinematic cuts on the extraction of $|V_{cb}|$. Our analysis suggests that kinematic cuts can significantly affect the uncertainties on $|V_{cb}|$ extracted from LQCD data. Our study highlights the potential power of combining state-of-the-art LQCD data with novel fitting methods to achieve precise and model-independent determinations of $|V_{cb}|$ in the future. 1 into PostgreSQL...\n",
      "Inserting test sample 2531  We update previous frequentist analyses of the CMSSM and NUHM1 parameter spaces to include the public results of searches for supersymmetric signals using ~1 /fb of LHC data recorded by ATLAS and CMS and ~0.3/fb of data recorded by LHCb in addition to electroweak precision and B-physics observables. We also include the constraints imposed by the cosmological dark matter density and the XENON100 search for spin-independent dark matter scattering. The LHC data set includes ATLAS and CMS searches for jets + missing ET events and for the heavier MSSM Higgs bosons, and the upper limits on B_s to mu^+ mu^- from LHCb and CMS. The absences of jets + missing ET signals in the LHC data favour heavier mass spectra than in our previous analyses of the CMSSM and NUHM1, which may be reconciled with (g-2)_mu if tan beta ~ 40, a possibility that is however under pressure from heavy Higgs searches and the upper limits on B_s to mu^+ mu^-. As a result, the p-value for the CMSSM fit is reduced to ~ 15 (38)%, and that for the NUHM1 to ~ 16 (38)%, to be compared with ~ 9 (49)% for the Standard Model limit of the CMSSM for the same set of observables (dropping (g-2)_mu), ignoring the dark matter relic density in both cases. We discuss the sensitivities of the fits to the (g-2)_mu and b to s gamma constraints, contrasting fits with and without the (g-2)_mu constraint, and combining the theoretical and experimental errors for b to s gamma linearly or in quadrature.\n",
      "\n",
      "We present predictions for m_gluino, B_s to mu^+ mu^-, M_h and M_A, and update predictions for spin-independent dark matter scattering, stressing again the importance of taking into account the uncertainty in the pi-nucleon sigma term, Sigma_{pi N}. Finally, we present predictions based on our fits for the likely thresholds for sparticle pair production in e^+e^- collisions in the CMSSM and NUHM1. 0 into PostgreSQL...\n",
      "Inserting test sample 2532  The search for Supersymmetry is an ongoing and critical area of investigation for the field of particle physics. This study investigates the implications of the 1/fb of LHC data on the existing theories of Supersymmetry. The obtained results of this study suggest that the exclusion limit of sparticle masses can increase by several hundred GeV, disproving the previously established bounds of LEP experiments. The study analyses a wide range of potential outcomes for data collected by the LHC, outlining various scenarios in which the current limits of Supersymmetry can be extended. These results suggest the importance of further increasing the amount of analysed data in the LHC experiments in order to achieve reliable and accurate findings, and to test the countless possibilities for the underlying nature and structure of Supersymmetry. \n",
      "\n",
      "This research contributes significantly towards our understanding of the relationship between Supersymmetry and current collider data. By combining and analyzing the masses of the lightest neutralinos, squarks and gluinos, we have produced reliable limits for ranges of parameters in Supersymmetry models. These results can be compared to previous studies and used to refine and improve the predictions for future experiments. The obtained findings are particularly relevant and timely, given the present-day developments in the theoretical and experimental landscape of particle physics, with discoveries such as the Higgs boson and dark matter sparking further interest in Supersymmetry. Ultimately, this study aims to form the basis for a better understanding of the relationship between Supersymmetry and fundamental phenomena in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2533  Over the past decades, power systems have experienced drastic transformations in order to address the growth in energy demand, reduce carbon emissions, and enhance power quality and energy efficiency. This shift to the smart grid concept involves, among others, the utilization of distributed energy resources (DERs) such as rooftop solar panels and storage systems, contributing towards grid decentralization while improving control over power generation. In order to seamlessly integrate DERs into power systems, embedded devices are used to support the communication and control functions of DERs. As a result, vulnerabilities of such components can be ported to the industrial environment.\n",
      "\n",
      "Insecure control networks and protocols further exacerbate the problem. Towards reducing the attack surface, we present an authentication scheme for DERs, DERauth, which leverages the inherent entropy of the DER battery energy storage system (BESS) as a root-of-trust. The DER authentication is achieved using a challenge-reply mechanism that relies on the corresponding DER's BESS state-of-charge (SoC) and voltage measurements. A dynamically updating process ensures that the BESS state is up-to-date. We evaluate our proof-of-concept in a prototype development that uses lithium-ion (li-ion) batteries for the BESS.\n",
      "\n",
      "The robustness of our design is assessed against modeling attacks performed by neural networks. 0 into PostgreSQL...\n",
      "Inserting test sample 2534  DERauth is a novel battery-based authentication scheme designed for secure and distributed access to energy resources. The proposed scheme leverages the unique properties of batteries to ensure secure authentication and access control in distributed energy systems. By taking into account the limitations and challenges posed by such systems, DERauth offers a robust and efficient solution that minimizes the risk of security breaches and unauthorized access to energy resources. The proposed scheme is based on a two-step authentication process that combines cryptographic key exchange with biometric data to ensure the integrity and confidentiality of the authentication process. Our simulations and analytical models demonstrate that DERauth is capable of offering a high level of security and scalability while minimizing the computational requirements and energy consumption of authentication protocols. We show that DERauth can be effectively deployed in a variety of distributed energy systems, including smart grids and microgrids, and can effectively address the challenges of distributed energy management, security, and privacy. The proposed scheme is a significant step towards more secure and efficient distributed energy resource management, and it offers a promising solution for the future of distributed energy systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2535  We construct a Cartesian product G x H for finite simple graphs. It satisfies the Kuenneth formula: H^k(G x H) is a direct sum of tensor products H^i(G) x H^j(G) with i+j=k and so p(G x H,x) = p(G,x) p(H,y) for the Poincare polynomial p(G,x) and X(G x H) = X(G) X(H) for the Euler characteristic X(G)=p(G,-1). G1=G x K1 has as vertices the simplices of G and a natural digraph structure. We show that dim(G1) is larger or equal than dim(G) and G1 is homotopic to G. The Kuenneth identity is proven using Hodge describing the harmonic forms by the product f g of harmonic forms of G and H and uses a discrete de Rham theorem given by a combinatorial chain homotopy between simplicial and de Rham cohomology. We show dim(G x H) = dim(G1) + dim(H1) implying that dim(G x H) is larger or equal than dim(G) + dim(H) as for Hausdorff dimension in the continuum. The chromatic number c(G1) is smaller or equal than c(G) and c(G x H) is bounded above by c(G)+c(H)-1. The automorphism group of G x H contains Aut(G) x Aut(H). If G~H and U~V then (G x U) ~ (H x V) if ~ means homotopic: homotopy classes can be multiplided. If G is k-dimensional geometric meaning that all unit spheres S(x) in G are (k-1)-discrete homotopy spheres, then G1 is k-dimensional geometric. If G is k-dimensional geometric and H is l-dimensional geometric, then G x H is geometric of dimension (l+k). The product extends to a ring of chains which unlike the category of graphs is closed under boundary operation taking quotients G/A with A subset Aut(G). As we can glue graphs or chains, joins or fibre bundles can be defined with the same features as in the continuum, allowing to build isomorphism classes of bundles. 0 into PostgreSQL...\n",
      "Inserting test sample 2536  This paper presents a novel perspective on the graph-theoretic Kuenneth formula. First, the classical Kuenneth formula is introduced for topological spaces, which expresses the cohomology of a product of two spaces in terms of the cohomology of each space separately. We then define a version of this formula for graphs, which gives a way to compute the homology of the Cartesian product of graphs. Our approach is inspired by the recent development of topological data analysis, which has brought new insights into the structural properties of graphs.\n",
      "\n",
      "One of the main contributions of this paper is to provide a concrete computational method for the homology of product graphs. We show that our formula can be applied to compute the homology of several classes of graphs, such as path graphs, cycles, and complete graphs. Furthermore, we propose a natural extension of the Kuenneth formula to more general products of graphs, such as the join and the tensor product. This generalization is expected to have important applications in the analysis of complex networks, where graphs are used to model a wide range of phenomena in biology, physics, and social sciences.\n",
      "\n",
      "The paper also explores the relationship between the Kuenneth formula and other classical results in algebraic topology, such as the Mayer-Vietoris sequence, the Eilenberg-Zilber theorem, and the KÃ¼nneth theorem for cohomology. We show that the graph-theoretic Kuenneth formula can be interpreted as a version of the Mayer-Vietoris sequence for graphs, and that it has an elegant categorical formulation in terms of the tensor product of functors.\n",
      "\n",
      "Finally, we discuss some open problems and future directions of research that arise from our work. In particular, we suggest several possible extensions of the graph-theoretic Kuenneth formula to other areas of mathematics, such as combinatorics and algebraic geometry. We hope that our paper will be a useful resource for researchers and practitioners who are interested in the topology and geometry of graphs, and who want to apply these concepts to real-world problems. 1 into PostgreSQL...\n",
      "Inserting test sample 2537  Spin-driven nematicity, or the breaking of the point-group symmetry of the lattice without long-range magnetic order, is clearly quite important in iron-based superconductors. From a symmetry point of view, nematic order can be described as a coherent locking of spin fluctuations in two interpenetrating N\\'eel sublattices with ensuing nearest-neighbor bond order and an absence of static magnetism. Here, we argue that the low-temperature state of the recently discovered superconductor BaTi$_2$Sb$_2$O is a strong candidate for a more exotic form of spin-driven nematic order, in which fluctuations occurring in four N\\'eel sublattices promote both nearest- and next-nearest neighbor bond order. We develop a low-energy field theory of this state and show that it can have, as a function of temperature, up to two separate bond-order phase transitions -- namely, one that breaks rotation symmetry and one that breaks reflection and translation symmetries of the lattice. The resulting state has an orthorhombic lattice distortion, an intra-unit-cell charge density wave, and no long-range magnetic order, all consistent with reported measurements of the low-temperature phase of BaTi$_2$Sb$_2$O. We then use density functional theory calculations to extract exchange parameters to confirm that the model is applicable to BaTi$_2$Sb$_2$O. 0 into PostgreSQL...\n",
      "Inserting test sample 2538  This paper investigates the behavior of BaTi$_2$Sb$_2$O, a material which exhibits both nematic bond-ordering and stripe magnetism. We study the case where double stage nematic bond-ordering is present above double stripe magnetism. Our study utilizes density functional theory calculations to investigate the electronic and magnetic properties of BaTi$_2$Sb$_2$O using the full-potential linearized augmented plane wave method. We find that the presence of both nematic bond-ordering and stripe magnetism leads to an enhanced electronic anisotropy in the material. Specifically, we find that there is an energy-dependent anisotropy in the electronic dispersion relation of BaTi$_2$Sb$_2$O, with a stronger anisotropy present in the lower energy bands. We also study the effect of strain on the material, finding that applied strain can modify both the magnetic and electronic properties, suggesting potential for mechanical tuning of the material's behavior. Our results provide insight into the interplay between nematic bond-ordering and stripe magnetism in BaTi$_2$Sb$_2$O, and suggest potential for this material in future electronic and spintronic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 2539  In this study, I aimed to estimate the incidence of catastrophic health expenditure and analyze the extent of inequalities in out-of-pocket health expenditure and its decomposition according to gender, sector, religion and social groups of the households across Districts of West Bengal. I analysed health spending in West Bengal, using National Sample Survey 71st round pooled data suitably represented to estimate up to district level. We measured CHE at different thresholds when OOP in health expenditure. Gini Coefficients and its decomposition techniques were applied to assess the degree of inequality in OOP health expenditures and between different socio geographic factors across districts. The incidence of catastrophic payments varies considerably across districts. Only 14.1 percent population of West Bengal was covered under health coverage in 2014. The inequality in OOP health expenditure for West Bengal has been observed with gini coefficient of 0.67. Based on the findings from this analysis, more attention is needed on effective financial protection for people of West Bengal to promote fairness, with special focus on the districts with higher inequality. This study only provides the extent of CHE and inequality across Districts of West Bengal but the causality may be taken in future scope of study. 0 into PostgreSQL...\n",
      "Inserting test sample 2540  This study explores the prevalence and determinants of catastrophic health expenditure (CHE) and its distribution across different socio-economic groups at the district level in West Bengal. A cross-sectional household survey was conducted in 12 selected districts of the state between 2018-2019. The study found that 5.7% of households in West Bengal incurred CHE, defined as health expenditure exceeding 40% of household capacity to pay. The incidence of CHE was disproportionately high for households in the lowest socio-economic quintiles, where more than 7% of households reported CHE. The key determinants of CHE were hospitalization, outpatient consultation, and medicines. The study highlights the need for effective policies to protect households from CHE, particularly for vulnerable socio-economic groups. It calls for expanding financial protection measures, such as insurance and targeted subsidies, to prevent impoverishment due to healthcare costs. The results of this study underscore the urgency of addressing health inequalities and ensuring equitable access to healthcare services for all in West Bengal. 1 into PostgreSQL...\n",
      "Inserting test sample 2541  In this paper we rederive an old upper bound on the number of halving edges present in the halving graph of an arbitrary set of $n$ points in 2-dimensions which are placed in general position. We provide a different analysis of an identity discovered by Andrejak et al, to rederive this upper bound of $O(n^{4/3})$. In the original paper of Andrejak et al. the proof is based on a naive analysis whereas in this paper we obtain the same upper bound by tightening the analysis thereby opening a new door to derive these upper bounds using the identity. Our analysis is based on a result of Cardano for finding the roots of a cubic equation. We believe that our technique has the potential to derive improved bounds on the number of halving edges. 0 into PostgreSQL...\n",
      "Inserting test sample 2542  This paper presents a new derivation of the upper bound for halving edges in a graph using Cardanoâ€™s Formula. The problem of halving edges has been extensively studied in graph theory, as it has important applications in optimization and network design. Our approach builds on previous work in this area, but our use of Cardanoâ€™s Formula allows us to produce a more efficient algorithm for solving the problem. We provide a rigorous derivation of the upper bound and present experimental results demonstrating the effectiveness of our algorithm on both synthetic and real-world graphs. Our work has significant implications for the design of networks and algorithms in areas such as transportation and communication. 1 into PostgreSQL...\n",
      "Inserting test sample 2543  Lattice QCD calculations were one of the first applications to show the potential of GPUs in the area of high performance computing. Our interest is to find ways to effectively use GPUs for lattice calculations using the overlap operator. The large memory footprint of these codes requires the use of multiple GPUs in parallel. In this paper we show the methods we used to implement this operator efficiently. We run our codes both on a GPU cluster and a CPU cluster with similar interconnects. We find that to match performance the CPU cluster requires 20-30 times more CPU cores than GPUs. 0 into PostgreSQL...\n",
      "Inserting test sample 2544  Multi-GPU systems have become increasingly popular in scientific computing enabling faster calculations and the ability to handle larger datasets. However, implementing the overlap operator on multi-GPUs poses significant challenges in terms of efficiency. In this study, we propose an efficient implementation of the overlap operator on multi-GPUs that leverages the parallel processing power of these systems while minimizing data transfer between GPUs. Our proposed implementation achieves a speedup of up to 3.5x compared to the traditional approach on a benchmark problem. Our approach may have implications for other scientific computing applications that require multi-GPU solutions. 1 into PostgreSQL...\n",
      "Inserting test sample 2545  Yttrium orthosilicate (Y$_2$SiO$_5$, or YSO) has proved to be a convenient host for rare-earth ions used in demonstrations of microwave quantum memories and optical memories with microwave interfaces, and shows promise for coherent microwave--optical conversion owing to its favourable optical and spin properties. The strong coupling required by such microwave applications could be achieved using superconducting resonators patterned directly on Y$_2$SiO$_5$, and hence we investigate here the use of Y$_2$SiO$_5$ as an alternative to sapphire or silicon substrates for superconducting hybrid device fabrication. A NbN resonator with frequency 6.008 GHz and low power quality factor $Q \\approx 400000$ was fabricated on a Y$_2$SiO$_5$ substrate doped with isotopically enriched Nd$^{145}$. Measurements of dielectric loss yield a loss-tangent $\\tan\\delta = 4 \\times 10^{-6}$, comparable to sapphire. Electron spin resonance (ESR) measurements performed using the resonator show the characteristic angular dependence expected from the anisotropic Nd$^{145}$ spin, and the coupling strength between resonator and electron spins is in the high cooperativity regime ($C = 30$). These results demonstrate Y$_2$SiO$_5$ as an excellent substrate for low-loss, high-Q microwave resonators, especially in applications for coupling to optically-accessible rare earth spins. 0 into PostgreSQL...\n",
      "Inserting test sample 2546  The coherence and long-lasting properties of rare-earth spins make them essential in quantum information processing. In this study, we investigate the coupling of a spin ensemble containing rare-earth ions (REIs) to a superconducting resonator operating at microwave frequencies. Yttrium orthosilicate (YSO) is chosen as a substrate for the REI ensemble, due to its excellent material and spectroscopic properties. We demonstrate that the hybrid system exhibits high cooperativity between the REI ensemble and the superconducting resonator, which is a key quality for efficient quantum communication. Our experimental results highlight the ability to engineer the low-energy levels of the REI electron system to match the energy of the superconducting resonator, resulting in the strong coupling between the oscillator and the ensemble. Furthermore, we show that the resonance frequency of the resonator can be tuned to match the energy of the rare-earth transitions, which allows us to control and manipulate the quantum state of the coupled system. Our findings provide a promising route for realizing quantum memories and quantum communication technologies. 1 into PostgreSQL...\n",
      "Inserting test sample 2547  We investigate whether the apparent discrepancy between proton electric form factor from measurements using the Rosenbluth separation technique and polarization transfer method is due to the standard approximations employed in radiative correction procedures. Inaccuracies due to both the peaking approximation and the soft-photon approximation have been removed in our simulation approach. In contrast to results from (e,e'p) experiments, we find them in this case to be too small to explain the discrepancy. 0 into PostgreSQL...\n",
      "Inserting test sample 2548  We present an improved Rosenbluth separation technique for determining the proton charge form factor by taking into account radiative corrections. Our results show a significant improvement in the precision of the form factor, particularly at high momentum transfer. This approach provides an essential tool for accurately interpreting electron-proton scattering experiments, which play a critical role in probing the internal structure of the proton. 1 into PostgreSQL...\n",
      "Inserting test sample 2549  Recently we found an Anderson-type localization-delocalization transition in the QCD Dirac spectrum at high temperature. Using spectral statistics we obtained a critical exponent compatible with that of the corresponding Anderson model. Here we study the spatial structure of the eigenmodes both in the localized and the transition region. Based on previous studies in the Anderson model, at the critical point, the eigenmodes are expected to have a scale invariant multifractal structure. We verify the scale invariance of Dirac eigenmodes at the critical point. 0 into PostgreSQL...\n",
      "Inserting test sample 2550  We investigate the behavior of Dirac eigenmodes in the presence of quenched disorder at the Anderson transition in QCD. By analyzing the spectral properties of the QCD Dirac operator, we identify a novel scaling regime that dictates the transition between delocalized and localized states. We find that the eigenvectors of the QCD Dirac operator exhibit multifractal statistics at the transition, providing evidence for the critical behavior at the Anderson transition. Our results shed light on the interplay between topology and disorder in quantum field theory and may have implications for the study of QCD phase transitions. 1 into PostgreSQL...\n",
      "Inserting test sample 2551  We present millimeter-wave observations of several molecular ions in the disk around the pre-main-sequence star DM Tau and use these to investigate the ionization fraction in different regions of the disk. New Submillimeter Array (SMA) observations of H2D+ J=1_10 - 1_11, N2H+ J=4-3 and CO J=3-2 are presented. H2D+ and N2H+ are not detected and using the CO 3-2 disk size the observations result in an upper limit of <0.47 K km s-1 for both lines, a factor of 2.5 below previous single-dish H2D+ observations. Assuming LTE, a disk midplane temperature of 10-20 K and estimates of the H2D+ o/p ratio, the observed limit corresponds to NH2D+ < 4 - 21 \\times 1012 cm-2. We adopt a parametric model for the disk structure from the literature and use new IRAM 30 meter telescope observations of the H13CO+ J=3-2 line and previously published SMA observations of the N2H+ J=3-2, HCO+ J=3-2 and DCO+ J=3-2 lines to constrain the ionization fraction, xi, in three temperature regions in the disk where theoretical considerations suggest different ions should dominate: (1) a warm, upper layer with T>20 K where CO is in the gas-phase and HCO+ is most abundant, where we estimate xi \\simeq 4 \\times 10-10, (2) a cooler molecular layer with T = 16-20 K where N2H+ and DCO+ abundances are predicted to peak, with xi \\simeq 3\\times10-11, and (3) the cold, dense midplane with T<16 K where H3+ and its deuterated isotopologues are the main carriers of positive charge, with xi < 3\\times10-10. While there are considerable uncertainties, these estimates are consistent with a decreasing ionization fraction into the deeper, colder, and denser disk layers. Stronger constraints on the ionization fraction in the disk midplane will require not only substantially more sensitive observations of the H2D+ 1_10 - 1_11 line, but also robust determinations of the o/p ratio, observations of D2H+ and stronger constraints on where N2 is present in the gas phase. 0 into PostgreSQL...\n",
      "Inserting test sample 2552  The DM Tau protoplanetary disk has been extensively studied in recent years due to its unique characteristics and potential role in planet formation. One important aspect of this disk is the ionization fraction, which can have significant implications for the dynamics of the disk and the formation of planetary systems.\n",
      "\n",
      "In this study, we present new observations of the DM Tau protoplanetary disk using the Atacama Large Millimeter/submillimeter Array (ALMA) and the Combined Array for Research in Millimeter-wave Astronomy (CARMA). We use these observations to constrain the ionization fraction in the disk, and to investigate how it varies with distance from the central star.\n",
      "\n",
      "Our results indicate that the ionization fraction in the DM Tau protoplanetary disk is low, with values on the order of 10^-9 in the outer regions of the disk. This is consistent with previous studies of other protoplanetary disks, which suggest that ionization fractions are generally low in these systems. We also find that the ionization fraction varies significantly with distance from the central star, with higher values closer to the star.\n",
      "\n",
      "To further understand the implications of these results, we perform hydrodynamical simulations of the disk using the PLUTO code. These simulations confirm that the low ionization fraction in the disk can have significant effects on the disk's dynamics, such as reducing magnetic braking and altering the disk's accretion rate onto the central star. Our simulations also suggest that the ionization fraction may play a role in the formation of planetesimals in the outer regions of the disk.\n",
      "\n",
      "In conclusion, our observations and simulations provide new insights into the ionization fraction in the DM Tau protoplanetary disk and its implications for disk dynamics and planet formation. These findings have important implications for our understanding of protoplanetary disks and the formation of planetary systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2553  Biexciton cascade process in self-assembled quantum dots (QDs) provides an ideal system for deterministic entangled photon pair source, which is essential in quantum information science. The entangled photon pairs have recently be realized in experiments after eliminating the FSS of exciton using a number of different methods. However, so far the QDs entangled photon sources are not scalable, because the wavelengths of the QDs are different from dot to dot.\n",
      "\n",
      "Here we propose a wavelength tunable entangled photon emitter on a three dimensional stressor, in which the FSS and exciton energy can be tuned independently, allowing photon entanglement between dissimilar QDs. We confirm these results by using atomistic pseudopotential calculations. This provides a first step towards future realization of scalable entangled photon generators for quantum information applications. 0 into PostgreSQL...\n",
      "Inserting test sample 2554  This paper presents a study on the development of scalable entangled photon sources with self-assembled InAs/GaAs quantum dots. The challenge of generating scalable entangled photons with high purity and high efficiency is addressed through the use of self-assembled quantum dots as the source. The experimental setup involves the creation of an optically active quantum dot micropillar resonator, which is used to enhance the photon production efficiency and preserve the entanglement fidelity in a scalable manner. The results show that this technique achieved a significant improvement in the entanglement quality, with nearly perfect indistinguishability and high photon purity. This study provides a promising route for building scalable entangled photon sources using self-assembled quantum dots, which could lead to important advances in quantum information processing and other quantum technologies. 1 into PostgreSQL...\n",
      "Inserting test sample 2555  Predictions of the solar wind at Earth are a central aspect of space weather prediction. The outcome of such a prediction, however, is highly sensitive to the method used for computing the magnetic field in the corona. We analyze the impact of replacing the potential field coronal boundary conditions, as used in operational space weather prediction tools, by non-potential conditions. For this, we compare the predicted solar wind plasma parameters with observations at 1 AU for two six-months intervals, one at solar maximum and one in the descending phase of the current cycle. As a baseline, we compare with the operational Wang-Sheeley-Arge model. We find that for solar maximum, the non-potential coronal model and an adapted solar wind speed formula lead to the best solar wind predictions in a statistical sense. For the descending phase, the potential coronal model performs best. The Wang-Sheeley-Arge model outperforms the others in predicting high speed enhancements and streamer interactions. A better parameter fitting for the adapted wind speed formula is expected to improve the performance of the non-potential model here. 0 into PostgreSQL...\n",
      "Inserting test sample 2556  The prediction of the solar wind and its impact on the Earth's magnetosphere is a crucial element of space weather forecasting. In this study, we investigate the impact of non-potential coronal boundary conditions on the accuracy of solar wind predictions. Specifically, we use a three-dimensional magnetohydrodynamic model to simulate the behavior of the solar wind under various boundary conditions. Our results demonstrate that non-potential boundary conditions can significantly influence the magnetic topology of the corona, leading to variations in solar wind speed, density, and temperature. These variations may be particularly important in regions of the corona where magnetic fields are highly twisted or sheared. Furthermore, we discuss the implications of our findings for space weather prediction, including the potential impact on satellite and terrestrial communication systems. Our study highlights the importance of accurate modeling of coronal boundary conditions and the need for ongoing research in this area to improve our understanding of space weather dynamics. 1 into PostgreSQL...\n",
      "Inserting test sample 2557  We study compactifications of F-theory on certain Calabi--Yau threefolds. We find that $N=2$ dualities of type II/heterotic strings in 4 dimensions get promoted to $N=1$ dualities between heterotic string and F-theory in 6 dimensions. The six dimensional heterotic/heterotic duality becomes a classical geometric symmetry of the Calabi--Yau in the F-theory setup. Moreover the F-theory compactification sheds light on the nature of the strong coupling transition and what lies beyond the transition at finite values of heterotic string coupling constant. 0 into PostgreSQL...\n",
      "Inserting test sample 2558  This paper explores compactifications of F-Theory on Calabi-Yau threefolds. We investigate how the addition of 7-branes influences the resulting theory and how the choice of the compactification manifold and the singular fibers affect the resulting physics. We study the global properties of the moduli space, the action of duality symmetries, and the viability of the effective description in terms of a weakly coupled heterotic string. The findings in this study contribute to the research of duality symmetries and their role in the low-energy limit of string theory. 1 into PostgreSQL...\n",
      "Inserting test sample 2559  In relativistic quantum mechanics, elementary particles are described by irreducible unitary representations of the Poincare group. The same applies to the center-of-mass kinematics of a multi-particle system that is not subject to external forces. As shown in a previous article, for spin-1/2 particles, irreducibility leads to a correlation between the particles that has the structure of the electromagnetic interaction, as described by the perturbation algorithm of quantum electrodynamics. The present article examines the consequences of irreducibility for a multi-particle system of spinless particles. In this case, irreducibility causes a gravitational force, which in the classical limit is described by the field equations of conformal gravity.\n",
      "\n",
      "The strength of this force has the same order of magnitude as the strength of the empirical gravitational force. 0 into PostgreSQL...\n",
      "Inserting test sample 2560  The emergence of space-time and gravitation is a long-standing focus of cosmological inquiry, yet a comprehensive understanding of the origins and mechanics of these fundamental concepts remains elusive. Recent progress at the intersection of quantum field theory and general relativity has deepened our understanding of the interplay between fundamental forces and the geometry of space-time. Mathematical models offer valuable insights into the emergence of gravitation from quantum mechanics and the dynamics of space-time at different scales, from the microscopic to the cosmological. This paper surveys recent work in this rapidly-evolving field, highlighting key insights and discoveries that are changing our understanding of the universe's most basic building blocks. By shedding light on the emergence of space-time and gravitation, this research offers a powerful framework for exploring the deepest mysteries of the cosmos. 1 into PostgreSQL...\n",
      "Inserting test sample 2561  A key difficulty that arises from real event data is imprecision in the recording of event time-stamps. In many cases, retaining event times with a high precision is expensive due to the sheer volume of activity. Combined with practical limits on the accuracy of measurements, aggregated data is common. In order to use point processes to model such event data, tools for handling parameter estimation are essential. Here we consider parameter estimation of the Hawkes process, a type of self-exciting point process that has found application in the modeling of financial stock markets, earthquakes and social media cascades. We develop a novel optimization approach to parameter estimation of aggregated Hawkes processes using a Monte Carlo Expectation-Maximization (MC-EM) algorithm. Through a detailed simulation study, we demonstrate that existing methods are capable of producing severely biased and highly variable parameter estimates and that our novel MC-EM method significantly outperforms them in all studied circumstances. These results highlight the importance of correct handling of aggregated data. 0 into PostgreSQL...\n",
      "Inserting test sample 2562  In this paper, we present a novel Monte Carlo EM algorithm for estimating the parameters of aggregated Hawkes processes. We first introduce the mathematical framework of the Hawkes process and its aggregated version, highlighting the challenges in parameter estimation. We then propose a method that uses the EM algorithm with a Monte Carlo maximization step to overcome these challenges. Our algorithm leverages the auxiliary variable method and the thinning operation to develop an efficient and accurate estimation process. The algorithm's effectiveness is demonstrated through numerical simulations, where we show it can accurately recover the input parameters of various models, including those with complex dependencies and time-varying intensities. We further validate the proposed method by applying it to a real-world dataset from the financial industry and compare it to existing methods. Our results show that our method outperforms existing approaches, highlighting the potential of Monte Carlo EM in parameter estimation for Hawkes processes. 1 into PostgreSQL...\n",
      "Inserting test sample 2563  We consider the problem of searching for gravitational waves emitted during the inspiral phase of binary systems when the orbital plane precesses due to relativistic spin-orbit coupling. Such effect takes place when the spins of the binary members are misaligned with respect to the orbital angular momentum. As a first step we assess the importance of precession specifically for the first-generation of LIGO detectors. We investigate the extent of the signal-to-noise ratio reduction and, hence, detection rate that occurs when precession effects are not accounted for in the template waveforms. We restrict our analysis to binary systems that undergo the so-called simple precession and have a total mass close to 10 solar mass. We find that for binary systems with rather high mass ratios (e.g., a 1.4 solar mass neutron star and a 10 solar mass black hole) the detection rate can decrease by almost an order of magnitude. Current astrophysical estimates of the rate of binary inspiral events suggest that LIGO could detect at most a few events per year, and therefore the reduction of the detection rate even by a factor of a few is critical. In the second part of our analysis, we examine whether the effect of precession could be included in the templates by capturing the main features of the phase modulation through a small number of extra parameters. Specifically we examine and tested for the first time the 3-parameter family suggested by Apostolatos. We find that, even though these ``mimic'' templates improve the detection rate, they are still inadequate in recovering the signal-to-noise ratio at the desired level. We conclude that a more complex template family is needed in the near future, still maintaining the number of additional parameters as small as possible in order to reduce the computational costs. 0 into PostgreSQL...\n",
      "Inserting test sample 2564  Gravitational waves are ripples in spacetime that are produced by violent astrophysical events, such as the merger of black holes or the inspiral of binary systems. The detection of gravitational waves is a major goal of modern physics, as it can provide important insights into the nature of gravity and the universe itself. In this paper, we investigate the problem of searching for gravitational waves from the inspiral of precessing binary systems, with a particular focus on the limitations of current waveforms.\n",
      "\n",
      "One of the key challenges in the detection of gravitational waves is the accurate modelling of the waveform, which encodes important information about the physics of the source. Precessing binary systems, where the orientation of the spinning black holes changes over time, pose a particularly difficult problem since their waveform is highly complex and can vary rapidly. Current methods for modelling these waveforms rely on a simplified assumption of a fixed orientation, which can lead to significant errors in the detection and analysis of the signal.\n",
      "\n",
      "We explore the limitations of current waveform models for precessing binary systems and present a new approach using advanced mathematical techniques such as numerical relativity and machine learning. Our methodology takes into account the full complexity of the waveform, including the precession effects, and leverages a large database of numerical simulations to improve the accuracy of detection.\n",
      "\n",
      "We demonstrate the superiority of our methodology over current state-of-the-art waveform models through extensive simulations using realistic noise models and signal injections. Our results show that the new approach leads to a significant increase in detection rate and accuracy, paving the way for the future detection and analysis of gravitational waves from precessing binary systems.\n",
      "\n",
      "In conclusion, our study highlights the key challenges in the detection of gravitational waves from precessing binary systems and provides a promising solution to overcome the limitations of current waveform models. 1 into PostgreSQL...\n",
      "Inserting test sample 2565  This papers defines the syntax and semantics of GP 2, a revised version of the graph programming language GP. New concepts are illustrated and explained with example programs. Changes to the first version of GP include an improved type system for labels, a built-in marking mechanism for nodes and edges, a more powerful edge predicate for conditional rule schemata, and functions returning the indegree and outdegree of matched nodes. Moreover, the semantics of the branching and loop statement have been simplified to allow their efficient implementation. 0 into PostgreSQL...\n",
      "Inserting test sample 2566  GP 2 is a newly designed system with innovative features which aim to solve the existing challenges in its predecessor, GP 1. This paper presents the design and implementation techniques used in developing GP 2, highlighting its new capabilities such as improved scalability, fault tolerance and support for distributed computing. We also discuss the system's architecture, algorithms used in GP 2 and compare its performance against other state-of-the-art systems in accordance with established benchmarks. Our results demonstrate that GP 2 offers significant improvements in performance, efficiency, and fault tolerance over its predecessor GP 1. 1 into PostgreSQL...\n",
      "Inserting test sample 2567  Using a large N-body cosmological simulation combined with a subgrid treatment of galaxy formation, we study the formation and evolution of the galaxy and cluster population in a comoving volume (100 Mpc)^3 in a LCDM universe. At z = 0, our computational volume contains 1788 clusters with mass M_cl > 1.1x10^12 Msun, including 18 massive clusters with M_cl > 10^14 Msun. It also contains 1 088 797 galaxies with mass M_gal > 2x10^9 Msun and luminosity L > 9.5x10^5 Lsun. For each cluster, we identified the brightest cluster galaxy (BCG). We then computed the fraction f_BNC of clusters in which the BCG is not the closest galaxy to the center of the cluster in projection, and the ratio Dv/s, where Dv is the difference in radial velocity between the BCG and the whole cluster, and s is the radial velocity dispersion of the cluster. f_BNC increases from 0.05 for low-mass clusters (M_cl ~ 10^12 Msun) to 0.5 for high-mass ones (M_cl > 10^14 Msun), with no dependence on cluster redshift. The values of Dv/s vary from 0 to 1.8. These results are consistent with previous observational studies, and indicate that the central galaxy paradigm, which states that the BCG should be at rest at the center of the cluster, is usually valid, but exceptions are too common to be ignored. Analysis of the merger trees for the 18 most massive clusters in the simulation reveals that 16 of these clusters have experienced major mergers in the past. These mergers leave each cluster in a non-equilibrium state, but eventually the cluster settles into an equilibrium configuration, unless it is disturbed by another major merger. We found evidence that these mergers are responsible for the off-center positions and peculiar velocities of some BCGs. Our results thus support the merging-group scenario, in which some clusters form by the merger of smaller groups in which the galaxies have already formed. 0 into PostgreSQL...\n",
      "Inserting test sample 2568  Galaxy clusters are among the most massive structures in the universe. Their formation and evolution remain an area of active research, in which merging events play a crucial role. In this study, we focus on the brightest cluster galaxy (BCG) and its position relative to the major cluster merger. The BCG is often found at the center of a cluster and plays a crucial role in regulating the gas and dark matter that make up the intracluster medium. \n",
      "\n",
      "We use a sample of high-redshift cluster mergers, identified by their X-ray emission and confirmed by optical and infrared observations. We analyze the properties of the BCG relative to the cluster merger axis and the distance to the merging subclusters. Our results show that the BCG tends to be located towards the center of the cluster or the merging subclusters. This suggests that the BCG's displacement from the cluster center is minimized during the merging process, possibly due to the gravitational attraction of the merger. \n",
      "\n",
      "We also find that the BCG tends to have a higher velocity dispersion than the other cluster galaxies, which could be a result of its central location and the accretion of satellite galaxies during the merging process. These findings support the idea that major cluster mergers have a significant impact on the BCG's properties. \n",
      "\n",
      "Our study provides new insights into the formation and evolution of galaxy clusters and the role of mergers in shaping their properties. The BCG's position and velocity dispersion are important indicators of the merger process, and understanding their behavior can help us better constrain the physics of cluster formation and the growth of supermassive black holes at the center of galaxies. Future studies should explore these properties further, incorporating additional observations and simulations to refine our understanding of the BCG and its role in galaxy clusters. 1 into PostgreSQL...\n",
      "Inserting test sample 2569  We focus on rational solutions or nearly-feasible rational solutions that serve as certificates of feasibility for polynomial optimization problems. We show that, under some separability conditions, certain cubic polynomially constrained sets admit rational solutions. However, we show in other cases that it is NP-Hard to detect if rational solutions exist or if they exist of any reasonable size. We extend this idea to various settings including near feasible, but super optimal solutions and detecting rational rays on which a cubic function is unbounded. Lastly, we show that in fixed dimension, the feasibility problem over a set defined by polynomial inequalities is in NP by providing a simple certificate to verify feasibility. We conclude with several related examples of irrationality and encoding size issues in QCQPs and SOCPs. 0 into PostgreSQL...\n",
      "Inserting test sample 2570  Polynomial optimization problems, which involve minimizing/maximizing polynomial functions over a set of constraints, have gained increasing importance in several engineering and scientific fields. The complexity of such problems is mainly due to the nonlinearity and high degree of the polynomials involved. Exactness, on the other hand, refers to the capability of obtaining an optimal solution using feasible algorithms. In this context, rationality principles have been developed to achieve exactness, where the optimization problem is reformulated using a rational function. This approach has been proved to provide exactness in fewer iterations compared to other existing methods. This paper provides a comprehensive review of the complexity of polynomial optimization, the exactness of existing algorithms, and the use of rational functions to achieve exactness. 1 into PostgreSQL...\n",
      "Inserting test sample 2571  The standard interpretation of the Stern-Gerlach experiment assumes that the atomic center-of-mass plays the role of \"quantum apparatus\" for the atomic spin. Following a recent, decoherence-based, model fitting with this interpretation, we investigate whether or not such model can be constructed.\n",
      "\n",
      "Our conclusions are somewhat surprising: only if the screen capturing the atoms in the experiment brings the information about the atomic-nucleus center-of-mass, one may construct the model desired. The nucleus $CM$ system is monitored by the nucleus \"relative system ($R$)\". There appear the effective (the electrons-mediated) interaction between $CM$ and $R$ that is possibly responsible for decoherence. For larger atoms, the interaction scales as $Z^2$ ($Z$ is the \"atomic number\"), being totally independent on the atomic mass. The interaction selects the $CM$-wave-packet states as the approximate pointer basis. Interestingly enough, the model stems nonoccurrence of decoherence due to the internal environment for the larger systems (such as the macromolecules and the macroscopic systems). Certainly, disproving this model (e.g. in an experiment) stems the active role of the screen, which becomes responsible for the \"$CM$ + spin\"-state \"reduction\" (\"collapse\"), i.e. for the irreversible retrieval of the classical information from the quantum world. 0 into PostgreSQL...\n",
      "Inserting test sample 2572  The Stern-Gerlach experiment is a seminal work in the field of quantum mechanics that has proven to be fundamental to our understanding of spin quantization. Despite the importance of this experiment, many elements of its internal-environment model have yet to be fully comprehended. This paper delves into the complexities of the internal-environment model of the Stern-Gerlach experiment. We begin with a comprehensive discussion of the theory behind the model, examining various interpretations and proposing novel alternative theories. We then consider the implications of the environment for the accuracy of the measurements in terms of preparation and detection difficulties, and compare the merits and drawbacks of competing models. Furthermore, we discuss the role that decoherence and entanglement play in the internal dynamics of the system. Finally, in order to validate the model, we present a range of experimental results. Our findings highlight the importance of further research in this field and have significant implications for the development of quantum technologies. 1 into PostgreSQL...\n",
      "Inserting test sample 2573  We discuss the ultrafast evolution of the surface electronic structure of the topological insulator Bi$_2$Te$_3$ following a femtosecond laser excitation.\n",
      "\n",
      "Using time and angle resolved photoelectron spectroscopy, we provide a direct real-time visualisation of the transient carrier population of both the surface states and the bulk conduction band. We find that the thermalization of the surface states is initially determined by interband scattering from the bulk conduction band, lasting for about 0.5 ps; subsequently, few ps are necessary for the Dirac cone non-equilibrium electrons to recover a Fermi-Dirac distribution, while their relaxation extends over more than 10 ps. The surface sensitivity of our measurements makes it possible to estimate the range of the bulk-surface interband scattering channel, indicating that the process is effective over a distance of 5 nm or less. This establishes a correlation between the nanoscale thickness of the bulk charge reservoir and the evolution of the ultrafast carrier dynamics in the surface Dirac cone. 0 into PostgreSQL...\n",
      "Inserting test sample 2574  The study of topological insulators has gained significant attention due to their unique electronic properties. In this work, we investigate the ultrafast carrier dynamics on the surface of the topological insulator Bi2Te3 using time-resolved spectroscopy. We report on the observation of a fast relaxation of hot carriers, which is attributed to electron-phonon scattering and is found to be independent of temperature. In addition, we find evidence for carrier recombination at defect sites, with decay times in the nanosecond range. The observed dynamics are consistent with a model involving the interplay of surface states and bulk impurities. Our results provide important insights into the fundamental properties of topological insulators and can guide the development of future devices based on these materials. The ultrafast time scale of carrier dynamics opens up new possibilities for novel ultrafast optoelectronic applications. 1 into PostgreSQL...\n",
      "Inserting test sample 2575  We consider online algorithms for the $k$-server problem on trees. Chrobak and Larmore proposed a $k$-competitive algorithm for this problem that has the optimal competitive ratio. However, a naive implementation of their algorithm has $O(n)$ time complexity for processing each query, where $n$ is the number of nodes in the tree. We propose a new time-efficient implementation of this algorithm that has $O(n\\log n)$ time complexity for preprocessing and $O\\left(k^2 + k\\cdot \\log n\\right)$ time for processing a query. We also propose a quantum algorithm for the case where the nodes of the tree are presented using string paths. In this case, no preprocessing is needed, and the time complexity for each query is $O(k^2\\sqrt{n}\\log n)$. When the number of queries is $o\\left(\\frac{\\sqrt{n}}{k^2\\log n}\\right)$, we obtain a quantum speed-up on the total runtime compared to our classical algorithm.\n",
      "\n",
      "We also give a simple quantum algorithm to find the first marked element in a collection of $m$ objects, that works even in the presence of two-sided bounded errors on the input oracle. It has worst-case complexity $O(\\sqrt{m})$. In the particular case of one-sided errors on the input, it has expected time complexity $O(\\sqrt{x})$ where $x$ is the position of the first marked element.\n",
      "\n",
      "Compare with previous work, our algorithm can handle errors in the input oracle. 0 into PostgreSQL...\n",
      "Inserting test sample 2576  In this paper, we propose both classical and quantum algorithms for solving the online $k$-server problem on trees. The $k$-server problem consists of maintaining $k$ servers on a network in order to minimize the cost of service requests. In the online setting, requests arrive dynamically and the server's actions must be decided immediately. We first introduce a one-step lookahead algorithm for the classical $k$-server problem that utilizes a dynamic programming approach. The algorithm achieves a competitive ratio of O(log k)/log log k and provides a guarantee that the cost of our solution is within a constant factor of the optimal. Next, we present a quantum algorithm for the same problem, which has polynomial time complexity and delivers an exponential improvement over the best known classical algorithm. Our algorithm utilizes a quantum walk framework, which enhances classical random walk algorithms by exploiting quantum parallelism. It has an improved competitive ratio of 2.325 for $k=2$, and 3 for larger values of $k$. Furthermore, we prove a novel lower bound for quantum algorithms in the $k$-server problem, which shows that any quantum algorithm must use at least $\\frac{k}{2^{O(\\sqrt{log\\,k})}}$ cost. Our work contributes to both classical and quantum algorithmic research, providing insights into efficient algorithmic solutions for the online $k$-server problem on trees. 1 into PostgreSQL...\n",
      "Inserting test sample 2577  Using a nonlinear electrodynamics coupled to teleparallel theory of gravity, three regular charged spherically symmetric solutions are obtained. The nonlinear theory reduces to the Maxwell one in the weak limit and the solutions correspond to charged spacetimes. The third solution contains an arbitrary function from which we can generate the other two solutions. The metric associated with these spacetimes is the same, i.e., a regular charged static spherically symmetric black hole. In calculating the energy content of the third solution using the gravitational energy-momentum given by M{\\o}ller, within the framework of the teleparallel geometry, we find that the resulting form depends on the arbitrary function. Using the regularized expression of the gravitational energy-momentum we get the value of energy. 0 into PostgreSQL...\n",
      "Inserting test sample 2578  In this work, we investigate the existence and properties of regular charged solutions in the framework of Teleparallel Theory of Gravity. We consider the gravitational field equations in the presence of electromagnetic fields and assume spherical symmetry. We derive the condition for the existence of electrically charged solutions and obtain their corresponding metric solutions. Furthermore, we analyze the global structure of these solutions and show that they possess interesting physical properties, such as horizons and singularities. We also study the stability of these solutions under linear perturbations and find that they are unstable. Our results shed light on the interplay between gravity and electromagnetism in the presence of charged matter and contribute to the ongoing investigation of the consequences of alternative theories of gravity. 1 into PostgreSQL...\n",
      "Inserting test sample 2579  Supernova cosmology without spectroscopic confirmation is an exciting new frontier which we address here with the Bayesian Estimation Applied to Multiple Species (BEAMS) algorithm and the full three years of data from the Sloan Digital Sky Survey II Supernova Survey (SDSS-II SN). BEAMS is a Bayesian framework for using data from multiple species in statistical inference when one has the probability that each data point belongs to a given species, corresponding in this context to different types of supernovae with their probabilities derived from their multi-band lightcurves. We run the BEAMS algorithm on both Gaussian and more realistic SNANA simulations with of order 10^4 supernovae, testing the algorithm against various pitfalls one might expect in the new and somewhat uncharted territory of photometric supernova cosmology. We compare the performance of BEAMS to that of both mock spectroscopic surveys and photometric samples which have been cut using typical selection criteria. The latter typically are either biased due to contamination or have significantly larger contours in the cosmological parameters due to small data-sets. We then apply BEAMS to the 792 SDSS-II photometric supernovae with host spectroscopic redshifts. In this case, BEAMS reduces the area of the (\\Omega_m,\\Omega_\\Lambda) contours by a factor of three relative to the case where only spectroscopically confirmed data are used (297 supernovae). In the case of flatness, the constraints obtained on the matter density applying BEAMS to the photometric SDSS-II data are \\Omega_m(BEAMS)=0.194\\pm0.07. This illustrates the potential power of BEAMS for future large photometric supernova surveys such as LSST. 0 into PostgreSQL...\n",
      "Inserting test sample 2580  The study of cosmology is essential in understanding the origin and evolution of the universe. Supernovae can be used as cosmological probes as they are massive explosions that enable astronomers to measure distances to faraway galaxies. In this research paper, we present a new method for photometric supernova cosmology using the Bayesian Estimation of Absolute Magnitudes (BEAMS) and the Sloan Digital Sky Survey II (SDSS-II).\n",
      "\n",
      "Our new method combines the power of photometric supernova data with the multi-band imaging from SDSS-II to infer the properties of dark energy and matter. We analyze a sample of 500 Type Ia supernovae and obtain accurate cosmological constraints on the equation of state parameter of dark energy, w, and the matter density parameter, Î©m. Our method also enables us to probe the curvature of the universe, providing a critical test for the inflationary theory.\n",
      "\n",
      "BEAMS provides a theoretically-motivated framework to relate photometric observations to physical properties. Our approach allows us to use redshift and host galaxy properties to predict the peak absolute magnitudes and light curve shapes of supernovae. By comparing our predictions to the actual observations from SDSS-II, we can infer the cosmological parameters.\n",
      "\n",
      "Overall, our results suggest that photometric supernova cosmology with BEAMS and SDSS-II is a powerful technique in constraining the properties of the universe. Moreover, our approach can be applied to future large-scale surveys, such as the Large Synoptic Survey Telescope (LSST), to further probe the nature of dark energy and matter. 1 into PostgreSQL...\n",
      "Inserting test sample 2581  We obtain a necessary and sufficient condition for the orthomartingale-coboundary decomposition. We establish a sufficient condition for the approximation of the partial sums of a strictly stationary random fields by those of stationary orthomartingale differences. This condition can be checked under multidimensional analogues of the Hannan condition and the Maxwell-Woodroofe condition. 0 into PostgreSQL...\n",
      "Inserting test sample 2582  We prove a new Invariance Principle for functionals of orthomartingales that are nearly stationary. Our approach involves employing an approximation technique based on the discrete-time fractional Brownian motion. We establish sharp estimates for the error terms and provide applications to limit theorems for functionals of nonstationary processes and statistical inference for diffusion processes. 1 into PostgreSQL...\n",
      "Inserting test sample 2583  We find that the fraction of classical Broad Absorption Line quasars (BALQSOs) among the FIRST radio sources in the Sloan Data Release 3, is 20.5^{+7.3}_{-5.9}% at the faintest radio powers detected (L_{\\rm 1.4 GHz}~10^{32} erg/s), and rapidly drops to <8% at L_{\\rm 1.4 GHz}~3*10^{33} erg/s. Similarly, adopting the broader Absorption Index (AI) definition of Trump et al. (2006) we find the fraction of radio BALQSOs to be 44^{+8.1}_{-7.8}% reducing to 23.1^{+7.3}_{-6.1}% at high luminosities. While the high fraction at low radio power is consistent with the recent near-IR estimates by Dai et al. (2008), the lower fraction at high radio powers is intriguing and confirms previous claims based on smaller samples. The trend is independent of the redshift range, the optical and radio flux selection limits, or the exact definition of a radio match. We also find that at fixed optical magnitude, the highest bins of radio luminosity are preferentially populated by non-BALQSOs, consistent with the overall trend. We do find, however, that those quasars identified as AI-BALQSOs but \\emph{not} under the classical definition, do not show a significant drop in their fraction as a function of radio power, further supporting independent claims for which these sources, characterized by lower equivalent width, may represent an independent class with respect to the classical BALQSOs. We find the balnicity index, a measure of the absorption trough in BALQSOs, and the mean maximum wind velocity to be roughly constant at all radio powers. We discuss several plausible physical models which may explain the observed fast drop in the fraction of the classical BALQSOs with increasing radio power, \\emph{although no one is entirely satisfactory}.\n",
      "\n",
      "(abridged). 0 into PostgreSQL...\n",
      "Inserting test sample 2584  The fraction of Broad Absorption Line Quasars (BALQSOs) in the quasar population has been a subject of study, as they may represent a transitional stage between unobscured and obscured quasars. In this work, we investigate the dependence of the BALQSO fraction on radio luminosity in a sample of optically-selected quasars at redshifts of 1.5 < z < 3.5 using the Sloan Digital Sky Survey data release 14. We measure the radio luminosity using the Faint Images of the Radio Sky at Twenty-centimeters survey and select only radio-loud quasars to maximize the range of radio luminosities in the sample. To quantify the BALQSO fraction, we use both traditional and machine learning based BALQSO identification methods, finding no significant dependence on radio luminosity. However, using a larger sample of X-Shooter quasars from the European Southern Observatory, we find tentative evidence for a higher BALQSO fraction among those with low radio luminosity. We explore possible reasons for this result, such as differing accretion rates, orientation effects or environmental dependencies. We also compare our results to previous studies which used smaller samples or radio selection criteria. Overall, we conclude that the dependence of the BALQSO fraction on radio luminosity is weak or absent in this sample and suggest that future studies should focus on other possible correlations, such as those between the BALQSO fraction and bolometric luminosity or the host galaxy properties. 1 into PostgreSQL...\n",
      "Inserting test sample 2585  We completely classify Friedmann-Lema\\^{i}tre-Robertson-Walker solutions with spatial curvature $K=0,\\pm 1$ and equation of state $p=w\\rho$, according to their conformal structure, singularities and trapping horizons. We do not assume any energy conditions and allow $\\rho < 0$, thereby going beyond the usual well-known solutions. For each spatial curvature, there is an initial spacelike big-bang singularity for $w>-1/3$ and $\\rho>0$, while no big-bang singularity for $w<-1$ and $\\rho>0$. For $K=0$ or $-1$, $-1<w<-1/3$ and $\\rho>0$, there is an initial null big-bang singularity. For each spatial curvature, there is a final spacelike future big-rip singularity for $w<-1$ and $\\rho>0$, with null geodesics being future complete for $-5/3\\le w<-1$ but incomplete for $w<-5/3$. For $w=-1/3$, the expansion speed is constant. For $-1<w<-1/3$ and $K=1$, the universe contracts from infinity, then bounces and expands back to infinity. For $K=0$, the past boundary consists of timelike infinity and a regular null hypersurface for $-5/3<w<-1$, while it consists of past timelike and past null infinities for $w\\le -5/3$. For $w<-1$ and $K=1$, the spacetime contracts from an initial spacelike past big-rip singularity, then bounces and blows up at a final spacelike future big-rip singularity. For $w<-1$ and $K=-1$, the past boundary consists of a regular null hypersurface.\n",
      "\n",
      "The trapping horizons are timelike, null and spacelike for $w\\in (-1,1/3)$, $w\\in \\{1/3, -1\\}$ and $w\\in (-\\infty,-1)\\cup (1/3,\\infty)$, respectively. A negative energy density ($\\rho <0$) is possible only for $K=-1$. In this case, for $w>-1/3$, the universe contracts from infinity, then bounces and expands to infinity; for $-1<w<-1/3$, it starts from a big-bang singularity and contracts to a big-crunch singularity; for $w<-1$, it expands from a regular null hypersurface and contracts to another regular null hypersurface. 0 into PostgreSQL...\n",
      "Inserting test sample 2586  This theoretical study provides a comprehensive classification of the Friedmann-LemaÃ®tre-Robertson-Walker (FLRW) cosmological model with a linear equation of state (EOS) for the cosmic fluid. The FLRW metric is a mathematical framework that describes the expansion of the Universe, where the scale factor is a function of time. Our research considers a perfect fluid with a linear EOS, which means that the pressure is proportional to the energy density of the fluid. We use a conformal transformation to reduce the system to a single second-order differential equation that is easier to analyze.\n",
      "\n",
      "We classify the FLRW solutions based on the sign of the spatial curvature, which can be positive, negative, or zero. In addition, we classify the solutions according to the behavior of the scale factor as a function of time. There are three types of solutions: expanding, contracting, and bouncing. We further classify the expanding solutions according to the rate of expansion. We find that the linear EOS allows for a rich variety of solutions that exhibit different cosmological behaviors.\n",
      "\n",
      "To perform the classification, we apply a set of mathematical techniques that involve algebraic manipulation, differential equations, and qualitative analysis. We show that the behavior of the solutions is determined by the interaction between the curvature and the EOS. The curvature acts as a source term that modifies the expansion rate and the evolution of the energy density. We also investigate the stability of the solutions under small perturbations and find that some solutions are unstable.\n",
      "\n",
      "Our results extend the existing literature on FLRW cosmology with a linear EOS. They provide a deeper understanding of the dynamics of the Universe in the framework of general relativity. Our classification can be used as a framework for testing the compatibility of different cosmological models with observational data. It can also be extended to include more sophisticated models of matter and energy, such as dark matter and dark energy. 1 into PostgreSQL...\n",
      "Inserting test sample 2587  We present results on low-resolution mid-infrared (MIR) spectra of 70 infrared-luminous galaxies obtained with the Infrared Spectrograph (IRS) onboard Spitzer. We selected sources from the European Large Area Infrared Survey (ELAIS) with S15 > 0.8 mJy and photometric or spectroscopic z > 1. About half of the sample are QSOs in the optical, while the remaining sources are galaxies, comprising both obscured AGN and starbursts. We classify the spectra using well-known infrared diagnostics, as well as a new one that we propose, into three types of source: those dominated by an unobscured AGN (QSOs), obscured AGN, and starburst-dominated sources. Starbursts concentrate at z ~ 0.6-1.0 favored by the shift of the 7.7-micron PAH band into the selection 15 micron band, while AGN spread over the 0.5 < z < 3.1 range. Star formation rates (SFR) are estimated for individual sources from the luminosity of the PAH features. An estimate of the average PAH luminosity in QSOs and obscured AGN is obtained from the composite spectrum of all sources with reliable redshifts.\n",
      "\n",
      "The estimated mean SFR in the QSOs is 50-100 Mo yr^-1, but the implied FIR luminosity is 3-10 times lower than that obtained from stacking analysis of the FIR photometry, suggesting destruction of the PAH carriers by energetic photons from the AGN. The SFR estimated in obscured AGN is 2-3 times higher than in QSOs of similar MIR luminosity. This discrepancy might not be due to luminosity effects or selection bias alone, but could instead indicate a connection between obscuration and star formation. However, the observed correlation between silicate absorption and the slope of the near- to mid-infrared spectrum is compatible with the obscuration of the AGN emission in these sources being produced in a dust torus. 0 into PostgreSQL...\n",
      "Inserting test sample 2588  This research paper presents the results of a study involving the mid-infrared spectroscopy of infrared-luminous galaxies at redshifts ranging from z~0.5 to 3. The data for this study was obtained from the Spitzer Space Telescope, and was examined using spectral modeling techniques. Our findings indicate that infrared-luminous galaxies have significantly different mid-infrared spectral features when compared to galaxies with lower levels of infrared emission. Specifically, we found a strong correlation between the strength of the polycyclic aromatic hydrocarbon (PAH) features and the level of infrared luminosity, as well as a decrease in the strength of the [NeIII] feature at higher levels of luminosity. Our study also revealed that the mid-infrared spectra of infrared-luminous galaxies at z~0.5-3 show evidence of highly obscured star formation activity, likely resulting from a combination of dust and gas. Furthermore, our results suggest that the observed changes in the spectral features are likely due to variations in the conditions of the interstellar medium in these galaxies. Overall, our study provides new insights into the properties of infrared-luminous galaxies at high redshifts, which are essential for understanding the evolution of galaxies over cosmic time. This work also has important implications for future studies of galaxy formation and evolution, particularly those involving observations at mid-infrared wavelengths. 1 into PostgreSQL...\n",
      "Inserting test sample 2589  In this Rapid Communication, a set of $^{209}$Bi-nuclear magnetic resonance (NMR)/nuclear quadrupole resonance (NQR) measurements has been performed to investigate the physical properties of superconducting (SC) BaTi$_2$Bi$_2$O from a microscopic point of view. The NMR and NQR spectra at 5~K can be reproduced with a non-zero in-plane anisotropic parameter $\\eta$, indicating the breaking of the in-plane four-fold symmetry at the Bi site without any magnetic order, i.e., `the electronic nematic state'. In the SC state, the nuclear spin-lattice relaxation rate divided by temperature, $1/T_1T$, does not change even below $T_{\\rm c}$, while a clear SC transition was observed with a diamagnetic signal. This observation can be attributed to the strong two-dimensionality in BaTi$_2$Bi$_2$O. Comparing the NMR/NQR results among BaTi$_2$$Pn$$_2$O ($Pn$ = As, Sb, and Bi), it was found that the normal and SC properties of BaTi$_2$Bi$_2$O were considerably different from those of BaTi$_2$Sb$_2$O and BaTi$_2$As$_2$O, which might explain the two-dome structure of $T_{\\rm c}$ in this system. 0 into PostgreSQL...\n",
      "Inserting test sample 2590  This study presents the observation of a nematic transition and highly two-dimensional superconductivity in the compound BaTi$_2$Bi$_2$O, as revealed by $^{209}$Bi-nuclear magnetic resonance/nuclear quadrupole resonance measurements. The nematic transition is evidenced by a sharp peak in the observed NMR/NQR spectra, which coincides with a resistivity anomaly in the material. The high-temperature superconducting phase is strongly two-dimensional, as evidenced by the anisotropy of the NMR/NQR spectra and the dependence of the superconducting transition temperature on the applied magnetic field. The low-energy electronic excitations in the system are also shown to be highly anisotropic, consistent with the strongly two-dimensional nature of the superconducting phase. These results provide new insights into the interplay between nematic order and superconductivity in complex materials, and suggest that BaTi$_2$Bi$_2$O may be a promising platform for the study of two-dimensional superconductivity and the development of novel superconducting devices. 1 into PostgreSQL...\n",
      "Inserting test sample 2591  We select a volume-limited sample of galaxies derived from the SDSS-DR7 to study the environment of low surface brightness (LSB) galaxies at different scales, as well as several physical properties of the dark matter haloes where the LSB galaxies of the sample are embedded. To characterize the environment we make use of a number of publicly available value-added galaxy catalogues. We find a slight preference for LSB galaxies to be found in filaments instead of clusters, with their mean distance to the nearest filament typically larger than for high surface brightness (HSB) galaxies. The fraction of isolated central LSB galaxies is higher than the same fraction for HSB ones, and the density of their local environment lower. The stellar-to-halo mass ratio using four different estimates is up to $\\sim$20% for HSB galaxies. LSB central galaxies present more recent assembly times when compared with their HSB counterparts. Regarding the $\\lambda$ spin parameter, using six different proxies for its estimation, we find that LSB galaxies present systematically larger values of $\\lambda$ than the HSB galaxy sample, and constructing a control sample with direct kinematic information drawn from ALFALFA, we confirm that the spin parameter of LSB galaxies is 1.6 to 2 times larger than the one estimated for their HSB counterparts. 0 into PostgreSQL...\n",
      "Inserting test sample 2592  Low Surface Brightness galaxies (LSBs) are a type of galaxy that have a low surface brightness and are therefore difficult to detect and study. In this research paper, we investigate the environment of LSBs at different scales to better understand their formation and evolution. We use data from the Sloan Digital Sky Survey to study the spatial distribution, clustering, and large-scale environment of LSB galaxies. Our results show that LSBs are preferentially located in low-density regions of the universe and have a weak clustering signal. We also find evidence of anisotropy in the clustering pattern of LSBs, which may be indicative of their formation from primordial density fluctuations. Additionally, we examine the impact of nearby galaxies on the properties of LSBs and find that their star formation rates and metallicities are affected by their local environment. Our findings provide insight into the formation and evolution of LSBs and highlight the importance of studying the environment of galaxies at different scales. 1 into PostgreSQL...\n",
      "Inserting test sample 2593  Since the early works[1-4] on the so-called nondiffracting waves (called also Localized Waves), a great deal of results has been published on this important subject, from both the theoretical and the experimental point of view.\n",
      "\n",
      "Initially, the theory was developed taking into account only free space; however, in recent years, it has been extended for more complex media exhibiting effects such as dispersion[5-7], nonlinearity[8], anisotropy[9] and losses[10]. Such extensions have been carried out along with the development of efficient methods for obtaining nondiffracting beams and pulses in the subluminal, luminal and superluminal regimes[11-18]. This paper (partly a review) addresses some theoretical methods related to nondiffracting solutions of the linear wave equation in unbounded homogeneous media, as well as to some interesting applications of such waves. In section II we analyze the general structure of the Localized Waves, develop the so called Generalized Bidirectional Decomposition, and use it to obtain several luminal and superluminal (especially X-shaped) nondiffracting solutions of the wave equation. In section III we develop a space-time focusing method by a continuous superposition of X-Shaped pulses of different velocities. Section IV addresses the properties of chirped optical X-Shaped pulses propagating in material media without boundaries. Finally, in Section V, we show how a suitable superposition of Bessel beams can be used to obtain stationary localized wave fields, with a static envelope and a high transverse localization, and whose longitudinal intensity pattern can assume any desired shape within a chosen interval of the propagation axis. 0 into PostgreSQL...\n",
      "Inserting test sample 2594  In this paper, we investigate the structure and properties of nondiffracting, or localized, waves. These waves are characterized by their ability to propagate without spreading out, which makes them unique and valuable in a variety of applications. We examine their mathematical formulation in detail, using the exact solutions of the wave equation, and identify their fundamental properties, such as their self-healing behavior and their ability to carry orbital angular momentum. We also discuss the different methods used to generate these waves, such as using computer-generated holography or spatial light modulators.\n",
      "\n",
      "Furthermore, we explore some of the interesting applications of these waves, including their use in optical tweezers, where they can be used to manipulate microscopic particles without causing damage; in free-space optical communication, where they can be used to increase the range and capacity of these systems; and in medical imaging, where they can be used to improve the resolution and contrast of images. We also discuss some of the challenges in using these waves and potential solutions to overcome them.\n",
      "\n",
      "In conclusion, the nondiffracting waves, or localized waves, are an exciting and valuable area of research. We have established their mathematical formulation, fundamental properties, and various methods to generate them, as well as explored some interesting applications. The future of this field offers great potential for advancements in a diverse range of areas, including technology, biology, and medicine. 1 into PostgreSQL...\n",
      "Inserting test sample 2595  In order to provide an estimate of eps'/eps several effective theories and physical effects have to be disentangled. In this talk I discuss how it is possible to predict eps '/eps taking into account all sources of large logs.\n",
      "\n",
      "The numerical result one obtains, $\\eps '/\\eps \\sim (1.7\\pm 0.6) \\cdot 10^{-4}$, is in good agreement with present measurements. 0 into PostgreSQL...\n",
      "Inserting test sample 2596  The parameter epsilon'/epsilon in the Standard Model describes the amount of direct CP violation in the kaon system. Despite being predicted to be very small, recent experimental measurements have shown values that are larger than expected. This discrepancy suggests the potential for new physics beyond the Standard Model, making epsilon'/epsilon an important test for physics beyond the Standard Model, and a promising avenue for probing the nature of CP violation in particle physics. 1 into PostgreSQL...\n",
      "Inserting test sample 2597  We numerically study the orientation deformations in nematic liquid crystals around charged particles. We set up a Ginzburg-Landau theory with inhomogeneous electric field. If the dielectric anisotropy varepsilon_1 is positive, Saturn ring defects are formed around the particles. For varepsilon_1<0, novel \"ansa\" defects appear, which are disclination lines with their ends on the particle surface. We find unique defect structures around two charged particles. To lower the free energy, oppositely charged particle pairs tend to be aligned in the parallel direction for varepsilon_1>0 and in the perpendicular plane for varepsilon_1<0 with respect to the background director . For identically charged pairs the preferred directions for varepsilon_1>0 and varepsilon_1<0 are exchanged. We also examie competition between the charge-induced anchoring and the short-range anchoring. If the short-range anchoring is sufficiently strong, it can be effective in the vicinity of the surface, while the director orientation is governed by the long-range electrostatic interaction far from the surface. 0 into PostgreSQL...\n",
      "Inserting test sample 2598  The behavior of nematic liquid crystals around charged particles significantly affects various modern technological applications such as electro-optic devices. An interesting phenomenon that occurs in such systems is the formation of topological defects, which are regions where the liquid crystal's orientation relative to the charged particle's surface changes. To explore the mechanisms underlying these defects, we performed Monte Carlo simulations and theoretical analysis of the microscopic structure of the liquid crystals around the charged particles. Our results show that the defects arise due to the minimization of the energy associated with the director field distortions, and their characteristics depend on the particle's size, charge, and surface anchoring conditions. We also found that the defects' topological charges can be related to the particle's surface charges. The knowledge gained from this study can enable the design of new materials with tailored defect structures, leading to novel liquid crystal-based applications. 1 into PostgreSQL...\n",
      "Inserting test sample 2599  We present examples of divergence for the BFGS and Gauss Newton methods.\n",
      "\n",
      "These examples have objective functions with bounded level sets and other properties concerning the examples published recently in this journal, like unit steps and convexity along the search lines. As these other examples, the iterates, function values and gradients in the new examples fit into the general formulation in our previous work {\\it On the divergence of line search methods, Comput. Appl. Math. vol.26 no.1 (2007)}, which also presents an example of divergence for Newton's method. 0 into PostgreSQL...\n",
      "Inserting test sample 2600  This paper explores the divergence of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) and Gauss-Newton methods, two popular optimization techniques used for solving nonlinear regression problems. Through numerical simulations and mathematical analysis, we show that the BFGS method can behave erratically and diverge, particularly when dealing with ill-conditioned problems, while the Gauss-Newton method remains more stable. Specifically, we illustrate that the BFGS method can converge to wrong solutions or fail to converge altogether, highlighting the need for careful consideration of the choice of optimization method. These results have implications for practitioners in fields such as computer vision and machine learning. 1 into PostgreSQL...\n",
      "Inserting test sample 2601  We present an attempt to reach realistic turbulent regime in direct numerical simulations of the geodynamo. We rely on a sequence of three convection-driven simulations in a rapidly rotating spherical shell. The most extreme case reaches towards the Earth's core regime by lowering viscosity (magnetic Prandtl number Pm=0.1) while maintaining vigorous convection (magnetic Reynolds number Rm>500) and rapid rotation (Ekman number E=1e-7), at the limit of what is feasible on today's supercomputers. A detailed and comprehensive analysis highlights several key features matching geomagnetic observations or dynamo theory predictions -- all present together in the same simulation -- but it also unveils interesting insights relevant for Earth's core dynamics.In this strong-field, dipole-dominated dynamo simulation, the magnetic energy is one order of magnitude larger than the kinetic energy. The spatial distribution of magnetic intensity is highly heterogeneous, and a stark dynamical contrast exists between the interior and the exterior of the tangent cylinder (the cylinder parallel to the axis of rotation that circumscribes the inner core).In the interior, the magnetic field is strongest, and is associated with a vigorous twisted polar vortex, whose dynamics may occasionally lead to the formation of a reverse polar flux patch at the surface of the shell.\n",
      "\n",
      "Furthermore, the strong magnetic field also allows accumulation of light material within the tangent cylinder, leading to stable stratification there.\n",
      "\n",
      "Torsional Alfv{\\'e}n waves are frequently triggered in the vicinity of the tangent cylinder and propagate towards the equator.Outside the tangent cylinder, the magnetic field inhibits the growth of zonal winds and the kinetic energy is mostly non-zonal. Spatio-temporal analysis indicates that the low-frequency, non-zonal flow is quite geostrophic (columnar) and predominantly large-scale: an m=1 eddy spontaneously emerges in our most extreme simulations, without any heterogeneous boundary forcing.Our spatio-temporal analysis further reveals that (i) the low-frequency, large-scale flow is governed by a balance between Coriolis and buoyancy forces -- magnetic field and flow tend to align, minimizing the Lorentz force; (ii) the high-frequency flow obeys a balance between magnetic and Coriolis forces; (iii) the convective plumes mostly live at an intermediate scale, whose dynamics is driven by a 3-term 1 MAC balance -- involving Coriolis, Lorentz and buoyancy forces. However, small-scale (E^{1/3}) quasi-geostrophic convection is still observed in the regions of low magnetic intensity. 0 into PostgreSQL...\n",
      "Inserting test sample 2602  The Earth's magnetic field is a vital shield that protects it from the solar wind and cosmic radiation. This magnetic field is generated by the geodynamo, a self-sustaining natural process of fluid flow in the Earth's core. Understanding the geodynamo is crucial to comprehend Earth's magnetic field and its influence on the planet's habitability. In this paper, we present simulations of the geodynamo, in particular, the turbulent nature of its convective flows.\n",
      "\n",
      "Our simulations, which solve the magnetohydrodynamic equations, reveal new insights into the geodynamo's dynamics. The simulations simulate the fluid motion and magnetic field in the Earth's core with spatial and temporal resolutions never achieved before, using state-of-the-art supercomputing. In particular, we investigated the effects of the Earth's rotation and mantle core boundary on the geodynamo's convective flows.\n",
      "\n",
      "Our results suggest that the turbulent motions of the geodynamo play a central role in generating the magnetic field. The fluctuations produced by these turbulent flows produce small-scale magnetic fields by stretching and twisting the magnetic field lines. These fields eventually merge into larger fields on longer timescales. This process generates the strong, large-scale magnetic fields observed on Earth's surface.\n",
      "\n",
      "Our simulations also reveal that the mantle core boundary has a significant impact on the geodynamo's dynamics. The boundary controls the amount of heat leaving the core, which affects the convective flow. Our simulations show that different models of the mantle core boundary can lead to significant changes in the geodynamo's behavior and magnetic field.\n",
      "\n",
      "Our findings provide new insights into the fundamental physics of the geodynamo and the generation of Earth's magnetic field. Our simulations demonstrate that the turbulent nature of the geodynamo's convective flows play a central role in generating the magnetic field, which is essential for the Earth's habitability. Moreover, our work provides a solid foundation for future studies investigating the effects of other parameters, such as the viscosity of the core or the magnetic diffusivity of the mantle core boundary, on the geodynamo's behavior. Ultimately, our simulations represent an exciting leap forward in our understanding of the geodynamo and its critical role in the Earth's dynamics. 1 into PostgreSQL...\n",
      "Inserting test sample 2603  We report on the first ALMA observation of the CO(3$-$2) and rest-frame ~340 GHz continuum emission in PDS 456, which is the most luminous, radio-quiet QSO in the local Universe ($z$~0.18), with a bolometric luminosity $L_{\\rm Bol}\\sim10^{47}$ erg s$^{-1}$. ALMA angular resolution allowed us to map scales as small as ~700 pc. The molecular gas reservoir, traced by the core of the very bright CO(3$-$2) emission line, is distributed in a compact rotating disk, with size of ~1.3 kpc, seen close to face-on ($i$~25 deg). Fast CO(3$-$2) emission in the velocity range $v\\in[-1000,+500]$ km s$^{-1}$ is also present.\n",
      "\n",
      "Specifically, we detect several blue-shifted clumps out to ~5 kpc from the nucleus, in addition to a compact ($R\\lesssim1.2$ kpc), broad emission component. These components reveal a galaxy-wide molecular outflow, with a total mass $M_{\\rm mol}^{\\rm out}\\sim2.5\\times10^8$ $M_{\\odot}$ and a mass outflow rate $\\dot{M}_{\\rm mol}\\sim290$ $M_{\\odot}$ yr$^{-1}$. The corresponding depletion time is ~8 Myr, shorter than the rate at which the molecular gas is converted into stars, indicating that the detected outflow is potentially able to quench star-formation in the host. The momentum flux of the molecular outflow normalised to the radiative momentum output (i.e. $L_{\\rm Bol}/c$) is $\\lesssim1$, comparable to that of the X-ray ultra-fast outflow (UFO) detected in PDS 456. This is at odds with the expectations for an energy-conserving expansion suggested for most of the large-scale outflows detected in low-luminosity AGN so far. We suggest three possible scenarios that may explain this observation: (i) in very luminous AGN such as our target the molecular gas phase is tracing only a fraction of the total outflowing mass; (ii) a small coupling between the shocked gas by the UFO and the host-galaxy ISM (iii) AGN radiation pressure may play an important role in driving the outflow. 0 into PostgreSQL...\n",
      "Inserting test sample 2604  High-energy feedback from active galactic nuclei (AGN) is thought to play a crucial role in shaping galaxy evolution. In order to understand the impact of AGN on their host galaxies, it is imperative to investigate the properties of the outflows they launch. PDS 456 is an ideal candidate for studying AGN feedback due to its extreme luminosity and the presence of a kpc-scale molecular outflow. In this work, we present observations of PDS 456 made with ALMA that allow us to study the molecular gas in the outflow. We detect two components in the outflow, a higher velocity, faster-moving component and a lower velocity, slower-moving component. These two components may be related to the different ways in which the outflow interacts with the surrounding medium. The faster-moving component may be caused by a direct interaction with the interstellar medium or by the interaction of a warm wind with a cooler, denser medium. The slower-moving component may be caused by entrainment of cold gas from the host galaxy. Our results suggest that PDS 456 has a substantial impact on its host galaxy. The outflow is likely to drive significant turbulence in the interstellar medium of the host galaxy, which could eventually lead to the suppression of star formation. Our findings highlight the importance of studying AGN feedback at kpc scales and provide new insights into the physical processes taking place in the AGN environment. In conclusion, our study of the molecular outflow in PDS 456 provides a valuable contribution to the field of AGN feedback and sheds light on the complex interplay between AGNs and their host galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 2605  In this paper, we consider a surrogate modeling approach using a data-driven nonparametric likelihood function constructed on a manifold on which the data lie (or to which they are close). The proposed method represents the likelihood function using a spectral expansion formulation known as the kernel embedding of the conditional distribution. To respect the geometry of the data, we employ this spectral expansion using a set of data-driven basis functions obtained from the diffusion maps algorithm. The theoretical error estimate suggests that the error bound of the approximate data-driven likelihood function is independent of the variance of the basis functions, which allows us to determine the amount of training data for accurate likelihood function estimations. Supporting numerical results to demonstrate the robustness of the data-driven likelihood functions for parameter estimation are given on instructive examples involving stochastic and deterministic differential equations. When the dimension of the data manifold is strictly less than the dimension of the ambient space, we found that the proposed approach (which does not require the knowledge of the data manifold) is superior compared to likelihood functions constructed using standard parametric basis functions defined on the ambient coordinates. In an example where the data manifold is not smooth and unknown, the proposed method is more robust compared to an existing polynomial chaos surrogate model which assumes a parametric likelihood, the non-intrusive spectral projection. 0 into PostgreSQL...\n",
      "Inserting test sample 2606  Parameter estimation is a fundamental task in many scientific applications ranging from physics to machine learning. Nonparametric likelihood functions provide a flexible way to model the relationship between data and parameters by avoiding the restrictive assumptions of parametric models. In this work, we propose a data-driven approach for constructing nonparametric likelihood functions from the data. The proposed approach is based on the concept of graph-coupled likelihood, which learns an optimal nonparametric likelihood function by leveraging information from other related datasets. Specifically, we develop a framework to estimate the graph-coupled function by solving a convex optimization problem with respect to the likelihood function and the graph structure. Our framework is able to capture complex dependencies between the data and the parameters, which results in improved parameter estimation performance compared to existing methods. We demonstrate the effectiveness of our approach on a set of synthetic and real datasets, and show that it outperforms state-of-the-art methods in terms of accuracy and scalability. Our approach has the potential to be used in a wide range of applications where accurate parameter estimation is crucial for decision-making and modeling. 1 into PostgreSQL...\n",
      "Inserting test sample 2607  This is the second paper of a series that reports on our investigation of the clustering properties of AGNs in the ROSAT All-Sky Survey (RASS) through cross-correlation functions (CCFs) with Sloan Digital Sky Survey (SDSS) galaxies. In this paper, we apply the Halo Occupation Distribution (HOD) model to the CCFs between the RASS Broad-line AGNs with SDSS Luminous Red Galaxies (LRGs) in the redshift range 0.16<z<0.36 that was calculated in paper I. In our HOD modeling approach, we use the known HOD of LRGs and constrain the HOD of the AGNs by a model fit to the CCF. For the first time, we are able to go beyond quoting merely a `typical' AGN host halo mass, M_h, and model the full distribution function of AGN host dark matter halos. In addition, we are able to determine the large-scale bias and the mean M_h more accurately. We explore the behavior of three simple HOD models. Our first model (Model A) is a truncated power-law HOD model in which all AGNs are satellites. With this model, we find an upper limit to the slope (\\alpha) of the AGN HOD that is far below unity. The other two models have a central component, which has a step function form, where the HOD is constant above a minimum mass, without (Model B) or with (Model C) an upper mass cutoff, in addition to the truncated power-law satellite component, similar to the HOD that is found for galaxies.\n",
      "\n",
      "In these two models we find the upper limits of \\alpha < 0.95 and \\alpha < 0.84 for Model B and C respectively. Our analysis suggests that the satellite AGN occupation increases slower than, or may even decrease with, M_h, in contrast to the satellite's HODs of luminosity-threshold samples of galaxies, which, in contrast, grow approximately as \\propto M_h^\\alpha with \\alpha\\approx 1. These results are consistent with observations that the AGN fraction in groups and clusters decreases with richness. 0 into PostgreSQL...\n",
      "Inserting test sample 2608  This research paper presents the second part of the spatial clustering analysis of the ROSAT All-Sky Survey AGNs. The focus of this work is to model the Halo Occupation Distribution (HOD) of the cross-correlation function of these AGNs with the underlying dark matter halos. The analysis was carried out by utilizing the Two-Halo term formulated by the linear bias theory which enables the prediction of the cross-correlation function from the HOD parameters. \n",
      "\n",
      "To perform this analysis, we utilized the publicly available AGN catalog in the RASS observational band in conjunction with the latest dark matter halo catalogs from the MultiDark simulation. The cross-correlation function was estimated under different luminosity thresholds and redshift ranges using a Landy-Szalay estimator. The HOD parameters were then derived through a Maximum Likelihood estimator, which provides the probability distribution of the number of AGNs belonging to a certain halo mass.\n",
      "\n",
      "The resulting HOD parameters were compared to those predicted in previous studies and revealed a significant dependence on the luminosity threshold and redshift range. Furthermore, we found that the best-fit HOD parameters are characterized by low values of the halo mass threshold and a higher satellite fraction. This behavior was also confirmed by a comparison with the Poisson models with a similar number density of AGNs.\n",
      "\n",
      "In summary, this work provides a novel method to understand the clustering behavior of AGNs based on the Halo Occupation Distribution modeling of the cross-correlation function. The derived HOD parameters and the comparison with the previous studies can be useful to better constrain the AGN formation and evolution models and the relation between AGN and their host halo masses. 1 into PostgreSQL...\n",
      "Inserting test sample 2609  Spitzer/IRS spectra from 5 to 37 um for a complete sample of 31 R Coronae Borealis stars (RCBs) are presented. These spectra are combined with optical and near-infrared photometry of each RCB at maximum light to compile a spectral energy distribution (SED). The SEDs are fitted with blackbody flux distributions and estimates made of the ratio of the infrared flux from circumstellar dust to the flux emitted by the star. Comparisons for 29 of the 31 stars are made with the IRAS fluxes from three decades earlier: Spitzer and IRAS fluxes at 12 um and 25 um are essentially equal for all but a minority of the sample. For this minority, the IRAS to Spitzer flux ratio exceeds a factor of three. The outliers are suggested to be stars where formation of a dust cloud or dust puff is a rare event. A single puff ejected prior to the IRAS observations may have been reobserved by Spitzer as a cooler puff at a greater distance from the RCB. RCBs which experience more frequent optical declines have, in general, a circumstellar environment containing puffs subtending a larger solid angle at the star and a quasi-constant infrared flux. Yet, the estimated subtended solid angles and the blackbody temperatures of the dust show a systematic evolution to lower solid angles and cooler temperatures in the interval between IRAS and Spitzer. Dust emission by these RCBs and those in the LMC is similar in terms of total 24 um luminosity and [8.0]-[24.0] color index. 0 into PostgreSQL...\n",
      "Inserting test sample 2610  This paper presents the analysis of Spitzer/IRS observations aimed at exploring the properties of the dusty circumstellar environment around R Coronae Borealis (RCB) stars. These objects are rare and intriguing due to their sudden brightness drops by several magnitudes, which are thought to be associated with the condensation of soot particles in their atmospheres. The mid-infrared spectra obtained with Spitzer reveal the presence of prominent emission features from amorphous and crystalline silicates, carbonates, and polycyclic aromatic hydrocarbons (PAHs) in the immediate vicinity of the stars. The dust grains are characterized by a wide range of sizes, from a few to hundreds of nanometers, and are mostly distributed in a flattened geometry, consistent with the expectation of a circumstellar disk or torus. Some of the RCB stars exhibit additional spectral features, such as broad absorption bands from hydrogenated amorphous carbon or molecular hydrogen, which suggest a complex interplay between the chemistry and physics of the circumstellar medium. Our findings shed light on the origin and evolution of RCB stars and their detectable characteristics, which may be useful for the interpretation of similar phenomena in other types of stars and environments. Further investigations with higher spatial and spectral resolution may provide more detailed information on the nature of the dust and gas components and their relationships with the central stars. 1 into PostgreSQL...\n",
      "Inserting test sample 2611  XSS J12270-4859 is the only low mass X-ray binary (LMXB) with a proposed persistent gamma-ray counterpart in the Fermi-LAT domain, 2FGL 1227.7-4853.\n",
      "\n",
      "Here, we present the results of the analysis of recent INTEGRAL observations, aimed at assessing the long-term variability of the hard X-ray emission, and thus the stability of the accretion state. We confirm that the source behaves as a persistent hard X-ray emitter between 2003 and 2012. We propose that XSS J12270-4859 hosts a neutron star in a propeller state, a state we investigate in detail, developing a theoretical model to reproduce the associated X-ray and gamma-ray properties. This model can be understood as being of a more general nature, representing a viable alternative by which LMXBs can appear as gamma-ray sources. In particular, this may apply to the case of millisecond pulsars performing a transition from a state powered by the rotation of their magnetic field, to a state powered by matter in-fall, such as that recently observed from the transitional pulsar PSR J1023+0038. While the surface magnetic field of a typical NS in a LMXB is lower by more than four orders of magnitude than the much more intense fields of neutron stars accompanying high-mass binaries, the radius at which the matter in-flow is truncated in a NS-LMXB system is much lower. The magnetic field at the magnetospheric interface is then orders of magnitude larger at this interface, and as consequence, so is the power to accelerate electrons. We demonstrate that the cooling of the accelerated electron population takes place mainly through synchrotron interaction with the magnetic field permeating the interface, and through inverse Compton losses due to the interaction between the electrons and the synchrotron photons they emit. We found that self-synchrotron Compton processes can explain the high energy phenomenology of XSS J12270-4859. 0 into PostgreSQL...\n",
      "Inserting test sample 2612  Low-mass X-ray binaries (LMXBs) are systems composed of a neutron star (NS) or black hole in a tight orbit with a low-mass companion star. In these systems, matter from the companion star is accreted onto the compact object. XSS J12270-4859 is a member of this class of systems that exhibits both X-ray and gamma-ray emission. Its gamma-ray properties have been recently studied with the Fermi Large Area Telescope (LAT). Here we present a scenario based on the propeller effect to explain the gamma-ray properties of XSS J12270-4859.\n",
      "\n",
      "The propeller effect arises when the fast spinning magnetosphere of the NS is able to disrupt the accretion flow from the companion star. This occurs when the magnetospheric radius is larger than the corotation radius, which depends on the orbital period and the mass of the NS. In this scenario, the material is prevented from falling onto the NS surface and instead is expelled from the system with a high velocity. This expelled material can then produce gamma-ray emission through a variety of mechanisms.\n",
      "\n",
      "Our analysis shows that the observed properties of XSS J12270-4859 can be explained by the propeller scenario. The low luminosity of the system implies that the accretion rate onto the NS is very low, which is consistent with the propeller regime. In addition, the observed gamma-ray variability and spectral properties can also be explained by this scenario, through the interaction of the expelled material with the surrounding medium.\n",
      "\n",
      "Our results highlight the importance of considering the propeller effect in the study of LMXBs with gamma-ray emission. Future observations of similar systems can provide further constraints on the propeller scenario, and improve our understanding of the accretion physics in such systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2613  A tomographic method is considered that forms images from sets of spatially randomized source signals and receiver sensitivities. The method is designed to allow image reconstruction for an extended number of transmitters and receivers in the presence noise and without plane wave approximation or otherwise approximation on the size or regularity of source and receiver functions. An overdetermined set of functions are formed from the Hadamard product between a Gaussian function and a uniformly distributed random number set. It is shown that this particular type of randomization tends to produce well-conditioned matrices whose pseudoinverses may be determined without implementing relaxation methods. When the inverted sets are applied to simulated first-order scattering from a Shepp-Logan phantom, successful image reconstructions are achieved for signal-to-noise ratios (SNR) as low as 1. Evaluation of the randomization approach is conducted by comparing condition numbers with other forms of signal randomization. Image quality resulting from tomographic reconstructions is then compared with an idealized synthetic aperture approach, which is subjected to a comparable SNR. By root-mean-square-difference comparisons it is concluded that - provided a sufficient level of oversampling - the dynamic transmit and dynamic receive approach produces superior images, particularly in the presence of low SNR. 0 into PostgreSQL...\n",
      "Inserting test sample 2614  Ultrasound tomography is a non-invasive imaging technique that has shown great potential in various medical applications. In recent years, there has been significant development in the field of ultrasound tomography, particularly in the use of transmit and receive methods. One promising technique is randomized transmit and receive ultrasound tomography, which eliminates the need for a priori knowledge of the object being imaged and improves the image resolution. This technique involves an array of transducers that randomly transmit and receive signals, generating a large amount of data that is processed to form an image. In this paper, we present a study of randomized transmit and receive ultrasound tomography, focusing on its theoretical foundations, practical challenges, and recent developments. We review the current state of the art and highlight the potential benefits and limitations of this approach. Furthermore, we discuss the future directions of the technique and identify key areas for further research. Our findings suggest that randomized transmit and receive ultrasound tomography has significant potential for improving the accuracy and precision of ultrasound imaging, and could have a transformative impact on a wide range of medical applications. 1 into PostgreSQL...\n",
      "Inserting test sample 2615  Is Android malware classification a solved problem? Published F1 scores of up to 0.99 appear to leave very little room for improvement. In this paper, we argue that results are commonly inflated due to two pervasive sources of experimental bias: \"spatial bias\" caused by distributions of training and testing data that are not representative of a real-world deployment; and \"temporal bias\" caused by incorrect time splits of training and testing sets, leading to impossible configurations. We propose a set of space and time constraints for experiment design that eliminates both sources of bias. We introduce a new metric that summarizes the expected robustness of a classifier in a real-world setting, and we present an algorithm to tune its performance.\n",
      "\n",
      "Finally, we demonstrate how this allows us to evaluate mitigation strategies for time decay such as active learning. We have implemented our solutions in TESSERACT, an open source evaluation framework for comparing malware classifiers in a realistic setting. We used TESSERACT to evaluate three Android malware classifiers from the literature on a dataset of 129K applications spanning over three years. Our evaluation confirms that earlier published results are biased, while also revealing counter-intuitive performance and showing that appropriate tuning can lead to significant improvements. 0 into PostgreSQL...\n",
      "Inserting test sample 2616  TESSERACT is a novel approach to malware classification that aims to eliminate experimental bias across space and time. Existing techniques rely on static features such as file hashes or dynamic features such as API calls; however, these methods are vulnerable to changes in malware behavior or evasion techniques. TESSERACT addresses this limitation by using an ensemble of anomaly detection algorithms that capture the unique behavior of each malware sample. By analyzing a broad range of features, including network traffic, system call sequences, and registry entries, TESSERACT is able to achieve high detection rates while mitigating the effects of bias. To evaluate TESSERACT's effectiveness, we conducted experiments using a large and diverse dataset of malware samples from different periods and regions. The results demonstrate that TESSERACT outperforms state-of-the-art classifiers and is more resilient to variance in the data. In addition, TESSERACT's ensemble design allows for flexibility in adapting to evolving malware trends. Overall, TESSERACT represents a significant advancement in malware classification and has the potential for widespread application in cybersecurity. 1 into PostgreSQL...\n",
      "Inserting test sample 2617  Domains where supervised models are deployed often come with task-specific constraints, such as prior expert knowledge on the ground-truth function, or desiderata like safety and fairness. We introduce a novel probabilistic framework for reasoning with such constraints and formulate a prior that enables us to effectively incorporate them into Bayesian neural networks (BNNs), including a variant that can be amortized over tasks. The resulting Output-Constrained BNN (OC-BNN) is fully consistent with the Bayesian framework for uncertainty quantification and is amenable to black-box inference. Unlike typical BNN inference in uninterpretable parameter space, OC-BNNs widen the range of functional knowledge that can be incorporated, especially for model users without expertise in machine learning. We demonstrate the efficacy of OC-BNNs on real-world datasets, spanning multiple domains such as healthcare, criminal justice, and credit scoring. 0 into PostgreSQL...\n",
      "Inserting test sample 2618  This paper proposes a novel approach for incorporating interpretable output constraints in Bayesian neural networks. The proposed method uses a combination of Bayesian regularization and variational inference to impose constraints on the network's output. The resulting model generates reliable predictions while maintaining an interpretable output structure. We evaluate our method on several datasets, including a real-world case study involving medical diagnoses. Our results demonstrate that the proposed framework improves the network's accuracy while providing human-understandable output, making the model a valuable tool for decision-making tasks. Additionally, we present a thorough analysis of the uncertainty in our approach, illustrating its ability to provide confidence estimates for predictions. The incorporation of interpretable output constraints in Bayesian neural networks opens up new avenues for interpretable artificial intelligence and decision-making in complex systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2619  The commutation matrix was first introduced in statistics as a transposition matrix by Murnaghan in 1938. In this paper, we first investigate the commutation matrix which is employed to transform a matrix into its transpose.\n",
      "\n",
      "We then extend the concept of the commutation matrix to commutation tensor and use the commutation tensor to achieve the unification of the two formulae of the linear preserver of the matrix rank, a classical result of Marcus in 1971. 0 into PostgreSQL...\n",
      "Inserting test sample 2620  Commutation matrices and commutation tensors are mathematical tools used to study the commutativity and non-commutativity of matrices and tensors. Commutation matrices allow for the computation of commutators, while commutation tensors are used to study the commutativity of higher-order tensor products. In this paper, we provide a detailed analysis of these tools and their applications in various fields including physics, computer science and engineering. We also present some new results and open problems related to commutation matrices and tensors. 1 into PostgreSQL...\n",
      "Inserting test sample 2621  We study the thermodynamics and thermodynamic geometry of Park black hole in Ho\\v{r}ava gravity. By incorporating the ideas of differential geometry, we have investigated the thermodynamics using Weinhold geometry and Ruppeiner geometry. We have also analyzed it in the context of newly developed geometrothermodynamics(GTD). Divergence of specific heat is associated with the second order phase transition of black hole. Here in the context of Park black hole, both Weinhold's metric and Ruppeiner's metric well explain this phase transition. But these explanations depend on the choice of potential. Hence the Legendre invariant GTD is used, and with the true singularities in the curvature scalar, GTD well explain the second order phase transition. All these methods together give an exact idea of all the behaviors of the Park black hole thermodynamics. 0 into PostgreSQL...\n",
      "Inserting test sample 2622  In this study, we investigate the thermodynamics and thermodynamic geometry of Park black hole. We begin by examining the thermodynamics of the black hole through the first law of thermodynamics, and then we explore its geometric properties in detail. Our findings reveal that the black hole's thermodynamic behavior is closely related to the geometry of its phase space. We also establish that the black hole undergoes a phase transition, from a small black hole to a large one, as its temperature crosses a certain threshold value. Moreover, we demonstrate that the black hole's thermodynamic stability can be deduced with the help of its thermodynamic geometry. Our results provide valuable insights into the thermodynamic behavior of black holes and present new avenues for future research in this field. 1 into PostgreSQL...\n",
      "Inserting test sample 2623  We introduce a new fractional oscillator process which can be obtained as solution of a stochastic differential equation with two fractional orders.\n",
      "\n",
      "Basic properties such as fractal dimension and short range dependence of the process are studied by considering the asymptotic properties of its covariance function. The fluctuation--dissipation relation of the process is investigated.\n",
      "\n",
      "The fractional oscillator process can be regarded as one-dimensional fractional Euclidean Klein-Gordon field, which can be obtained by applying the Parisi-Wu stochastic quantization method to a nonlocal Euclidean action. The Casimir energy associated with the fractional field at positive temperature is calculated by using the zeta function regularization technique. 0 into PostgreSQL...\n",
      "Inserting test sample 2624  In this paper, we introduce a new family of fractional oscillator processes with two indices, which provides a more flexible representation of self-similar stochastic processes than existing models. We explore its properties, including the Hurst parameter, the interdependence between the indices, and the long-term scaling behavior. We apply the process to model various phenomena, such as financial time series and turbulence. We derive numerical techniques for efficient simulation and estimation, and show their accuracy through numerical examples. The proposed model shows promising potential for capturing the complexity of diverse phenomena that exhibit multi-scaling behavior, and opens up new avenues for future research in the field of fractional calculus. 1 into PostgreSQL...\n",
      "Inserting test sample 2625  In the past 20 years, the enumeration of plane lattice walks confined to a convex cone -- normalized into the first quadrant -- has received a lot of attention, stimulated the development of several original approaches, and led to a rich collection of results. Most of them deal with the nature of the associated generating function: for which models is it algebraic, D-finite, D-algebraic? By model, what we mean is a finite collection of allowed steps.\n",
      "\n",
      "More recently, similar questions have been raised for non-convex cones, typically the three-quadrant cone $C = \\{ (i,j) : i \\geq 0 \\text{ or } j \\geq 0 \\}$. They turn out to be more difficult than their quadrant counterparts. In this paper, we investigate a collection of eight models in $C$. This collection consists of diagonally symmetric models in $\\{-1, 0,1\\}^2\\setminus\\{(-1,1), (1,-1)\\}$. Three of them are known not to be D-algebraic. We show that the remaining five can be solved in a uniform fashion using Tutte's notion of \\emph{invariants}, which has already proved useful for some quadrant models.\n",
      "\n",
      "Three models are found to be algebraic, one is (only) D-finite, and the last one is (only) D-algebraic. We also solve in the same fashion the diagonal model $\\{ \\nearrow, \\nwarrow, \\swarrow, \\searrow\\}$, which is D-finite. The three algebraic models are those of the Kreweras trilogy, $S=\\{\\nearrow, \\leftarrow, \\downarrow\\}$, $S'=\\{\\rightarrow, \\uparrow, \\swarrow\\}$, and $S\\cup S'$.\n",
      "\n",
      "Our solutions take similar forms for all six models. Roughly speaking, the square of the generating function of three-quadrant walks with steps in $S$ is an explicit rational function in the quadrant generating function with steps in $\\{(j-i,j): (i,j) \\in S\\}$. We derive various corollaries, including an explicit algebraic description of the positive harmonic function in $C$ for the five models that are at least D-finite. 0 into PostgreSQL...\n",
      "Inserting test sample 2626  This paper outlines a new approach to enumerating three-quadrant walks utilizing invariants and a set of diagonally symmetric models. The strategy we propose offers an efficient and straightforward method for determining the number of paths in three-quadrant walks, which has wide application in various fields, including statistical physics, probability theory, and combinatorial enumeration. Our method involves the identification of useful invariants, which can be easily characterized through a set of recursive rules, to build a formula that counts the number of three-quadrant walks for different lengths. Our formula is versatile and can be applied to a set of new cases, deemed unsolvable by other methods, giving us an excellent tool for solving otherwise intractable problems. \n",
      "\n",
      "We also explore a set of diagonally symmetric models that are applicable in corner cases of three-quadrant walks. Our models consider the effects of symmetries on the total count of three-quadrant walks by restricting the allowable positions of the walks. In this way, a more specific count can be obtained while lowering computational complexity. Through these models, we can see the relationships between pairwise symmetries and generate explicit formulas that reflect these symmetries. As a result, we are better equipped to identify critical points or singularities where these symmetries fail, illustrating the usefulness of our approach.\n",
      "\n",
      "Our proposed method is more versatile and computationally efficient than current methods. We demonstrate its success in computing the count of three-quadrant walks on various grids, providing precise results that match up with previous results in the literature. Furthermore, our diagonally symmetric models provide insight into the behaviors of symmetry regimes on three-quadrant walks and are potentially useful in other areas that study symmetry breaking in combinatorial structures. 1 into PostgreSQL...\n",
      "Inserting test sample 2627  Lorentz violation is motivated by quantum gravity and it is generically described by nondynamical tensors. In this work a Lorentz violating extension of general relativity is studied where a nondynamical tensor couples to the Weyl tensor. A family of static and spherically symmetric solutions in vacuum is found, confirming that there are consistent solutions with explicit Lorentz violation in dynamical spacetimes. These solutions produce an unconventional dependence of the gravitational redshift, which, in turn, leads to the first bounds on such nondynamical tensor that do not rely on the physics of the early universe. Moreover, the bounds obtained in this work are competitive with respect to limits on similar nondynamical tensors. 0 into PostgreSQL...\n",
      "Inserting test sample 2628  In this work, we analyze a static and spherically-symmetric spacetime exhibiting explicit Lorentz violation. We consider a tensor field as a background field to represent the spontaneous breaking of Lorentz symmetry in the gravitational sector. The resulting theory is invariant under diffeomorphisms and reparametrizations, and we obtain the equations of motion describing the dynamics of test particles under the influence of the background field. After that, we investigate the classical solutions of the system and verify the propagation of gravitational waves in this framework. Finally, we consider some cosmological implications of this theory. Our results illustrate the possibility of explicit Lorentz violation in the gravitational sector, which has important theoretical and observational consequences. 1 into PostgreSQL...\n",
      "Inserting test sample 2629  The univariate extreme value theory deals with the convergence in type of powers of elements of sequences of cumulative distribution functions on the real line when the power index gets infinite. In terms of convergence of random variables, this amounts to the the weak convergence, in the sense of probability measures weak convergence, of the partial maximas of a sequence of independent and identically distributed random variables. In this monograph, this theory is comprehensively studied in the broad frame of weak convergence of random vectors as exposed in Lo et al.(2016). It has two main parts. The first is devoted to its nice mathematical foundation. Most of the materials of this part is taken from the most essential Lo\\`eve(1936,177) and Haan (1970), based on the stunning theory of regular, pi or gamma variation. To prepare the statistical applications, a number contributions I made in my PhD and my Doctorate of Sciences are added in the last chapter of the last chapter of that part. Our real concern is to put these materials together with others, among them those of the authors from his PhD dissertations and Science doctorate thesis, in a way to have an almost full coverage of the theory on the real line that may serve as a master course of one semester in our universities. As well, it will help the second part of the monograph. This second part will deal with statistical estimations problems related to extreme values. It addresses various estimation questions and should be considered as the beginning of a survey study to be updated progressively. Research questions are tackled therein. Many results of the author, either unpublished or not sufficiently known, are stated and/or updated therein. 0 into PostgreSQL...\n",
      "Inserting test sample 2630  The univariate extreme value theory has emerged as a new framework that is increasingly used in many fields such as economics, finance, engineering, and environmental science, among others. Characterized by its ability to model the behavior of extreme events, the theory has established a distinctive approach to dealing with rare but highly impactful events in real systems. This paper examines both the functional and random aspects of the theory's weak convergence (IIA) as they relate to the univariate extreme value distributions.\n",
      "\n",
      "Functional aspects of the extreme value theory relate to the behavior of the distribution in the tails. The theory provides a suitable model to describe the distribution of maximum values in data from real systems, such as floods, earthquakes, or financial risks. The theory enables practitioners to estimate the probability of extreme events that lie outside the range of available data. Random aspects of the theory relate to the distribution of those extreme events and the associated uncertainty of such events. The paper examines the different types of models that have been developed to describe the random behavior of extreme values, such as the popular generalized extreme value (GEV) model, and their limitations.\n",
      "\n",
      "The weak convergence (IIA) is also a key aspect of the univariate extreme value theory. The paper explores the interpretation of the IIA, its significance, and the ways in which it has been used in many applications. The theory proves that the maxima of random samples converge to an extreme value distribution, which is a fundamental concept in the theory's applicability.\n",
      "\n",
      "In summary, this paper provides insights into the functional and random aspects of the univariate extreme value theory, specifically focusing on the theory's weak convergence (IIA). The paper concludes with a discussion of the implications of the findings for the applications of the theory in real systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2631  We study the stochastic total variation flow (STVF) equation with linear multiplicative noise. By considering a limit of a sequence of regularized stochastic gradient flows with respect to a regularization parameter $\\varepsilon$ we obtain the existence of a unique variational solution of the STVF equation which satisfies a stochastic variational inequality. We propose an energy preserving fully discrete finite element approximation for the regularized gradient flow equation and show that the numerical solution converges to the solution of the unregularized STVF equation. We perform numerical experiments to demonstrate the practicability of the proposed numerical approximation. 0 into PostgreSQL...\n",
      "Inserting test sample 2632  This paper proposes a novel numerical method to compute stochastic total variation flow. By introducing a convergent difference scheme based on the alternating direction method of multipliers, we provide an accurate simulation of the system's evolution. We establish the well-posedness and stability of the proposed method in the full strong sense. Extensive numerical experiments and comparisons with several state-of-the-art algorithms are conducted, verifying the effectiveness and robustness of this new approach for solving stochastic total variation flow problems. Our method has significant applications in various fields such as image processing, computer vision, and finance. 1 into PostgreSQL...\n",
      "Inserting test sample 2633  Magic state distillation and the Shor factoring algorithm make essential use of logical diagonal gates. We introduce a method of synthesizing CSS codes that realize a target logical diagonal gate at some level $l$ in the Clifford hierarchy. The method combines three basic operations: concatenation, removal of $Z$-stabilizers, and addition of $X$-stabilizers. It explicitly tracks the logical gate induced by a diagonal physical gate that preserves a CSS code. The first step is concatenation, where the input is a CSS code and a physical diagonal gate at level $l$ inducing a logical diagonal gate at the same level.\n",
      "\n",
      "The output is a new code for which a physical diagonal gate at level $l+1$ induces the original logical gate. The next step is judicious removal of $Z$-stabilizers to increase the level of the induced logical operator. We identify three ways of climbing the logical Clifford hierarchy from level $l$ to level $l+1$, each built on a recursive relation on the Pauli coefficients of the induced logical operators. Removal of $Z$-stabilizers may reduce distance, and the purpose of the third basic operation, addition of $X$-stabilizers, is to compensate for such losses. For the coherent noise model, we describe how to switch between computation and storage of intermediate results in a decoherence-free subspace by simply applying Pauli $X$ matrices. The approach to logical gate synthesis taken in prior work focuses on the code states, and results in sufficient conditions for a CSS code to be fixed by a transversal $Z$-rotation. In contrast, we derive necessary and sufficient conditions by analyzing the action of a transversal diagonal gate on the stabilizer group that determines the code. The power of our approach is demonstrated by two proofs of concept: the $[[2^{l+1}-2,2,2]]$ triorthogonal code family, and the $[[2^m,\\binom{m}{r},2^{\\min\\{r,m-r\\}}]]$ quantum Reed-Muller code family. 0 into PostgreSQL...\n",
      "Inserting test sample 2634  The theory of Diagonal Clifford Hierarchy has recently emerged as an exciting area of research in theoretical computer science. In this paper, we propose an algorithm for climbing the hierarchy of Diagonal Clifford matrices. Our approach is based on a combination of techniques from matrix theory and graph theory to develop a systematic process for constructing higher-order diagonal Clifford matrices. We show that our algorithm is efficient in terms of computational complexity and can be applied to a broad class of problems involving high-dimensional matrices.\n",
      "\n",
      "Our algorithm is based on the observation that every diagonal Clifford matrix can be decomposed into a tensor product of simpler matrices, each of which can be generated from a smaller set of basic matrices. We use this property to develop an iterative algorithm for constructing higher-order diagonal Clifford matrices. At each iteration, we generate a set of candidate matrices and select the best candidate based on a suitable criterion, such as the Frobenius norm of the difference between the candidate and the target matrix.\n",
      "\n",
      "We demonstrate the effectiveness of our algorithm on a variety of benchmark problems, including the simulation of quantum circuits and the computation of graph Laplacians. Our results show that our algorithm can efficiently generate high-dimensional diagonal Clifford matrices that are not easily constructed by other methods. We also discuss the theoretical implications of our work and suggest several open problems for future research.\n",
      "\n",
      "In summary, our paper presents a novel algorithm for climbing the Diagonal Clifford Hierarchy, which uses a combination of matrix and graph theory techniques. Our approach is efficient, applicable to a broad range of problems, and has significant theoretical and practical implications. We hope that our work will stimulate further research in this exciting area of theoretical computer science. 1 into PostgreSQL...\n",
      "Inserting test sample 2635  We present the results of the Infrared Space Observatory Short Wavelength Spectrometer (ISO-SWS) observations of Jupiter related to ammonia. We focus on two spectral regions; the first one (the 10-micron region), probes atmospheric levels between 1 and 0.2 bar, while the second one (the 5-micron window), sounds the atmosphere between 8 and 2 bars. The two spectral windows cannot be fitted with the same ammonia vertical distribution. From the 10-micron region we infer an ammonia distribution of about half the saturation profile above the 1-bar level, where the N/H ratio is roughly solar. A totally different picture is derived from the 5-micron window, where we determine an upper limit of 3.7E-5 at 1 bar and find an increasing NH3 abundance at least down to 4 bar.\n",
      "\n",
      "This profile is similar to the one measured by the Galileo probe (Folkner et al. 1998). The discrepancy between the two spectral regions most likely arises from the spatial heterogeneity of Jupiter, the 5-micron window sounding dry areas unveiled by a locally thin cloud cover (the 5-micron hot spots), and the 10-micron region probing the mean jovian atmosphere above 1 bar. The 15NH3 mixing ratio is measured around 400 mbar from nu_2 band absorptions in the 10-micron region. We find the atmosphere of Jupiter highly depleted in 15N at this pressure level (15N/14N)_{Jupiter}=(1.9^{+0.9}_{-1.))E-3, while (15N/14N)_{earth}=3.68E-3. It is not clear whether this depletion reveals the global jovian (15N/14N) ratio. Instead an isotopic fractionation process, taking place during the ammonia cloud condensation, is indicated as a possible mechanism. A fractionation coefficient alpha higher than 1.08 would explain the observed isotopic ratio, but the lack of laboratory data does not allow us to decide unambiguously on the origin of the observed low (15N/14N) ratio. 0 into PostgreSQL...\n",
      "Inserting test sample 2636  The ammonia tropospheric profile and the isotopic ratio of 15N/14N of Jupiter are measured in this study using data collected by the ISO-SWS (Infrared Space Observatory - Short Wavelength Spectrometer) instrument. The observations were made in various regions including the equator, mid-latitude and polar regions of the planet. The analysis of the data reveals that the ammonia mixing ratio decreases as altitude increases in the troposphere. The derived tropospheric profile indicates that ammonia is well mixed within the equatorial region, while it is more concentrated in the mid-latitude and polar regions.\n",
      "\n",
      "The isotopic ratio of 15N/14N is found to be constant within the error bars across the observed regions. Specifically, the value obtained for the ratio is 2.7 Ã— 10^-3, which is in agreement with values measured for Jupiter and other gas giants in our Solar System. These results corroborate previous studies, and contribute to the current understanding of the global circulation and composition of the Jovian atmosphere.\n",
      "\n",
      "The observations were made with the ISO-SWS spectrometer, which covered the wavelength range of 2.4 to 45 microns, and held a spectral resolution ranging from 2000 to 20000. The data analysis relied on a combination of radiative transfer modeling and curve fitting techniques. The results of this study provide valuable information regarding the vertical distribution and isotopic composition of ammonia in Jupiter's troposphere and contribute to a better understanding of the Jovian atmosphere.\n",
      "\n",
      "In conclusion, this research paper discusses the analysis of ammonia tropospheric profiles and isotopic ratios of 15N/14N across Jupiter's different regions. The observations were conducted using the ISO-SWS spectrometer and resulted in accurate values that can contribute to the current understanding of the planet's atmosphere. These findings can be useful in developing and testing models of planetary atmospheres and in studying the composition and evolution of the Solar System. 1 into PostgreSQL...\n",
      "Inserting test sample 2637  Taking advantage of the advances in array detector technology, an imaging polarimeter (IMPOL) has been constructed for measuring linear polarization in the wavelength band from 400-800 nm. It makes use of a Wollaston prism as the analyser to measure simultaneously the two orthogonal polarization components that define a Stoke's parameter. An achromatic half-wave plate is used to rotate the plane of polarization with respect to the axis of the analyser so that the second Stoke's parameter also can be determined. With a field of view correponding to about 30x30 sq. mm for a 1.2 m, f/13 telescope, a sensitive, liquid-nitrogen cooled CCD camera as the detector and a built-in acquisition and guidance unit, the instrument can be used for studying stellar fields or extended objects with an angular resolution close to 2 arcsec. The instrumental polarization is less than 0.05% and the accuracies of measurement are primarily limited by photon noise for typical observations. 0 into PostgreSQL...\n",
      "Inserting test sample 2638  An Imaging Polarimeter (IMPOL) has been developed to perform multi-wavelength observations of celestial optical and near-infrared radiation. This device is capable of polarimetric measurements in the wavelength range between 400 and 1000 nm with a high degree of precision. The instrument has been designed to meet the requirements of modern astronomical research, which demands high efficiency and high-quality imaging polarimetry. The cutting-edge technology employed in the IMPOL allows for simultaneous imaging in four different wavelength bands. The unique capabilities of this instrument make it well-suited for observations of extended objects, such as galaxies and star-forming regions, as well as point sources, including stars and exoplanets. The versatility of the IMPOL sets it apart as a valuable tool for the study of a wide range of astrophysical phenomena, including the interstellar medium, stellar magnetic fields, and the polarization properties of the cosmic microwave background. 1 into PostgreSQL...\n",
      "Inserting test sample 2639  We provide observations that Finsler geometry could be useful tools to construct higher-spin theories. We suggest that a Finsler metric of constant flag curvature can be regarded as a metric encoding higher-spin fields. We also show that the Fronsdal's equations for free higher-spin fields can be derived from equations of motion of constant curvature in Finsler geometry. 0 into PostgreSQL...\n",
      "Inserting test sample 2640  This paper investigates the connection between higher-spin theories and Finsler geometry. We analyze the geometric structures underlying gauge fields of arbitrary spin, and explore how they relate to generalized Finsler spacetimes. We provide a detailed derivation for the action functional for higher-spin fields in Finsler backgrounds and demonstrate how this approach leads to a consistent set of equations of motion. Our results hold promise for a deeper understanding of the interplay between geometry and particle physics. 1 into PostgreSQL...\n",
      "Inserting test sample 2641  We have developed a method for recovering polarization structures from the NRAO Very Large Array Sky Survey (NVSS) on larger angular scales than the nominal 15 arc minute survey limit. The technique depends on the existence of smaller scale fluctuations in polarization angle, to which the interferometer is sensitive, while the undetected total intensity of the structures can be arbitrarily large. We recover the large scale structure of the polarized Milky Way, as seen in single dish surveys, as well as a wide variety of smaller scale galactic and extragalactic features. We present a brief discussion of the uncertainties and limitations of the reprocessed NVSS polarization survey, a comparison of single-dish and NVSS results, and a sampling of the new polarization structures. We show a companion feature 1.8 Mpc outside of Abell cluster 3744, apparent Mpc-scale extensions to the tailed radio galaxy 3C31, a possible new giant galactic loop,and a new bright polarized patch in supernova remnant CTA1. We note that there is little quantitative information from these detections, and followup investigations would be necessary to measure reliable polarized fluxes and position angles. Some of the new features discovered in this NVSS reanalysis could provide a foreground for CMB polarization studies, but the internal foreground modeling for the next generation of experiments should have no difficulty accounting for them. 0 into PostgreSQL...\n",
      "Inserting test sample 2642  The NVSS, or the NRAO VLA Sky Survey, is a large-scale astronomical project aimed at mapping the radio sky in the frequency range of 1.4 GHz. With its high sensitivity and resolution, it has become a valuable tool in studying the polarization properties of radio sources. In this work, we present the discovery of diffuse radio polarization structures in the NVSS, which are characterized by extended regions of low degree of polarization. These structures are found to be associated with Galactic foreground emissions and are not typically observed in extragalactic sources. By analyzing the polarization properties of these structures, we demonstrate that they are consistent with the predictions of the \"dust foreground model,\" which describes the polarization properties of emission from Galactic dust. Our findings have important implications for future studies of the Cosmic Microwave Background polarization, as these structures could potentially represent a significant foreground contamination. The detection of these diffuse structures in the NVSS highlights the importance of careful foreground modeling in the analysis of sensitive polarization experiments, and further underscores the ongoing need for more accurate measurements and better understanding of Galactic emission. 1 into PostgreSQL...\n",
      "Inserting test sample 2643  We study linear Batalin-Vilkovisky (BV) quantization, which is a derived and shifted version of the Weyl quantization of symplectic vector spaces. Using a variety of homotopical machinery, we implement this construction as a symmetric monoidal functor of $\\infty$-categories. We also show that this construction has a number of pleasant properties: It has a natural extension to derived algebraic geometry, it can be fed into the higher Morita category of $E_n$-algebras to produce a \"higher BV quantization\" functor, and when restricted to formal moduli problems, it behaves like a determinant. Along the way we also use our machinery to give an algebraic construction of $E_n$-enveloping algebras for shifted Lie algebras. 0 into PostgreSQL...\n",
      "Inserting test sample 2644  In this paper, we establish the relationship between the linear Batalin-Vilkovisky quantization and $\\infty$-categories. By developing a categorical framework for the quantization process, we introduce a functor between the category of $\\infty$-categories and the category of Poisson manifolds. The functor is shown to preserve symplectic and Batalin-Vilkovisky structures, providing a new approach for quantization in the $\\infty$-categorical setting. Furthermore, we present an explicit example of a functorial quantization on the category of derived schemes, highlighting the potential for further applications of our approach. This work establishes an important connection between quantum field theory and higher category theory, and sheds light on the relationship between geometry and quantization. 1 into PostgreSQL...\n",
      "Inserting test sample 2645  LHCb has recorded large samples of semileptonic {\\textit B} decays. These provide the possibility to study {\\textit CP} violation effects in the $B^{0}_s$ and $B^{0}$ systems. Decay-time-integrated or decay-time-dependent asymmetries between charge conjugated final states probe {\\textit CP} violation in $B^{0}_{(s)}$ mixing through the measurement of the parameter $a_{\\textrm sl}. These measurements rely on data-driven techniques to control possible detection asymmetries. 0 into PostgreSQL...\n",
      "Inserting test sample 2646  The LHCb collaboration has measured {\\textit CP} violation in semileptonic decays of heavy-flavored hadrons at the LHC. These measurements provide important constraints on the parameters of the Standard Model and tests for new physics beyond it. The results are in good agreement with the Standard Model predictions and represent the most precise measurements to date in the field. Our study opens up new avenues for exploring the physics of the strong interactions at the LHC. 1 into PostgreSQL...\n",
      "Inserting test sample 2647  Mobile edge computing (MEC) is a key player in low latency 5G networks with the task to resolve the conflict between computationally-intensive mobile applications and resource-limited mobile devices (MDs). As such, there has been intense interest in this topic, especially in multi-user single-server and homogeneous multi-server scenarios. However, the research in the heterogeneous multi-server scenario is limited, where the servers are located at small base-stations (SBSs), macro base-stations (MBSs), or the cloud with different computing and communication capabilities. On the other hand, computational-tasks offloading is limited by the type of MD-BS association with almost all previous works focusing on offloading the MD's computational tasks to the MEC servers/cloudlets at its serving BS. However, in multi-BS association, or downlink/uplink decoupled (DUDe) scenarios, an MD can be served by multiple BSs and hence has multiple offloading choices. Motivated by this, we proposed a joint BS association and subchannel allocation algorithm based on a student-project allocation (SPA) matching approach to minimize the network sum-latency, which break the constraint that one MD must connect to the same BS in the UL and DL, and jointly consider the communication and computational disparity of SBS and MBS cloudlets in heterogeneous MEC networks. Moreover, an optimal power allocation scheme is proposed to optimize the system performance subject to the predefined quality of service constraints. Our results show that the proposed scheme is superior to benchmark techniques in enabling effective use of the computational and communication resources in heterogeneous MEC networks. 0 into PostgreSQL...\n",
      "Inserting test sample 2648  The increasing number of connected devices and the need for real-time data processing has led to the emergence of Mobile Edge Computing (MEC). MEC aims to reduce response times, lower network congestion, and improve network efficiency by locating compute and storage resources closer to the end user. To further enhance network performance, end-to-end communication needs to be carefully optimized, especially when dealing with heterogeneous networks.\n",
      "\n",
      "One way to achieve this optimization is by using a Uplink/Downlink Decoupled Access (UDDA) mechanism. With UDDA, the uplink and downlink communication channels are decoupled and each channel is allocated separately. UDDA provides flexibility in the allocation of resources for each channel, which can lead to better utilization of network resources and improved system performance.\n",
      "\n",
      "In this paper, we investigate the application of UDDA in Heterogeneous Mobile Edge Computing (H-MEC) environments. We propose a UDDA-based protocol for H-MEC that improves both reliability and scalability by enabling efficient resource allocation and dynamic transmission power control. Through simulations and experiments, we show that our proposed UDDA-based protocol outperforms traditional protocols in terms of latency, network throughput, and energy efficiency.\n",
      "\n",
      "Our findings suggest that UDDA is a promising approach for addressing the challenges of H-MEC. We therefore recommend the adoption of our proposed protocol for future H-MEC networks in order to improve performance and enhance user experience. These findings also have implications for the broader field of edge computing, suggesting that UDDA has the potential to improve network performance and resource utilization in various edge computing scenarios. 1 into PostgreSQL...\n",
      "Inserting test sample 2649  It has been argued that there is biological and modeling evidence that a non-linear diffusion coefficient of the type D(b) = D_0 b^{k} underlies the formation of a number of growth patterns of bacterial colonies. We study a reaction-diffusion system with a non-linear diffusion coefficient introduced by Ben-Jacob et al. Due to the fact that the bacterial diffusion coefficient vanishes when the bacterial density b -> 0, the standard linear stability analysis for fronts cannot be used. We introduce an extension of the stability analysis which can be applied to such singular fronts, map out the region of stability in the D-k-plane and derive an interfacial approximation in some limits. Our linear stability analysis and sharp interface formulation will also be applicable to other examples of interface formation due to nonlinear diffusion, like in porous media or in the problem of vortex motion in superconductors. 0 into PostgreSQL...\n",
      "Inserting test sample 2650  This study investigates the morphological instability and dynamics of bacterial growth models with nonlinear diffusion. We propose a continuum model that takes into account the interplay between bacterial growth, nutrient diffusion, and motility. We show that the nonlinear diffusion term introduced in the model can lead to the emergence of morphological instabilities, such as finger-like protrusions and labyrinthine patterns, in the bacterial front. Our numerical simulations illustrate the interplay between these instabilities and the dynamics of bacterial growth. Specifically, we observe that geometric features of the bacterial front affect the rate of nutrient consumption and waste production, leading to heterogeneity in bacterial density. Our findings suggest that the proposed model can provide insights into the mechanisms that underlie the complex patterns observed in bacterial colonies and biofilms. 1 into PostgreSQL...\n",
      "Inserting test sample 2651  Our Milky Way (MW) has witnessed a series of major accretion events. One of the later additions, Gaia-Enceladus, has contributed a considerable mass to the inner Galaxy, but also generously donated to the outer halo. So far, associations with present-day MW globular clusters (GCs) have been chiefly based on their kinematics and ages. Here, we present a chemical abundance study of the outer halo (R$_{\\rm GC}$=18 kpc) GC NGC 1261, which has been suggested to be an accreted object. We measured 31 species of 29 elements in two stars from high-resolution Magellan/MIKE spectra and find that the cluster is moderately metal poor, at [Fe/H]=-1.26. NGC 1261 is moderately $\\alpha$-enhanced to the 0.3-dex level. While from the small sample alone it is difficult to assert any abundance correlations, the light elements Na,O,Mg, and Al differ significantly between the two stars in contrast to the majority of other elements with smaller scatter; this argues in favour of multiple generations of stars coexisting in this GC. Intriguingly for its metallicity, NGC 1261 shows heavy element abundances that are consistent with $r$-process nucleosynthesis and we discuss their origin in various sites. In particular the Eu overabundance quantitatively suggests that one single $r$-process event, such as a neutron-star neutron-star merger or a rare kind of supernova, can be responsible for the stellar enhancement or even the enrichment of the cluster with the excess $r$-material. Its heavy element pattern makes NGC 1261 resemble the moderately enhanced r-I stars that are commonly found in the halo and have been detected in Gaia-Enceladus as well. Therefore, combining all kinematical, age, and chemical evidence we conclude that NGC 1261 is a chemically intriguing GC that was born in Gaia-Enceladus and has been subsequently accreted into the MW halo. [abridged] 0 into PostgreSQL...\n",
      "Inserting test sample 2652  NGC 1261 is a globular cluster that has drawn considerable attention due to its unique properties. In this study, we investigate the cluster's chemical composition and provide evidence that it is enhanced in r-process elements. This enhancement can be explained by the Gaia-Enceladus event, a recent the merger of the Milky Way and a dwarf galaxy.\n",
      "\n",
      "Using high-resolution spectroscopy data obtained from the MIKE instrument on the Magellan Clay telescope, we measured the abundances of several elements in the cluster. We found that NGC 1261 is significantly enhanced in Eu, a neutron-capture element that is mainly produced by the r-process. This enrichment of r-process elements in NGC 1261 is consistent with those observed in other Galactic halo stars that are thought to have originated from Gaia-Enceladus.\n",
      "\n",
      "Our findings indicate that NGC 1261 may have formed from the same gas cloud that gave rise to Gaia-Enceladus. The presence of r-process elements suggests that the cluster was likely enriched by the same type of nucleosynthesis process that occurred in Gaia-Enceladus. This unearths a promising avenue in understanding the formation and evolution of Globular clusters.\n",
      "\n",
      "Our results also reveal that NGC 1261 is an intriguing target for further research. The globular cluster's close proximity to the Earth and abundance of metal-poor halo stars make it a hotbed of activity for astrophysical studies. With the advent of large, sensitive, and high-resolution spectroscopic instruments, it is now possible to investigate the chemical composition of a large sample of similar globular clusters to fully understand their formation and evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 2653  Deep neural networks (DNNs) have shown their success as high-dimensional function approximators in many applications; however, training DNNs can be challenging in general. DNN training is commonly phrased as a stochastic optimization problem whose challenges include non-convexity, non-smoothness, insufficient regularization, and complicated data distributions. Hence, the performance of DNNs on a given task depends crucially on tuning hyperparameters, especially learning rates and regularization parameters. In the absence of theoretical guidelines or prior experience on similar tasks, this requires solving many training problems, which can be time-consuming and demanding on computational resources. This can limit the applicability of DNNs to problems with non-standard, complex, and scarce datasets, e.g., those arising in many scientific applications. To remedy the challenges of DNN training, we propose slimTrain, a stochastic optimization method for training DNNs with reduced sensitivity to the choice hyperparameters and fast initial convergence. The central idea of slimTrain is to exploit the separability inherent in many DNN architectures; that is, we separate the DNN into a nonlinear feature extractor followed by a linear model. This separability allows us to leverage recent advances made for solving large-scale, linear, ill-posed inverse problems. Crucially, for the linear weights, slimTrain does not require a learning rate and automatically adapts the regularization parameter. Since our method operates on mini-batches, its computational overhead per iteration is modest. In our numerical experiments, slimTrain outperforms existing DNN training methods with the recommended hyperparameter settings and reduces the sensitivity of DNN training to the remaining hyperparameters. 0 into PostgreSQL...\n",
      "Inserting test sample 2654  Deep neural networks (DNNs) are an important class of machine learning models used in a variety of applications. Training such networks involves optimization of a complex, high-dimensional non-convex function. Existing optimization methods are often sensitive to the initial configuration and can get stuck in poor local optima, causing the optimization process to be slow or fail altogether. In this paper, we propose slimTrain, a novel stochastic approximation method for training separable deep neural networks.\n",
      "\n",
      "SlimTrain is based on a new set of update rules that enable efficient optimization of the separable structure in DNNs. Specifically, we decompose the weight matrix between two adjacent layers into a product of two lower-rank matrices, resulting in a separable structure. This enables us to optimize the network by updating its low-rank factors in a way that preserves the separability property. We show that slimTrain converges in probability to a stationary point of the objective function, and present experimental results demonstrating the effectiveness and efficiency of slimTrain compared to existing methods.\n",
      "\n",
      "Our proposed method offers several advantages over current techniques. SlimTrain is computationally efficient as it operates on the low-rank factors rather than the full weight matrix. Additionally, it is robust to the choice of initialization, and can be easily extended to networks with more complex structures. Overall, slimTrain presents a promising approach for training DNNs with superior performance over existing techniques. 1 into PostgreSQL...\n",
      "Inserting test sample 2655  In the formal approach to reactive controller synthesis, a symbolic controller for a possibly hybrid system is obtained by algorithmically computing a winning strategy in a two-player game. Such game-solving algorithms scale poorly as the size of the game graph increases. However, in many applications, the game graph has a natural hierarchical structure. In this paper, we propose a modeling formalism and a synthesis algorithm that exploits this hierarchical structure for more scalable synthesis.\n",
      "\n",
      "We define local games on hierarchical graphs as a modeling formalism which decomposes a large-scale reactive synthesis problem in two dimensions. First, the construction of a hierarchical game graph introduces abstraction layers, where each layer is again a two-player game graph. Second, every such layer is decomposed into multiple local game graphs, each corresponding to a node in the higher level game graph. While local games have the potential to reduce the state space for controller synthesis, they lead to more complex synthesis problems where strategies computed for one local game can impose additional requirements on lower-level local games.\n",
      "\n",
      "Our second contribution is a procedure to construct a dynamic controller for local game graphs over hierarchies. The controller computes assume-admissible winning strategies that satisfy local specifications in the presence of environment assumptions, and dynamically updates specifications and strategies due to interactions between games at different abstraction layers at each step of the play. We show that our synthesis procedure is sound: the controller constructs a play which satisfies all local specifications. We illustrate our results through an example controlling an autonomous robot in a known, multistory building. 0 into PostgreSQL...\n",
      "Inserting test sample 2656  Dynamic Hierarchical Reactive Controller Synthesis is a critical aspect of control theory, which aims to create efficient and reliable controllers for complex industrial processes. This paper presents a novel approach for synthesizing controllers for such systems. The proposed method involves designing a hierarchical structure of subsystems, each with its set of controllers, to deal with uncertainty and variations in the process.\n",
      "\n",
      "The method utilizes a reactive synthesis technique that enables the controllers to respond quickly to any changes in the system's state. Unlike the conventional static approach, the dynamic hierarchical structure can react and adapt to real-time changes, guaranteeing better performance, and robustness under various conditions.\n",
      "\n",
      "The paper evaluates the effectiveness of the proposed technique through simulations. We demonstrate that the dynamic hierarchical reactive controller synthesis method outperforms conventional methods in terms of the speed of response, feasibility, and scalability. Moreover, the proposed technique reduces computational complexity, thereby lowering the cost of implementing the controllers.\n",
      "\n",
      "The findings of this paper have significant implications for various industrial applications requiring the control of complex and dynamic systems, such as chemical processes, transportation, and power systems. The proposed method provides an effective and straightforward way to design controllers that can deal with real-time uncertainties, thereby improving reliability and performance. We anticipate that this novel approach will be useful in developing controllers for other highly dynamic systems in the future. 1 into PostgreSQL...\n",
      "Inserting test sample 2657  When a star comes too close to a supermassive black hole, it gets torn apart by strong tidal forces in a tidal disruption event, or TDE. Half of the elongated stream of debris comes back to the stellar pericenter where relativistic apsidal precession induces a self-crossing shock. As a result, the gas gets launched into an outflow that can experience additional interactions, leading to the formation of an accretion disc. We carry out the first radiation-hydrodynamics simulations of this process, making use of the same injection procedure to treat the self-crossing shock as in our previous adiabatic study (Bonnerot & Lu 2020). Two sets of realistic parameters of the problem are considered that correspond to different strengths of this initial interaction. In both cases, we find that the injected matter has its trajectories promptly circularized by secondary shocks taking place near the black hole. However, the generated internal energy efficiently diffuses away in the form of radiation, which results in a thin vertical profile of the formed disc. The diffusing photons promptly irradiate the surrounding debris until they emerge with a bolometric luminosity of $L\\approx 10^{44} \\, \\rm erg\\, s^{-1}$. Towards the self-crossing shock, diffusion is however slowed that results in a shallower luminosity increase, with a potentially significant component in the optical band. Matter launched to large distances continuously gains energy through radiation pressure, which can cause a significant fraction to become unbound. This work provides direct insight into the origin of the early emission from TDEs, which is accessed by a rapidly increasing number of observations. 0 into PostgreSQL...\n",
      "Inserting test sample 2658  Tidal disruption events (TDEs) are rare astrophysical phenomena that occur when a star becomes too close to a massive black hole and is gravitationally torn apart, producing a bright flare of electromagnetic radiation. The early stages of these events, known as \"first light\", provide valuable insights into the physical processes involved in TDEs and can help reveal the properties of the central black hole, the disrupted star, and the surrounding environment.\n",
      "\n",
      "In this study, we report on the first detection of TDEs in their early stages using a combination of high-cadence optical and X-ray observations. We present detailed light curve analyses and spectral modeling of the observed flares, which allow us to constrain the properties of the accretion disk and the black hole.\n",
      "\n",
      "Our results provide strong evidence for the presence of compact accretion disks and highly relativistic outflows in the early phases of TDEs, supporting theoretical models that predict these features. We also find evidence for multiple components in the X-ray spectra, suggesting that the accretion process may be complex and dynamic.\n",
      "\n",
      "In addition, we use our observations to estimate the mass and spin of the central black hole, as well as the mass and radius of the disrupted star. Our findings provide important constraints on models of black hole growth and dynamics, and offer new opportunities for studying the properties of compact objects and their environments.\n",
      "\n",
      "Overall, our work demonstrates the power of early-time observations of TDEs for advancing our understanding of these fascinating and enigmatic phenomena. 1 into PostgreSQL...\n",
      "Inserting test sample 2659  The current trend in object detection and localization is to learn predictions with high capacity deep neural networks trained on a very large amount of annotated data and using a high amount of processing power. In this work, we propose a new neural model which directly predicts bounding box coordinates. The particularity of our contribution lies in the local computations of predictions with a new form of local parameter sharing which keeps the overall amount of trainable parameters low. Key components of the model are spatial 2D-LSTM recurrent layers which convey contextual information between the regions of the image. We show that this model is more powerful than the state of the art in applications where training data is not as abundant as in the classical configuration of natural images and Imagenet/Pascal VOC tasks.\n",
      "\n",
      "We particularly target the detection of text in document images, but our method is not limited to this setting. The proposed model also facilitates the detection of many objects in a single image and can deal with inputs of variable sizes without resizing. 0 into PostgreSQL...\n",
      "Inserting test sample 2660  Object detection and localization has advanced tremendously with the advent of deep learning. However, these approaches often require a large number of labeled examples for training. In many real-world scenarios, obtaining sufficient labeled data for all objects of interest can be both expensive and impractical. In this paper, we propose a learning approach for object detection and localization that requires only a few examples per object category. Our method is based on an attention mechanism that learns to selectively attend to the most informative parts of the example images. To alleviate the risk of overfitting, we further incorporate a regularization term that encourages the attention maps to be semantically meaningful and interpretable. We show that our approach outperforms existing methods on challenging object detection and localization tasks, such as those found in aerial imagery and medical imaging. Our work contributes to the development of learning methods that enable efficient and effective object detection and localization with limited labeled data. 1 into PostgreSQL...\n",
      "Inserting test sample 2661  Summary Background Claims made in science papers are coming under increased scrutiny with many claims failing to replicate. Meta-analysis studies that use unreliable observational studies should be in question. We examine the reliability of the base studies used in an air quality/heart attack meta-analysis and the resulting meta-analysis.\n",
      "\n",
      "Methods A meta-analysis study that includes 14 observational air quality/heart attack studies is examined for its statistical reliability. We use simple counting to evaluate the reliability of the base papers and a p-value plot of the p-values from the base studies to examine study heterogeneity.\n",
      "\n",
      "Findings We find that the based papers have massive multiple testing and multiple modeling with no statistical adjustments. Statistics coming from the base papers are not guaranteed to be unbiased, a requirement for a valid meta-analysis. There is study heterogeneity for the base papers with strong evidence for so called p-hacking.\n",
      "\n",
      "Interpretation We make two observations: there are many claims at issue in each of the 14 base studies so uncorrected multiple testing is a serious issue.\n",
      "\n",
      "We find the base papers and the resulting meta-analysis are unreliable. 0 into PostgreSQL...\n",
      "Inserting test sample 2662  Environmental epidemiology meta-analyses are becoming increasingly popular to investigate the association between environmental exposures and health outcomes. Despite being a valuable analytical tool, there are concerns about their reliability and reproducibility. In this case study, we sought to assess the reliability of an environmental epidemiology meta-analysis on a specific health outcome. Our analysis revealed that the meta-analysis had limitations due to variability in study quality and inclusion criteria. However, despite these limitations, the meta-analysis provided valuable insights into the magnitude of association between environmental exposures and the outcome of interest. We also provide recommendations for future meta-analyses in environmental epidemiology and emphasize the importance of rigorously assessing the quality of included studies. Our findings suggest that caution should be exercised in interpreting the results of meta-analyses in environmental epidemiology, and that the reliability of such studies should be critically evaluated. Overall, this case study reinforces the need for standardization and transparency in environmental epidemiology research. 1 into PostgreSQL...\n",
      "Inserting test sample 2663  We present dimensional circuit synthesis, a new method for generating digital logic circuits that improve the efficiency of training and inference of machine learning models from sensor data. The hardware accelerators that the method generates are compact enough (a few thousand gates) to allow integration within low-cost miniaturized sensor integrated circuits, right next to the sensor transducer. The method takes as input a description of physical properties of relevant signals in the sensor transduction process and generates as output a Verilog register transfer level (RTL) description for a circuit that computes low-level features that exploit the units of measure of the signals in the system.\n",
      "\n",
      "We implement dimensional circuit synthesis as a backend to the compiler for Newton, a language for describing physical systems. We evaluate the backend implementation and the hardware it generates, on descriptions of 7 physical systems. The results show that our implementation of dimensional circuit synthesis generates circuits of as little as 1662 logic cells / 1239 gates for the systems we evaluate.\n",
      "\n",
      "We synthesize the designs generated by the dimensional circuit synthesis compilation backend for a low-power miniature FPGA targeted by its manufacturer at sensor interface applications. The circuits which the method generated use as little as 27% of the resources of the 2.15x2.5 mm FPGA. We measure the power dissipation of the FPGA's isolated core supply rail and show that, driven with a pseudorandom signal input stream, the synthesized designs use as little as 1.0 mW and no more than 5.8 mW. These results show the feasibility of integrating physics-inspired machine learning methods within low-cost miniaturized sensor integrated circuits, right next to the sensor transducer. 0 into PostgreSQL...\n",
      "Inserting test sample 2664  This research paper explores the implementation of compact hardware for accelerating inference from physical signals in sensors. Specifically, we propose a novel approach that is based on the principles of signal processing and machine learning. Our approach offers several distinct advantages over existing methods, including lower computational complexity, reduced power consumption, and increased reliability.\n",
      "\n",
      "To achieve these benefits, we begin by developing a set of mathematical models that describe the physical signals that are commonly encountered in sensor applications. These models take into account the unique characteristics of each signal, such as its frequency, amplitude, and phase, and allow them to be processed efficiently using our novel hardware architecture.\n",
      "\n",
      "We then integrate these models with state-of-the-art machine learning algorithms to enable rapid inference and classification of sensor data. Our approach utilizes techniques such as neural networks, decision trees, and support vector machines to ensure accurate and robust interpretation of sensor data.\n",
      "\n",
      "To validate our approach, we present experimental results demonstrating its efficacy on a range of sensor applications, including environmental monitoring, industrial control, and medical imaging. We show that our approach is able to achieve significantly faster inference times compared to existing solutions, while maintaining high levels of accuracy and reliability.\n",
      "\n",
      "Overall, our research presents a promising avenue for accelerating inference from physical signals in sensors using compact hardware. We believe that our approach can have significant practical applications in a range of fields, from environmental monitoring and automation to medical diagnosis and treatment. 1 into PostgreSQL...\n",
      "Inserting test sample 2665  Reliability is a critical consideration to DL-based systems. But the statistical nature of DL makes it quite vulnerable to invalid inputs, i.e., those cases that are not considered in the training phase of a DL model. This paper proposes to perform data sanity check to identify invalid inputs, so as to enhance the reliability of DL-based systems. We design and implement a tool to detect behavior deviation of a DL model when processing an input case. This tool extracts the data flow footprints and conducts an assertion-based validation mechanism. The assertions are built automatically, which are specifically-tailored for DL model data flow analysis. Our experiments conducted with real-world scenarios demonstrate that such an assertion-based data sanity check mechanism is effective in identifying invalid input cases. 0 into PostgreSQL...\n",
      "Inserting test sample 2666  Deep learning systems have become increasingly popular across multiple domains, but their reliability is often hindered by the lack of proper validation approaches. In this paper, we propose a method for performing data sanity checks via learnt assertions, which enables the identification of potential issues in deep learning models before deployment. Our approach relies on a combination of statistical analysis and machine learning, and it allows for the automatic generation of assertions that capture the expected properties of the input data based on a training set. By evaluating these assertions on a test set, our method provides a quantitative measure of the quality of the data and helps practitioners avoid common pitfalls such as overfitting or noisy inputs. We present experimental results on several benchmarks and show that our method achieves high accuracy in detecting data issues. 1 into PostgreSQL...\n",
      "Inserting test sample 2667  The continuous motorization of traffic has led to a sustained increase in the global number of road related fatalities and injuries. To counter this, governments are focusing on enforcing safe and law-abiding behavior in traffic.\n",
      "\n",
      "However, especially in developing countries where the motorcycle is the main form of transportation, there is a lack of comprehensive data on the safety-critical behavioral metric of motorcycle helmet use. This lack of data prohibits targeted enforcement and education campaigns which are crucial for injury prevention. Hence, we have developed an algorithm for the automated registration of motorcycle helmet usage from video data, using a deep learning approach. Based on 91,000 annotated frames of video data, collected at multiple observation sites in 7 cities across the country of Myanmar, we trained our algorithm to detect active motorcycles, the number and position of riders on the motorcycle, as well as their helmet use. An analysis of the algorithm's accuracy on an annotated test data set, and a comparison to available human-registered helmet use data reveals a high accuracy of our approach. Our algorithm registers motorcycle helmet use rates with an accuracy of -4.4% and +2.1% in comparison to a human observer, with minimal training for individual observation sites. Without observation site specific training, the accuracy of helmet use detection decreases slightly, depending on a number of factors. Our approach can be implemented in existing roadside traffic surveillance infrastructure and can facilitate targeted data-driven injury prevention campaigns with real-time speed. Implications of the proposed method, as well as measures that can further improve detection accuracy are discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 2668  This study proposes a novel approach to detect motorcycle helmet use with deep learning techniques. Helmet use is a crucial safety measure to reduce the likelihood and severity of head injuries in motorcycle accidents. However, conventional methods of surveillance and reporting on helmet use rely heavily on human observation and can be prone to error.\n",
      "\n",
      "To address this issue, we developed a deep learning algorithm using Convolutional Neural Networks (CNNs) to automatically detect helmet use from camera footage. The model was trained on a dataset of over 10,000 images of motorcyclists in various settings, with ground truth labeling of helmet use.\n",
      "\n",
      "Our results show that our model achieved an accuracy of 94% in detecting helmet use, outperforming traditional surveillance methods. We also conducted experiments to evaluate the robustness of our model under different scenarios such as varying lighting conditions and helmet designs.\n",
      "\n",
      "Furthermore, we demonstrate the potential of our model in real-world applications through a case study in a major city where we deployed the model on surveillance cameras at traffic intersections. Our approach allowed us to accurately estimate the prevalence of helmet use, which can inform policy and interventions to promote motorcycle safety.\n",
      "\n",
      "In conclusion, this study presents a promising approach to detect motorcycle helmet use using deep learning, which has the potential to improve the accuracy and efficiency of monitoring helmet use. Further research can explore the generalizability and scalability of our approach to different regions and contexts. 1 into PostgreSQL...\n",
      "Inserting test sample 2669  The presentations, discussions and findings from the inaugural `PhyStat-$\\nu$' workshop held at the Kavli Institute for the Physics and Mathematics of the Universe (IPMU) near Tokyo in 2016 are described.\n",
      "\n",
      "PhyStat-$\\nu$ was the first workshop to focus solely on statistical issues across the broad range of modern neutrino physics, bringing together physicists who are active in the analysis of neutrino data with experts in statistics to explore statistical issues in the field. It is a goal of PhyStat-$\\nu$ to help serve the neutrino physics community by providing a forum within which such statistical issues can be discussed and disseminated broadly.\n",
      "\n",
      "This paper is adapted from a summary document that was initially circulated amongst the participants soon after the workshop. Another PhyStat-$\\nu$ workshop is being held at CERN in January 2019, building on the discussions in 2016.\n",
      "\n",
      "Advances in experimental neutrino physics in recent years have led to much larger datasets and more diversity in the properties of neutrinos that are being investigated. The discussions here raised several areas where improved statistical errors and more complicated interpretations of the data require statistical methods to be revisited, as well as topics where broader discussions between experimentalists, phenomenologists and theorists will required, which are summarised here. It is important to record the state of the field as it stands today, as much is expected to change over the coming years, including the emergence of more inter-collaborational studies and increasing sophistication in global parameter fitting and model selection methods. The document is also intended to serve as a reference for pedagogical material for those who are new to the use of modern statistical techniques to describe experimental data, as well as those who are well-versed in these techniques and wish to apply them to new data. 0 into PostgreSQL...\n",
      "Inserting test sample 2670  The PhyStat-$\\nu$ 2016 conference brought together researchers in the fields of particle physics, cosmology and statistics at the Institute for the Physics and Mathematics of the Universe (IPMU). The focus was on discussing the latest developments in statistical methods for analyzing and interpreting data in neutrino research. This paper provides a summary of the discussions that took place over the course of the conference. \n",
      "\n",
      "One of the key topics discussed was the use of Bayesian inference in neutrino physics. Many researchers presented their work on constructing Bayesian models to better understand the properties of neutrinos, such as their masses and mixing angles. Additionally, techniques for estimating systematic uncertainties in neutrino experiments were also discussed. This is an important issue as systematic uncertainties can have a significant impact on the final results of such experiments.\n",
      "\n",
      "Another topic of discussion was the use of machine learning algorithms in neutrino data analysis. This approach has become increasingly popular in recent years due to the large amount of data being produced in neutrino experiments. Several researchers presented their work on using machine learning techniques such as neural networks and decision trees to improve the classification of neutrino events.\n",
      "\n",
      "Finally, the conference also featured discussions on the future of neutrino experiments and the statistical methods that will be needed to analyze the data. Many researchers emphasized the importance of developing new techniques to deal with increasingly complex data sets, as well as the need for collaboration between the particle physics and statistics communities.\n",
      "\n",
      "Overall, the PhyStat-$\\nu$ 2016 conference provided a valuable forum for researchers to discuss new developments and techniques in statistical methods for neutrino physics. The discussions highlighted the need for continued collaboration and innovation in this field in order to better understand the properties of these elusive particles. 1 into PostgreSQL...\n",
      "Inserting test sample 2671  Here presented is a unified approach to Stirling numbers and their generalizations as well as generalized Stirling functions by using generalized factorial functions, $k$-Gamma functions, and generalized divided difference.\n",
      "\n",
      "Previous well-known extensions of Stirling numbers due to Riordan, Carlitz, Howard, Charalambides-Koutras, Gould-Hopper, Hsu-Shiue, Tsylova Todorov, Ahuja-Enneking, and Stirling functions introduced by Butzer and Hauss, Butzer, Kilbas, and Trujilloet and others are included as particular cases of our generalization. Some basic properties related to our general pattern such as their recursive relations and generating functions are discussed. Three algorithms for calculating the Stirling numbers based on our generalization are also given, which include a comprehensive algorithm using the characterization of Riordan arrays. 0 into PostgreSQL...\n",
      "Inserting test sample 2672  This paper explores the generalized Stirling numbers and functions, which are mathematical entities that have proven to be valuable tools in combinatorics, probability theory, and analysis. We investigate their properties and derive several identities related to these numbers and functions. We show that the generalized Stirling numbers and functions are intimately related to classical combinatorial objects, such as partitions and permutations, and we provide a number of interpretations of our results within the context of combinatorial theory. Our analysis is motivated by a desire to better understand the role of these entities in a range of mathematical disciplines. Our results have potential applications in fields such as computer science and statistical physics. 1 into PostgreSQL...\n",
      "Inserting test sample 2673  We show that if the derivative of the Riemann zeta function has sufficiently many zeros close to the critical line, then the zeta function has many closely spaced zeros. This gives a condition on the zeros of the derivative of the zeta function which implies a lower bound of the class numbers of imaginary quadratic fields. 0 into PostgreSQL...\n",
      "Inserting test sample 2674  We investigate the distribution of zeros of the derivative of the Riemann zeta function, specifically the Landau-Siegel zeros. By analyzing the behavior of these zeros in various regions, we uncover important properties of the zeta function and its derivatives. Our findings deepen our understanding of the complex behavior of the Riemann zeta function and contribute to the broader field of number theory. 1 into PostgreSQL...\n",
      "Inserting test sample 2675  We review the properties of fractals, the Mandelbrot set and how deterministic chaos ties to the picture. A detailed study on three body systems, one of the major applications of chaos theory was undertaken. Systems belonging to different families produced till date were studied and their properties were analysed. We then segregated them into three classes according to their properties. We suggest that such reviews be carried out in regular intervals of time as there are an infinite number of solutions for three body systems and some of them may prove to be useful in various domains apart from hierarchical systems. 0 into PostgreSQL...\n",
      "Inserting test sample 2676  This research paper explores the interplay between chaos and the three-body problemâ€”a longstanding issue in celestial mechanics. By employing advanced numerical techniques, we investigate the complex behaviors within this system, identifying the conditions under which chaos arises and its effects on the long-term dynamical evolution of the system. Our findings reveal novel insights into the fundamental physics of the three-body problem and suggest new avenues for future research. Ultimately, this work seeks to contribute to a deeper understanding of the intricate dynamics governing celestial bodies, with broad implications for astrophysics, cosmology, and planetary science. 1 into PostgreSQL...\n",
      "Inserting test sample 2677  Massless perturbative QCD forbids, at leading order, the exclusive annihilation of proton-antiproton into some charmonium states, which, however, have been observed in the $p\\bar p$ channel, indicating the significance of higher order and non perturbative effects in the few GeV energy region. The most well known cases are those of the $^1S_0$ ($\\eta_c$) and the $^1P_1$. The case of the $^1D_2$ is considered here and a way of detecting such a state through its typical angular distribution in the radiative decay $^1D_2 \\to$ $^1P_1 \\gamma$ is suggested. Estimates of the branching ratio $BR(^1D_2 \\to p\\bar p)$, as given by a quark-diquark model of the nucleon, mass corrections and an instanton induced process are presented. 0 into PostgreSQL...\n",
      "Inserting test sample 2678  In this paper, we investigate the formation and decay of an interesting charmonium state, specifically $p \\bar p \\to ^1D_2 \\to ^1P_1 \\gamma$. By analyzing production data and examining decay mechanisms, we aim to understand the characteristics that distinguish this state from other charmonium states. Our findings suggest that the $^1D_2$ state has a narrow width and a unique decay mode to $^1P_1$. Furthermore, our study sheds light on potential future experimental strategies to probe the properties of this state, including measurements of the cross section and polarization. We conclude that the $^1D_2$ state should be a valuable system for further investigations into the dynamics of charmonium and decays involving radiative transitions. 1 into PostgreSQL...\n",
      "Inserting test sample 2679  We consider a system of $d$ coupled non-linear stochastic heat equations in spatial dimension 1 driven by $d$-dimensional additive space-time white noise.\n",
      "\n",
      "We establish upper and lower bounds on hitting probabilities of the solution $\\{u(t, x)\\}_{t \\in \\mathbb{R}_+, x \\in [0, 1]}$, in terms of respectively Hausdorff measure and Newtonian capacity. We also obtain the Hausdorff dimensions of level sets and their projections. A result of independent interest is an anisotropic form of the Kolmogorov continuity theorem. 0 into PostgreSQL...\n",
      "Inserting test sample 2680  We consider systems of non-linear stochastic heat equations with additive noise, and investigate the corresponding hitting probabilities. Specifically, we study the probability that the solution of the system will reach a given subset of the state space at a specified time. We use the Feynman-Kac formula to derive a representation of the hitting probability in terms of a two-point boundary value problem. Furthermore, we provide conditions under which the solution exists and is unique. Our results demonstrate the importance of understanding the hitting probabilities in the dynamics of non-linear stochastic heat equations. 1 into PostgreSQL...\n",
      "Inserting test sample 2681  We found that whisker crystals of Mo-doped Nb4SiTe4 show high thermoelectric performances at low temperatures, indicated by the largest power factor of 70 microW cm-1 K-2 at 230-300 K, much larger than those of Bi2Te3-based practical materials. This power factor is smaller than the maximum value in the 5d analogue Ta4SiTe4, but is comparable to that with a similar doping level. First principles calculation results suggest that the difference in thermoelectric performances between Nb and Ta compounds is caused by the much smaller band gap in Nb4SiTe4 than that in Ta4SiTe4, due to the weaker spin-orbit coupling in the former. We also demonstrated that the solid solution of Nb4SiTe4 and Ta4SiTe4 shows a large power factor, indicating that their combination is promising as a practical thermoelectric material, as in the case of Bi2Te3 and Sb2Te3. These results advance our understanding of the mechanism of high thermoelectric performances in this one-dimensional telluride system, as well as indicating the high potential of this system as a practical thermoelectric material for low temperature applications. 0 into PostgreSQL...\n",
      "Inserting test sample 2682  Recent developments in thermoelectric materials have opened new possibilities for waste heat recovery and energy harvesting. In this study, we report on the promising thermoelectric performance of one-dimensional telluride Nbâ‚„SiTeâ‚„ and its substituted compounds. Our experiments show that the power factor of Nbâ‚„SiTeâ‚„ can reach up to 66.3 ÂµW/cmKÂ² at room temperature, which is among the highest reported values for tellurides. Moreover, we found that the power factor can be significantly enhanced by substituting Te with Se or Sb, leading to values of up to 115 ÂµW/cmKÂ² for Nbâ‚„SiSeâ‚„ and 219 ÂµW/cmKÂ² for Nbâ‚„SiSbâ‚„. Our analysis indicates that the high thermoelectric performance primarily arises from the highly anisotropic crystal structure and the electronic band structure characteristics. These findings suggest that the Nbâ‚„SiTeâ‚„ system and its substituted variants have great potential for thermoelectric applications. 1 into PostgreSQL...\n",
      "Inserting test sample 2683  Supermassive black holes (SMBHs) are found in most galactic nuclei. A significant fraction of these nuclei also contain a nuclear stellar cluster (NSC) surrounding the SMBH. In this paper, we consider the idea that the NSC forms first, from the merger of several stellar clusters that may contain intermediate-mass black holes (IMBHs). These IMBHs can subsequently grow in the NSC and form an SMBH. We carry out $N$-body simulations of the simultaneous merger of three stellar clusters to form an NSC, and investigate the outcome of simulated runs containing zero, one, two and three IMBHs. We find that IMBHs can efficiently sink to the centre of the merged cluster. If multiple merging clusters contain an IMBH, we find that an IMBH binary is likely to form and subsequently merge by gravitational wave emission. We show that these mergers are catalyzed by dynamical interactions with surrounding stars, which systematically harden the binary and increase its orbital eccentricity. The seed SMBH will be ejected from the NSC by the recoil kick produced when two IMBHs merge, if their mass ratio $q\\gtrsim 0.15$. If the seed is ejected then no SMBH will form in the NSC. This is a natural pathway to explain those galactic nuclei that contain an NSC but apparently lack an SMBH, such as M33.\n",
      "\n",
      "However, if an IMBH is retained then it can seed the growth of an SMBH through gas accretion and tidal disruption of stars. 0 into PostgreSQL...\n",
      "Inserting test sample 2684  Super-massive black holes (SMBHs) â€“ black holes with masses greater than a million times that of the sun â€“ are ubiquitous in the nuclei of galaxies, including the Milky Way. The origins of these massive objects remain a mystery, but there is evidence to suggest that they may form through a hierarchical process involving the merging of smaller black holes. In this work, we explore the idea that intermediate-mass black holes (IMBHs), with masses between 100 and 100,000 times that of the sun, could act as seeds for SMBHs. We study the delivery of such IMBHs to the centers of massive stellar clusters via two-body relaxation processes. Using analytic models and extensive N-body simulations, we find that IMBHs can indeed sink to the centers of clusters on relatively short timescales, where they can form binaries and eventually grow to SMBH masses through multiple mergers. Our results suggest that this mechanism could be a significant contributor to the growth of SMBHs in galactic nuclei. We also discuss the implications of our work for the detection of IMBHs in globular clusters and dwarf galaxies. Our study provides a foundation for future, more detailed investigations of the formation and growth of SMBHs in galactic nuclei. 1 into PostgreSQL...\n",
      "Inserting test sample 2685  This 3rd paper in the CHANG-ES series shows the first results from our regular data taken with the Karl G. Jansky Very Large Array (JVLA). The edge-on galaxy, UGC 10288, has been observed in the B, C, and D configurations at L-band (1.5 GHz) and in the C and D configurations at C-band (6 GHz) in all polarization products. We show the first spatially resolved images in these bands,the first polarization images, and the first composed image at an intermediate frequency (4.1 GHz) which has been formed from a combination of all data sets.\n",
      "\n",
      "A surprising new result is the presence of a strong, polarized, double-lobed extragalactic radio source ({\\it CHANG-ES A}) almost immediately behind the galaxy and perpendicular to its disk. The core of {\\it CHANG-ES A} has an optical counterpart at a photometric redshift of $z_{phot}\\,=\\,0.39$; the southern radio lobe is behind the disk of UGC 10288 and the northern lobe is behind the halo region. This background `probe' has allowed us to do a preliminary Faraday Rotation analysis of the foreground galaxy, putting limits on the regular magnetic field and electron density in the halo of UGC 10288 in regions in which there is no direct detection of a radio continuum halo.\n",
      "\n",
      "We have revised the flux densities of the two sources individually as well as the star formation rate (SFR) for UGC 10288. UGC 10288 would have fallen well below the CHANG-ES flux density cutoff, had it been considered without the brighter contribution of the background source.\n",
      "\n",
      "UGC 10288 shows discrete high-latitude radio continuum features, but it does not have a {\\it global} radio continuum halo. The total minimum magnetic field strength at a sample position in the arc is $\\sim$ 10 $\\mu$G. Thus, this galaxy still appears to be able to form substantial high latitude, localized features in spite of its relatively low SFR. 0 into PostgreSQL...\n",
      "Inserting test sample 2686  This paper presents new radio observations of the edge-on galaxy UGC10288, obtained as part of the continuing CHANG-ES project. Our observations reveal the presence of a partially resolved, double-lobed radio source located behind and offset from the plane of the galaxy. This radio source is likely associated with a background active galactic nucleus or a star-forming galaxy. The size and morphology of the radio lobes suggest that the source is likely oriented close to the plane of the sky, or at a moderate inclination angle relative to our line of sight. \n",
      "\n",
      "We also discuss the properties of the diffuse, steep-spectrum radio emission in the disk of UGC10288, which is likely powered by cosmic-ray electrons generated in supernova remnants. The radio scale height of the disk emission is found to be consistent with that of other nearby edge-on galaxies, and we comment on the implications of this result for models of cosmic-ray transport and magnetic field structure in galactic disks.\n",
      "\n",
      "The presence of a background radio source behind UGC10288 provides an opportunity to study the effects of an intervening galaxy on the radio structure of a distant object. We compare the observed morphology of the radio lobes with predictions from simple models of radio source propagation through a foreground screen of ionized gas and dust. Preliminary results suggest that the radio lobes are heavily distorted by the gravitational potential of UGC10288, indicating a significant concentration of mass close to the plane of the galaxy.\n",
      "\n",
      "In summary, our observations of UGC10288 reveal a complex radio morphology that is likely shaped by both the intrinsic properties of the background radio source and the gravitational potential of the foreground galaxy. Further studies of this object at different wavelengths will provide valuable insights into the interplay between galactic disks and their environment, and the role of gravitational lensing in shaping the observed properties of distant sources. 1 into PostgreSQL...\n",
      "Inserting test sample 2687  In this paper, we investigate the relationship between star formation and structure, using a mass-complete sample of 27,893 galaxies at $0.5<z<2.5$ selected from 3D-HST. We confirm that star-forming galaxies are larger than quiescent galaxies at fixed stellar mass (M$_{\\star}$). However, in contrast with some simulations, there is only a weak relation between star formation rate (SFR) and size within the star-forming population: when dividing into quartiles based on residual offsets in SFR, we find that the sizes of star-forming galaxies in the lowest quartile are 0.27$\\pm$0.06 dex smaller than the highest quartile. We show that 50% of star formation in galaxies at fixed M$_{\\star}$ takes place within a narrow range of sizes (0.26 dex). Taken together, these results suggest that there is an abrupt cessation of star formation after galaxies attain particular structural properties. Confirming earlier results, we find that central stellar density within a 1 kpc fixed physical radius is the key parameter connecting galaxy morphology and star formation histories: galaxies with high central densities are red and have increasingly lower SFR/M$_{\\star}$, whereas galaxies with low central densities are blue and have a roughly constant (higher) SFR/M$_{\\star}$ at a given redshift. We find remarkably little scatter in the average trends and a strong evolution of $>$0.5 dex in the central density threshold correlated with quiescence from $z\\sim0.7-2.0$. Neither a compact size nor high-$n$ are sufficient to assess the likelihood of quiescence for the average galaxy; rather, the combination of these two parameters together with M$_{\\star}$ results in a unique quenching threshold in central density/velocity. 0 into PostgreSQL...\n",
      "Inserting test sample 2688  This research paper investigates the dependence of the specific star formation rate (sSFR) on the size and central density of galaxies for the redshift range of 0.5 < z < 2.5. The study is based on deep UV and optical imaging from the Cosmic Assembly Near-infrared Deep Extragalactic Legacy Survey (CANDELS) and uses a sample of over 38,000 galaxies.\n",
      "\n",
      "Our analysis reveals that sSFR is strongly correlated with galaxy size and central density, implying that both factors play significant roles in regulating star formation activity. Specifically, we find that larger galaxies tend to have lower sSFR, while those with higher central density exhibit higher sSFR. Additionally, at a fixed size, the strength of the sSFR-density relation is found to weaken with increasing redshift.\n",
      "\n",
      "Based on our results, we present a novel machine learning algorithm capable of predicting quiescent galaxies, which are galaxies with low sSFR and minimal ongoing star formation. Our predictive model achieves a recall rate of 0.87 and a precision rate of 0.80, indicating its high accuracy in identifying quiescence.\n",
      "\n",
      "This paper provides new insights into the role of galaxy size and central density in regulating star formation at intermediate redshifts. It also proposes a practical application in the form of a machine learning model for predicting whether a given galaxy is quiescent or not. These findings are expected to facilitate a better understanding of the galaxy formation and evolution processes in the early universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2689  We estimate the mass of the inner ($<20$ kpc) Milky Way and the axis ratio of its dark matter halo using globular clusters as tracers. At the same time, we constrain the phase-space distribution of the globular cluster system. We use the Gaia DR2 catalogue of 75 globular clusters' proper motions and recent measurements of the proper motions of another 20 distant clusters obtained with the Hubble Space Telescope. We describe the globular cluster system with a 2-component distribution function (DF), with a flat, rotating disc and a rounder, more extended halo. While fixing the Milky Way's disc and bulge, we let the mass and shape of the dark matter halo and we fit these two parameters, together with other six describing the DF, with a Bayesian method. We find the mass of the Galaxy within 20 kpc to be $M(<20{\\,\\rm kpc})=1.91^{+0.18}_{-0.17} \\times 10^{11} M_\\odot$, of which $M_{\\rm DM}(<20{\\,\\rm kpc})=1.37^{+0.18}_{-0.17}\\times 10^{11}M_\\odot$ is in dark matter, and the density axis ratio of the dark matter halo to be $q=1.30 \\pm 0.25$. This implies a virial mass $M_{\\rm via} = 1.3 \\pm 0.3 \\times 10^{12} M_\\odot$. Our analysis rules out oblate ($q<0.8$) and strongly prolate halos ($q>1.9$) with 99\\% probability. Our preferred model reproduces well the observed phase-space distribution of globular clusters and has a disc component that closely resembles that of the Galactic thick disc. The halo component follows a power-law density profile $\\rho \\propto r^{-3.3}$, has a mean rotational velocity of $V_{\\rm rot}\\simeq -14\\,\\rm km\\,s^{-1}$ at 20 kpc, and has a mildly radially biased velocity distribution ($\\beta\\simeq 0.2 \\pm 0.07$, fairly constant outside 15 kpc). We also find that our distinction between disc and halo clusters resembles, although not fully, the observed distinction in metal-rich ([Fe/H]$>-0.8$) and metal-poor ([Fe/H]$\\leq-0.8$) cluster populations. 0 into PostgreSQL...\n",
      "Inserting test sample 2690  The Milky Way is a massive galaxy that resides in the Local Group. Understanding its structure and composition is crucial for our understanding of the universe. In this study, we explore the mass and shape of the Milky Way's dark matter halo using globular clusters as tracers. Specifically, we use data from Gaia and Hubble to extract kinematic and photometric information of globular clusters and combine it with theoretical models of the dark matter halo.\n",
      "\n",
      "We first take advantage of the precise astrometric measurements provided by Gaia to derive the velocities of globular clusters. By combining these velocities with Hubble's high-resolution imaging, we obtain accurate positions and photometry for each cluster. We then use this information, along with theoretical models of the dark matter halo, to constrain its mass and shape.\n",
      "\n",
      "Our results indicate that the Milky Way's dark matter halo is oblate, with an axis ratio of about 0.7. We also find that its mass within a certain radius is approximately 1.1 trillion solar masses, suggesting that the Milky Way is more massive than previously thought.\n",
      "\n",
      "Furthermore, we find that the distribution of globular clusters in the halo is not isotropic. Instead, it shows a clear elongation along the axis of the disk, which could be an indicator of the formation history of the Milky Way.\n",
      "\n",
      "Our study demonstrates the power of combining Gaia's astrometry with Hubble's high-resolution imaging to explore the Milky Way's structure and composition. By using globular clusters as tracers, we are able to constrain the properties of the dark matter halo and shed light on the formation history of our galaxy.\n",
      "\n",
      "In conclusion, our work provides crucial insights into the structure and composition of the Milky Way's dark matter halo. With future surveys like the Vera C. Rubin Observatory's Legacy Survey of Space and Time, we will be able to extend and refine our analyses and further our understanding of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2691  Let $\\mathbf{f}=(f\\_1,\\ldots,f\\_m)$ and $\\mathbf{g}=(g\\_1,\\ldots,g\\_m)$ be two sets of $m\\geq 1$ nonlinear polynomials over $\\mathbb{K}[x\\_1,\\ldots,x\\_n]$ ($\\mathbb{K}$ being a field). We consider the computational problem of finding -- if any -- an invertible transformation on the variables mapping $\\mathbf{f}$ to $\\mathbf{g}$. The corresponding equivalence problem is known as {\\tt Isomorphism of Polynomials with one Secret} ({\\tt IP1S}) and is a fundamental problem in multivariate cryptography. The main result is a randomized polynomial-time algorithm for solving {\\tt IP1S} for quadratic instances, a particular case of importance in cryptography and somewhat justifying {\\it a posteriori} the fact that {\\it Graph Isomorphism} reduces to only cubic instances of {\\tt IP1S} (Agrawal and Saxena). To this end, we show that {\\tt IP1S} for quadratic polynomials can be reduced to a variant of the classical module isomorphism problem in representation theory, which involves to test the orthogonal simultaneous conjugacy of symmetric matrices. We show that we can essentially {\\it linearize} the problem by reducing quadratic-{\\tt IP1S} to test the orthogonal simultaneous similarity of symmetric matrices; this latter problem was shown by Chistov, Ivanyos and Karpinski to be equivalent to finding an invertible matrix in the linear space $\\mathbb{K}^{n \\times n}$ of $n \\times n$ matrices over $\\mathbb{K}$ and to compute the square root in a matrix algebra. While computing square roots of matrices can be done efficiently using numerical methods, it seems difficult to control the bit complexity of such methods. However, we present exact and polynomial-time algorithms for computing the square root in $\\mathbb{K}^{n \\times n}$ for various fields (including finite fields). We then consider \\\\#{\\tt IP1S}, the counting version of {\\tt IP1S} for quadratic instances. In particular, we provide a (complete) characterization of the automorphism group of homogeneous quadratic polynomials. Finally, we also consider the more general {\\it Isomorphism of Polynomials} ({\\tt IP}) problem where we allow an invertible linear transformation on the variables \\emph{and} on the set of polynomials. A randomized polynomial-time algorithm for solving {\\tt IP} when \\(\\mathbf{f}=(x\\_1^d,\\ldots,x\\_n^d)\\) is presented. From an algorithmic point of view, the problem boils down to factoring the determinant of a linear matrix (\\emph{i.e.}\\ a matrix whose components are linear polynomials). This extends to {\\tt IP} a result of Kayal obtained for {\\tt PolyProj}. 0 into PostgreSQL...\n",
      "Inserting test sample 2692  This paper discusses the development of deterministic polynomial-time algorithms for solving the isomorphism problem on quadratic polynomials. The focus is on the regular case, where all roots of the polynomials in question are distinct. The isomorphism problem on quadratic polynomials is the task of determining whether two given polynomials are equivalent, i.e., whether one can be obtained from the other by a permutation of its variables and a scaling of its coefficients.\n",
      "\n",
      "First, we present an algorithm that solves the problem for polynomials with rational coefficients by reducing it to the problem of solving Diophantine equations. We then extend this method to work with arbitrary fields. Next, we show how the isomorphism problem on quadratic polynomials can be reduced to the problem of computing roots of univariate polynomials. We provide algorithmic solutions to this problem based on classical algorithms for polynomial root-finding.\n",
      "\n",
      "We also describe a new algorithm for the isomorphism problem on quadratic polynomials that exploits the connection to the problem of computing Abelian varieties. This algorithm works over arbitrary fields and is based on counting points on the Jacobian varieties of hyperelliptic curves. Finally, we show that under some natural assumptions, the polynomial-time algorithms presented in this paper are optimal.\n",
      "\n",
      "The techniques developed in this paper have applications in algebraic geometry, cryptography, and computer algebra systems. The results also shed light on the complexity of other important problems in algebra, such as the polynomial identity testing problem and the determination of isomorphism of higher-degree polynomials.\n",
      "\n",
      "Overall, this paper makes important contributions to the study of the isomorphism problem on quadratic polynomials, and provides new insights into the complexity of isomorphism problems for more general classes of polynomials. 1 into PostgreSQL...\n",
      "Inserting test sample 2693  Spin networks are at the core of quantum gravity. Our aim is to plug the mathematical community at large into the procedures turn to create a finite quantum theory of general relativity. For this, because of the different cultural backgraund, we would like to change the tack: to relate discrete (combinatorial) objects to the standard \"contineous\" geometry. 0 into PostgreSQL...\n",
      "Inserting test sample 2694  This paper explores the three mathematical faces of SU(2) in spin networks. We analyze the interplay between the group and its representations, including the algebraic, topological, and geometrical aspects. Additionally, we investigate the relationships between spin networks, quantum geometry, and loop quantum gravity. These findings provide new insights into the fundamental nature of space and time. 1 into PostgreSQL...\n",
      "Inserting test sample 2695  Searching for the signal of primordial gravitational waves in the B-modes (BB) power spectrum is one of the key scientific aims of the cosmic microwave background (CMB) polarization experiments. However, this could be easily contaminated by several foreground issues, such as the thermal dust emission.\n",
      "\n",
      "In this paper we study another mechanism, the cosmic birefringence, which can be introduced by a CPT-violating interaction between CMB photons and an external scalar field. Such kind of interaction could give rise to the rotation of the linear polarization state of CMB photons, and consequently induce the CMB BB power spectrum, which could mimic the signal of primordial gravitational waves at large scales. With the recent polarization data of BICEP2 and the joint analysis data of BICEP2/Keck Array and Planck, we perform a global fitting analysis on constraining the tensor-to-scalar ratio $r$ by considering the polarization rotation angle which can be separated into a background isotropic part and a small anisotropic part. Since the data of BICEP2 and Keck Array experiments have already been corrected by using the \"self-calibration\" method, here we mainly focus on the effects from the anisotropies of CMB polarization rotation angle. We find that including the anisotropies in the analysis could slightly weaken the constraints on $r$, when using current CMB polarization measurements. We also simulate the mock CMB data with the BICEP3-like sensitivity. Very interestingly, we find that if the effects of the anisotropic polarization rotation angle can not be taken into account properly in the analysis, the constraints on $r$ will be dramatically biased. This implies that we need to break the degeneracy between the anisotropies of the CMB polarization rotation angle and the CMB primordial tensor perturbations, in order to measure the signal of primordial gravitational waves accurately. 0 into PostgreSQL...\n",
      "Inserting test sample 2696  This research paper deals with the measurements of primordial gravitational waves and anisotropies of cosmic microwave background (CMB) polarization rotation. The search for primordial gravitational waves has been one of the most exciting areas of cosmological research in recent years, as it provides a window into the very early universe. By measuring the patterns in the polarization of the CMB, it is possible to infer the existence of these waves and to learn important information about the universe at the time of inflation, when the universe underwent a rapid expansion.\n",
      "\n",
      "One of the most important goals of this research is the detection of B-mode polarization in the CMB, which arises from the interaction of primordial gravitational waves with the polarized light from the CMB. The detection of this B-mode polarization has been a major focus of cosmological experiments, as it would provide strong evidence for the existence of primordial gravitational waves.\n",
      "\n",
      "This paper presents the results of new measurements of B-mode polarization in the CMB, along with an investigation of its anisotropies. Using data from the Planck satellite, the authors report the detection of a significant B-mode polarization signal at large angular scales. The observed signal is consistent with the expected signal from primordial gravitational waves, providing strong evidence for their existence.\n",
      "\n",
      "In addition, the authors analyze the anisotropies in the polarization rotation angle of the CMB, which can also provide important information about the early universe. They report the detection of statistically significant correlations between these anisotropies and the B-mode polarization signal, which suggests a common origin for these phenomena.\n",
      "\n",
      "Overall, these results represent a major advance in our understanding of the early universe and the nature of primordial gravitational waves. The detection of B-mode polarization provides strong evidence for the existence of these waves, while the analysis of anisotropies provides important clues about the physical processes that occurred in the very early universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2697  The goal of Machine Learning to automatically learn from data, extract knowledge and to make decisions without any human intervention. Such automatic (aML) approaches show impressive success. Recent results even demonstrate intriguingly that deep learning applied for automatic classification of skin lesions is on par with the performance of dermatologists, yet outperforms the average. As human perception is inherently limited, such approaches can discover patterns, e.g. that two objects are similar, in arbitrarily high-dimensional spaces what no human is able to do. Humans can deal only with limited amounts of data, whilst big data is beneficial for aML; however, in health informatics, we are often confronted with a small number of data sets, where aML suffer of insufficient training samples and many problems are computationally hard. Here, interactive machine learning (iML) may be of help, where a human-in-the-loop contributes to reduce the complexity of NP-hard problems. A further motivation for iML is that standard black-box approaches lack transparency, hence do not foster trust and acceptance of ML among end-users. Rising legal and privacy aspects, e.g. with the new European General Data Protection Regulations, make black-box approaches difficult to use, because they often are not able to explain why a decision has been made. In this paper, we present some experiments to demonstrate the effectiveness of the human-in-the-loop approach, particularly in opening the black-box to a glass-box and thus enabling a human directly to interact with an learning algorithm. We selected the Ant Colony Optimization framework, and applied it on the Traveling Salesman Problem, which is a good example, due to its relevance for health informatics, e.g. for the study of protein folding. From studies of how humans extract so much from so little data, fundamental ML-research also may benefit. 0 into PostgreSQL...\n",
      "Inserting test sample 2698  In recent years, the problem of solving NP-hard problems has received significant attention among researchers. However, existing methodologies for addressing these problems have limitations that constrain their performance. To address these limitations, we propose a glass-box interactive approach that leverages the predictive power of machine learning algorithms and human expertise to solve these problems efficiently.\n",
      "\n",
      "Our approach involves using a glass-box machine learning model that is trained on problem instances and human performance data to make informed decisions. The glass-box model increases transparency and interpretability in the decision-making process, allowing domain experts to provide feedback and improve the model's performance. Additionally, our approach enables the human-in-the-loop by allowing for human interaction and feedback throughout the problem-solving process. \n",
      "\n",
      "To evaluate the effectiveness of our approach, we conducted experiments on various NP-hard problems, such as the Traveling Salesman Problem (TSP) and the Knapsack Problem. The results indicate that our approach outperforms existing methodologies in terms of speed, efficiency, and accuracy. By combining the predictive power of machine learning with the expertise of human decision-makers, our glass-box interactive approach represents a promising solution to address NP-hard problems in practice.\n",
      "\n",
      "Overall, this paper presents a novel approach for solving NP-hard problems by integrating machine learning and human expertise. By leveraging the strengths of both, our approach provides a powerful tool for addressing challenges in a wide range of domains. Furthermore, the glass-box model increases transparency and understanding of the underlying decision-making process, making it an attractive option for real-world applications. 1 into PostgreSQL...\n",
      "Inserting test sample 2699  Recent technological advancements have allowed to implement in solid-state cavity-based devices phenomena of quantum nature such as vacuum Rabi splitting, controllable single photon emission and quantum entanglement. For a sufficiently strong coupling between a quantum emitter and a cavity, large quality factors ($Q$) along with small modal volume ($V_{eff}$) are essential.\n",
      "\n",
      "Here we show that by applying a 5nm Al coating to the sidewalls of a submicrometer-sized Fabry-P\\'{e}rot microcavity, the cavity $Q$ can be temperature-tuned from few hundreds at room temperatures to 2$\\times$10$^5$ below 30~K. This is achieved by, first, a complete shielding of the sidewall loss with ideally reflecting lateral metallic mirrors and, secondly, a dramatic decrease of the cavity's axial loss for small-sized devices due to the largely off-axis wavevector within the multilayered structure. Our findings offer a novel temperature-tunable platform to study quantum electrodynamical phenomena of emitter-cavity coupling. We demonstrate that a Rabi splitting of 2g=24~GHz (0.142~nm) can be readily achieved at 40~K in a 0.8$\\mu$m-sized device, which has an $V_{eff}\\approx0.0845~\\mu$m$^3$, comparable to best 2D photonic crystal (PhC) nanocavities. 0 into PostgreSQL...\n",
      "Inserting test sample 2700  In this study, the temperature-dependent properties of a quantum emitter-cavity system in a photonic wire microcavity with shielded sidewall loss are investigated. The temperature tunability of the strong coupling regime is examined in detail, where the emitter-cavity interaction is maximized. It is shown that the coupling strength is highly sensitive to changes in temperature, leading to a significant red shift of the resonant mode frequency and an increase in the quality factor of the cavity. Theoretical simulations are used to explain the coupling behavior, and experimental data from fabricated devices are compared to these simulations. The results show that temperature can be used as a powerful tool to precisely control the coupling strength and frequency of the emitter-cavity system in photonic wire microcavities with shielded loss. Potential applications of this technology include the development of high-sensitivity sensors, as well as the realization of on-chip optoelectronic devices for quantum information processing and quantum communication. Overall, this research provides valuable insights into the mechanisms behind temperature tunability in quantum emitter-cavity systems, and showcases the potential of such systems for a variety of photonic and quantum technologies. 1 into PostgreSQL...\n",
      "Inserting test sample 2701  We use bank-level balance sheet data from 2005 to 2010 to study interactions within the banking system of five emerging countries: Argentina, Brazil, Mexico, South Africa, and Taiwan. For each country we construct a financial network based on the leverage ratio dependence between each pair of banks, and find results that are comparable across countries. Banks present a variety of leverage ratio behaviors. This leverage diversity produces financial networks that exhibit a modular structure characterized by one large bank community, some small ones and isolated banks. There exist compact structures that have synchronized dynamics. Many groups of banks merge together creating a financial network topology that converges to a unique big cluster at a relatively low leverage dependence level. Finally, we propose a model that includes corporate and interbank loans for studying the banking system. This model generates networks similar to the empirical ones. Moreover, we find that faster-growing banks tend to be more highly interconnected between them, and this is also observed in empirical data. 0 into PostgreSQL...\n",
      "Inserting test sample 2702  This research investigates the relationship between banking networks and leverage dependence across emerging economies. We use a sample of selected emerging countries over the period of 2000-2018. We find that an increase in the degree of interbank connectivity or density of a banking network leads to an increase in systemic leverage dependence. This result is robust to numerous control variables and alternative measures of leverage dependence. Additionally, we document that common shocks to individual bank balance sheets stemming from the network increase the propensity of leverage dependence and amplify the impact of macroeconomic shocks. Our findings provide a valuable contribution to the literature on the determinants of leverage dependence in emerging market countries and suggest that policymakers should be mindful of the impact of banking networks on systemic risk. Furthermore, our analysis may serve as a useful tool for anticipating the likelihood of crises in emerging economies. 1 into PostgreSQL...\n",
      "Inserting test sample 2703  Offshoring the proprietary Intellectual property (IP) has recently increased the threat of malicious logic insertion in the form of Hardware Trojan (HT). A potential and stealthy HT is triggered with nets that switch rarely during regular circuit operation. Detection of HT in the host design requires exhaustive simulation to activate the HT during pre- and postsilicon. Although the nets with variable switching probability less than a threshold are primarily chosen as a good candidate for Trojan triggering, there is no systematic fine-grained approach for earlier detection of rare nets from word-level measures of input signals. In this paper, we propose a high-level technique to estimate the nets with the rare activity of arithmetic modules from word-level information. Specifically, for a given module, we use the knowledge of internal construction of the architecture to detect \"low activity\" and \"local regions\" without resorting to expensive RTL and other low-level simulations. The presented heuristic method abstracts away from the low-level details of design and describes the rare activity of bits (modules) in a word (architecture) as a function of signal statistics. The resulting quick estimates of nets in rare regions allows a designer to develop a compact test generation algorithm without the knowledge of the bit-level activity. We determine the effect of different positions of the breakpoint in the input signal to calculate the accuracy of the approach. We conduct a set of experiments on six adder architectures and four multiplier architectures. The average error to calculate the rare nets between RTL simulation and estimated values are below 2% in all architectures. 0 into PostgreSQL...\n",
      "Inserting test sample 2704  Hardware Trojans are malicious modifications in electronic circuits aimed at undermining the security and reliability of integrated circuits (ICs). As the demand for secure and trustworthy ICs grows, it becomes crucial to detect and defend against hardware trojans. In this paper, we propose an analytical approach to estimate the vulnerability of Register Transfer Level (RTL) designs to hardware trojans.\n",
      "\n",
      "Our methodology defines a set of metrics and uses them to identify potential vulnerabilities in RTL designs. The metrics account for the complexity of the design, such as the number of modules, the number of signals, and the signal activity. By analyzing the design at this level of abstraction, we can detect trojan vulnerabilities early in the design process.\n",
      "\n",
      "Additionally, we propose a localization technique that identifies the specific parts of the design that are susceptible to hardware trojans. By analyzing the design's input-output behavior, we can pinpoint potential trojan locations. Our approach is fully automated and efficient, making it suitable for industrial use.\n",
      "\n",
      "To evaluate our proposed approach, we conduct experiments on several benchmark designs. The results demonstrate the effectiveness of our methodology in estimating and localizing hardware trojan vulnerabilities. Our approach provides a means to detect and remove hardware trojans at an early stage, ensuring the security and reliability of integrated circuits. Overall, our work contributes to the advancement of hardware security and lays the foundation for future research in this field. 1 into PostgreSQL...\n",
      "Inserting test sample 2705  A new method is proposed for switching on interactions that are compatible with global symmetries and conservation laws of the original free theory. The method is applied to the control of stability in Lagrangian and non-Lagrangian theories with higher derivatives. By way of illustration, a wide class of stable interactions is constructed for the Pais-Uhlenbeck oscillator. 0 into PostgreSQL...\n",
      "Inserting test sample 2706  This research paper focuses on the role of proper deformations in establishing stable interactions between objects. We propose a novel mathematical framework to quantify the stability of these interactions, and demonstrate its effectiveness through simulation and experimental results. The use of this framework has vast implications in fields such as material science and robotics, enabling the design of more robust and reliable systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2707  We examine anomalous dimensions of higher spin currents in the critical O(N) scalar model and the Gross-Neveu model in arbitrary d dimensions. These two models are proposed to be dual to the type A and type B Vasiliev theories, respectively. We reproduce the known results on the anomalous dimensions to the leading order in 1/N by using conformal perturbation theory. This work can be regarded as an extension of previous work on the critical O(N) scalars in 3 dimensions, where it was shown that the bulk computation for the masses of higher spin fields on AdS_4 can be mapped to the boundary one in conformal perturbation theory. The anomalous dimensions of the both theories agree with each other up to an overall factor depending only on d, and we discuss the coincidence for d=3 by utilizing N=2 supersymmetry. 0 into PostgreSQL...\n",
      "Inserting test sample 2708  In large N conformal field theories (CFTs), the anomalous dimensions of higher spin currents have been shown to play a fundamental role in the behavior of the theories. These anomalous dimensions are related to the scaling dimensions of the operators and characterize the nonperturbative dynamics of the CFTs. In this paper, we investigate the anomalous dimensions of higher spin currents in large N CFTs using the bootstrap approach. We present an analytic method to study the spectrum of anomalous dimensions for spin-l operators with l > 2, where previous methods have faced difficulties. We find that the anomalous dimensions exhibit nontrivial behavior, including a mixing of operators with different spins. Our results provide insight into the nonperturbative dynamics of large N CFTs and can be used to guide future studies. 1 into PostgreSQL...\n",
      "Inserting test sample 2709  Classical separability problem involving multi-color point sets is an important area of study in computational geometry. In this paper, we study different separability problems for bichromatic point set P=P_r\\cup P_b on a plane, where $P_r$ and $P_b$ represent the set of n red points and m blue points respectively, and the objective is to compute a monochromatic object of the desired type and of maximum size. We propose in-place algorithms for computing (i) an arbitrarily oriented monochromatic rectangle of maximum size in R^2, (ii) an axis-parallel monochromatic cuboid of maximum size in R^3. The time complexities of the algorithms for problems (i) and (ii) are O(m(m+n)(m\\sqrt{n}+m\\log m+n \\log n)) and O(m^3\\sqrt{n}+m^2n\\log n), respectively. As a prerequisite, we propose an in-place construction of the classic data structure the k-d tree, which was originally invented by J. L.\n",
      "\n",
      "Bentley in 1975. Our in-place variant of the $k$-d tree for a set of n points in R^k supports both orthogonal range reporting and counting query using O(1) extra workspace, and these query time complexities are the same as the classical complexities, i.e., O(n^{1-1/k}+\\mu) and O(n^{1-1/k}), respectively, where \\mu is the output size of the reporting query. The construction time of this data structure is O(n\\log n). Both the construction and query algorithms are non-recursive in nature that do not need O(\\log n) size recursion stack compared to the previously known construction algorithm for in-place k-d tree and query in it. We believe that this result is of independent interest. We also propose an algorithm for the problem of computing an arbitrarily oriented rectangle of maximum weight among a point set P=P_r \\cup P_b, where each point in P_b (resp. P_r) is associated with a negative (resp. positive) real-valued weight that runs in O(m^2(n+m)\\log(n+m)) time using O(n) extra space. 0 into PostgreSQL...\n",
      "Inserting test sample 2710  This research paper investigates the problem of recognizing the largest rectangle within a bichromatic point set, a fundamental task of computational geometry with several applications in image processing, pattern recognition, and computer vision. The problem consists in finding the largest rectangle within a set of points such that one side is parallel to the x-axis, the other side is parallel to the y-axis, and all the points lie within its interior. \n",
      "\n",
      "The main contribution of this work is the study of different variations of this problem, focusing on their complexity, algorithms, and applications. We consider four variations of the problem according to the types of constraints imposed on the point set. First, we assume that each point is colored with red or blue and that the rectangles must have sides of different colors. Second, we assume that the points belong to two different sets, and the rectangles must cover one set entirely and only overlap the other set. Third, we assume that the rectangles can be rotated by an arbitrary angle, and the goal is to find the angle that maximizes their area. Fourth, we assume that the points may have weights, and the area of the rectangles is evaluated based on the sum of weights of points inside.\n",
      "\n",
      "To solve these variations, we propose efficient algorithms based on geometric and combinatorial techniques, including dynamic programming, convex hull computation, and binary search. We analyze the time and space complexity of each algorithm, proving their optimality and providing experimental results. Moreover, we discuss the applications of the proposed algorithms in several domains, such as image segmentation, shape recognition, and data visualization. \n",
      "\n",
      "In summary, this research paper presents a comprehensive study of the well-known problem of largest rectangle recognition amidst a bichromatic point set, extending it to four interesting variations and proposing efficient algorithms for their solution. The results of this work contribute to the theoretical and practical foundations of computational geometry and offer new opportunities for interdisciplinary research. 1 into PostgreSQL...\n",
      "Inserting test sample 2711  A certain identification of points in a planar Schwarzschild-anti de Sitter (AdS) black hole generates a four-dimensional static black string. In turn, a rotating black string can be obtained from a static one by means of an `illegitimate coordinate transformation', a local boost in the compact direction. On the basis of the gauge/gravity duality, these black strings are dual to rotating thermal states of a strongly interacting conformal field theory (CFT) that lives on a cylinder. In this work, we obtain the complete quasinormal mode (QNM) spectrum of the gravitational perturbations of rotating black strings. Analytic solutions for the dispersion relations are found in the hydrodynamic limit, characterized by fluctuations with wavenumber and frequency much smaller than the Hawking temperature of the string (or the temperature in the CFT dual description). We obtain these dispersion relations both by studying the gravitational perturbations of rotating black strings and by investigating the hydrodynamic fluctuations of a moving fluid living on the boundary of the AdS spacetime. Relativistic effects like the Doppler shift of the frequencies, wavelength contraction, and dilation of the thermalization time are shown explicitly in such a regime. We also investigate the behavior of a sound wave propagating in a viscous fluid for several values of the rotation parameter. The numerical solutions for the fundamental QNMs show a crossover (a transition) from a hydrodynamic-like behavior to a linear relativistic scaling for large wavenumbers. Additionally, we find a new family of QNMs which are purely damped in the zero wavenumber limit and that does not follow as a continuation of QNMs of the static black string, but that appears to be closely related to the algebraically special perturbation modes. 0 into PostgreSQL...\n",
      "Inserting test sample 2712  This paper explores the relationship between quasinormal modes of rotating black strings and the hydrodynamics of a moving conformal field theory plasma. We begin by introducing the basics of black string perturbation theory and its associated quasinormal modes. We then study the behavior of these modes under the influence of rotation, and demonstrate how they can be used to extract information about the thermodynamic properties of the black string. \n",
      "\n",
      "Moving on, we discuss the hydrodynamics of a conformal field theory plasma and the mathematical tools that are used to model it. We show that the equations governing the plasmaâ€™s behavior are analogous to those describing the motion of the black string, and explore the implications of this correspondence. In particular, we investigate the consequences of applying the AdS/CFT correspondence to this system, and discuss its implications for the study of strongly coupled plasmas.\n",
      "\n",
      "Finally, we combine our knowledge of quasinormal modes and hydrodynamics to study the behavior of a rotating black string in a moving plasma. We show that the quasinormal modes can be used to extract important information about the plasma, such as its viscosity and other transport coefficients. We also demonstrate how the AdS/CFT correspondence can be used to shed light on the behavior of black holes in strongly coupled plasmas.\n",
      "\n",
      "Overall, our work contributes to a deeper understanding of the connections between black hole physics and the behavior of strongly interacting systems. By studying the properties of quasinormal modes and applying them to the study of hydrodynamics, we are able to gain new insights into the fundamental physics that governs the universe at its most basic level. 1 into PostgreSQL...\n",
      "Inserting test sample 2713  We construct several Milky Way-like galaxy models containing a gas halo (as well as gaseous and stellar disks, a dark matter halo, and a stellar bulge) following either an isothermal or an NFW density profile with varying mass and initial spin. In addition, galactic winds associated with star formation are tested in some of the simulations. We evolve these isolated galaxy models using the GADGET-3 $N$-body/hydrodynamic simulation code, paying particular attention to the effects of the gas halo on the evolution. We find that the evolution of the models is strongly affected by the adopted gas halo component. The model without a gas halo shows an increasing star formation rate (SFR) at the beginning of the simulation for some hundreds of millions of years and then a continuously decreasing rate to the end of the run at 3 Gyr. On the other hand, the SFRs in the models with a gas halo emerge to be either relatively flat throughout the simulations or increasing over a gigayear and then decreasing to the end. The models with the more centrally concentrated NFW gas halo show overall higher SFRs than those with the isothermal gas halo of the equal mass.\n",
      "\n",
      "The gas accretion from the halo onto the disk also occurs more in the models with the NFW gas halo, however, this is shown to take place mostly in the inner part of the disk and not to contribute significantly to the star formation unless the gas halo has very high density at the central part. The rotation of a gas halo is found to make SFR lower in the model. The SFRs in the runs including galactic winds are found to be lower than the same runs but without winds. We conclude that the effects of a hot gaseous halo on the evolution of galaxies are generally too significant to be simply ignored, and expect that more hydrodynamical processes in galaxies could be understood through numerical simulations employing both gas disk and gas halo components. 0 into PostgreSQL...\n",
      "Inserting test sample 2714  This paper investigates the effects of the hot gas halo on the initial conditions and evolution of isolated galaxy models. The presence of a hot gas halo is a fundamental aspect of galaxy formation and evolution, yet its precise role in shaping the properties of galaxies remains uncertain. \n",
      "\n",
      "We use state-of-the-art numerical simulations to study the impact of the hot gas halo on the formation and evolution of isolated galaxy models. Our simulations include a variety of physical processes such as gas cooling, star formation, and feedback from supernovae and active galactic nuclei.\n",
      "\n",
      "Our results show that the presence of a hot gas halo can have a significant impact on the properties of isolated galaxy models. In particular, we find that the hot gas halo can influence the rate of gas accretion onto the galaxy, the efficiency of star formation, and the properties of the resulting stellar populations. \n",
      "\n",
      "We also find that the formation and evolution of the hot gas halo itself is closely tied to the properties of the underlying galaxy. In particular, the size and mass of the hot gas halo are strongly correlated with the mass and metallicity of the central galaxy. \n",
      "\n",
      "Overall, our simulations suggest that the hot gas halo plays a crucial role in shaping the properties of isolated galaxy models, and that a complete understanding of galaxy formation and evolution requires a thorough understanding of the formation and evolution of the hot gas halo. Our results have important implications for future observational studies of isolated galaxies, and provide important constraints for theoretical models of galaxy formation and evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 2715  Bernstein-Sato polynomial of a hypersurface is an important object with numerous applications. It is known, that it is complicated to obtain it computationally, as a number of open questions and challenges indicate. In this paper we propose a family of algorithms called \\texttt{checkRoot} for optimized check of whether a given rational number is a root of Bernstein-Sato polynomial and the computations of its multiplicity. This algorithms are used in the new approach to compute the whole global or local Bernstein-Sato polynomial and $b$-function of a holonomic ideal with respect to weights. They are applied in numerous situations, where there is a possibility to compute an upper bound for the polynomial. Namely, it can be achieved by means of embedded resolution, for topologically equivalent singularities or using the formula of A'Campo and spectral numbers. We also present approaches to the logarithmic comparison problem and the intersection homology D-module. Several applications are presented as well as solutions to some challenges which were intractable with the classical methods. One of the main applications consists of computing of a stratification of affine space with the local $b$-function being constant on each stratum. Notably, the algorithm we propose does not employ primary decomposition. Also we apply our results for the computation of Bernstein-Sato polynomials for varieties. The methods from this paper have been implemented in {\\sc Singular:Plural} as libraries {\\tt dmod.lib} and {\\tt bfun.lib}. All the examples from the paper have been computed with this implementation. 0 into PostgreSQL...\n",
      "Inserting test sample 2716  This paper explores the development of algorithms for checking rational roots of $b$-functions and their various applications. The motivation for creating these algorithms stems from the fact that $b$-functions are essential tools in the study of various mathematical objects, such as algebraic curves and singularities. By developing a method for efficiently testing integral and rational roots of $b$-functions, we provide a valuable computational tool for mathematicians interested in studying these objects. The proposed algorithms build upon existing techniques such as the Newton-Puiseux iteration and resultant theory, while also introducing new ideas that significantly improve efficiency. One key application of these algorithms is in the construction of deformation spaces of $b$-functions, which play an important role in the study of moduli spaces of algebraic curves. Additionally, the algorithms are used to compute the topological Euler characteristic of a certain class of varieties known as toric varieties. These applications demonstrate the usefulness of our algorithms in various areas of mathematics. The paper concludes with experimental results that showcase the effectiveness of our algorithms in practice. Overall, our contributions provide a significant step towards a better understanding of $b$-functions and their many applications in modern mathematics. 1 into PostgreSQL...\n",
      "Inserting test sample 2717  The task of Chinese text spam detection is very challenging due to both glyph and phonetic variations of Chinese characters. This paper proposes a novel framework to jointly model Chinese variational, semantic, and contextualized representations for Chinese text spam detection task. In particular, a Variation Family-enhanced Graph Embedding (VFGE) algorithm is designed based on a Chinese character variation graph. The VFGE can learn both the graph embeddings of the Chinese characters (local) and the latent variation families (global). Furthermore, an enhanced bidirectional language model, with a combination gate function and an aggregation learning function, is proposed to integrate the graph and text information while capturing the sequential information. Extensive experiments have been conducted on both SMS and review datasets, to show the proposed method outperforms a series of state-of-the-art models for Chinese spam detection. 0 into PostgreSQL...\n",
      "Inserting test sample 2718  As spam content creators become more adept at camouflaging their messages to evade filters, new methods are required to accurately identify and remove it. This research paper presents a novel approach to detecting camouflaged spam content through the use of StoneSkipping, a graph and text joint embedding method for Chinese character variation representation. The proposed method overcomes limitations of existing graph representation methods, taking both semantic and visual information into account to improve the accuracy of spam detection. Experimental results demonstrate the superiority of StoneSkipping over other state-of-the-art embedding techniques, achieving high precision and recall scores in spam content detection. This approach shows promise for future research in the growing field of spam detection, providing a new tool for identifying and combating harmful content online. 1 into PostgreSQL...\n",
      "Inserting test sample 2719  We carried out the largest ($>3.5\\times10^5$ Mpc$^3$, 26 deg$^2$) H$\\alpha$ narrow band survey to date at $z\\sim0.2$ in the SA22, W2 and XMMLSS extragalactic fields. Our survey covers a large enough volume to overcome cosmic variance and to sample bright and rare H$\\alpha$ emitters up to an observed luminosity of $\\sim10^{42.4}$ erg s$^{-1}$, equivalent to $\\sim11 M_\\odot$ yr$^{-1}$. Using our sample of $220$ sources brighter than $>10^{41.4}$ erg s$^{-1}$ ($>1 M_\\odot$ yr$^{-1}$), we derive H$\\alpha$ luminosity functions, which are well described by a Schechter function with $\\phi^* = 10^{-2.85\\pm0.03}$ Mpc$^{-3}$ and $L^*_{H\\alpha} = 10^{41.71\\pm0.02}$ erg s$^{-1}$ (with a fixed faint end slope $\\alpha=-1.35$). We find that surveys probing smaller volumes ($\\sim3\\times10^4$ Mpc$^3$) are heavily affected by cosmic variance, which can lead to errors of over $100$ per cent in the characteristic density and luminosity of the H$\\alpha$ luminosity function.\n",
      "\n",
      "We derive a star formation rate density of $\\rho_\\mathrm{SFRD} = 0.0094\\pm0.0008$ $M_\\odot$ yr$^{-1}$, in agreement with the redshift-dependent H$\\alpha$ parametrisation from Sobral et al. (2013). The two-point correlation function is described by a single power law $\\omega(\\theta) = (0.159\\pm0.012) \\theta^{(-0.75\\pm0.05)}$, corresponding to a clustering length of $r_0 = 3.3\\pm0.8$ Mpc/h. We find that the most luminous H$\\alpha$ emitters at $z\\sim0.2$ are more strongly clustered than the relatively fainter ones. The $L^*_{H\\alpha}$ H$\\alpha$ emitters at $z\\sim0.2$ in our sample reside in $\\sim10^{12.5-13.5}$ $M_\\odot$ dark matter haloes. This implies that the most star forming galaxies always reside in relatively massive haloes or group-like environments and that the typical host halo mass of star-forming galaxies is independent of redshift if scaled by $L_\\mathrm{H\\alpha}/L^*_{H\\alpha}(z)$, as proposed by Sobral et al. (2010). 0 into PostgreSQL...\n",
      "Inserting test sample 2720  In this research paper, we present the results of a large narrow band H$\\alpha$ survey of galaxies at a redshift of $z\\sim0.2$. Our main focus is on studying the bright end of the luminosity function, as well as investigating cosmic variance and clustering across cosmic time. \n",
      "\n",
      "Our survey covers a total of $\\sim$100 square degrees on the sky and probes the H$\\alpha$ emission line for hundreds of thousands of galaxies in this redshift range. By carefully accounting for variations in survey depth and other observational effects, we construct an accurate and well-characterized luminosity function for H$\\alpha$ at $z\\sim0.2$.\n",
      "\n",
      "Our analysis reveals a sharp drop-off in the number density of bright ($L\\gtrsim10^{42}$ erg/s) H$\\alpha$ emitters at this redshift, consistent with previous studies. However, our survey is the first to probe this regime with such a wide field of view, enabling us to measure the slope and normalization of the luminosity function with high precision.\n",
      "\n",
      "We also investigate the impact of cosmic variance on measurements of the luminosity function and clustering statistics, through the use of mock catalogs constructed from state-of-the-art cosmological simulations. We find that cosmic variance can have a significant impact on these measurements, particularly on small scales and for relatively rare objects.\n",
      "\n",
      "Finally, we discuss the implications of our results for our understanding of galaxy formation and evolution at $z\\sim0.2$, as well as the use of H$\\alpha$ emission as a probe of star formation activity in galaxies. We argue that our survey provides an important new dataset for future studies in these areas, and we highlight several key questions that our results raise for future investigation. 1 into PostgreSQL...\n",
      "Inserting test sample 2721  While loopy belief propagation (LBP) performs reasonably well for inference in some Gaussian graphical models with cycles, its performance is unsatisfactory for many others. In particular for some models LBP does not converge, and in general when it does converge, the computed variances are incorrect (except for cycle-free graphs for which belief propagation (BP) is non-iterative and exact). In this paper we propose {\\em feedback message passing} (FMP), a message-passing algorithm that makes use of a special set of vertices (called a {\\em feedback vertex set} or {\\em FVS}) whose removal results in a cycle-free graph. In FMP, standard BP is employed several times on the cycle-free subgraph excluding the FVS while a special message-passing scheme is used for the nodes in the FVS. The computational complexity of exact inference is $O(k^2n)$, where $k$ is the number of feedback nodes, and $n$ is the total number of nodes. When the size of the FVS is very large, FMP is intractable. Hence we propose {\\em approximate FMP}, where a pseudo-FVS is used instead of an FVS, and where inference in the non-cycle-free graph obtained by removing the pseudo-FVS is carried out approximately using LBP. We show that, when approximate FMP converges, it yields exact means and variances on the pseudo-FVS and exact means throughout the remainder of the graph. We also provide theoretical results on the convergence and accuracy of approximate FMP.\n",
      "\n",
      "In particular, we prove error bounds on variance computation. Based on these theoretical results, we design efficient algorithms to select a pseudo-FVS of bounded size. The choice of the pseudo-FVS allows us to explicitly trade off between efficiency and accuracy. Experimental results show that using a pseudo-FVS of size no larger than $\\log(n)$, this procedure converges much more often, more quickly, and provides more accurate results than LBP on the entire graph. 0 into PostgreSQL...\n",
      "Inserting test sample 2722  Gaussian graphical models (GGMs) have been extensively utilized in various scientific fields for modeling high-dimensional data. Inference in GGMs involves computing the precision matrix, which is the inverse of the covariance matrix. Due to the high-dimensional nature of GGMs, the computation of the precision matrix can be challenging, especially when the sample size is much smaller than the number of variables.\n",
      "\n",
      "This paper proposes a novel feedback message passing algorithm for efficient inference in GGMs. In this algorithm, messages are passed between the variables in the graph and feedback is propagated back to improve the message passing accuracy. The use of feedback allows for more accurate messages to be passed, resulting in a more accurate precision matrix.\n",
      "\n",
      "The proposed algorithm is compared to other state-of-the-art algorithms using both synthetic and real-world datasets. The experimental results show that the proposed algorithm outperforms the existing algorithms in terms of both accuracy and computation time. The algorithm also scales well with the size of the GGM, making it suitable for large-scale inference problems.\n",
      "\n",
      "Moreover, the proposed algorithm is easily parallelizable and can be implemented on distributed systems for efficient computation. This is particularly relevant for big data applications, where the size of the data may be too large to fit into memory.\n",
      "\n",
      "In summary, this paper presents a feedback message passing algorithm for efficient inference in GGMs, which outperforms the existing state-of-the-art algorithms in terms of accuracy and computation time. The proposed algorithm is scalable and can be easily parallelized, making it suitable for large-scale inference problems. This work advances the state-of-the-art in GGM inference and has potential applications in various scientific fields. 1 into PostgreSQL...\n",
      "Inserting test sample 2723  We simulate liquid water between hydrophobic walls separated by 0.5 nm, to study how the diffusion constant D_\\parallel parallel to the walls depends on the microscopic structure of water. At low temperature T, water diffusion can be associated with the number of defects in the hydrogen bonds network.\n",
      "\n",
      "However, the number of defects solely does not account for the peculiar diffusion of water, with maxima and minima along isotherms. Here, we calculate a relation that quantitatively reproduces the behavior of D_\\parallel, focusing on the high-T regime. We clarify how the interplay between breaking of hydrogen bonds and cooperative rearranging regions of 1-nm size gives rise to the diffusion extrema, possibly relevant for both bulk and nanoconfined water. 0 into PostgreSQL...\n",
      "Inserting test sample 2724  This study investigates the influence of hydrophobic nanoconfinement on the diffusion anomaly and cooperative rearrangement regions in water monolayers. Using molecular dynamics simulations, we found that as the confinement of the water monolayer increased, so did the extent of its diffusion anomaly behavior. We also found that cooperative rearrangement regions increased with decreasing temperature and increasing confinement. We further explored the origin of these phenomena and found evidence of enhanced hydrogen bond lifetimes in the confined nano-water system. Our observations are consistent with the premise that the diffusion anomaly and cooperative rearrangement regions in water are the result of structural transitions in the hydrogen bond network. These findings offer novel insights into the behavior of water in confined environments, with potential implications in materials science and nanofluidics. 1 into PostgreSQL...\n",
      "Inserting test sample 2725  The ground, one- and two-particle states of the (1+1)-dimensional massive sine-Gordon field theory are investigated within the framework of the Gaussian wave-functional approach. We demonstrate that for a certain region of the model-parameter space, the vacuum of the field system is asymmetrical.\n",
      "\n",
      "Furthermore, it is shown that two-particle bound state can exist upon the asymmetric vacuum for a part of the aforementioned region. Besides, for the bosonic equivalent to the massive Schwinger model, the masses of the one boson and two-boson bound states agree with the recent second-order results of a fermion-mass perturbation calculation when the fermion mass is small. 0 into PostgreSQL...\n",
      "Inserting test sample 2726  This paper presents a comprehensive study of the (1+1)-dimensional massive sine-Gordon field theory utilizing the Gaussian wave-functional approach. Specifically, we investigate the behavior of the model's dynamical degrees of freedom and develop analytic expressions for their equations of motion. Our study reveals the existence of topological structures in the model's classical solutions and their persistence in the quantum regime. Additionally, we derive the exact functional form of the one-loop effective potential and compare it with the semiclassical approximation. The findings demonstrate the usefulness of the Gaussian wave-functional for analyzing field theory models and provide a deeper understanding of the behavior of the massive sine-Gordon model in (1+1) dimensions. 1 into PostgreSQL...\n",
      "Inserting test sample 2727  The article has two parts. The first part is devoted to proving a singular version of the logarithmic Kodaira-Akizuki-Nakano vanishing theorem of Esnault and Viehweg. This is then used to prove other vanishing theorems. In the second part these vanishing theorems are used to prove an Arakelov-Parshin type boundedness result for families of canonically polarized varieties with rational Gorenstein singularities. 0 into PostgreSQL...\n",
      "Inserting test sample 2728  We prove the logarithmic Kodaira-Akizuki-Nakano vanishing theorem for a proper morphism with relative log canonical singularities, and establish the Arakelov-Parshin boundedness theorem for singular projective varieties. Our main result extends these classical results to the singular setting and has potential applications in the study of algebraic cycles and rational points. 1 into PostgreSQL...\n",
      "Inserting test sample 2729  In this article we develop some new existence results for the Einstein constraint equations using the Lichnerowicz-York conformal rescaling method.\n",
      "\n",
      "The mean extrinsic curvature is taken to be an arbitrary smooth function without restrictions on the size of its spatial derivatives, so that it can be arbitrarily far from constant. The rescaled background metric belongs to the positive Yamabe class, and the freely specifiable part of the data given by the traceless-transverse part of the rescaled extrinsic curvature and the matter fields are taken to be sufficiently small, with the matter energy density not identically zero. Using topological fixed-point arguments and global barrier constructions, we then establish existence of solutions to the constraints. Two recent advances in the analysis of the Einstein constraint equations make this result possible: A new type of topological fixed-point argument without smallness conditions on spatial derivatives of the mean extrinsic curvature, and a new construction of global super-solutions for the Hamiltonian constraint that is similarly free of such conditions on the mean extrinsic curvature. For clarity, we present our results only for strong solutions on closed manifolds.\n",
      "\n",
      "However, our results also hold for weak solutions and for other cases such as compact manifolds with boundary; these generalizations will appear elsewhere.\n",
      "\n",
      "The existence results presented here for the Einstein constraints are apparently the first such results that do not require smallness conditions on spatial derivatives of the mean extrinsic curvature. 0 into PostgreSQL...\n",
      "Inserting test sample 2730  In this paper, we construct far-from-constant mean curvature solutions of Einstein's constraint equations in the context of positive Yamabe metrics. Our approach is motivated by the study of the Lichnerowicz conjecture, which suggests that the solution space of the constraint equations is isomorphic to the space of conformal classes of positive Yamabe metrics. By analyzing the conformal geometry of the solution space, we construct a family of solutions in a far-from-constant mean curvature regime. We also investigate the properties of these solutions, such as their stability and the relation between the mean curvature and the scalar curvature of the associated metric. Furthermore, we show that our solutions provide a natural framework for understanding the behavior of the conformal Laplacian operator on manifolds with positive Yamabe metrics, a topic of great interest in mathematical physics and geometry. Our results shed new light on the interplay between the geometry and topology of manifolds and the behavior of physical fields on them, and pave the way for further exploration of the Lichnerowicz conjecture and related problems. 1 into PostgreSQL...\n",
      "Inserting test sample 2731  We present a detailed spectroscopic analysis of the luminous blue variable AG Carinae during the last two visual minimum phases of its S-Dor cycle (1985-1990 and 2000-2003). The analysis reveals an overabundance of He, N, and Na, and a depletion of H, C, and O, on the surface of AG Car, indicating the presence of CNO-processed material. Furthermore, the ratio N/O is higher on the stellar surface than in the nebula. We found that the minimum phases of AG Car are not equal to each other, since we derived a noticeable difference between the maximum effective temperature achieved during 1985-1990 (22,800 K) and 2000-2001 (17,000 K). While the wind terminal velocity was 300 km/s in 1985-1990, it was as low as 105 km/s in 2001. The mass-loss rate, however, was lower from 1985-1990 (1.5 x 10^(-5) Msun/yr) than from 2000-2001 (3.7 x 10^(-5) Msun/yr). We found that the wind of AG Car is significantly clumped (f=0.10 - 0.25) and that clumps must be formed deep in the wind. We derived a bolometric luminosity of 1.5 x 10^6 Lsun during both minimum phases which, contrary to the common assumption, decreases to 1.0 x 10^6 Lsun as the star moves towards maximum flux in the V band. Assuming that the decrease in the bolometric luminosity of AG Car is due to the energy used to expand the outer layers of the star (Lamers 1995), we found that the expanding layers contain roughly 0.6 - 2 Msun. Such an amount of mass is an order of magnitude lower than the nebular mass around AG Car, but is comparable to the nebular mass found around lower-luminosity LBVs and to that of the Little Homunculus of Eta Car. If such a large amount of mass is indeed involved in the S Dor-type variability, we speculate that such instability could be a failed Giant Eruption, with several solar masses never becoming unbound from the star.(abridged) 0 into PostgreSQL...\n",
      "Inserting test sample 2732  This paper presents a study of the luminous blue variable AG Carinae. Our analysis focuses on fundamental parameters during visual minimum phases and changes in bolometric luminosity during the S-Doradus cycle. We use a comprehensive dataset of multi-band photometry and spectroscopy to determine the effective temperature, luminosity, radius, and mass of the star during visual minimum phases. We find that AG Carinae displays significant variations in these parameters over the course of its S-Doradus cycle. Additionally, we analyze the star's spectral energy distribution and calculate the bolometric luminosity during different stages of the cycle. Our results show that AG Carinae experiences a significant increase in bolometric luminosity during the hotter and brighter phases of its cycle. We also investigate the spectral characteristics of the star and identify changes in both the strength and shape of spectral lines during the S-Doradus cycle. We discuss possible explanations for these observations, including mass loss, changes in opacity due to elemental abundance variations, and non-radial pulsations. We conclude that AG Carinae is a complex system that presents a rich variety of observational phenomena. Our study provides important insights into the physical processes driving the variability of this prototype luminous blue variable. Future work will focus on obtaining complementary observations and using advanced modeling techniques to further unravel the mysteries of this fascinating object. 1 into PostgreSQL...\n",
      "Inserting test sample 2733  We present details of the phase diagrams of fermionic systems with random and frustrated interactions, emphasizing the important role of the chemical potential. The insulating fermionic Ising spin glass model is shown to reveal different entangled magnetic instabilities and phase transitions. We review tricritical phenomena related to the strong correspondence between charge and spin fluctuations, being controlled by quantum statistics. We compare the spin density diluted Sherrington-Kirkpatrick spin glass with classical spin 1 models such as the BEG model. We analyse in detail the infinite range model and show that spin glass order must decay discontinuously as the chemical potential exceeds a critical value, provided the temperature is below the tricritical one, and that the T=0 transition is of classical type. Parisi replica permutation symmetry breaking (RPSB) governs the thermal spin glass transitions and fermionic modifications of the SK-models AT-line emerge. RPSB takes place everywhere within the fermionic spin glass phase. Although the critical field theory of the quantum paramagnet to spin glass transition in metallic systems remains replica--symmetric at T=0, with only small corrections at low T from RPSB, the phase diagram is affected at O(T^0) by RPSB. Generalizing our results for the fermionic Ising spin glass we consider aspects of models with additional spin and charge quantum--dynamics such as metallic spin glasses. 0 into PostgreSQL...\n",
      "Inserting test sample 2734  This paper investigates the phase diagram of randomly interacting fermionic systems. These systems are often found in condensed matter and particle physics, and their properties are still not fully understood. The focus of this research is to determine how the random interactions affect the system's phase transitions. Using a combination of analytical and numerical techniques, we study the behavior of these systems in various dimensions and under different interaction strengths. Our results show that the random interactions can drastically alter the phase diagram, causing new phases to emerge and changing the character of phase transitions. We find evidence of unconventional phases, such as topological superfluids and spin liquids, which may have practical applications in quantum information processing. Our study provides insights into the complex behavior of randomly interacting fermionic systems and lays the foundation for further research. In addition, our results could impact our understanding of many physical systems, including high-temperature superconductors and cold atomic gases. 1 into PostgreSQL...\n",
      "Inserting test sample 2735  Atomic magnetometers are highly sensitive detectors of magnetic fields that monitor the evolution of the macroscopic magnetic moment of atomic vapors, and opening new applications in biological, physical, and chemical science.\n",
      "\n",
      "However, the performance of atomic magnetometers is often limited by hidden systematic effects that may cause misdiagnosis for a variety of applications, e.g., in NMR and in biomagnetism. In this work, we uncover a hitherto unexplained interference effect in atomic magnetometers, which causes an important systematic effect to greatly deteriorate the accuracy of measuring magnetic fields. We present a standard approach to detecting and characterizing the interference effect in, but not limited to, atomic magnetometers. As applications of our work, we consider the effect of the interference in NMR structural determination and locating the brain electrophysiological symptom, and show that it will help to improve the measurement accuracy by taking interference effects into account. Through our experiments, we indeed find good agreement between our prediction and the asymmetric amplitudes of resonant lines in ultralow-field NMR spectra -- an effect that has not been understood so far. We anticipate that our work will stimulate interesting new researches for magnetic interference phenomena in a wide range of magnetometers and their applications. 0 into PostgreSQL...\n",
      "Inserting test sample 2736  Atomic magnetometry is a powerful tool for measuring magnetic fields, with applications ranging from biomagnetism to materials science. However, magnetic fields can be affected by environmental factors or other sources of interference, leading to inaccurate measurements. This paper explores the various sources of interference in atomic magnetometry and proposes techniques for mitigating their effects.\n",
      "\n",
      "One common source of interference is magnetic noise from the environment, which can be caused by electrical equipment or even natural fluctuations in the Earth's magnetic field. Several methods for reducing this type of interference are discussed, including shielding techniques and noise cancellation algorithms.\n",
      "\n",
      "In addition to environmental noise, interference can also arise from other sources within the system itself. One example is laser noise, which can be caused by fluctuations in the intensity or frequency of the laser used to probe the atoms. The paper describes methods for minimizing this type of interference through careful design of the experimental setup.\n",
      "\n",
      "Overall, this paper provides a comprehensive overview of the sources of interference in atomic magnetometry and suggests strategies for improving the accuracy of measurements. These techniques will be useful for researchers in a wide range of fields who rely on atomic magnetometry to study magnetic phenomena in materials or living systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2737  We propose a framework that extends Blender to exploit Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques for image-based modeling tasks such as sculpting or camera and motion tracking. Applying SfM allows us to determine camera motions without manually defining feature tracks or calibrating the cameras used to capture the image data. With MVS we are able to automatically compute dense scene models, which is not feasible with the built-in tools of Blender. Currently, our framework supports several state-of-the-art SfM and MVS pipelines. The modular system design enables us to integrate further approaches without additional effort. The framework is publicly available as an open source software package. 0 into PostgreSQL...\n",
      "Inserting test sample 2738  This paper presents a photogrammetry-based framework that streamlines image-based modeling and automatic camera tracking. Aiming at scenarios where manual tracking and modeling are not feasible, the proposed method enables the generation of accurate 3D models and camera path estimation with minimal user interaction. Based on an initial set of sparse 3D points, the proposed technique extracts rich feature descriptors from multiple images, allowing precise camera pose estimation and robust 3D point triangulation. Our experimental results demonstrate the effectiveness and efficiency of our method by providing precise tracking and reconstruction results in various challenging scenarios, such as long-action sequences with significant viewpoint changes, indoor and outdoor scenes, and large-scale scenes with few texture features. 1 into PostgreSQL...\n",
      "Inserting test sample 2739  The imprint of Baryonic Acoustic Oscillations (BAO) on the matter power spectrum can be constrained using the neutral hydrogen density in the intergalactic medium as a tracer of the matter density. One of the goals of the Baryon Oscillation Spectroscopic Survey (BOSS) of the Sloan Digital Sky Survey (SDSS-III) is to derive the Hubble expansion rate and the angular scale from the BAO signal in the IGM. To this aim, the Lyman-alpha forest of 10^5 quasars will be observed in the redshift range 2.2<z<3.5 and over 10,000 deg^2. We simulated the BOSS QSO survey to estimate the statistical accuracy on the BAO scale determination provided by such a large scale survey. In particular, we discuss the effect of the poorly constrained estimate of the unabsorbed intrinsic quasar spectrum. The volume of current N-body simulations being too small for such studies, we resorted to Gaussian random field (GRF) simulations.\n",
      "\n",
      "We validated the use of GRFs by comparing the output of GRF simulations with that of the Horizon N-body simulation with the same initial conditions.\n",
      "\n",
      "Realistic mock samples of QSO Lyman-\\alpha forest were generated; their power spectrum was computed and fitted to obtain the BAO scale. The rms of the results for 100 different simulations provides an estimate of the statistical error expected from the BOSS survey. We confirm the results from Fisher matrix estimate. In the absence of error on the unabsorbed quasar spectrum, the BOSS quasar survey should measure the BAO scale with an error of the order of 2.3%, or the transverse and radial BAO scales separately with errors of the order of 6.8% and 3.9%, respectively. The significance of the BAO detection is assessed by an average \\Delta\\chi^2=17 but for individual realizations \\Delta\\chi^2 ranges from 2 t o 35. The error on the unabsorbed quasar spectrum increases the error on the BAO scale by 10 to 20% and results in a sub percent bias. 0 into PostgreSQL...\n",
      "Inserting test sample 2740  This study presents the results of simulations of Baryonic Acoustic Oscillation (BAO) reconstruction with a quasar Lyman-alpha survey. The BAO feature in the power spectrum of matter distribution is a standard ruler that can be used to measure the expansion history of the Universe and the nature of dark energy. However, the BAO signal is affected by non-linear gravitational evolution and redshift-space distortions, which can degrade the precision of the cosmological constraints. \n",
      "\n",
      "In this work, we use mock quasar Lyman-alpha forest observations to study the impact of these effects on the BAO signal. We generate realistic synthetic spectra of quasar absorption lines, including the forest and the Lyman-alpha emission line, at different redshifts from z=2 to z=3.5. The Lyman-alpha forest is a tracer of the underlying matter distribution, and can be used to reconstruct the BAO signal using novel statistical methods such as the Fuzzy Reconstruction Technique (FRT). The FRT is able to mitigate the effects of non-linearities and redshift distortions by smoothing the observed spectra in a smart way that preserves the BAO signal.\n",
      "\n",
      "Our simulations show that the FRT can successfully recover the BAO feature from the quasar Lyman-alpha forest, with a precision that is competitive with the state-of-the-art BAO measurements from galaxy surveys. We also investigate the impact of various systematics, such as continuum fitting, signal-to-noise ratio, and metal absorption lines, on the reconstruction accuracy. Our results demonstrate the potential of quasar Lyman-alpha surveys, such as the ongoing DESI and future PFS projects, in improving the precision of cosmological constraints from BAO measurements. \n",
      "\n",
      "In summary, our study presents a novel approach to BAO reconstruction using quasar Lyman-alpha forest observations, and demonstrates its feasibility and potential in future cosmological surveys. 1 into PostgreSQL...\n",
      "Inserting test sample 2741  We consider the main factors which cause the variation of the value of the local slope of the elastic pp cross section B(t)=d[\\ln(d\\sigma_el(pp)/dt]/dt with t. Namely, we discuss the role of the pion-loop insertion in the pomeron trajectory, the t-dependence of the pomeron-nucleon coupling and the role of the eikonalization of the proton-proton amplitude in both the one- and two-channel eikonal models. 0 into PostgreSQL...\n",
      "Inserting test sample 2742  The dependence of the slope of the high energy elastic proton-proton cross-section on the momentum transfer t is investigated using data from the TOTEM experiment. The measured t-slope value is found to decrease with increasing center-of-mass energy, confirming the predictions of several theoretical models. The results suggest that the nature of the proton-proton interaction is changing with energy, and provide valuable insights for further theoretical developments. 1 into PostgreSQL...\n",
      "Inserting test sample 2743  The James Webb Space Telescope will provide deep imaging and spectroscopy for sources at redshifts above 6, covering the Epoch of Reionization (EoR, 6 < z < 10). The Mid-IR instrument (MIRI) integral field spectrograph (MRS) will be the only instrument on board JWST able to observe the brightest optical emission lines H$\\alpha$ and [OIII]0.5007$\\mu$m at redshifts above 7 and 9, respectively. This paper presents a study of the H$\\alpha$ fluxes predicted by FIRSTLIGHT cosmological simulations for galaxies at redshifts of 6.5 to 10.5, and its detectability with MIRI. Deep (40 ks) spectroscopic integrations with MRS will be able to detect (S/N > 5) EoR sources at redshifts above 7 with intrinsic star formation rates of more than 2 M$_{\\odot}$ yr$^{-1}$, and stellar masses above 4-9 $\\times$ 10$^7$ M$_{\\odot}$. In addition, the paper presents realistic MRS simulated observations of the expected (rest-frame) optical and near-infrared spectra for some spectroscopically confirmed EoR sources detected by ALMA as [OIII]88$\\mu$m emitters. The MRS simulated spectra cover a wide range of low metallicities from about 0.2 to 0.02Z$_{\\odot}$, and different [OIII]88$\\mu$m/[OIII]0.5007$\\mu$m line ratios. The simulated 10ks MRS spectra show S/N in the range of 5 to 90 for H$\\beta$, [OIII]0.4959,0.5007$\\mu$m, H$\\alpha$ and HeI1.083$\\mu$m emission lines of MACS1149-JD1 at z = 9.11, independent of metallicity. In addition, deep 40 ks simulated spectra of the luminous merger candidate B14-65666 at z=7.15 shows the MRS capabilities of detecting, or putting strong upper limits, on the [NII]0.6584$\\mu$m, [SII]0.6717,0.6731$\\mu$m, and [SIII]0.9069,0.9532$\\mu$m emission lines. In summary, MRS will enable the detailed study of key physical properties like internal extinction, instantaneous star formation, hardness of the ionising continuum, and metallicity, in bright (intrinsic or lensed) EoR sources. 0 into PostgreSQL...\n",
      "Inserting test sample 2744  The Universe during its Epoch of Reionization, or EoR, presents a unique opportunity to study the nature of galaxies in the early stages of their evolution. In particular, we can investigate the physical properties of these galaxies in order to better understand the processes that shape their formation and behavior. In this paper, we present the results of a study aimed at investigating the physical properties of galaxies in the EoR using MIRI/JWST spectroscopy.\n",
      "\n",
      "Our study takes advantage of the capabilities of the James Webb Space Telescope (JWST), specifically the Mid-Infrared Instrument (MIRI), which is uniquely sensitive to the rest-frame mid-infrared emission of distant galaxies. By combining spectral measurements in the mid-infrared with other observations at longer wavelengths, we are able to study the physical properties of galaxies in the EoR.\n",
      "\n",
      "Using a sample of galaxies selected based on their strong Lyman-alpha emission, we analyze their rest-frame mid-infrared spectra to determine several key physical properties, including their dust content, star-formation rate, and gas-phase metallicity. Our analysis reveals a clear correlation between these properties, with galaxies exhibiting higher dust content also showing higher star-formation rates and gas-phase metallicities.\n",
      "\n",
      "In addition to these results, our study also provides insight into the physical processes driving galaxy evolution during the EoR. We find evidence for the presence of significant dust enrichment in the interstellar medium of these galaxies, which suggests the importance of dust production by supernovae and other astrophysical processes. We also observe a strong correlation between the shape of the Lyman-alpha emission line and the properties of the mid-infrared emission, indicating a close connection between the ionization state of the galaxy and its overall physical properties.\n",
      "\n",
      "Overall, our study provides important insights into the physical properties of galaxies during the EoR and highlights the unique capabilities of MIRI/JWST spectroscopy in probing the early Universe. By continuing to study the physical properties of these galaxies, we can gain a better understanding of the processes that drive galaxy formation and evolution throughout cosmic time. 1 into PostgreSQL...\n",
      "Inserting test sample 2745  We present a workable model for the fermion-photon vertex, which is expressed solely in terms of functions that appear in the fermion propagator and independent of the angle between the relative momenta, and does not explicitly depend on the covariant-gauge parameter. It nevertheless produces a critical coupling for dynamical chiral symmetry breaking that is practically independent of the covariant-gauge parameter and an anomalous magnetic moment distribution for the dressed fermion that agrees in important respects with realistic numerical solutions of the inhomogeneous vector Bethe-Salpeter equation. 0 into PostgreSQL...\n",
      "Inserting test sample 2746  This study explores the relationship between the fermion-gauge-boson vertex and dynamical chiral symmetry breaking. We investigate the effective vertices and analyze the limits of the fermionic momenta. The relationship found here is useful in analyzing phenomena in a variety of fields, including QCD, condensed matter physics, and cosmology. We hope that our findings will provide new insights for future theoretical and experimental work in this field, ultimately resulting in a deeper understanding of the fundamental forces of the universe and the nature of matter itself. 1 into PostgreSQL...\n",
      "Inserting test sample 2747  We present our first results from 130 ks of X-ray observations obtained with the Advanced CCD Imaging Spectrometer on the Chandra X-ray Observatory. We reach a flux of 2 X 10^(-16) erg s^(-1) cm^(-2) in the 0.5-2 keV soft band and 2 X 10^(-15) erg s^(-1) cm^(-2) in the 2-10 keV hard band. For the optical identification we carried out a survey in VRI with the FORS-1 imaging-spectrometer on the ANTU telescope (UT-1 at VLT) complete to R <= 26.\n",
      "\n",
      "This dataset was complemented with data from the ESO Imaging Survey (EIS) in the UBJK bands and the ESO Wide Field Imager Survey (WFI) in the B band. The positional accuracy of the X-ray detections is of order of 1'' in the central 6'. Optical identifications are found for ~90% of the sources. We obtain the cumulative spectra of the faint and bright X-ray sources in the sample. A power law fit in the range 2-10 keV using the galactic value of N_H ~ 8 x 10^(19) cm^(-2), yields a photon index of Gamma = 1.70+-0.06 and 1.35+-0.10 for the bright and faint sample respectively, showing a flattening of the spectrum at lower fluxes. We discuss the LogN-LogS relationship and the discrete source contribution to the integrated X-ray sky flux. In the soft band, the sources now detected at fluxes below 10^(-15) erg s^(-1) cm^(-2) contribute (4.0 +- 0.3) X 10^(-12) erg cm^(-2)s^(-1) deg^(-2) to the total XRB. The flux resolved in the hard band down to the flux limit of 2 X 10^(-15) erg s^(-1) cm^(-2) now contributes a total of 1.6 X 10^(-11) erg cm^(-2) s^(-1) deg^(-2) which amounts to a fraction of 60-80% of the total measured background. This result confirms that the XRB is due to the integrated contribution of discrete sources, but shows that there is still a relevant fraction (at least 20%) of the hard XRB to be resolved at fluxes below 10^(-15) erg s^(-1) cm^(-2). (ABRIDGED) 0 into PostgreSQL...\n",
      "Inserting test sample 2748  This research paper presents the first set of results obtained from the X-ray and optical survey of the Chandra Deep Field South. The primary goal of this study is to identify and characterize the X-ray sources detected within this region. The survey combines data from the Chandra X-ray Observatory and ground-based optical telescopes. \n",
      "\n",
      "The X-ray data were acquired using the Advanced CCD Imaging Spectrometer (ACIS) on board Chandra. The survey covers an area of approximately 0.1 square degrees and reaches a sensitivity limit of âˆ¼10^-16 ergs/s/cm^2 in the 0.5-2 keV energy band. A total of 464 X-ray point sources were detected above this limit, of which roughly 80% have optical counterparts.\n",
      "\n",
      "The optical observations were obtained using the Very Large Telescope (VLT) and the 8-meter Gemini South telescope. The VLT images cover the entire Chandra survey area in the B, V, R, and I bands, while the Gemini South data cover a smaller area in the V and I bands. The optical photometry was performed using the SExtractor software package, and the resulting catalog contains âˆ¼13000 sources down to a magnitude limit of âˆ¼27 in the V band.\n",
      "\n",
      "Cross-matching the X-ray and optical catalogs revealed âˆ¼3700 X-ray/optical associations, with a matching probability of âˆ¼70%. The majority of these associations are likely to be AGNs, as suggested by their X-ray spectral properties and optical colors. A significant fraction (âˆ¼20%) of the X-ray sources, however, do not have obvious optical counterparts and may represent a new population of X-ray emitters.\n",
      "\n",
      "We have performed a preliminary spectral analysis of the X-ray sources using the XSPEC package. The X-ray spectra of the majority of the AGNs are well fitted by a power-law model, with an average photon index of âˆ¼1.7. We have also identified a small number of X-ray sources with more complex spectra, which may be indicative of the presence of additional emission components.\n",
      "\n",
      "In conclusion, our study represents the first comprehensive X-ray and optical survey of the Chandra Deep Field South, and provides an important dataset for studying the properties of X-ray sources in the distant universe. Future work will focus on a detailed analysis of the X-ray and optical data, and on the interpretation of the results in terms of the nature and evolution of the X-ray source population. 1 into PostgreSQL...\n",
      "Inserting test sample 2749  Context. Grains in circumstellar disks are believed to grow by mutual collisions and subsequent sticking due to surface forces. Results of many fields of research involving circumstellar disks, such as radiative transfer calculations, disk chemistry, magneto-hydrodynamic simulations largely depend on the unknown grain size distribution.\n",
      "\n",
      "Aims. As detailed calculations of grain growth and fragmentation are both numerically challenging and computationally expensive, we aim to find simple recipes and analytical solutions for the grain size distribution in circumstellar disks for a scenario in which grain growth is limited by fragmentation and radial drift can be neglected.\n",
      "\n",
      "Methods. We generalize previous analytical work on self-similar steady-state grain distributions. Numerical simulations are carried out to identify under which conditions the grain size distributions can be understood in terms of a combination of power-law distributions. A physically motivated fitting formula for grain size distributions is derived using our analytical predictions and numerical simulations.\n",
      "\n",
      "Results. We find good agreement between analytical results and numerical solutions of the Smoluchowski equation for simple shapes of the kernel function. The results for more complicated and realistic cases can be fitted with a physically motivated \"black box\" recipe presented in this paper. Our results show that the shape of the dust distribution is mostly dominated by the gas surface density (not the dust-to-gas ratio), the turbulence strength and the temperature and does not obey an MRN type distribution. 0 into PostgreSQL...\n",
      "Inserting test sample 2750  In this study, we investigate the dust size distribution in coagulation/fragmentation equilibrium through both numerical solutions and analytical fits. Our research focuses on the dynamics of the dust grains in protoplanetary disks and molecular clouds, using a comprehensive model that includes both coagulation and fragmentation processes. We employ state-of-the-art numerical simulations to study the time evolution of the particle size distribution and obtain a detailed understanding of the evolution of the dust. In particular, we investigate the effects of important physical parameters such as the gas density and temperature, as well as the strength of the fragmentation process. Our results reveal the complex behavior of the system, highlighting the competition between the coagulation and fragmentation processes and their impact on the overall distribution of the dust grains. Furthermore, we propose analytical fits to the numerical solutions, providing a convenient way for researchers to incorporate our findings into their own studies. Our work sheds new light on the dynamics of dust formation and growth, offering new insights into the early stages of planet formation. 1 into PostgreSQL...\n",
      "Inserting test sample 2751  Focke, Goldberg, and \\v{Z}ivn\\'y (arXiv 2017) prove a complexity dichotomy for the problem of counting surjective homomorphisms from a large input graph G without loops to a fixed graph H that may have loops. In this note, we give a short proof of a weaker result: Namely, we only prove the #P-hardness of the more general problem in which G may have loops. Our proof is an application of a powerful framework of Lov\\'asz (2012), and it is analogous to proofs of Curticapean, Dell, and Marx (STOC 2017) who studied the \"dual\" problem in which the pattern graph G is small and the host graph H is the input. Independently, Chen (arXiv 2017) used Lov\\'asz's framework to prove a complexity dichotomy for counting surjective homomorphisms to fixed finite structures. 0 into PostgreSQL...\n",
      "Inserting test sample 2752  The paper presents new results on the complexity of problems concerning surjective homomorphisms and compactions. The authors focus on finding an algorithm for computing the number of surjective homomorphisms from a given graph G to another graph H. They show that this problem is hard to solve in general, even when restricted to certain classes of graphs. They also study the so-called \"compactions\" of graphs, which are transformations that preserve certain properties while reducing the size of the graph. They give a complete characterization of the class of graphs that can be \"compacted\" in polynomial time, and show that finding the smallest such \"compaction\" is NP-hard in general. Finally, they prove a dichotomy theorem that classifies the complexity of counting surjective homomorphisms into two classes: either it is #P-complete or it can be computed in polynomial time. 1 into PostgreSQL...\n",
      "Inserting test sample 2753  As the amount of digital devices suspected of containing digital evidence increases, case backlogs for digital investigations are also increasing in many organizations. To ensure timely investigation of requests, this work proposes the use of signature-based methods for automated action instance approximation to automatically reconstruct past user activities within a compromised or suspect system. This work specifically explores how multiple instances of a user action may be detected using signature-based methods during a post-mortem digital forensic analysis. A system is formally defined as a set of objects, where a subset of objects may be altered on the occurrence of an action. A novel action-trace update time threshold is proposed that enables objects to be categorized by their respective update patterns over time. By integrating time into event reconstruction, the most recent action instance approximation as well as limited past instances of the action may be differentiated and their time values approximated. After the formal theory if signature-based event reconstruction is defined, a case study is given to evaluate the practicality of the proposed method. 0 into PostgreSQL...\n",
      "Inserting test sample 2754  In digital investigations, it is essential to know which actions have been taken in the past in order to determine the actors, motives, and impact of digital crimes. However, it is often difficult and time-consuming to manually infer past action instances from relevant digital evidence. In this paper, we propose an automated approach to infer past action instances based on a combination of machine learning and natural language processing techniques. Specifically, we identify action-related keywords and phrases from digital evidence, and then use a supervised algorithm to classify these instances into different action categories such as exfiltration, installation, or modification. We evaluate our approach with a dataset of real-world digital crimes and demonstrate that our approach achieves high accuracy in inferring past actions instances. Our automated approach can significantly reduce the time and resources needed for digital investigations, and enable investigators to more effectively identify the perpetrators and understand the intricacies of digital crimes. 1 into PostgreSQL...\n",
      "Inserting test sample 2755  Social-distancing to combat the COVID-19 pandemic has led to widespread reductions in air pollutant emissions. Quantifying these changes requires a business as usual counterfactual that accounts for the synoptic and seasonal variability of air pollutants. We use a machine learning algorithm driven by information from the NASA GEOS-CF model to assess changes in nitrogen dioxide (NO$_{2}$) and ozone (O$_{3}$) at 5,756 observation sites in 46 countries from January through June 2020. Reductions in NO$_{2}$ correlate with timing and intensity of COVID-19 restrictions, ranging from 60% in severely affected cities (e.g., Wuhan, Milan) to little change (e.g., Rio de Janeiro, Taipei). On average, NO$_{2}$ concentrations were 18% lower than business as usual from February 2020 onward. China experienced the earliest and steepest decline, but concentrations since April have mostly recovered and remained within 5% to the business as usual estimate. NO$_{2}$ reductions in Europe and the US have been more gradual with a halting recovery starting in late March. We estimate that the global NO$_{x}$ (NO+NO$_{2}$) emission reduction during the first 6 months of 2020 amounted to 2.9 TgN, equivalent to 5.1% of the annual anthropogenic total. The response of surface O$_{3}$ is complicated by competing influences of non-linear atmospheric chemistry. While surface O$_{3}$ increased by up to 50% in some locations, we find the overall net impact on daily average O$_{3}$ between February - June 2020 to be small. However, our analysis indicates a flattening of the O$_{3}$ diurnal cycle with an increase in night time ozone due to reduced titration and a decrease in daytime ozone, reflecting a reduction in photochemical production. The O$_{3}$ response is dependent on season, time scale, and environment, with declines in surface O$_{3}$ forecasted if NO$_{x}$ emission reductions continue. 0 into PostgreSQL...\n",
      "Inserting test sample 2756  This study aims to investigate the impact of COVID-19 restrictions on the global atmospheric concentrations of nitrogen dioxide (NO2) and ozone (O3), two of the most significant air pollutants that have wide-ranging effects on human health and the environment. We used satellite observations from the TROPOspheric Monitoring Instrument (TROPOMI) and ground-based measurements from various air quality monitoring networks to estimate the changes in NO2 and O3 levels during the lockdown periods in different regions of the world, compared to the same time periods in the previous years.\n",
      "\n",
      "Our results show a significant reduction in NO2 concentrations during the lockdown periods, particularly in highly populated and industrialized regions such as China, India, Europe, and the United States. The largest reduction in NO2 levels was observed in China, where the strict confinement measures resulted in up to a 50% decrease in NO2 levels in some cities. Similarly, Europe experienced a substantial decline in NO2 concentrations during the lockdown, with some regions experiencing reductions of up to 60% compared to the same time last year. The United States saw a 30-50% reduction in NO2 levels in major cities.\n",
      "\n",
      "In contrast, we observed a mixed picture for O3 concentrations during the lockdown period. While some areas, such as India and Europe, saw relatively small increases in O3 levels due to reduced NO2 levels, other regions, such as China and the eastern United States, saw a reduction in O3 levels, likely due to changes in meteorological conditions.\n",
      "\n",
      "Overall, our findings provide evidence of the positive impact of COVID-19 restrictions on air quality, particularly in reducing NO2 concentrations. However, the effects on O3 are more complex and require further investigation. This study contributes to a better understanding of the relationship between human activities, air quality, and the environment, and highlights the potential for targeted policies and measures to mitigate air pollution and improve public health and environmental outcomes. 1 into PostgreSQL...\n",
      "Inserting test sample 2757  The production yield and angular anisotropy of prompt ${D_s^+}$ mesons were measured as a function of transverse momentum ($p_{ T}$) in Pb-Pb collisions at a centre-of-mass energy per nucleon pair $\\sqrt{s_{ NN}} = 5.02$ TeV collected with the ALICE detector at the LHC. ${D_s^+}$ mesons and their charge conjugates were reconstructed at midrapidity ($|y|<0.5$) from their hadronic decay channel ${D_s^+ \\to \\phi \\pi^+}$, with ${\\phi \\to K^-K^+}$, in the $p_{ T}$ intervals $2<p_{ T}<50$ GeV/$c$ and $2<p_{ T}<36$ GeV/$c$ for the 0-10% and 30-50% centrality intervals. For $p_{ T}>10$ GeV/$c$, the measured ${D_s^+}$-meson nuclear modification factor $R_{ AA}$ is consistent with the one of non-strange D mesons within uncertainties, while at lower $p_{ T}$ a hint for a ${D_s^+}$-meson $R_{ AA}$ larger than that of non-strange D mesons is seen. The enhanced production of ${D_s^+}$ relative to non-strange D mesons is also studied by comparing the $p_{ T}$-dependent ${D_s^+/D^0}$ production yield ratios in Pb-Pb and in pp collisions. The ratio measured in Pb-Pb collisions is found to be on average higher than that in pp collisions in the interval $2<p_{ T} <8$ GeV/$c$ with a significance of 2.3$\\sigma$ and 2.4$\\sigma$ for the 0-10% and 30-50% centrality intervals. The azimuthal anisotropy coefficient $v_2$ of prompt ${D_s^+}$ mesons was measured in Pb-Pb collisions in the 30-50% centrality interval and is found to be compatible with that of non-strange D mesons. The main features of the measured $R_{ AA}$, ${D_s^+/D^0}$ ratio, and $v_2$ as a function of $p_{ T}$ are described by theoretical calculations of charm-quark transport in a hydrodynamically expanding quark-gluon plasma including hadronisation via charm-quark recombination with light quarks from the medium. The $p_{ T}$-integrated production yield of ${D_s^+}$ mesons is compatible with the prediction of the statistical hadronisation model. 0 into PostgreSQL...\n",
      "Inserting test sample 2758  The production of prompt $\\rm{D_{s}^{+}}$-mesons and their azimuthal anisotropy have been studied in Pb-Pb collisions at $\\sqrt{s_{\\rm NN}} = 5.02$ TeV utilizing the ALICE detector at the LHC. The measurement of the production yields and transverse momentum spectra of prompt $\\rm{D_{s}^{+}}$-meson was performed in nine centrality classes. The statistical and systematic uncertainties were evaluated and found to be in agreement with previously published results. The $\\rm{D_{s}^{+}}$-meson nuclear modification factor and elliptic flow coefficient ($v_{2}$) were computed as a function of transverse momentum ($p_{\\rm T}$) and centrality. The results exhibit that both nuclear modification factor and $v_{2}$ value are smaller than unity inmost central collisions. Furthermore, the $v_{2}$ values of $\\rm{D_{s}^{+}}$ mesons for 20-40% and 40-60% centrality classes were compared to those of charged hadrons, and an indication of a mass ordering effect for heavy-flavour hadrons was observed.\n",
      "\n",
      "The study of $\\rm{D_{s}^{+}}$-meson production and their azimuthal anisotropy in heavy ion collisions provide insights into the properties of the Quark-Gluon Plasma (QGP). The $\\rm{D_{s}^{+}}$-mesons originate from the fragmentation of charm quarks which interact with the QGP. Hence, the study of $\\rm{D_{s}^{+}}$-mesons aids in understanding the energy loss mechanism of heavy quarks in the QGP, which can be used to constrain theoretical models. Additionally, the measurement of $\\rm{D_{s}^{+}}$-mesons at LHC energies provides a comparison to lower energies and serves to extend our understanding of heavy-flavour physics.\n",
      "\n",
      "In summary, this paper reports the measurement of prompt $\\rm{D_{s}^{+}}$-meson production and their azimuthal anisotropy in Pb-Pb collisions at $\\sqrt{s_{\\rm NN}} = 5.02$ TeV, which provides insight into the properties of the QGP and heavy-flavour physics. The results exhibit signatures of the energy loss mechanism of heavy quarks in the QGP, and provide a comparison to lower energies, extending our knowledge of heavy-flavour physics. 1 into PostgreSQL...\n",
      "Inserting test sample 2759  Conditions for the validity of the quantum adiabatic approximation are analyzed. For the case of linear Hamiltonians, a simple and general sufficient condition is derived, which is valid for arbitrary spectra and any kind of time variation. It is shown that in some cases the found condition is necessary and sufficient. The adiabatic theorem is generalized for the case of nonlinear Hamiltonians. 0 into PostgreSQL...\n",
      "Inserting test sample 2760  This paper discusses adiabatic theorems for both linear and nonlinear Hamiltonians. The adiabatic theorem for linear Hamiltonians provides a sufficient condition for the system to remain close to its instantaneous eigenspace during a slow change of the Hamiltonian parameters. However, the adiabatic theorem cannot be applied to nonlinear Hamiltonians. We investigate alternative adiabatic theorems that apply to nonlinear Hamiltonians and provide examples. 1 into PostgreSQL...\n",
      "Inserting test sample 2761  We present results on the variation of 7.7 micron Polycyclic Aromatic Hydrocarbon (PAH) emission in galaxies spanning a wide range in metallicity at z ~ 2. For this analysis, we use rest-frame optical spectra of 476 galaxies at 1.37 < z < 2.61 from the MOSFIRE Deep Evolution Field (MOSDEF) survey to infer metallicities and ionization states. Spitzer/MIPS 24 micron and Herschel/PACS 100 and 160 micron observations are used to derive rest-frame 7.7 micron luminosities (L(7.7)) and total IR luminosities (L(IR)), respectively. We find significant trends between the ratio of L(7.7) to L(IR) (and to dust-corrected SFR) and both metallicity and [OIII]/[OII] (O32) emission-line ratio. The latter is an empirical proxy for the ionization parameter. These trends indicate a paucity of PAH emission in low metallicity environments with harder and more intense radiation fields. Additionally, L(7.7)/L(IR) is significantly lower in the youngest quartile of our sample (ages of 500 Myr) compared to older galaxies, which may be a result of the delayed production of PAHs by AGB stars. The relative strength of L(7.7) to L(IR) is also lower by a factor of ~ 2 for galaxies with masses $M_* < 10^{10}M_{\\odot}$, compared to the more massive ones. We demonstrate that commonly-used conversions of L(7.7) (or 24 micron flux density; f(24)) to L(IR) underestimate the IR luminosity by more than a factor of 2 at $M_*$ ~ $10^{9.6-10.0} M_{\\odot}$. We adopt a mass-dependent conversion of L(7.7) to L(IR) with L(7.7)/L(IR)= 0.09 and 0.22 for $M_* < 10^{10}$ and $> 10^{10} M_{\\odot}$, respectively. Based on the new scaling, the SFR-$M_*$ relation has a shallower slope than previously derived.\n",
      "\n",
      "Our results also suggest a higher IR luminosity density at z ~ 2 than previously measured, corresponding to a ~ 30% increase in the SFR density. 0 into PostgreSQL...\n",
      "Inserting test sample 2762  The MOSDEF survey is a multi-wavelength study of the distant universe that explores the properties of galaxies at high redshift. In this paper, we investigate the metallicity dependence of the polycyclic aromatic hydrocarbon (PAH) emission, which is commonly used as a tracer of star formation activity. We present a sample of galaxies at z~2 with spectroscopic measurements of both the PAH emission and the gas-phase metallicity obtained via rest-frame optical emission lines. Our analysis shows a clear correlation between metallicity and PAH luminosity, with high-metallicity galaxies exhibiting lower PAH equivalent widths for a given infrared luminosity. We further demonstrate that this trend has important implications for the use of the 24 micron-inferred infrared luminosities and star formation rates (SFRs) as proxies for the true values at high redshift.\n",
      "\n",
      "Specifically, we find that the use of the 24 micron flux to estimate the total infrared luminosity systematically overestimates the SFR for high-metallicity galaxies. The bias is significant and increases with metallicity, with a median correction factor of ~0.4 dex at 12+log(O/H)=8.5. We propose a metallicity-dependent calibration to correct for this effect and provide a simple formula to estimate the \"corrected\" SFR from the 24 micron flux density. Our results have important implications for the interpretation of observations of high-redshift galaxies, as they suggest that the commonly used 24 micron flux density is not a reliable tracer of ongoing star formation activity in high-metallicity systems. We discuss the possible physical mechanisms that could underlie the observed trend and explore its potential impact on our understanding of the star formation history of the universe. Overall, our findings highlight the complex interplay between metallicity, dust, and star formation activity in galaxies at high redshift. 1 into PostgreSQL...\n",
      "Inserting test sample 2763  In this paper we investigate the problem of building a static data structure that represents a string s using space close to its compressed size, and allows fast access to individual characters of s. This type of structures was investigated by the recent paper of Bille et al. Let n be the size of a context-free grammar that derives a unique string s of length L. (Note that L might be exponential in n.) Bille et al. showed a data structure that uses space O(n) and allows to query for the i-th character of s using running time O(log L). Their data structure works on a word RAM with a word size of logL bits. Here we prove that for such data structures, if the space is poly(n), then the query time must be at least (log L)^{1-\\epsilon}/log S where S is the space used, for any constant eps>0. As a function of n, our lower bound is \\Omega(n^{1/2-\\epsilon}). Our proof holds in the cell-probe model with a word size of log L bits, so in particular it holds in the word RAM model. We show that no lower bound significantly better than n^{1/2-\\epsilon} can be achieved in the cell-probe model, since there is a data structure in the cell-probe model that uses O(n) space and achieves O(\\sqrt{n log n}) query time. The \"bad\" setting of parameters occurs roughly when L=2^{\\sqrt{n}}. We also prove a lower bound for the case of not-as-compressible strings, where, say, L=n^{1+\\epsilon}. For this case, we prove that if the space is n polylog(n), then the query time must be at least \\Omega(log n/loglog n).\n",
      "\n",
      "The proof works by reduction to communication complexity, namely to the LSD problem, recently employed by Patrascu and others. We prove lower bounds also for the case of LZ-compression and Burrows-Wheeler (BWT) compression. All of our lower bounds hold even when the strings are over an alphabet of size 2 and hold even for randomized data structures with 2-sided error. 0 into PostgreSQL...\n",
      "Inserting test sample 2764  In recent years, the increasing use of compressed representations of data structures has led to several studies of the lower bounds on access to these representations. One such area of interest is the study of lower bounds on random access to grammar-compressed strings, which have proven to have important applications in various fields of computer science including information retrieval, data compression, and computational biology.\n",
      "\n",
      "In this paper, we investigate the lower bounds on random access to grammar-compressed strings using information theoretic techniques and communication complexity. We show that any data structure providing random access to any position of a grammar-compressed string must have space complexity of at least n log(n) bits, where n is the length of the string, regardless of the underlying grammar. This result implies that it is impossible to compress the information contained in grammar-compressed strings without loss of access efficiency.\n",
      "\n",
      "We demonstrate the applicability of our lower bounds to compressed pattern matching, a fundamental problem in computer science. We show that any random access algorithm for pattern matching over grammar-compressed text, regardless of the underlying grammar, must have query time of at least log(n) bits per query. This implies that the space-time tradeoff for compressed pattern matching is inherently limited by the space-time tradeoff for grammar compression.\n",
      "\n",
      "Our results have significant implications for the design of data structures for compressed text, and imply that further research is needed to provide compressed data structures with more efficient access to compressed text. We conclude by outlining some open problems and directions for future work, and emphasize the importance of language and communication complexity techniques in the analysis of compressed data structures. 1 into PostgreSQL...\n",
      "Inserting test sample 2765  We prove that the periodic quantum Toda lattice corresponding to any extended Dynkin diagram is completely integrable. This has been conjectured and proved in all classical cases and $E_6$ by Goodman and Wallach at the beginning of the 1980's. As a direct application, in the context of quantum cohomology of affine flag manifolds, results that were known to hold only for some particular Lie types can now be extended to all types. 0 into PostgreSQL...\n",
      "Inserting test sample 2766  We investigate the complete integrability of the periodic quantum Toda lattice. Our study focuses on understanding the behavior of its quantum states and deriving the algebraic relations that govern its dynamics. Using symplectic geometry, we prove that the periodic Toda lattice is completely integrable. Our results provide valuable insights into the fundamental properties of this model and have potential applications in various fields of theoretical physics and engineering. 1 into PostgreSQL...\n",
      "Inserting test sample 2767  To reveal the detail of the internal structure, the relationship between chromospheric activity and the Rossby number, N_R (= rotational period P / convective turnover time tau_c), has been extensively examined for main-sequence stars. The goal of our work is to apply the same methods to pre-main-sequence (PMS) stars and identify the appropriate model of tau_c for them. Yamashita et al. (2020) investigated the relationship between N_R and strengths of the Ca II infrared triplet (IRT; lambda 8498, 8542, 8662 A) emission lines of 60 PMS stars. Their equivalent widths are converted into the emission line to stellar bolometric luminosity ratio (R'). The 54 PMS stars have N_R < 10^{-1.0} and show R' \\sim 10^{-4.2} as large as the maximum R' of the zero-age main-sequence (ZAMS) stars. However, because all R' was saturated against N_R, it was not possible to estimate the appropriate tau_c model for the PMS stars. We noticed that Mg I emission lines at 8808 A is an optically thin chromospheric line, appropriate for determination of the adequate tau_c for PMS stars. Using the archive data of the Anglo-Australian Telescope (AAT)/the University College London Echelle Spectrograph (UCLES), we investigated the Mg I line of 52 ZAMS stars. After subtracting photospheric absorption component, the Mg I line is detected as an emission line in 45 ZAMS stars, whose R' is between 10^{-5.9} and 10^{-4.1}. The Mg I line is not saturated yet in \"the saturated regime for the Ca II emission lines\", i.e.\n",
      "\n",
      "10^{-1.6} < N_R < 10^{-0.8}. Therefore, the adequate tau_c for PMS stars can be determined by measuring of their R' values. 0 into PostgreSQL...\n",
      "Inserting test sample 2768  The rotational activity of young stars is closely related to the emission of infrared radiation from certain elements, such as calcium and magnesium. In this study, we investigate the relationship between the Ca II and Mg I infrared emission lines and the rotational activity of young stars. Our analysis is based on spectral observations obtained from the Apache Point Observatory Galactic Evolution Experiment (APOGEE).\n",
      "\n",
      "We find a clear correlation between the equivalent width of the Ca II and Mg I emission lines and the rotation period of the stars. Specifically, we observe a decrease in the equivalent width of both emission lines as the rotation period increases. This correlation can be explained by the decrease in the filling factor of the emission region as the stars spin more slowly. Furthermore, we find that the correlation between rotational activity and emission is stronger for stars with higher masses.\n",
      "\n",
      "Our results have important implications for the study of young stars and their evolution. The relationship between rotational activity and emission lines can be used to estimate the rotation period of stars and their ages. Additionally, our results suggest that the emission of infrared radiation from elements such as Ca II and Mg I could be used as a proxy for the rotational activity of young stars, providing a valuable diagnostic tool for future studies.\n",
      "\n",
      "In conclusion, our analysis of the rotation-activity relation of Ca II and Mg I infrared emission lines of young stars reveals a clear correlation between the equivalent width of these lines and the rotational activity of the stars. These findings shed light on the connection between rotation and emission in young stars and provide insights into their evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 2769  {\\bf Background:} Using the chiral (Kyushu) $g$-matrix folding model with the densities calculated with GHFB+AMP, we determined $r_{\\rm skin}^{208}=0.25$fm from the central values of $\\sigma_{\\rm R}$ of p+$^{208}$Pb scattering in $E_{\\rm in}=40-81$MeV. The high-resolution $E1$ polarizability experiment ($E1$pE) yields $r_{\\rm skin}^{48}(E1{\\rm pE}) =0.14-0.20$fm. The data on $\\sigma_{\\rm R}$ are available as a function of $E_{\\rm in}$ for $p$+$^{48}$Ca scattering. {\\bf Aim:} Our aim is to determine $r_{\\rm skin}^{48}$ from the central values of $\\sigma_{\\rm R}$ for $p$+$^{48}$Ca scattering by using the folding model. {\\bf Results:} As for $^{48}$Ca, we determine $r_n(E1{\\rm pE})=3.56$fm from the central value 0.17fm of $r_{\\rm skin}^{48}(E1{\\rm pE})$ and $r_p({\\rm EXP})=3.385$fm of electron scattering, and evaluate $r_m(E1{\\rm pE})=3.485$fm from the $r_n(E1{\\rm pE})$ and the $r_p({\\rm EXP})$ of electron scattering. The folding model with GHFB+AMP densities reproduces $\\sigma_{\\rm R}$ in $23 \\leq E_{\\rm in} \\leq 25.3$ MeV in one-$\\sigma$ level, but slightly overestimates the central values of $\\sigma_{\\rm R}$ there. In $23 \\leq E_{\\rm in} \\leq 25.3$MeV, the small deviation allows us to scale the GHFB+AMP densities to the central values of $r_p({\\rm EXP})$ and $r_n(E1{\\rm pE})$. The $\\sigma_{\\rm R}(E1{\\rm pE})$ obtained with the scaled densities almost reproduce the central values of $\\sigma_{\\rm R}$ when $E_{\\rm in}=23-25.3$MeV, so that the $\\sigma_{\\rm R}({\\rm GHFB+AMP})$ and the $\\sigma_{\\rm R}(E1{\\rm pE})$ are in 1-$\\sigma$ of $\\sigma_{\\rm R}$ there. In $E_{\\rm in}=23-25.3$MeV, we determine the $r_{m}({\\rm EXP})$ from the central values of $\\sigma_{\\rm R}$ and take the average for the $r_{m}({\\rm EXP})$. The averaged value is $r_{m}({\\rm EXP})=3.471$fm. Eventually, we obtain $r_{\\rm skin}^{48}({\\rm EXP})=0.146$fm from $r_{m}({\\rm EXP})=3.471$fm and $r_p({\\rm EXP})=3.385$fm. 0 into PostgreSQL...\n",
      "Inserting test sample 2770  The neutron skin thickness of an atomic nucleus refers to the difference between the radii of the neutron and proton distribution in the nucleus. This skin thickness is a vital quantity needed to understand the nuclear symmetry energy, nuclear equation-of-state, and the neutron star structure. A new methodology has been proposed where neutron skins are obtained by measuring the reaction cross-section of proton-nucleus elastic scattering at proton energies between 0.5 and 2.5 GeV. In the present work, the neutron skin thickness of $^{48}$Ca is measured at the RIKEN Radioactive Isotope Beam Factory (RIBF) through proton elastic scattering using an isotope-separated beam of $^{48}$Ca. The technique involves measuring the cross-section for proton scattering at very small angles, below 2 degrees. The thickness of the neutron skin of $^{48}$Ca was deduced from the slope of the electric form factor at zero momentum transfer.\n",
      "\n",
      "Using the above methodology, they determined the neutron skin thickness of $^{48}$Ca to be $r_{\\rm skin}^{48}=0.161^{+0.024}_{-0.027}\\textnormal{(stat.)}\\pm0.005\\textnormal{(syst.)}$ fm. The statistical uncertainties are due to the uncertainties in the measured cross-sections, whereas the systematic uncertainty in the neutron skin thickness is dominated by the radii of protons and neutrons Interaction Densities used in the theoretical calculation.\n",
      "\n",
      "In summary, this work corrected previous measurements and utilized isotope-separated beams to measure the reaction cross-section of proton-nucleus elastic scattering to determine the neutron skin thickness of $^{48}$Ca. The final values agreed well with other measurements done on different nuclei, including mass A=48 and odd-even nuclei. This work provides essential input to the study of properties of neutron-rich nuclei, nuclear symmetry energy, and neutron stars. 1 into PostgreSQL...\n",
      "Inserting test sample 2771  Let $M^n$ be a closed Riemannian manifold. Larry Guth proved that there exists $c(n)$ with the following property: if for some $r>0$ the volume of each metric ball of radius $r$ is less than $({r\\over c(n)})^n$, then there exists a continuous map from $M^n$ to a $(n-1)$-dimensional simplicial complex such that the inverse image of each point can be covered by a metric ball of radius $r$ in $M^n$. It was previously proven by Gromov that this result implies two by now famous Gromov's inequalities: $Fill Rad(M^n)\\leq c(n)vol(M^n)^{1\\over n}$ and, if $M^n$ is essential, then also $sys_1(M^n)\\leq 6c(n)vol(M^n)^{1\\over n}$ with the same constant $c(n)$. Here $sys_1(M^n)$ denotes the length of a shortest non-contractible closed curve in $M^n$.\n",
      "\n",
      "We prove that these results hold with $c(n)=({n!\\over 2})^{1\\over n}\\leq {n\\over 2}$. We demonstrate that for essential Riemannian manifolds $sys_1(M^n) \\leq n\\ vol^{1\\over n}(M^n)$. All previously known upper bounds for $c(n)$ were exponential in $n$.\n",
      "\n",
      "Moreover, we present a qualitative improvement: In Guth's theorem the assumption that the volume of every metric ball of radius $r$ is less than $({r\\over c(n)})^n$ can be replaced by a weaker assumption that for every point $x\\in M^n$ there exists a positive $\\rho(x)\\leq r$ such that the volume of the metric ball of radius $\\rho(x)$ centered at $x$ is less than $({\\rho(x)\\over c(n)})^n$ (for $c(n)=({n!\\over 2})^{1\\over n}$).\n",
      "\n",
      "Also, if $X$ is a boundedly compact metric space such that for some $r>0$ and an integer $n\\geq 1$ the $n$-dimensional Hausdorff content of each metric ball of radius $r$ in $X$ is less than $({r\\over 4n})^n$, then there exists a continuous map from $X$ to a $(n-1)$-dimensional simplicial complex such that the inverse image of each point can be covered by a metric ball of radius $r$. 0 into PostgreSQL...\n",
      "Inserting test sample 2772  This paper establishes linear bounds for constants in Gromov's systolic inequality and related results. Specifically, we investigate closed Riemannian manifolds of dimension greater than five and obtain new lower bounds for the systolic volume and the first non-zero Steklov eigenvalue in terms of the dimension, the maximum sectional curvature, and the diameter. Our main result is a linear upper bound for the systolic volume constant, namely, we obtain a bound in terms of the dimension and maximum sectional curvature that improves the previously known exponential upper bound. This has important implications in geometric group theory and topology, as it sheds new light on the isoperimetric and growth functions of groups. We also investigate related results such as the systolic ratio and the equivariant Gromov systolic inequality. These results rely heavily on geometric and analytic techniques such as Morse theory, spectral theory, and comparison geometry. Our proofs are based on a combination of geometric inequalities, spectral estimates, and geometric analysis. We provide detailed calculations and intuitive explanations to facilitate the understanding of the technical details. Our results have potential applications in fields such as robotics, mathematical physics, and computer science, where understanding the shape and topology of high-dimensional spaces is essential. We conclude by pointing out several open problems and directions for future research. In summary, this paper provides valuable insights into the quantitative aspects of systolic geometry and related subjects, and opens up new avenues for investigation and discovery in these fields. 1 into PostgreSQL...\n",
      "Inserting test sample 2773  HH 111 is a Class I protostellar system at a distance of ~ 400 pc, with the central source VLA 1 associated with a rotating disk deeply embedded in a flattened envelope. Here we present the observations of this system at ~ 0.6\" (240 AU) resolution in C18O (J=2-1) and 230 GHz continuum obtained with Atacama Large Millimeter/Submillimeter Array, and in SO obtained with Submillimeter Array. The observations show for the first time how a Keplerian rotating disk can be formed inside a flattened envelope. The flattened envelope is detected in C18O, extending out to >~ 2400 AU from the VLA 1 source. It has a differential rotation, with the outer part (>~ 2000 AU) better described by a rotation that has constant specific angular momentum and the innermost part (<~ 160 AU) by a Keplerian rotation. The rotationally supported disk is therefore relatively compact in this system, which is consistent with the dust continuum observations. Most interestingly, if the flow is in steady state, there is a substantial drop in specific angular momentum in the envelope-disk transition region from 2000 AU to 160 AU, by a factor of ~ 3. Such a decrease is not expected outside a disk formed from simple hydrodynamic core collapse, but can happen naturally if the core is significantly magnetized, because magnetic fields can be trapped in the transition region outside the disk by the ram pressure of the protostellar accretion flow, which can lead to efficient magnetic braking. In addition, SO shock emission is detected around the outer radius of the disk and could trace an accretion shock around the disk. 0 into PostgreSQL...\n",
      "Inserting test sample 2774  The envelope-disk transition region in protostellar systems plays a crucial role in regulating the accretion and angular momentum evolution of the system. We investigate the angular momentum loss mechanism in the transition region of the HH 111 protostellar system. Our observations reveal the presence of a rotating disk in the inner region, and an outflowing envelope in the outer region of the system.\n",
      "\n",
      "We find that the total angular momentum in the HH 111 system is not conserved and suggest that angular momentum loss is taking place in the envelope-disk transition region. One possibility for this loss is magnetic braking, which occurs due to the interaction between the disk and the magnetic field of the system. We propose that the magnetic field lines may be anchored in the disk, enabling the transfer of angular momentum from the disk to the envelope.\n",
      "\n",
      "Our findings also suggest that the magnetic field in the transition region is likely to be complex due to the presence of multiple sources of magnetic fields. We discuss the implications of our results for theories of disk formation and evolution, and for understanding the role of magnetic fields in the formation of protostellar systems.\n",
      "\n",
      "Overall, our observations provide new insights into the dynamical processes occurring in the envelope-disk transition region of protostellar systems. Our findings support the hypothesis of magnetic braking as a possible mechanism for angular momentum loss, and highlight the importance of further investigation into the role of magnetic fields in star formation. 1 into PostgreSQL...\n",
      "Inserting test sample 2775  We extend unified correspondence theory to Kripke frames with impossible worlds and their associated regular modal logics. These are logics the modal connectives of which are not required to be normal: only the weaker properties of additivity and multiplicativity are required. Conceptually, it has been argued that their lacking necessitation makes regular modal logics better suited than normal modal logics at the formalization of epistemic and deontic settings. From a technical viewpoint, regularity proves to be very natural and adequate for the treatment of algebraic canonicity J\\'onsson-style. Indeed, additivity and multiplicativity turn out to be key to extend J\\'onsson's original proof of canonicity to the full Sahlqvist class of certain regular distributive modal logics naturally generalizing Distributive Modal Logic. Most interestingly, additivity and multiplicativity are key to J\\'onsson-style canonicity also in the original (i.e. normal) DML. Our contributions include: the definition of Sahlqvist inequalities for regular modal logics on a distributive lattice propositional base; the proof of their canonicity following J\\'onsson's strategy; the adaptation of the algorithm ALBA to the setting of regular modal logics on two non-classical (distributive lattice and intuitionistic) bases; the proof that the adapted ALBA is guaranteed to succeed on a syntactically defined class which properly includes the Sahlqvist one; finally, the application of the previous results so as to obtain proofs, alternative to Kripke's, of the strong completeness of Lemmon's epistemic logics E2-E5 with respect to elementary classes of Kripke frames with impossible worlds. 0 into PostgreSQL...\n",
      "Inserting test sample 2776  Sahlqvist theory has proven to be an invaluable tool in modal logic, providing a means of expressing complex properties and relationships between objects and worlds. However, limitations arise when attempting to apply this theory to impossible worlds, where truth values are ambiguous or undefined. \n",
      "\n",
      "In this paper, we explore the extension of Sahlqvist theory to impossible worlds, providing a framework for analyzing and describing properties of impossible objects. Through a series of logical proofs and examples, we demonstrate the applicability and benefits of this approach to the field of modal logic. \n",
      "\n",
      "We begin by presenting a formal definition of impossible worlds and their properties, drawing upon existing theories in the field. We provide a discussion of the limitations of traditional Sahlqvist theory when applied to impossible worlds, highlighting the need for an expanded approach. \n",
      "\n",
      "We then introduce the concept of \"impossible Sahlqvist formulas\", which allow us to reason about impossible worlds using the Sahlqvist framework. We provide a range of examples which illustrate the usefulness of this approach, including the analysis of impossible objects such as square circles and married bachelors. \n",
      "\n",
      "Our results demonstrate the applicability of Sahlqvist theory to broader conceptual domains, opening up new avenues of research in modal logic and related fields. We conclude with a brief discussion of potential applications and future directions in this area of study, highlighting the importance of continued research and exploration. 1 into PostgreSQL...\n",
      "Inserting test sample 2777  A special surface discharge is proposed based on the multi-anode electrode geometry. Instead of the traditional surface flashover of creepage on the insulator surface between the electrodes, a surface discharge with one of the electrodes being placed far away from the insulator is achieved in this paper.\n",
      "\n",
      "The unique electric field distribution due to the multi-anode electrode geometry has a significant influence on the discharge process of the surface discharge. It changes the generation and propagation process of the plasma, forming a plasma plume contributes to the propulsion performance of the thruster. Through theoretical analysis of the obtained plume data, it is indicated that the ablative pulsed plasma thruster based on multi-anode electrode geometry (short for multi-anode APPT) promotes the internal pressure of the plasma jet during its propagation and significantly increases the density and energy of charged particles. The discharge phenomena manifest that the multi-anode APPT and the helix-coil multi-anode APPT effectively increase the intensity of the plasma plume. Through electron density spatial distribution measurement, it has been found that the helix-coil multi-anode APPT increases the density of plasma in the axial direction to more than 4 times of the conventional coaxial APPT and reduces the electron density in other directions. In the propulsion test, it has been demonstrated that the multi-anode APPT and the helix-coil multi-anode APPT have better performance in terms of the impulse bit and the thrust-to-power ratio. In addition, it is also identified that the pinch effect will be enhanced with the increase of discharge power and the propulsion performance is promoted more distinctly. The multi-anode APPT and the helixcoil multi-anode APPT have been proved to have potential application advantage in the field of micro-satellite propulsion. 0 into PostgreSQL...\n",
      "Inserting test sample 2778  This paper presents a study on the optimization of the plasma plume characteristics of a multi-anode coaxial ablation pulsed plasma thruster (MAPPT). The aim is to enhance the thrust efficiency while minimizing power consumption, and examining how the plume characteristics affect the thruster's performance. \n",
      "\n",
      "The thruster comprises of multiple electrodes arranged in a coaxial configuration, each of which is made up of an ablative material. A pulsed electrical discharge applied to the electrodes causes the ablative material to vaporize, producing a high-velocity plasma plume that generates thrust. The plasma plume's characteristics, such as the velocity, temperature, and density, strongly affect the thruster's performance.\n",
      "\n",
      "To optimize the plasma plume characteristics of the MAPPT, numerical simulations were performed using ANSYS Fluent software. The simulations analyzed the effects of various parameters, including the discharge voltage, discharge frequency, and electrode geometry, on the plasma plume's characteristics and the thruster's performance.\n",
      "\n",
      "The simulation results indicate that by varying the electrical discharge parameters, the plasma plume characteristics can be optimized to maximize the specific impulse while reducing the power consumption. Additionally, the electrode geometry was found to affect the plasma plume's velocity and temperature distribution significantly.\n",
      "\n",
      "Experimental tests were also conducted to validate the results obtained from the numerical simulations. A MAPPT prototype was designed, built, and tested under various operating conditions. The experimental results show a good agreement with the simulation results, confirming the effectiveness of the optimization strategy proposed.\n",
      "\n",
      "The findings of this study could benefit the development of more efficient and reliable plasma thrusters for space applications, such as orbit raising, maintaining the attitude of the spacecraft, and interplanetary missions. 1 into PostgreSQL...\n",
      "Inserting test sample 2779  We investigate the use of data-driven likelihoods to bypass a key assumption made in many scientific analyses, which is that the true likelihood of the data is Gaussian. In particular, we suggest using the optimization targets of flow-based generative models, a class of models that can capture complex distributions by transforming a simple base distribution through layers of nonlinearities. We call these flow-based likelihoods (FBL). We analyze the accuracy and precision of the reconstructed likelihoods on mock Gaussian data, and show that simply gauging the quality of samples drawn from the trained model is not a sufficient indicator that the true likelihood has been learned.\n",
      "\n",
      "We nevertheless demonstrate that the likelihood can be reconstructed to a precision equal to that of sampling error due to a finite sample size. We then apply FBLs to mock weak lensing convergence power spectra, a cosmological observable that is significantly non-Gaussian (NG). We find that the FBL captures the NG signatures in the data extremely well, while other commonly used data-driven likelihoods, such as Gaussian mixture models and independent component analysis, fail to do so. This suggests that works that have found small posterior shifts in NG data with data-driven likelihoods such as these could be underestimating the impact of non-Gaussianity in parameter constraints. By introducing a suite of tests that can capture different levels of NG in the data, we show that the success or failure of traditional data-driven likelihoods can be tied back to the structure of the NG in the data. Unlike other methods, the flexibility of the FBL makes it successful at tackling different types of NG simultaneously. Because of this, and consequently their likely applicability across datasets and domains, we encourage their use for inference when sufficient mock data are available for training. 0 into PostgreSQL...\n",
      "Inserting test sample 2780  In many fields of science and engineering, it is crucial to perform inference in situations where the underlying probability distribution is non-Gaussian. While there is a vast literature on methods for Gaussian inference, the non-Gaussian case is much less well understood. In this paper, we develop a new approach to non-Gaussian inference based on flow-based likelihoods.\n",
      "\n",
      "The key idea behind our approach is to use normalizing flows, a flexible class of transformations, to map a simple standard Gaussian distribution onto the target non-Gaussian distribution. Once this mapping has been computed, we can compute likelihoods for new observations by simply transforming their input features through the inverse of the flow. This approach is computationally efficient, easy to implement, and can be applied to a wide range of non-Gaussian distributions.\n",
      "\n",
      "To demonstrate the effectiveness of our approach, we apply it to several standard benchmark datasets in both regression and classification settings. We show that our method outperforms several state-of-the-art methods on several of these datasets. Moreover, we also show that our approach exhibits good calibration properties, which is particularly important in many real-world applications.\n",
      "\n",
      "Overall, our paper provides a powerful new approach to non-Gaussian inference based on flow-based likelihoods. We believe that our method will find broad application in many fields of science and engineering where non-Gaussian inference is required. 1 into PostgreSQL...\n",
      "Inserting test sample 2781  In last passage percolation models lying in the Kardar-Parisi-Zhang universality class, maximizing paths that travel over distances of order $n$ accrue energy that fluctuates on scale $n^{1/3}$; and these paths deviate from the linear interpolation of their endpoints on scale $n^{2/3}$. These maximizing paths and their energies may be viewed via a coordinate system that respects these scalings. What emerges by doing so is a system indexed by $x,y \\in \\mathbb{R}$ and $s,t \\in \\mathbb{R}$ with $s < t$ of unit order quantities $W_n\\big( x,s ; y,t \\big)$ specifying the scaled energy of the maximizing path that moves in scaled coordinates between $(x,s)$ and $(y,t)$. The space-time Airy sheet is, after a parabolic adjustment, the putative distributional limit $W_\\infty$ of this system as $n \\to \\infty$. The Airy sheet has recently been constructed in [15] as such a limit of Brownian last passage percolation. In this article, we initiate the study of fractal geometry in the Airy sheet. We prove that the scaled energy difference profile given by $\\mathbb{R} \\to \\mathbb{R}: z \\to W_\\infty \\big( 1,0 ; z,1 \\big) - W_\\infty \\big( -1,0 ; z,1 \\big)$ is a non-decreasing process that is constant in a random neighbourhood of almost every $z \\in \\mathbb{R}$; and that the exceptional set of $z \\in \\mathbb{R}$ that violate this condition almost surely has Hausdorff dimension one-half. Points of violation correspond to special behaviour for scaled maximizing paths, and we prove the result by investigating this behaviour, making use of two inputs from recent studies of scaled Brownian LPP; namely, Brownian regularity of profiles, and estimates on the rarity of pairs of disjoint scaled maximizing paths that begin and end close to each other. 0 into PostgreSQL...\n",
      "Inserting test sample 2782  Fractal geometry is a fundamental concept in the theory of stochastic processes. In this paper, we investigate the fractal properties of Airy_2 processes coupled via the Airy sheet. The Airy_2 process is a stationary random process that occurs in the study of random matrix theory and has been found to exhibit fractal behavior. The Airy sheet is a two-dimensional random surface with the same covariance structure as the Airy_2 process.\n",
      "\n",
      "We show that the coupling of Airy_2 processes via the Airy sheet leads to a new fractal structure that is a generalization of the individual fractal properties of the processes. Using numerical simulations, we demonstrate that the scaling properties of the coupled system are consistent with those predicted by the theory of stochastic processes with long-range dependencies.\n",
      "\n",
      "Furthermore, we investigate the multifractal properties of the coupled system by analyzing its singularity spectrum. We show that the presence of the Airy sheet induces a crossover in the multifractal behavior of the system. Our results suggest that the coupling of Airy_2 processes via the Airy sheet leads to a novel fractal structure with rich geometric properties.\n",
      "\n",
      "Finally, we discuss the potential application of our findings in physics, engineering, and finance. the fractal structure of the Airy_2 processes coupled via the Airy sheet may provide a useful framework for modeling complex systems with long-range dependencies. In conclusion, our study provides new insights into the fractal geometry of stochastic processes and suggests new avenues for future research in this field. 1 into PostgreSQL...\n",
      "Inserting test sample 2783  We develop the method for the calculation of the total reaction cross sections induced by the halo nuclei and stable nuclei. This approach is based on the Glauber theory, which is valid for nuclear reactions at high energy. It is extended for nuclear reactions at low energy and intermediate energy by including both the quantum correction and Coulomb correction under the assumption of the effective nuclear density distribution. The calculated results of the total reaction cross section induced by stable nuclei agree well with the 30 experimental data within 10 percent accuracy.The comparison between the numerical results and the 20 experimental data for the total nuclear reaction cross section induced by the neutron halo nuclei and the proton halo nuclei indicates a satisfactory agreement after considering the halo structure of these nuclei, which implies the quite different mean fields for the nuclear reactions induced by halo nuclei and stable nuclei. The halo nucleon distributions and the root mean square radii of these nuclei can be extracted from above comparison based on the improved Glauber model, which indicate clearly the halo structures of these nuclei. Especially, it is clear to see that the medium correction of the nucleon-nucleon collision has little effect on the total reaction cross sections induced by the halo nuclei due to the very weak binding and the very extended density distribution. 0 into PostgreSQL...\n",
      "Inserting test sample 2784  This study aims to investigate the total nuclear reaction cross-sections induced by halo nuclei and stable nuclei. The interaction of these nuclei at different energies was analyzed, and a comparison was drawn between their behaviors. The experiment was performed using a tandem accelerator, and the particle identification was carried out by a time-of-flight method.\n",
      "\n",
      "The experimental results revealed that the total nuclear reaction cross-sections of halo nuclei were significantly larger than that of stable nuclei. Moreover, it was observed that the cross-sections increased with the decrease of the incident energy of the projectiles. The contribution of the elastic scattering and the reaction channels were also explored, which helped in understanding the mechanism of the nuclear reactions.\n",
      "\n",
      "The present study brings forward important insights into the behavior of halo nuclei and stable nuclei induced nuclear reactions. The findings suggest that the halo nuclei could play a significant role in astrophysical processes such as the nucleosynthesis of light elements. The results of this research have significant implications for the advancement of nuclear physics and are expected to contribute towards the development of accurate models of nuclear reactions. 1 into PostgreSQL...\n",
      "Inserting test sample 2785  An often-overlooked characteristic of the human mind is its propensity to wander. Despite growing interest in the science of mind-wandering, most studies operationalize mind-wandering by its task-unrelated contents, which may be orthogonal to the processes constraining how thoughts are evoked and unfold over time. In this chapter, we emphasize the importance of incorporating such processes into current definitions of mind-wandering, and proposing that mind-wandering and other forms of spontaneous thought (such as dreaming and creativity) are mental states that arise and transition relatively freely due to an absence of constraints on cognition. We review existing psychological, philosophical, and neuroscientific research on spontaneous thought through the lens of this framework, and call for additional research into the dynamic properties of the mind and brain. 0 into PostgreSQL...\n",
      "Inserting test sample 2786  The study of spontaneous thought, or the stream of thoughts that occur without external stimulation, is an evolving, interdisciplinary field in neuroscience. Recent advances in neuroimaging techniques have allowed for a deeper understanding of the neural correlates underlying spontaneous thinking. The default mode network, a group of interconnected brain regions, is thought to play a crucial role in the generation of spontaneous thought. Additionally, cognitive and psychological factors, such as creativity and mind-wandering, have been found to influence spontaneous thinking. Understanding the neural and cognitive mechanisms underlying spontaneous thought has implications for our understanding of brain function in everyday life, as well as potential clinical applications in disorders such as depression and anxiety. This paper reviews recent developments in the neuroscience of spontaneous thought, highlighting its interdisciplinary nature and potential for future research directions. 1 into PostgreSQL...\n",
      "Inserting test sample 2787  Despite over 50 years of Gamma-Ray Burst (GRB) observations many open questions remain about their nature and the environments in which the emission takes place. Polarization measurements of the GRB prompt emission have long been theorized to be able to answer most of these questions. The POLAR detector was a dedicated GRB polarimeter developed by a Swiss, Chinese and Polish collaboration. The instrument was launched, together with the second Chinese Space Lab, the Tiangong-2, in September 2016 after which it took 6 months of scientific data. During this period POLAR detected 55 GRBs as well as several pulsars. From the analysis of the GRB polarization catalog we see that the prompt emission is lowly polarized or fully unpolarized. There is, however, the caveat that within single pulses there are strong hints of an evolving polarization angle which washes out the polarization degree in the time integrated analysis. Building on the success of the POLAR mission, the POLAR-2 instrument is currently under development. POLAR-2 is a Swiss, Chinese, Polish and German collaboration and was recently approved for launch in 2024. Thanks to its large sensitivity POLAR-2 will produce polarization measurements of at least 50 GRBs per year with a precision equal or higher than the best results published by POLAR. POLAR-2 thereby aims to make the prompt polarization a standard observable and produce catalogs of the gamma-ray polarization of GRBs.\n",
      "\n",
      "Here we will present an overview of the POLAR mission and all its scientific measurement results. Additionally, we will present an overview of the future POLAR-2 mission, and how it will answer some of the questions raised by the POLAR results. 0 into PostgreSQL...\n",
      "Inserting test sample 2788  The POLAR mission is a space-based observatory dedicated to detecting and measuring high-energy gamma rays and their polarization. The mission has recently released its latest results, which have significantly advanced our understanding of this elusive property of gamma rays. In particular, POLAR has provided key insights into the polarization of gamma-ray bursts, the most energetic explosions in the universe. \n",
      "\n",
      "The results show that the polarization fraction of gamma-ray bursts is generally low, indicating that the emission is likely produced in a highly turbulent and disordered magnetic field environment. However, the polarimetric data also reveal interesting features that cannot be explained by simple models and may point towards more complex physics at play. Future observations with POLAR and other gamma-ray telescopes will be crucial for disentangling the various mechanisms contributing to gamma-ray burst emission and polarization. \n",
      "\n",
      "Moreover, the POLAR mission is expected to make significant contributions to other important astrophysical phenomena, such as active galactic nuclei, pulsars, and gamma-ray binaries. The combination of high sensitivity and polarization capabilities makes POLAR a unique tool for probing the most extreme and energetic objects in the universe. The upcoming launch of POLAR-2, a new and improved version of the observatory, will further enhance its capabilities and enable even more exciting discoveries. \n",
      "\n",
      "Overall, the gamma-ray polarization results obtained by the POLAR mission highlight the importance of studying this property in order to shed light on the physics of the most extreme and violent processes in the universe. The future prospects for such investigations look promising, with continued advancements in technology and the upcoming launch of new observatories like POLAR-2. 1 into PostgreSQL...\n",
      "Inserting test sample 2789  Cycle expansions are an efficient scheme for computing the properties of chaotic systems. When enumerating the orbits for a cycle expansion not all orbits that one would expect at first are present --- some are pruned. This pruning leads to convergence difficulties when computing properties of chaotic systems. In numerical schemes, I show that pruning reduces the number of reliable eigenvalues when diagonalizing quantum mechanical operators, and that pruning slows down the convergence rate of cycle expansion calculations. I then exactly solve a diffusion model that displays chaos and show that its cycle expansion develops a branch point. 0 into PostgreSQL...\n",
      "Inserting test sample 2790  The study deals with cycle expansions that employ pruned orbits in order to obtain more efficient and accurate computations. The research finds that these types of expansions can exhibit branch points, which can have a significant impact on the numerical results. The investigation explores the mathematical properties of such branch points, their location and classification, as well as the consequences of their appearance. Through various numerical experiments and simulations, the paper demonstrates the effectiveness and limitations of the proposed methodology, highlighting the importance of considering the presence of branch points in cycle expansions with pruned orbits. 1 into PostgreSQL...\n",
      "Inserting test sample 2791  The Gaia satellite is a high-precision astrometry, photometry and spectroscopic ESA cornerstone mission, currently scheduled for launch in 2012.\n",
      "\n",
      "Its primary science drivers are the composition, formation and evolution of the Galaxy. Gaia will achieve its unprecedented accuracy requirements with detailed calibration and correction for CCD radiation damage and CCD geometric distortion. In this paper, the third of the series, we present our 3D Silvaco ATLAS model of the Gaia e2v CCD91-72 pixel. We publish e2v's design model predictions for the capacities of one of Gaia's pixel features, the supplementary buried channel (SBC), for the first time. Kohley et al. (2009) measured the SBC capacities of a Gaia CCD to be an order of magnitude smaller than e2v's design. We have found the SBC doping widths that yield these measured SBC capacities. The widths are systematically 2 {\\mu}m offset to the nominal widths. These offsets appear to be uncalibrated systematic offsets in e2v photolithography, which could either be due to systematic stitch alignment offsets or lateral ABD shield doping diffusion. The range of SBC capacities were used to derive the worst-case random stitch error between two pixel features within a stitch block to be \\pm 0.25 {\\mu}m, which cannot explain the systematic offsets. It is beyond the scope of our pixel model to provide the manufacturing reason for the range of SBC capacities, so it does not allow us to predict how representative the tested CCD is. This open question has implications for Gaia's radiation damage and geometric calibration models. 0 into PostgreSQL...\n",
      "Inserting test sample 2792  This research paper presents a Silvaco ATLAS model of the e2v CCD91-72 pixels utilized by the European Space Agency's (ESA) Gaia satellite. The Gaia satellite is a crucial astronomy mission that aims to create a comprehensive 3D map of our Milky Way Galaxy. The functioning of Gaia's Payload Module (PM) depends on the accurate behavior of its CCD pixels; as such, the CCD must be well-characterized using computer models. The Silvaco ATLAS software, a device simulation tool, is utilized to create this model using a comprehensive understanding of the CCD's operating principles. The resulting model provides increased insight into the pixel's performance, including their response to radiation-induced traps and the threshold voltage of each pixel. The ATLAS model also reveals that the pixel's sensitivity varies according to the level of impurities in the CCD material. Comparing the ATLAS model's results to those from laboratory tests validates our model's accuracy and demonstrates the necessity of calibration before launch. Our work has contributed to enhancing the performance of the e2v CCD91-72 pixels and providing more accurate measurements of astronomical objects. The resulting data from the Gaia satellite will be an invaluable resource for exploring our galaxy's structure and history, and revealing many secrets of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2793  We study the evolution of cooperation in the spatial public goods game, focusing on the effects that are brought about by the delayed distribution of goods that accumulate in groups due to the continuous investments of cooperators. We find that intermediate delays enhance network reciprocity because of a decelerated invasion of defectors, who are unable to reap the same high short-term benefits as they do in the absence of delayed distribution.\n",
      "\n",
      "Long delays, however, introduce a risk because the large accumulated wealth might fall into the wrong hands. Indeed, as soon as the curvature of a cooperative cluster turns negative, the engulfed defectors can collect the heritage of many generations of cooperators, and by doing so start a waning moon pattern that nullifies the benefits of decelerated invasion. Accidental meeting points of growing cooperative clusters may also act as triggers for the waning moon effect, thus linking the success of cooperators with their propensity to fail in a rather bizarre way. Our results highlight that \"investing into the future\" is a good idea only if that future is sufficiently near and not likely to be burdened by inflation. 0 into PostgreSQL...\n",
      "Inserting test sample 2794  In Public Goods Games (PGGs) with Delayed Distribution (DD), individuals' contributions towards a shared resource are constrained by their time preferences. In this paper, we analyze the effects of decelerated invasion and waning moon patterns on cooperation in PGGs with DD. We discover that cooperation potential is impacted by the timing of delayed incentives, and the likelihood of individuals contributing increases when incentives are closer in time. Moreover, we observe that decelerated invasion - a phenomenon in which the rate of contribution decrease is slower than the rate of increase - improves cooperation levels. Meanwhile, we find that waning moon patterns - where a decrease in contributions during the middle stages of the PGG occur - negatively affect overall cooperation. Furthermore, we introduce a new model of DD in PGGs that accounts for heterogeneous time preferences among individuals. Our findings have significant implications for organizations that rely on PGGs and for future studies exploring the behavioral motivations behind delayed gratification in social dilemma-type situations. 1 into PostgreSQL...\n",
      "Inserting test sample 2795  Prostate radiotherapy is a well established curative oncology modality, which in future will use Magnetic Resonance Imaging (MRI)-based radiotherapy for daily adaptive radiotherapy target definition. However the time needed to delineate the prostate from MRI data accurately is a time consuming process.\n",
      "\n",
      "Deep learning has been identified as a potential new technology for the delivery of precision radiotherapy in prostate cancer, where accurate prostate segmentation helps in cancer detection and therapy. However, the trained models can be limited in their application to clinical setting due to different acquisition protocols, limited publicly available datasets, where the size of the datasets are relatively small. Therefore, to explore the field of prostate segmentation and to discover a generalisable solution, we review the state-of-the-art deep learning algorithms in MR prostate segmentation; provide insights to the field by discussing their limitations and strengths; and propose an optimised 2D U-Net for MR prostate segmentation. We evaluate the performance on four publicly available datasets using Dice Similarity Coefficient (DSC) as performance metric. Our experiments include within dataset evaluation and cross-dataset evaluation. The best result is achieved by composite evaluation (DSC of 0.9427 on Decathlon test set) and the poorest result is achieved by cross-dataset evaluation (DSC of 0.5892, Prostate X training set, Promise 12 testing set). We outline the challenges and provide recommendations for future work. Our research provides a new perspective to MR prostate segmentation and more importantly, we provide standardised experiment settings for researchers to evaluate their algorithms. Our code is available at https://github.com/AIEMMU/MRI\\_Prostate. 0 into PostgreSQL...\n",
      "Inserting test sample 2796  Prostate segmentation in magnetic resonance imaging (MRI) has garnered considerable attention recently, owing to its potential to improve diagnosis and treatment of prostate cancer. Deep learning techniques have emerged as a promising solution to the inherent challenges of automated segmentation in MRI. In this paper, we provide a comprehensive review of the existing literature on deep learning-based prostate segmentation, highlighting the key contributions and limitations. Additionally, we present a novel perspective on the application of deep learning to prostate segmentation. Specifically, we propose a new encoder-decoder model that incorporates attention mechanisms to capture fine-grained details in prostate MRI images. In our experimental evaluations, we demonstrate the effectiveness of the proposed model in achieving state-of-the-art performance on several publicly available datasets. Furthermore, we show that our attention-based encoder-decoder architecture can be trained with limited labeled data, making it more practical for clinical applications. Finally, we discuss the potential impact of this new approach on future research and clinical practice. Our review and proposal offer a valuable contribution to the growing body of literature on deep learning-based segmentation in MRI and pave the way for improved diagnosis and treatment of prostate cancer. 1 into PostgreSQL...\n",
      "Inserting test sample 2797  Remnants of neutron-star mergers are essentially massive, hot, differentially rotating neutron stars, which are initially strongly oscillating. They represent a unique probe for high-density matter because the oscillations are detectable via gravitational-wave measurements and are strongly dependent on the equation of state. The impact of the equation of state is apparent in the frequency of the dominant oscillation mode of the remnant. For a fixed total binary mass a tight relation between the dominant postmerger frequency and the radii of nonrotating neutron stars exists. Inferring observationally the dominant postmerger frequency thus determines neutron star radii with high accuracy of the order of a few hundred meters. By considering symmetric and asymmetric binaries of the same chirp mass, we show that the knowledge of the binary mass ratio is not critical for this kind of radius measurements. We summarize different possibilities to deduce the maximum mass of nonrotating neutron stars. We clarify the nature of the three most prominent features of the postmerger gravitational-wave spectrum and argue that the merger remnant can be considered to be a single, isolated, self-gravitating object that can be described by concepts of asteroseismology. The understanding of the different mechanisms shaping the gravitational-wave signal yields a physically motivated analytic model of the gravitational-wave emission, which may form the basis for template-based gravitational-wave data analysis. We explore the observational consequences of a scenario of two families of compact stars including hadronic and quark stars. We find that this scenario leaves a distinctive imprint on the postmerger gravitational-wave signal. In particular, a strong discontinuity in the dominant postmerger frequency as function of the total mass will be a strong indication for two families of compact stars. (abridged) 0 into PostgreSQL...\n",
      "Inserting test sample 2798  When neutron stars collide, they create some of the most violent and energetic phenomena in the universe. But what happens to the high-density matter that is created during such mergers? In this paper, we present our research on exploring the properties of high-density matter through the remnants of neutron-star mergers. \n",
      "\n",
      "Our findings indicate that these mergers can provide us with a unique opportunity to study the behavior of nuclear matter under extreme conditions. By analyzing the gravitational waves and electromagnetic radiation emitted by these events, we can gain insights into the equation of state of high-density matter and the properties of dense hadronic matter. \n",
      "\n",
      "We also investigate the implications of these findings on our understanding of neutron stars and their formation. Our studies suggest that neutron-star mergers may play a crucial role in the production of heavy elements in the universe, as well as in the origin of short gamma-ray bursts. \n",
      "\n",
      "Furthermore, the remnants of neutron-star mergers offer a valuable source of information on the behavior of matter under extreme conditions, which has significant implications for astrophysics and particle physics. The insights we gain from studying these mergers can help us better understand the nature of high-density matter, the behavior of nuclear matter in extreme environments, and the mechanisms behind some of the most violent phenomena in the universe. \n",
      "\n",
      "In conclusion, our research highlights the importance of studying the remnants of neutron-star mergers in exploring the properties of high-density matter and advancing our understanding of astrophysics and particle physics. Through this work, we hope to contribute to the ongoing efforts to unravel some of the greatest mysteries of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2799  A warm corona at the surface of an accretion disc has been proposed as a potential location for producing the soft excess commonly observed in the X-ray spectra of active galactic nuclei (AGNs). In order to fit the observed data the gas must be at temperatures of $\\sim 1$ keV and have an optical depth of $\\tau_{\\mathrm{T}}\\approx 10$--$20$. We present one-dimensional calculations of the physical conditions and emitted spectra of a $\\tau_{\\mathrm{T}}=10$ or $20$ gas layer subject to illumination from an X-ray power-law (from above), a blackbody (from below) and a variable amount of internal heating. The models show that a warm corona with $kT \\sim 1$ keV can develop, producing a strong Comptonized soft excess, but only if the internal heating flux is within a relatively narrow range. Similarly, if the gas density of the layer is too large then efficient cooling will stop a warm corona from forming. The radiation from the hard X-ray power-law is crucial in producing a warm corona, indicating that a warm and hot corona may co-exist in AGN accretion discs, and their combined effect leads to the observed soft excess. Intense heating of a warm corona leads to steep X-ray spectra with ionised Fe K$\\alpha$ lines, similar to those seen in some narrow-line Seyfert 1 galaxies. 0 into PostgreSQL...\n",
      "Inserting test sample 2800  Active Galactic Nuclei (AGNs) are known to emit radiation across the electromagnetic spectrum, from radio waves to gamma-rays. The radiation is produced by accretion of matter onto supermassive black holes (SMBHs) at the centers of galaxies. In this study, we examine the physical conditions of the warm coronae in AGN accretion disks, which are believed to play a critical role in the AGN energy output.\n",
      "\n",
      "We analyze the X-ray spectra of nine AGNs obtained using the Chandra X-ray Observatory. We fit the spectra with a model consisting of a power law component plus several emission lines that are characteristic of ionized iron and other heavy elements. Our analysis reveals that the X-ray spectra are well-described by a three-component model, consisting of a power law, a â€˜soft excessâ€™, and a line component.\n",
      "\n",
      "We find evidence for a correlation between the strength of the soft excess and the ionization state of the accretion disk. This implies that the soft excess is produced by Compton upscattering of soft photons by the warm corona in the accretion disk. Comparing these results to models of the accretion flow and the structure of the corona, we derive constraints on the size and temperature of the corona. Our results suggest that the warm corona is a key component in the AGN accretion flow and plays a crucial role in regulating the AGN energy output. 1 into PostgreSQL...\n",
      "Inserting test sample 2801  We theoretically consider the substrate-induced Majorana localization length renormalization in nanowires in contact with a bulk superconductor in the strong tunnel-coupled regime, showing explicitly that this renormalization depends strongly on the transverse size of the one-dimensional nanowires. For metallic (e.g. Fe on Pb) or semiconducting (e.g. InSb on Nb) nanowires, the renormalization effect is found to be very strong and weak respectively because the transverse confinement size in the two situations happens to be 0.5nm (metallic nanowire) and 20nm (semiconducting nanowire). Thus, the Majorana localization length could be very short (long) for metallic (semiconducting) nanowires even for the same values of all other parameters (except for the transverse wire size). We also show that any tunneling conductance measurements in such nanowires, carried out at temperatures and/or energy resolutions comparable to the induced superconducting energy gap, cannot distinguish between the existence of the Majorana modes or ordinary subgap fermionic states since both produce very similar broad and weak peaks in the subgap tunneling conductance independent of the localization length involved. Only low temperature (and high resolution) tunneling measurements manifesting sharp zero bias peaks can be considered to be signatures of Majorana modes in topological nanowires. 0 into PostgreSQL...\n",
      "Inserting test sample 2802  In this paper, we investigate the effect of substrate-induced Majorana renormalization in topological nanowires. Topological nanowires have recently gained significant attention due to their potential for Majorana zero modes, which may be used for topological quantum computation. However, the presence of a substrate can significantly affect the properties of these nanowires and their ability to host Majorana modes. To study this effect, we employ a self-consistent numerical method which takes into account the parameters of the substrate, such as its dielectric constant and material properties. Our results show that the presence of the substrate leads to renormalization of the Majorana modes, which can affect their robustness against various types of perturbations. Furthermore, we find that the nature of this renormalization depends strongly on the type of substrate used.\n",
      "\n",
      "Our findings suggest that substrate-induced Majorana renormalization in topological nanowires is an important factor to consider when designing and optimizing these nanowires for quantum computing applications. Our numerical approach provides a useful tool for predicting the behavior of these systems under different substrate conditions. We believe that our work will stimulate further investigation into the effects of substrates on Majorana zero modes, ultimately leading to the development of more robust and efficient topological qubits. 1 into PostgreSQL...\n",
      "Inserting test sample 2803  The ground state cooling of a mechanical oscillator in an optomechanical cavity containing an ensemble of identical two-level ground-state atoms is studied in the highly unresolved-sideband regime. The system exhibits electromagnetically-induced transparency-like quantum interference effect. The mutual interaction with the cavity optical field gives rise to an indirect coupling between the atomic and mechanical modes. In presence of this interaction, the noise spectrum gets modified and leads to asymmetric cooling and heating rates. Using the quantum master equation, time evolution of the average phonon number is studied and it is observed that the average phonon occupancy in the mechanical resonator exhibits ground-state cooling. 0 into PostgreSQL...\n",
      "Inserting test sample 2804  We present a study on the impact of atom-assisted cavity cooling of a micromechanical oscillator in the unresolved sideband regime. By exploiting the strong interaction between the mechanical motion and the light field in the optical cavity, we demonstrate a significant improvement in the cooling rate and final temperature of the oscillator. Moreover, we elucidate the underlying theoretical framework and explore the dependence of the cooling performance on the system parameters. Our results highlight the potential of this technique for enhancing the cooling capabilities of micro and nanomechanical devices, with implications for a wide range of applications, including quantum information processing and sensing. 1 into PostgreSQL...\n",
      "Inserting test sample 2805  Important clues to the chemical and dynamical history of elliptical galaxies are encoded in the abundances of heavy elements in the X-ray emitting plasma.\n",
      "\n",
      "We derive the hot ISM abundance pattern in inner and outer regions of NGC 4472 from analysis of Suzaku spectra, supported by analysis of co-spatial XMM-Newton spectra. The low background and relatively sharp spectral resolution of the Suzaku XIS detectors, combined with the high luminosity and temperature in NGC 4472, enable us to derive a particularly extensive abundance pattern that encompasses O, Ne, Mg, Al, Si, S, Ar, Ca, Fe, and Ni in both regions. We apply simple chemical evolution models to these data, and conclude that the abundances are best explained by a combination of alpha-element enhanced stellar mass loss and direct injection of Type Ia supernova (SNIa) ejecta. We thus confirm the inference, based on optical data, that the stars in elliptical galaxies have supersolar alpha/Fe ratios, but find that that the present-day SNIa rate is 4-6 times lower than the standard value. We find SNIa yield sets that reproduce Ca and Ar, or Ni, but not all three simultaneously. The low abundance of O relative to Ne and Mg implies that standard core collapse nucleosynthesis models overproduce O by a factor of 2. 0 into PostgreSQL...\n",
      "Inserting test sample 2806  In this study, we analyze the abundance pattern in the hot interstellar medium (ISM) of NGC 4472, an elliptical galaxy located in the Virgo cluster. Our data was obtained using the Chandra X-ray Observatory, which allowed us to measure the relative abundances of carbon, nitrogen, oxygen, neon, magnesium, silicon, sulfur, and iron. Our results reveal remarkable insights into the chemical composition of the ISM.\n",
      "\n",
      "We observe that the abundance pattern of NGC 4472 is consistent with the solar abundance pattern, with the exception of oxygen, which is about 40% less abundant than expected. This anomaly is likely due to oxygen depletion onto dust grains or a lower initial abundance of oxygen in the galaxy's progenitor. We also find evidence of enhanced alpha-element abundance (oxygen, neon, magnesium, silicon, and sulfur) in the central region of NGC 4472, which may indicate a recent merger event.\n",
      "\n",
      "Overall, our findings significantly contribute to the understanding of the chemical evolution of galaxies, particularly those that are massive and old like NGC 4472. Further studies of the hot ISM in similar galaxies will be crucial to confirm and expand upon our findings. 1 into PostgreSQL...\n",
      "Inserting test sample 2807  This paper investigates ARQ (Automatic Repeat request) designs for PNC (Physical-layer Network Coding) systems. We have previously found that, besides TWRC (Two-Way Relay Channel) operated on the principle of PNC, there are many other PNC building blocks--building blocks are simple small network structures that can be used to construct a large network. In some of these PNC building blocks, receivers can obtain side information through overhearing. Although such overheard information is not the target information that the receivers desire, the receivers can exploit the overheard information together with a network-coded packet received to obtain a desired native packet. This leads to throughput gain. Our previous study, however, assumed what is sent always get received. In practice, that is not the case. Error control is needed to ensure reliable communication. This paper focuses on the use of ARQ to ensure reliable PNC communication. The availability of overheard Information and its potential exploitation make the ARQ design of a network-coded system different from that of a non-network-coded system. In this paper, we lay out the fundamental considerations for such ARQ design: 1) We address how to track the stored coded packets and overheard packets to increase the chance of packet extraction, and derive the throughput gain achieved by tracking 2) We investigate two variations of PNC ARQ, coupled and non-coupled ARQs, and prove that non-coupled ARQ is more efficient; 3) We show how to optimize parameters in PNC ARQ--specifically the window size and ACK frequency--to minimize the throughput degradation caused by ACK feedback overhead and wasteful retransmissions due to lost ACK. 0 into PostgreSQL...\n",
      "Inserting test sample 2808  ARQ (Automatic Repeat Request) is a well-established error control method used in networks to provide a reliable communication link between the nodes. Recently, Physical-layer Network Coding (PNC) techniques have been proposed to enhance the network throughput in wireless networks. In this paper, we investigate the application of ARQ in PNC systems to further improve the network performance. We propose a joint decoding and retransmission scheme that is based on a time division multiple access (TDMA) protocol. This scheme is designed to guarantee maximal diversity gain in the case where PNC is employed. It is shown that this joint ARQ-PNC protocol can achieve higher throughput and reliability compared to conventional PNC or ARQ schemes.\n",
      "\n",
      "Moreover, we provide a comprehensive analysis of theoretical and practical aspects of the proposed scheme. We present a lower bound on the outage probability of the system and show that our proposed scheme is optimal in terms of maximizing the diversity gain. We also evaluate the system performance through computer simulations, and our results show that the proposed scheme outperforms traditional ARQ techniques with 50% higher throughput. Furthermore, we demonstrate that our joint ARQ-PNC scheme is more robust to channel errors and improves the outage performance by 3 dB compared to conventional PNC. \n",
      "\n",
      "In conclusion, our study provides a new insight into the potential benefits of combining ARQ and PNC techniques in wireless networks. Our proposed scheme highlights that the joint implementation of ARQ and PNC can significantly improve the network's reliability and throughput, making it a promising alternative for future wireless communication systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2809  This paper describes a method for defining representative load profiles for domestic electricity users in the UK. It considers bottom up and clustering methods and then details the research plans for implementing and improving existing framework approaches based on the overall usage profile. The work focuses on adapting and applying analysis framework approaches to UK energy data in order to determine the effectiveness of creating a few (single figures) archetypical users with the intention of improving on the current methods of determining usage profiles. The work is currently in progress and the paper details initial results using data collected in Milton Keynes around 1990.\n",
      "\n",
      "Various possible enhancements to the work are considered including a split based on temperature to reflect the varying UK weather conditions. 0 into PostgreSQL...\n",
      "Inserting test sample 2810  This paper investigates the application of data mining techniques to analyze energy usage patterns in domestic residences using UK data. A framework for energy usage profiling is proposed, incorporating data preprocessing, feature selection, and classification techniques. The proposed framework is evaluated on a large dataset of UK households, demonstrating its effectiveness in accurately identifying and profiling different energy usage patterns. The results of this study contribute to a better understanding of household-level energy usage behavior, which can inform the development of energy-efficient policies and technologies. Overall, the proposed framework provides a useful tool for energy providers, policymakers, and researchers to identify and target households with high energy consumption and develop targeted interventions to reduce energy consumption and associated carbon emissions. 1 into PostgreSQL...\n",
      "Inserting test sample 2811  High-resolution specific-heat measurements of the organic superconductor kappa-(BEDT-TTF)_2Cu[N(CN)_2]Br in the superconducting (B = 0) and normal (B = 14 T) state show a clearly resolvable anomaly at Tc = 11.5 K and an electronic contribution, Ces, which can be reasonably well described by strong-coupling BCS theory. Most importantly, Ces vanishes exponentially in the superconducting state which gives evidence for a fully gapped order parameter. 0 into PostgreSQL...\n",
      "Inserting test sample 2812  We report on the physical properties of kappa-(BEDT-TTF)_2Cu[N(CN)_2]Br, a fully gapped strong-coupling superconductor. Our measurements indicate a high critical temperature and a large energy gap for this material, consistent with previous observations of strong-coupling superconductivity. Additionally, we observe that the resistivity and penetration depth exhibit power-law behaviors below the critical temperature. Our results suggest that kappa-(BEDT-TTF)_2Cu[N(CN)_2]Br is a promising candidate for further investigation into superconductivity in strongly correlated electronic systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2813  For a real non-signdefinite function $B(z)$, $z\\in \\C$, we investigate the dimension of the space of entire analytical functions square integrable with weight $e^{\\pm 2F}$, where the function $F(z)=F(x_1,x_2)$ satisfies the Poisson equation $\\D F=B$. The answer is known for the function $B$ with constant sign.\n",
      "\n",
      "We discuss some classes of non-signdefinite positively homogeneous functions $B$, where both infinite and zero dimension may occur. In the former case we present a method of constructing entire functions with prescribed behavior at infinity in different directions. The topic is closely related with the question of the dimension of the zero energy subspace (zero modes) for the Pauli operator. 0 into PostgreSQL...\n",
      "Inserting test sample 2814  In this paper, we investigate entire functions in weighted $L_2$ with non-signdefinite magnetic field. Our analysis of zero modes of the Pauli operator yields a deeper understanding of the properties of these functions. We demonstrate the existence of zero modes for magnetic fields which are not signdefinite and provide necessary and sufficient conditions for their existence. These findings contribute to the ongoing quest for a comprehensive understanding of the behavior of zero modes in magnetic systems. Our results have potential applications in the fields of mathematical physics and quantum mechanics. 1 into PostgreSQL...\n",
      "Inserting test sample 2815  Generative Adversarial Networks (GANs) are a machine learning approach capable of generating novel example outputs across a space of provided training examples. Procedural Content Generation (PCG) of levels for video games could benefit from such models, especially for games where there is a pre-existing corpus of levels to emulate. This paper trains a GAN to generate levels for Super Mario Bros using a level from the Video Game Level Corpus. The approach successfully generates a variety of levels similar to one in the original corpus, but is further improved by application of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Specifically, various fitness functions are used to discover levels within the latent space of the GAN that maximize desired properties. Simple static properties are optimized, such as a given distribution of tile types. Additionally, the champion A* agent from the 2009 Mario AI competition is used to assess whether a level is playable, and how many jumping actions are required to beat it. These fitness functions allow for the discovery of levels that exist within the space of examples designed by experts, and also guide the search towards levels that fulfill one or more specified objectives. 0 into PostgreSQL...\n",
      "Inserting test sample 2816  This paper proposes a novel approach to generate Mario game levels using deep convolutional generative adversarial networks (DCGANs). Our method revolves around mapping the latent space of a pre-trained DCGAN model to Mario levels, by exploiting the underlying statistical distribution learned by the network from a large corpus of level examples. The generative power of DCGANs allows us to generate novel Mario levels automatically and to fine-tune them based on various level properties such as difficulty, length, and novelty. We also explore the effect of different DCGAN architectures on the quality and diversity of generated levels, and show that our method produces levels of varying styles and complexity. Furthermore, we introduce a new reward function based on level completion time and score, and evaluate our generated levels using this metric. Our experiments demonstrate that our approach outperforms existing level generation approaches in terms of quality, diversity, and challenge. Overall, this work opens up new avenues for game design and generates exciting possibilities for creating novel game content in an automated manner. 1 into PostgreSQL...\n",
      "Inserting test sample 2817  A long standing question in the theory of orthogonal matrix polynomials is the matrix Bochner problem, the classification of $N \\times N$ weight matrices $W(x)$ whose associated orthogonal polynomials are eigenfunctions of a second order differential operator. Based on techniques from noncommutative algebra (semiprime PI algebras of Gelfand-Kirillov dimension one), we construct a framework for the systematic study of the structure of the algebra $\\mathcal D(W)$ of matrix differential operators for which the orthogonal polynomials of the weight matrix $W(x)$ are eigenfunctions. The ingredients for this algebraic setting are derived from the analytic properties of the orthogonal matrix polynomials. We use the representation theory of the algebras $\\mathcal D(W)$ to resolve the matrix Bochner problem under the two natural assumptions that the sum of the sizes of the matrix algebras in the central localization of $\\mathcal D(W)$ equals $N$ (fullness of $\\mathcal D(W)$) and the leading coefficient of the second order differential operator multiplied by the weight $W(x)$ is positive definite. In the case of $2\\times 2$ weights, it is proved that fullness is satisfied as long as $\\mathcal D(W)$ is noncommutative. The two conditions are natural in that without them the problem is equivalent to much more general ones by artificially increasing the size of the matrix $W(x)$. 0 into PostgreSQL...\n",
      "Inserting test sample 2818  The Matrix Bochner problem pertains to the question of determining when an arbitrary matrix valued function is the Fourier transform of a positive measure. This problem has been in existence since the 1950s and has been a subject of interest both in pure and applied mathematics. Several variants of this problem have been studied, such as the inverse problem of trying to recover the measure from the matrix function. In this paper, we investigate the Matrix Bochner problem in the setting of Euclidean spaces by exploring the properties of the Fourier transforms of measures that are supported on different geometric sets. We provide explicit examples of matrix functions that arise from measures supported both on smooth and nonsmooth sets, and study their properties in the Fourier domain. Our results reveal surprising connections between the geometries of the sets and the decay rates of their Fourier transforms. We also develop a new framework for studying the Matrix Bochner problem on more general settings, paving the way for future research. This work demonstrates the importance of the study of matrix valued functions and its applications in various fields including signal processing, spectral theory, and quantum mechanics. 1 into PostgreSQL...\n",
      "Inserting test sample 2819  In this study we discuss two key issues related to a small-scale dynamo instability at low magnetic Prandtl numbers and large magnetic Reynolds numbers, namely: (i) the scaling for the growth rate of small-scale dynamo instability in the vicinity of the dynamo threshold; (ii) the existence of the Golitsyn spectrum of magnetic fluctuations in small-scale dynamos. There are two different asymptotics for the small-scale dynamo growth rate: in the vicinity of the threshold of the excitation of the small-scale dynamo instability, $\\lambda \\propto \\ln({\\rm Rm}/ {\\rm Rm}^{\\rm cr})$, and when the magnetic Reynolds number is much larger than the threshold of the excitation of the small-scale dynamo instability, $\\lambda \\propto {\\rm Rm}^{1/2}$, where ${\\rm Rm}^{\\rm cr}$ is the small-scale dynamo instability threshold in the magnetic Reynolds number ${\\rm Rm}$. We demonstrated that the existence of the Golitsyn spectrum of magnetic fluctuations requires a finite correlation time of the random velocity field. On the other hand, the influence of the Golitsyn spectrum on the small-scale dynamo instability is minor. This is the reason why it is so difficult to observe this spectrum in direct numerical simulations for the small-scale dynamo with low magnetic Prandtl numbers. 0 into PostgreSQL...\n",
      "Inserting test sample 2820  In this research paper, we investigate the growth rate of small-scale dynamo at low magnetic Prandtl numbers. Small-scale dynamo operates on sub-viscous scales and is associated with the amplification of magnetic fields in conducting fluids. The magnetic Prandtl number (Pm) is defined as the ratio of viscosity to magnetic diffusivity, and measures the efficiency of the fluid to transport magnetic fields. In astrophysical systems, such as galaxies and the interstellar medium, Pm is known to have low values. We perform numerical simulations of a turbulent, shearing flow to investigate the growth dynamics of small-scale dynamo at different Pm values. Our results show an increase in the critical magnetic Reynolds number (Rm) for decreasing Pm. The critical Rm marks the transition from an initial kinematic phase where the magnetic field grows exponentially, to a saturated phase where the magnetic field is limited by nonlinear effects. Furthermore, we find that the scaling of the growth rate with Rm depends on the value of Pm. Our findings provide insights into the behavior of magnetic fields in astrophysical systems with low Pm and can help improve our understanding of their observed properties. 1 into PostgreSQL...\n",
      "Inserting test sample 2821  We make use of $N$-body simulations to determine the relationship between two observable parameters that are used to quantify mass segregation and energy equipartition in star clusters. Mass segregation can be quantified by measuring how the slope of a cluster's stellar mass function $\\alpha$ changes with clustercentric distance r, and then calculating $\\delta_\\alpha = \\frac{d \\alpha(r)}{d ln(r/r_m)}$ where $r_m$ is the cluster's half-mass radius. The degree of energy equipartition in a cluster is quantified by $\\eta$, which is a measure of how stellar velocity dispersion $\\sigma$ depends on stellar mass m via $\\sigma(m) \\propto m^{-\\eta}$. Through a suite of $N$-body star cluster simulations with a range of initial sizes, binary fractions, orbits, black hole retention fractions, and initial mass functions, we present the co-evolution of $\\delta_\\alpha$ and $\\eta$. We find that measurements of the global $\\eta$ are strongly affected by the radial dependence of $\\sigma$ and mean stellar mass and the relationship between $\\eta$ and $\\delta_\\alpha$ depends mainly on the cluster's initial conditions and the tidal field. Within $r_m$, where these effects are minimized, we find that $\\eta$ and $\\delta_\\alpha$ initially share a linear relationship. However, once the degree of mass segregation increases such that the radial dependence of $\\sigma$ and mean stellar mass become a factor within $r_m$, or the cluster undergoes core collapse, the relationship breaks down. We propose a method for determining $\\eta$ within $r_m$ from an observational measurement of $\\delta_\\alpha$. In cases where $\\eta$ and $\\delta_\\alpha$ can be measured independently, this new method offers a way of measuring the cluster's dynamical state. 0 into PostgreSQL...\n",
      "Inserting test sample 2822  This paper investigates the connection between energy equipartition and radial variation in the stellar mass function of star clusters. We show that, due to energy equipartition, massive stars tend to be found near the center of a cluster, while lower-mass stars tend to migrate away from the center. Our theoretical analysis suggests that this effect can be significant for young clusters with ages up to a few tens of millions of years. We also demonstrate that the shape and slope of the mass function can be affected by this process, particularly in the inner regions of a cluster.\n",
      "\n",
      "To investigate this phenomenon, we performed N-body simulations of star clusters with different initial conditions. We found that, in agreement with theoretical predictions, the radial variation of the mass function is indeed influenced by energy equipartition. The observed trends are also consistent with some recent observations of young star clusters.\n",
      "\n",
      "Our simulations also reveal that the mass function is not the only property of clusters that is affected by energy equipartition; other observables, such as the velocity dispersion or the binary fraction, can also vary as a function of cluster radius. These results have implications for our understanding of the dynamical evolution of star clusters and the formation of their constituent stellar populations. Specifically, they suggest that energy equipartition may play an important role in shaping the distribution of stellar masses and properties within clusters, and may even provide a clue to their initial conditions. Further observational and theoretical work is necessary to fully explore the implications of these findings. 1 into PostgreSQL...\n",
      "Inserting test sample 2823  We have measured the 3.6 micron luminosity evolution of about 1000 galaxies in 32 clusters at 0.2<z<1.25, without any a priori assumption about luminosity evolution, i.e. in a logically rigorous way. We find that the luminosity of our galaxies evolves as an old and passively evolving population formed at high redshift without any need for additional redshift-dependent evolution. Models with a prolonged stellar mass growth are rejected by the data with high confidence. The data also reject models in which the age of the stars is the same at all redshifts. Similarly, the characteristic stellar mass evolves, in the last two thirds of the universe age, as expected for a stellar population formed at high redshift. Together with the old age of stellar populations derived from fundamental plane studies, our data seems to suggest that early-type cluster galaxies have been completely assembled at high redshift, and not only that their stars are old. The quality of the data allows us to derive the LF and mass evolution homogeneously over the whole redshift range, using a single estimator. The Schechter function describes the galaxy luminosity function well. The characteristic luminosity at z=0.5 is is found to be 16.30 mag, with an uncertainty of 10 per cent. 0 into PostgreSQL...\n",
      "Inserting test sample 2824  In this study, we investigate the relationship between the buildup of stellar mass and the 3.6 micron luminosity function in clusters over a wide range of redshifts. By analyzing data from the IRAC Shallow Cluster Survey, we examine the evolution of these properties in clusters from z=1.25 to z=0.2. Our results reveal a significant increase in stellar mass with decreasing redshift, consistent with the hierarchical growth of structure. We also find that the 3.6 micron luminosity function evolves strongly with redshift, particularly at the bright end. This suggests that star formation activity in the brightest galaxies of clusters is declining with time. These findings have important implications for our understanding of galaxy evolution in clusters and the formation of massive galaxies. Our study contributes to the growing body of literature on the properties of galaxy clusters, and provides insights into the physical processes that shape the evolution of these complex systems over cosmic time. 1 into PostgreSQL...\n",
      "Inserting test sample 2825  The Geostationary Orbital Environmental Satellites (GOES) Soft X-ray (SXR) sensors have provided data relating to, inter alia, the time, intensity and duration of solar flares since the 1970s. The GOES SXR Flare List has become the standard reference catalogue for solar flares and is widely used in solar physics research and space weather. We report here that in the current version of the list there are significant differences between the mean duration of flares which occurred before May 1997 and the mean duration of flares thereafter. Our analysis shows that the reported flare timings for the pre-May 1997 data were not based on the same criteria as is currently the case.\n",
      "\n",
      "This finding has serious implications for all those who used flare duration (or fluence, which depends on the chosen start and end times) as part of their analysis of pre-May 1997 solar events, or statistical analyses of large samples of flares, e.g. as part of the assessment of a Solar Energetic Particle forecasting algorithm. 0 into PostgreSQL...\n",
      "Inserting test sample 2826  This study aims to investigate the reported durations of soft X-ray flares observed by the Geostationary Operational Environmental Satellite (GOES) in various solar cycles. The data analyzed consists of GOES X-ray flux measurements obtained during the period between 1986 and 2001. We use a sample of over 3000 flares with a minimum flux of 10^-6 W/m^2 to study the dependence of the total flare duration on the flare magnitude and solar activity. Our analysis revealed that the reported duration of GOES soft X-ray flares varies significantly from one solar cycle to another. Additionally, we observed a positive correlation between the flare magnitude and total duration, indicating the dominance of energy build-up processes. These results contribute to a better understanding of the complex dynamics of the solar atmosphere and are important in the context of space weather forecasting. 1 into PostgreSQL...\n",
      "Inserting test sample 2827  In a recent study, (Jain et al 2007 Phys. Rev. Lett. 99 190601), a symmetric exclusion process with time-dependent hopping rates was introduced. Using simulations and a perturbation theory, it was shown that if the hopping rates at two neighboring sites of a closed ring vary periodically in time and have a relative phase difference, there is a net DC current which decreases inversely with the system size. In this work, we simplify and generalize our earlier treatment. We study a model where hopping rates at all sites vary periodically in time, and show that for certain choices of relative phases, a DC current of order unity can be obtained. Our results are obtained using a perturbation theory in the amplitude of the time-dependent part of the hopping rate. We also present results obtained in a sudden approximation that assumes large modulation frequency. 0 into PostgreSQL...\n",
      "Inserting test sample 2828  The symmetric exclusion process with time-dependent hopping rates is a widely-studied model in statistical physics that investigates the dynamics of particle current in various physical systems. In this paper, we analyze the behavior of the particle current when the hopping rates between neighboring sites are varied in time. We start with the derivation of an exact expression for the generating function of the particle current using the matrix product ansatz. This enables us to evaluate various statistical properties of the current, including its cumulants, higher moments, and spatial correlations. We investigate the behavior of these properties as functions of time, system size, and rate parameters. Our results demonstrate the richness of the dynamics in this model and reveal several interesting features, such as transient oscillations and long-range correlations. We expect our findings to have implications for the understanding of various physical phenomena, including transport of particles in biological systems and traffic flow in cities. 1 into PostgreSQL...\n",
      "Inserting test sample 2829  We further develop a form factor formalism characterizing anomalous interactions of the Higgs-like boson (h) to massive electroweak vector bosons (V) and generic bilinear fermion states (F). Employing this approach, we examine the sensitivity of pp -> F ->Vh associated production to physics beyond the Standard Model, and compare it to the corresponding sensitivity of h -> V F decays. We discuss how determining the Vh invariant-mass distribution in associated production at LHC is a key ingredient for model-independent determinations of h V F interactions. We also provide a general discussion about the power counting of the form factor's momentum dependence in a generic effective field theory approach, analyzing in particular how effective theories based on a linear and non-linear realization of the SU(2)_L x U(1)_Y gauge symmetry map into the form factor formalism. We point out how measurements of the differential spectra characterizing h -> V F decays and pp -> F -> Vh associated production could be the leading indication of the presence of a nonlinear realization of the SU(2)_L x U(1)_Y gauge symmetry. 0 into PostgreSQL...\n",
      "Inserting test sample 2830  The form factors of the Higgs boson in associated production processes are of paramount importance for the characterization of this recently discovered elementary particle and to unravel its unique properties. In this work, we investigate the impact of such form factors on the production of ZH, WH and ttH events at the LHC in the context of the standard model. By taking into account the relevant theoretical uncertainties, we perform a state-of-the-art theoretical prediction for the Higgs form factors at the LHC for center-of-mass energies of 13 TeV. Our results show that the form factors can significantly affect the kinematics of the Higgs boson. In particular, we observe large deviations from the prediction of the standard model, thus suggesting that this observable could serve as a sensitive probe of new physics beyond the standard model. Our study sheds light on the potential of measuring the Higgs form factors in associated production to test the nature of the Higgs particle and the existence of new physics beyond the standard model. 1 into PostgreSQL...\n",
      "Inserting test sample 2831  We report on the low-temperature magnetic susceptibilities and specific heats of the isostructural spin-ladder molecular complexes L$_{2}$[M(opba)]$_{3\\cdot x$DMSO$\\cdot y$H$_{2}$O, hereafter abbreviated with L$_{2}$M$_{3}$ (where L = La, Gd, Tb, Dy, Ho and M = Cu, Zn). The results show that the Cu containing complexes (with the exception of La$_{2}$Cu$_{3}$) undergo long range magnetic order at temperatures below 2 K, and that for Gd$_{2}$Cu$_{3}$ this ordering is ferromagnetic, whereas for Tb$_{2}$Cu$_{3}$ and Dy$_{2}$Cu$_{3}$ it is probably antiferromagnetic. The susceptibilities and specific heats of Tb$_{2}$Cu$_{3}$ and Dy$_{2}$Cu$_{3}$ above $T_{C}$ have been explained by means of a model taking into account nearest as well as next-nearest neighbor magnetic interactions. We show that the intraladder L--Cu interaction is the predominant one and that it is ferromagnetic for L = Gd, Tb and Dy. For the cases of Tb, Dy and Ho containing complexes, strong crystal field effects on the magnetic and thermal properties have to be taken into account. The magnetic coupling between the (ferromagnetic) ladders is found to be very weak and is probably of dipolar origin. 0 into PostgreSQL...\n",
      "Inserting test sample 2832  This study investigates the magnetic and thermal properties of ladder-type molecular compounds that incorporate both 4f- and 3d-metal sites. The focus is on two such compounds which begin with a trinuclear building block comprised of a 4f metal ion and two 3d metal ions. Each trinuclear cluster is then bridged by a neutral ligand to create the ladder motif. Both compounds display strong intra-ladder antiferromagnetic interactions leading to pronounced spin frustration, likely attributable to their one-dimensional nature. Additionally, the 4f-3d interaction coupling has a significant effect on the magnetic properties. The anisotropy of each 4f-ion modifies the magnetic properties of its 3d neighbor, leading to non-trivial magnetism that is unique to this new class of 3d-4f molecular materials. Furthermore, these molecular compounds exhibit a complex thermal behavior. High-temperature peaks in the magnetic susceptibility data coincide with features in heat capacity data, a trend not observed in previous ladder-type compounds. These results demonstrate a promising new direction for future study of molecular-based magnetic materials. 1 into PostgreSQL...\n",
      "Inserting test sample 2833  TV white spaces refer to the allocated but locally unused TV spectrum and can be used by unlicensed devices as secondary users. Thanks to their lower frequencies (e.g., 54 -- 698 MHz in the US), communication over the TV spectrum has excellent propagation characteristics over long distances and through obstacles. These characteristics along with their wide availability make the TV white spaces a great choice and alternative to many existing wireless technologies, especially the ones that need long range and high bandwidth communication. In the last decade, there have been numerous efforts from academia, industries, and standards bodies for exploiting the potentials of the TV white spaces for several applications including wireless broadband Internet access. Their characteristics and features also hold potentials for many new applications including sensing and monitoring, Internet of Things, wireless control, smart utility, location-based services, and transportation and logistics. In this paper, we perform a retrospective review and comparative study of existing work on networking in the TV white spaces. Additionally, we discuss the associated research challenges such as dealing with the interference between primary and secondary TV spectrum users, TV white space temporal and spatial variations and fragmentation, antenna design, mobility, and security. We also describe the future research directions to handle the above challenges. To the best of our knowledge, this is the first comprehensive survey on the literature of TV white space networking research. 0 into PostgreSQL...\n",
      "Inserting test sample 2834  Wireless networks have seen rapid growth and have become an important aspect of modern life. However, network congestion is a growing concern and has resulted in the spectrum becoming a scarce resource. With the increasingly popular use of TV white spaces (TVWS) over the past decade, they have been identified as a potential solution for spectrum scarcity. This paper presents a survey of the key developments in networking over TVWS, as well as a comprehensive analysis of the available literature in this area. The paper reviews the technologies utilized in networking over TVWS, the regulatory and standardization aspects of TVWS, and the challenges posed by the unique propagation characteristics of TVWS. In addition, the study highlights the key applications of TVWS networking such as rural connectivity, smart grid, and emergency communication. Furthermore, the paper presents a detailed comparison of the performance of TVWS networking and traditional wireless networking. The results show that TVWS networking has a significant potential to offer cost-effective and long-range wireless connectivity, especially for rural areas. Although there are challenges to the adoption of TVWS networking, the study concludes that networking over TVWS is a promising research area that can enhance the available spectrum for wireless communication. 1 into PostgreSQL...\n",
      "Inserting test sample 2835  Future observations of CMB anisotropies will be able to probe high multipole regions of the angular power spectrum, corresponding to a resolution of a few arcminutes. Dust emission from merging haloes is one of the foregrounds that will affect such very small scales. We estimate the contribution to CMB angular fluctuations from objects that are bright in the sub-millimeter band due to intense star formation bursts following merging episodes. We base our approach on the Lacey-Cole merger model and on the Kennicutt relation which connects the star formation rate in galaxies with their infrared luminosity. We set the free parameters of the model in order to not exceed the SCUBA source counts, the Madau plot of star formation rate in the universe and COBE/FIRAS data on the intensity of the sub-millimeter cosmic background radiation. We show that the angular power spectrum arising from the distribution of such star-forming haloes will be one of the most significant foregrounds in the high frequency channels of future CMB experiments, such as PLANCK, ACT and SPT. The correlation term, due to the clustering of multiple haloes at redshift z~2-6, is dominant in the broad range of angular scales 200<l<3000. Poisson fluctuations due to bright sub-millimeter sources are more important at higher l, but since they are generated from the bright sources, such contribution could be strongly reduced if bright sources are excised from the sky maps. The contribution of the correlation term to the angular power spectrum depends strongly on the redshift evolution of the escape fraction of UV photons and the resulting temperature of the dust. The measurement of this signal will therefore give important information about galaxies in the early stage of their evolution. 0 into PostgreSQL...\n",
      "Inserting test sample 2836  This research paper investigates the clustering of merging star-forming haloes and its effects on dust emission as high-frequency arcminute CMB foreground. The merging of haloes, which are regions of high-density gas and dust that collapse to form a galaxy, has been observed to occur frequently in the early universe. We study the characteristics of these merging haloes and their impact on the foreground emission in the cosmic microwave background (CMB) radiation. \n",
      "\n",
      "Our work uses computer simulations that incorporate the complex physics of star formation and dust emission in merging haloes. We find that, as a result of the merging process, these haloes can emit a significant amount of dust at high frequencies, which can interfere with observations of the CMB radiation. The resulting foreground emission is not uniform, but exhibits clustering patterns that depend on the properties of the merging haloes and their distribution.\n",
      "\n",
      "Our analysis shows that the clustering of merging star-forming haloes is a key factor that needs to be accounted for in order to accurately interpret CMB observations. We propose a novel method to identify and separate this foreground emission from the CMB signal, based on the observed clustering patterns. Our results demonstrate that it is possible to isolate the CMB signal from the foreground contamination and improve the precision of cosmological parameter estimation.\n",
      "\n",
      "Overall, our work sheds light on the complex interplay between star formation, dust emission, halo merging, and CMB observations. It highlights the importance of understanding these processes in order to extract cosmological information from CMB data with high precision. 1 into PostgreSQL...\n",
      "Inserting test sample 2837  We conduct a comprehensive theoretical and numerical investigation of the pollution of pristine gas in turbulent flows, designed to provide new tools for modeling the evolution of the first generation of stars. The properties of such Population III (Pop III) stars are thought to be very different than later generations, because cooling is dramatically different in gas with a metallicity below a critical value Z_c, which lies between ~10^-6 and 10^-3 solar value. Z_c is much smaller than the typical average metallicity, <Z>, and thus the mixing efficiency of the pristine gas in the interstellar medium plays a crucial role in the transition from Pop III to normal star formation. The small critical value, Z_c, corresponds to the far left tail of the probability distribution function (PDF) of the metallicity. Based on closure models for the PDF formulation of turbulent mixing, we derive equations for the fraction of gas, P, lying below Z_c, in compressible turbulence. Our simulation data shows that the evolution of the fraction P can be well approximated by a generalized self-convolution model, which predicts dP/dt = -n/tau_con P (1-P^(1/n)), where n is a measure of the locality of the PDF convolution and the timescale tau_con is determined by the rate at which turbulence stretches the pollutants. Using a suite of simulations with Mach numbers ranging from M = 0.9 to 6.2, we provide accurate fits to n and tau_con as a function of M, Z_c/<Z>, and the scale, L_p, at which pollutants are added to the flow. For P>0.9, mixing occurs only in the regions surrounding the pollutants, such that n=1. For smaller P, n is larger as mixing becomes more global. We show how the results can be used to construct one-zone models for the evolution of Pop III stars in a single high-redshift galaxy, as well as subgrid models for tracking the evolution of the first stars in large cosmological simulations. 0 into PostgreSQL...\n",
      "Inserting test sample 2838  The early universe was a pristine environment, filled with hydrogen and helium gas. Investigating the pollution of this gas can provide insights into the formation and evolution of the first galaxies and stars. In this paper, we present a novel model for studying the pollution of pristine gas in the early universe.\n",
      "\n",
      "Our model is based on a hydrodynamic simulation that tracks the evolution of gas as it cools and collapses into structures. To study the pollution of this gas, we incorporate a self-consistent chemical network that accounts for the formation and destruction of various atomic and molecular species.\n",
      "\n",
      "Using our model, we investigate the effects of feedback mechanisms such as stars and supernovae on the pollution of pristine gas. We find that these mechanisms play a crucial role in enriching the gas with heavy elements such as carbon, oxygen, and nitrogen. Our simulation also reveals that the pollution process is heavily influenced by the underlying structure of the gas, with pollution occurring more rapidly in denser regions.\n",
      "\n",
      "Furthermore, our model can be used to predict observational signatures of pollution within the early universe. We find that the pollution of gas can significantly alter its absorption and emission spectra, providing a potential avenue for detection in future observations.\n",
      "\n",
      "In summary, this paper presents a new model for studying the pollution of pristine gas in the early universe. Our model incorporates a self-consistent chemical network and investigates the effects of different feedback mechanisms on pollution processes. We also discuss potential observational signatures of pollution and provide suggestions for future work. 1 into PostgreSQL...\n",
      "Inserting test sample 2839  A method is presented here for investigating variations in the upper end of the stellar Initial Mass Function (IMF) by probing the production rate of ionizing photons in unresolved, compact star clusters with ages<10 Myr and covering a range of masses. We test this method on the young cluster population in the nearby galaxy M51a, for which multi-wavelength observations from the Hubble Space Telescope are available. Our results indicate that the proposed method can probe the upper end of the IMF in galaxies located out to at least 10 Mpc, i.e., a factor 200 further away than possible by counting individual stars in young compact clusters. Our results for this galaxy show no obvious dependence of the upper mass end of the IMF on the mass of the star cluster, down to ~1000 M_sun, although more extensive analyses involving lower mass clusters and other galaxies are needed to confirm this conclusion. 0 into PostgreSQL...\n",
      "Inserting test sample 2840  This paper proposes a novel method for measuring the upper end of the IMF. The IMF is an important economic indicator to assess the stability of financial institutions and the health of the economy. However, existing methods have limitations, particularly when it comes to measuring the upper end of the IMF. Our proposed method addresses these limitations by leveraging machine learning techniques and data from financial markets to provide more accurate measurements. We demonstrate the effectiveness of our method through simulations and empirical studies. Our approach offers significant improvements in terms of both accuracy and efficiency, which should prove valuable to policy-makers, regulators, and financial institutions. Additionally, our method has the potential for wider applications, such as risk management and predicting financial crises. Overall, this paper presents an important contribution to the field of macroeconomics and financial economics. 1 into PostgreSQL...\n",
      "Inserting test sample 2841  High-precision planetary densities are key to derive robust atmospheric properties for extrasolar planets. Measuring precise masses is the most challenging part, especially in multi-planetary systems. We measure the masses and densities of a four-planet near resonant chain system (K2-32), and a young ($\\sim400$ Myr old) planetary system consisting of three close-in small planets (K2-233). We obtained 199 new HARPS observations for K2-32 and 124 for K2-233 covering a more than three year baseline. We find that K2-32 is a compact scaled-down version of the Solar System's architecture, with a small rocky inner planet (M$_e=2.1^{+1.3}_{-1.1}$~M$_{\\oplus}$, P$_e\\sim4.35$~days) followed by an inflated Neptune-mass planet (M$_b=15.0^{+1.8}_{-1.7}$~M$_{\\oplus}$, P$_b\\sim8.99$~days) and two external sub-Neptunes (M$_c=8.1\\pm2.4$~M$_{\\oplus}$, P$_c\\sim20.66$~days; M$_d=6.7\\pm2.5$~M$_{\\oplus}$, P$_d\\sim31.72$~days). K2-32 becomes one of the few multi-planetary systems with four or more planets known with measured masses and radii. Additionally, we constrain the masses of the three planets in K2-233. For the two inner Earth-size planets we constrain their masses to be smaller than M$_b<11.3$ M$_{\\oplus}$ (P$_b\\sim2.47$~days), M$_c<12.8$ M$_{\\oplus}$ (P$_c\\sim7.06$~days). The outer planet is a sub-Neptune size planet with an inferred mass of M$_d=8.3^{+5.2}_{-4.7}$ M$_{\\oplus}$ (M$_d<21.1$ M$_{\\oplus}$, P$_d\\sim24.36$~days). Our observations of these two planetary systems confirm for the first time the rocky nature of two planets orbiting a young star, with relatively short orbital periods ($<7$ days). They provide key information for planet formation and evolution models of telluric planets. Additionally, the Neptune-like derived masses of the three planets K2-32 b, c, d puts them in a relatively unexplored regime of incident flux and planet mass, key for transmission spectroscopy studies. 0 into PostgreSQL...\n",
      "Inserting test sample 2842  The K2 mission has been instrumental in the discovery of numerous exoplanets. In this research paper, we present the masses for seven planets orbiting two stars in the K2 field: K2-32 and K2-233. The four planets orbiting K2-32 are in a resonant chain, while the three planets orbiting K2-233 are isolated. \n",
      "\n",
      "We find that the masses of the planets orbiting K2-32 are particularly important in testing theories of planet formation and migration. The resonant chain is also of special interest since it provides insights on the dynamics of the system as a whole. Our results indicate that the two outer planets have similar masses, while the two inner planets have very different masses. This suggests that the resonant chain formed after the dispersal of the protoplanetary disk.\n",
      "\n",
      "The three isolated planets orbiting K2-233 are the first young rocky worlds discovered around a young star. Our analysis shows that the masses of these planets are consistent with rocky compositions, indicating that they are likely to be predominantly made up of solid material. These terrestrial worlds are also relatively close to their host star, with orbital periods ranging from 1.6 to 5.8 days.\n",
      "\n",
      "The diverse nature of the planets in these two systems provides valuable insights into the processes of planetary formation and evolution. In particular, the resonant chain observed in K2-32 reflects the impact of the disk's dispersal on the dynamics of the system. The discovery of the first young rocky worlds in K2-233 expands our understanding of the formation of planets around young stars.\n",
      "\n",
      "In conclusion, our study provides crucial data on the masses of seven planets orbiting two stars in the K2 field. The four planets in a resonant chain offer insight into dynamical processes in exoplanet systems, while the discovery of the first young rocky worlds provides valuable information on planet formation around young stars. 1 into PostgreSQL...\n",
      "Inserting test sample 2843  We present results of experimental and theoretical investigations of electron transport through stub-shaped waveguides or electron stub tuners (ESTs) in the ballistic regime. Measurements of the conductance G as a function of voltages, applied to different gates V_i (i=bottom, top, and side) of the device, show oscillations in the region of the first quantized plateau which we attribute to reflection resonances. The oscillations are rather regular and almost periodic when the height h of the EST cavity is small compared to its width. When h is increased, the oscillations become less regular and broad depressions in G appear. A theoretical analysis, which accounts for the electrostatic potential formed by the gates in the cavity region, and a numerical computation of the transmission probabilities successfully explains the experimental observations.\n",
      "\n",
      "An important finding for real devices, defined by surface Schottky gates, is that the resonance nima result from size quantization along the transport direction of the EST. 0 into PostgreSQL...\n",
      "Inserting test sample 2844  This study presents an investigation of ballistic electron transport (BET) in stubbed quantum waveguides (QWs), through a combined approach of experimental measurements and theoretical modeling. The impact of the QW stubs is examined, revealing that they can significantly impact BET and induce mode mixing. The experimental measurements are carried out on fabricated QW devices, and the results are in good agreement with theoretical predictions. Furthermore, the authors develop a model that quantifies the impact of the stub on BET and establish that the electron density and the Fermi wave vector are the primary factors affecting the transport behavior. The study highlights the importance of carefully designing QW structures and the potential applications of BET in future electronic and optoelectronic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 2845  Over the last decade, digital media (web or app publishers) generalized the use of real time ad auctions to sell their ad spaces. Multiple auction platforms, also called Supply-Side Platforms (SSP), were created. Because of this multiplicity, publishers started to create competition between SSPs. In this setting, there are two successive auctions: a second price auction in each SSP and a secondary, first price auction, called header bidding auction, between SSPs.In this paper, we consider an SSP competing with other SSPs for ad spaces. The SSP acts as an intermediary between an advertiser wanting to buy ad spaces and a web publisher wanting to sell its ad spaces, and needs to define a bidding strategy to be able to deliver to the advertisers as many ads as possible while spending as little as possible. The revenue optimization of this SSP can be written as a contextual bandit problem, where the context consists of the information available about the ad opportunity, such as properties of the internet user or of the ad placement.Using classical multi-armed bandit strategies (such as the original versions of UCB and EXP3) is inefficient in this setting and yields a low convergence speed, as the arms are very correlated. In this paper we design and experiment a version of the Thompson Sampling algorithm that easily takes this correlation into account. We combine this bayesian algorithm with a particle filter, which permits to handle non-stationarity by sequentially estimating the distribution of the highest bid to beat in order to win an auction. We apply this methodology on two real auction datasets, and show that it significantly outperforms more classical approaches.The strategy defined in this paper is being developed to be deployed on thousands of publishers worldwide. 0 into PostgreSQL...\n",
      "Inserting test sample 2846  Header bidding has become a popular technique among publishers to enhance programmatic advertising revenues. Supply-side platforms (SSPs) utilize this technique to allow real-time bidding for ad inventory by numerous demand-side platforms (DSPs) simultaneously, optimizing the selling process. However, setting up an optimal SSP header bidding strategy is challenging since it requires balancing the selection of DSPs, the number of requests sent, and the minimum price floor that will maximize revenue. In this study, we propose the use of the Thompson Sampling technique to address the SSP header bidding strategy optimization problem. This Bayesian algorithm enables the selection of the most profitable DSPs within a mutually exclusive consideration set, bypassing the weakness of the existing less-efficient heuristic methods and fully exploring the underlying distribution of the problem. We conducted a real-world experiment, applying the Thompson Sampling technique to a data set of 8 million bid requests obtained from the OpenX ad exchange. The results show that when compared with the widely used heuristic techniques, such as maximum revenue and secondary price auctions, the proposed method significantly outperforms them in terms of gross revenue. The proposed technique outperforms the benchmark methods by 220% and 83%, respectively. Additionally, we provide a detailed analysis of the models' performance by discussing their impact on different metrics, and analyzing scenarios when the proposed technique may not yield the best results. Our results show the effectiveness of using Thompson Sampling techniques to optimize the SSP header bidding strategy and shed light on the benefits and limitations of the approach. 1 into PostgreSQL...\n",
      "Inserting test sample 2847  We present a model for wave propagation in a monolayer of spheres on an elastic substrate. The model, which considers sagittally polarized waves, includes: horizontal, vertical, and rotational degrees of freedom; normal and shear coupling between the spheres and substrate, as well as between adjacent spheres; and the effects of wave propagation in the elastic substrate. For a monolayer of interacting spheres, we find three contact resonances, whose frequencies are given by simple closed-form expressions. For a monolayer of isolated spheres, only two resonances are present. The contact resonances couple to surface acoustic waves in the substrate, leading to mode hybridization and \"avoided crossing\" phenomena. We present dispersion curves for a monolayer of silica microspheres on a silica substrate, assuming adhesive, Hertzian interactions, and compare calculations using an effective medium approximation to a discrete model of a monolayer on a rigid substrate.\n",
      "\n",
      "While the effective medium model does not account for discrete lattice effects at short wavelengths, we find that it is well suited for describing the interaction between the monolayer and substrate in the long wavelength limit.\n",
      "\n",
      "We suggest that a complete picture of the dynamics of a discrete monolayer adhered to an elastic substrate can be found using a combination of the results presented for the discrete and effective medium descriptions. This model is potentially scalable for use with both micro- and macroscale systems, and offers the prospect of experimentally extracting contact stiffnesses from measurements of acoustic dispersion. 0 into PostgreSQL...\n",
      "Inserting test sample 2848  This paper investigates the dynamics of a monolayer of microspheres placed on an elastic substrate and their behavior under various conditions. The study aims to explore the interactions between the microspheres and the substrate, and the impact of external forces. The experimental analysis is conducted by utilizing microscopy techniques to monitor the movements of the microspheres and the substrate under different conditions, such as varying confinement and compression forces. The results show that the microspheres exhibit unique behaviors as a function of the substrate's elastic properties, including current velocity, forces, and confinement conditions. The observations are in line with prior studies of the contact mechanics of other soft condensed matter systems. This research provides valuable insights into the underlying principles and dynamics of monolayer formation and the response from an elastic substrate. Understanding this behavior is crucial, as it plays a vital role in the design of new materials and devices that rely on this type of self-organized assembly. The results obtained in this study can further aid in engineering applications, including the development of elastic sensors and filters, and self-assembled microfluidic devices. 1 into PostgreSQL...\n",
      "Inserting test sample 2849  For a Riemannian covering $\\pi\\colon M_1\\to M_0$, the bottoms of the spectra of $M_0$ and $M_1$ coincide if the covering is amenable. The converse implication does not always hold. Assuming completeness and a lower bound on the Ricci curvature, we obtain a converse under a natural condition on the spectrum of $M_0$. 0 into PostgreSQL...\n",
      "Inserting test sample 2850  We study the bottom of the spectrum of a covering operator algebra, and its relation to amenability. Specifically, we show that if the underlying group of the covering is infinite, then the amenability of the group is equivalent to the bottom of the spectrum containing the scalar multiples of the identity. This generalizes recent work on property (T) to a much broader class of groups. 1 into PostgreSQL...\n",
      "Inserting test sample 2851  The controller of an input-affine system is determined through minimizing a time-varying objective function, where stabilization is ensured via a Lyapunov function decay condition as constraint. This constraint is incorporated into the objective function via a barrier function. The time-varying minimum of the resulting relaxed cost function is determined by a tracking system. This system is constructed using derivatives up to second order of the relaxed cost function and improves the existing approaches in time-varying optimization.\n",
      "\n",
      "Under some mild assumptions, the tracking system yields a solution which is feasible for all times, and it converges to the optimal solution of the relaxed objective function in a user-defined fixed-time. The effectiveness of these results in comparison to exponential convergence is demonstrated in a case study. 0 into PostgreSQL...\n",
      "Inserting test sample 2852  This research paper presents a methodology for the design of a fixed-time stabilizing optimal controller based on a time-varying objective function. The proposed technique aims to combine the advantages of optimal control theory with the benefits of fixed-time stability concerning robustness against external disturbances and fast transient responses. The time-varying objective function is fully defined according to the desired performance specifications, which allows for a wide range of applications in different control engineering fields. The presented methodology consists of a two-step approach, where first, a state-dependent performance index is defined, and then extended to a suitable cost function. Next, the optimal controller is designed using the methodologies of convex optimization and canonical forms. Finally, numerical simulations are provided to demonstrate the effectiveness of the approach for different system models. 1 into PostgreSQL...\n",
      "Inserting test sample 2853  We report the first detection of an optical afterglow of a GRB (060108) that would have been classified as 'dark' in the absence of deep, rapid ground-based optical imaging with the 2-m robotic Faulkes Telesscope (FTN). Our multiwavelength analysis reveals an X-ray light curve typical of many Swift long GRBs (3-segments plus flare). Its optical afterglow, however, was already fainter than the detection limit of the UVOT within 100s of the burst. Optical imaging in BVRi' filters with the FTN began 2.75 minutes after the burst and resulted in the detection of the optical afterglow at 5.3 minutes, with a UKIRT K-band identification at ~45 mins. R and i'-band light curves are consistent with a single power law decay in flux, F(t) prop t^-a where a=0.43+/-0.08, or a 2-segment light curve with a steep decay a_1 <0.88, flattening to a_2 ~ 0.31, with evidence for rebrightening at i' band. Deep VLT R-band imaging at ~12 days reveals a faint, extended object (R ~23.5 mag) at the location of the afterglow. Although the brightness is compatible with the extrapolation of the a_2 slow decay, significant flux is likely due to a host galaxy. This implies that the optical light curve had a break before 12 days, akin to what observed in the X-rays. We derive a maximum photometric redshift z<3.2 for GRB 060108 and a best-fitting optical-to-X-ray SED at 1000 s after the burst consistent with a power law with index beta_OX = 0.54 and a small amount of extinction.\n",
      "\n",
      "The unambiguous detection at B-band and the derived photometric redshift rule out a high redshift as the reason for the optical faintness of GRB 060108.\n",
      "\n",
      "Instead, the hard opt/X-ray spectral index confirms it as one of the optically-darkest bursts detected and with modest host extinction explains the UVOT non-detection (abridged). 0 into PostgreSQL...\n",
      "Inserting test sample 2854  The purpose of this study is to present an in-depth analysis of the afterglow of GRB 060108, exploring its physical characteristics and the mechanisms behind its production. Dark bursts are unique events in the realm of gamma-ray bursts, as they are characterized by a weak or non-existent optical afterglow. However, observations of GRB 060108 revealed a bright and long-lasting afterglow, challenging the existing understanding of these events.\n",
      "\n",
      "Our analysis of the afterglow of GRB 060108 utilized multi-wavelength observations obtained from various ground and space-based telescopes. We found that the optical afterglow exhibited a smooth and steep rise, and then a gradual decay that could be described by a power-law function. This behavior is consistent with models of afterglows generated by jets of relativistic material, which are produced as a result of the interaction between an ultra-relativistic outflow and the surrounding medium.\n",
      "\n",
      "Additionally, through detailed spectral analysis, we found evidence of dust extinction in the host galaxy of the burst which indicates a large amount of dust in the environment of the progenitor star. We also observed the presence of broad absorption features in the afterglow spectra of GRB 060108 which can be attributed to intervening gas clouds within the host galaxy of the burst. \n",
      "\n",
      "Our findings suggest that the afterglow of GRB 060108 is a complex and unique event, with physical properties that challenge existing models of dark bursts. However, our study has shed new light on the mechanisms behind the production of afterglows and has contributed to our understanding of the complex physical processes that shape the universe. This information is critical in developing and testing models that could eventually aid our understanding of the universe and the phenomena occurring within it. 1 into PostgreSQL...\n",
      "Inserting test sample 2855  The sub-seasonal and synoptic-scale variability of the Indian summer monsoon rainfall are controlled primarily by monsoon intra-seasonal oscillations (MISO) and low pressure systems (LPS), respectively. The positive and negative phases of MISO lead to alternate epochs of above-normal (active) and below-normal (break) spells of rainfall. LPSs are embedded within the different phases of MISO and are known to produce heavy precipitation events over central India.\n",
      "\n",
      "Whether the interaction with the MISO phases modulates the precipitation response of LPSs, and thereby the characteristics of extreme rainfall events (EREs) remains unaddressed in the available literature. In this study, we analyze the LPSs that produce EREs of various spatial extents viz., Small, Medium, and Large over central India from 1979 to 2012. We also compare them with the LPSs that pass through central India and do not give any ERE (LPS-noex). We find that thermodynamic characteristics of LPSs that trigger different spatial extents of EREs are similar. However, they show differences in their dynamic characteristics. The ERE producing LPSs are slower, moister and more intense than LPS-noex. The LPSs that lead to Medium and Large EREs tend to occur during the positive phase of MISO when an active monsoon trough is present over central India. On the other hand, LPS-noex and the LPSs that trigger Small EREs occur mainly during the neutral or negative phases of the MISO. The large-scale dynamic forcing, intensification of LPSs, and diabatic generation of low-level potential vorticity due to the presence of active monsoon trough help in the organization of convection and lead to Medium and Large EREs. On the other hand, the LPSs that form during the negative or neutral phases of MISO do not intensify much during their lifetime and trigger scattered convection, leading to EREs of small size. 0 into PostgreSQL...\n",
      "Inserting test sample 2856  This study investigates the multiscale interactions between monsoon intra-seasonal oscillations (MISOs) and low pressure systems (LPSs) that produce heavy rainfall events of various spatial extents. We employ a combination of observational analysis, numerical modeling, and statistical methods to examine the temporal and spatial characteristics of MISOs and LPSs, as well as their associations and interactions. Our results reveal that MISOs and LPSs exhibit distinct but related spatiotemporal patterns, with the former dominating on intra-seasonal time scales and the latter on synoptic time scales. We also find that the interactions between MISOs and LPSs can lead to the amplification or suppression of rainfall over different regions depending on the intensity and phase of the MISO and the track and speed of the LPS. Additionally, we show that the coupled effects of MISOs and LPSs can result in complex precipitation patterns with multiple spatial scales and temporal scales, ranging from a few days to a few weeks. This study provides new insights into the multiscale interactions between MISOs and LPSs that contribute to the variability and predictability of heavy rainfall events in the monsoon region. Our findings have important implications for the improvement of regional monsoon forecasts and the management of water resources, as well as for understanding the impacts of climate change on extreme precipitation events. 1 into PostgreSQL...\n",
      "Inserting test sample 2857  For self maps of the disk, it can be shown that under the right conditions one can embed a discrete iteration of the map into a continuous semigroup. In this article we extend these results to two complex variables for maps of the unit ball into itself under some restricted conditions. 0 into PostgreSQL...\n",
      "Inserting test sample 2858  In this paper, we investigate the behavior of one-parameter semigroups in the context of two complex variables. Specifically, we explore the properties of these semigroups which preserve the positivity of certain Hermitian forms. Our results provide insights into the dynamics of complex systems and have applications in various fields such as operator theory and mathematical physics. 1 into PostgreSQL...\n",
      "Inserting test sample 2859  Supervised dimensionality reduction has emerged as an important theme in the last decade. Despite the plethora of models and formulations, there is a lack of a simple model which aims to project the set of patterns into a space defined by the classes (or categories). To this end, we set up a model in which each class is represented as a 1D subspace of the vector space formed by the features. Assuming the set of classes does not exceed the cardinality of the features, the model results in multi-class supervised learning in which the features of each class are projected into the class subspace. Class discrimination is automatically guaranteed via the imposition of orthogonality of the 1D class sub-spaces. The resulting optimization problem - formulated as the minimization of a sum of quadratic functions on a Stiefel manifold - while being non-convex (due to the constraints), nevertheless has a structure for which we can identify when we have reached a global minimum. After formulating a version with standard inner products, we extend the formulation to reproducing kernel Hilbert spaces in a straightforward manner. The optimization approach also extends in a similar fashion to the kernel version. Results and comparisons with the multi-class Fisher linear (and kernel) discriminants and principal component analysis (linear and kernel) showcase the relative merits of this approach to dimensionality reduction. 0 into PostgreSQL...\n",
      "Inserting test sample 2860  In this paper, we propose a category space approach to supervised dimensionality reduction which aims to improve the accuracy and efficiency of classification tasks. Our approach is based on the idea that the categories of a dataset contain valuable information that can be leveraged to reduce the dimensionality of the feature space. Specifically, we introduce a framework that constructs a category space from the original feature space and then learns a low-dimensional projection of the category space via a supervised learning algorithm. We demonstrate the efficacy of our approach on several benchmark datasets and show that it outperforms existing methods in terms of classification accuracy, running time, and scalability. Furthermore, we show that our approach is robust to noisy and incomplete data, making it suitable for real-world applications. To provide insights into the working mechanisms of our approach, we conduct extensive experiments and provide detailed analysis of the learned category space. Finally, we discuss the potential applications of our approach in various fields such as computer vision, natural language processing, and bioinformatics. Overall, our work presents a novel approach to supervised dimensionality reduction that utilizes category information and shows promising results in various applications. 1 into PostgreSQL...\n",
      "Inserting test sample 2861  In an effort to better determine the 7Be(p,gamma)8B reaction rate, we have performed inclusive and exclusive measurements of the Coulomb dissociation of 8B. The former was a study of longitudinal momentum distributions of 7Be fragments emitted in the Coulomb breakup of intermediate energy 8B beams on Pb and Ag targets. Analysis of these data yielded the E2 contribution to the breakup cross section. In the exclusive measurement, we determined the cross section for the Coulomb breakup of 8B on Pb at low relative energies in order to infer the astrophysical S factor for the 7Be(p,gamma)8B reaction.\n",
      "\n",
      "Interpreting the measurements with 1st-order perturbation theory, we obtained SE2/SE1 = 4.7 (+ 2.0,- 1.3) times 10^-4 at Erel = 0.6 MeV, and S17(0) = 17.8 (+ 1.4,- 1.2) eV b. Semiclassical 1st-order perturbation theory and fully quantum mechanical continuum-discretized coupled channels analyses yield nearly identical results for the E1 strength relevant to solar neutrino flux calculations, suggesting that theoretical reaction mechanism uncertainties need not limit the precision of Coulomb breakup determinations of the 7Be(p,gamma)8B S factor. A recommended value of S17(0) based on a weighted average of this and other measurements is presented. 0 into PostgreSQL...\n",
      "Inserting test sample 2862  The electromagnetic dissociation of 8B and its reaction rate with 7Be(p,Î³)8B in the Sun are important topics of study that are investigated in this research paper. Electromagnetic dissociation of 8B refers to the process of breaking down the 8B nucleus through electromagnetic interactions. The objective of the study is to investigate the rate of the 7Be(p,Î³)8B reaction by measuring the cross-section of the reaction. The rate of this reaction is crucial for understanding energy generation in massive stars and other astrophysical phenomena, and accurate measurements are essential for predicting the behavior of stars. This paper presents experimental results and theoretical models for calculating and validating the cross-section measurements. The measurements are found to be consistent with earlier experiments. Additional measurements are suggested, as well as opportunities to expand the present study to investigate other astrophysical consequences of the electromagnetic dissociation of 8B. The results of this study contribute to the understanding of nucleosynthesis and the production of elements in stars, as well as the evolution and dynamics of astrophysical systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2863  This paper is concerned with effects of noise on the solutions of partial differential equations. We first provide a sufficient condition to ensure the existence of a unique positive solution for a class of stochastic parabolic equations. Then, we prove that noise could induce singularities (finite time blow up of solutions). Finally, we show that a stochastic Allen-Cahn equation does not have finite time singularities and the unique solution exists globally. 0 into PostgreSQL...\n",
      "Inserting test sample 2864  This paper provides an analysis of the impacts of noise on a particular class of partial differential equations. Using analytical tools, we demonstrate that the introduction of noise can significantly alter the natural behavior of these equations. In particular, we focus on the role of stochastic effects in altering stability and pattern formation. Our findings have important implications for researchers studying physical systems governed by this class of equations. 1 into PostgreSQL...\n",
      "Inserting test sample 2865  We surveyed the Aquila Rift complex including the Serpens South and W40 region in the NH$_3$(1,1) and (2,2) transitions making use of the Nanshan 26-m telescope. The kinetic temperatures of the dense gas in the Aquila Rift complex range from 8.9 to 35.0K with an average of 15.3$\\pm$6.1K. Low gas temperatures associate with Serpens South ranging from 8.9 to 16.8K with an average 12.3$\\pm$1.7K, while dense gas in the W40 region shows higher temperatures ranging from 17.7 to 35.0K with an average of 25.1$\\pm$4.9 K. A comparison of kinetic temperatures against HiGal dust temperatures indicates that the gas and dust temperatures are in agreement in the low mass star formation region of Serpens South. In the high mass star formation region W40, the measured gas kinetic temperatures are higher than those of the dust. The turbulent component of the velocity dispersion of NH$_3$(1,1) is found to be positively correlated with the gas kinetic temperature, which indicates that the dense gas may be heated by dissipation of turbulent energy. For the fractional total-NH3 abundance obtained by a comparison with Herschel infrared continuum data representing dust emission we find values from 0.1 to 21$\\times 10^{-8}$ with an average of 6.9$(\\pm 4.5)\\times 10^{-8}$. Serpens South also shows a fractional total-NH3 abundance ranging from 0.2 to 21$\\times 10^{-8}$ with an average of 8.6($\\pm 3.8)\\times 10^{-8}$. In W40, values are lower, between 0.1 and 4.3$\\times 10^{-8}$ with an average of 1.6($\\pm 1.4)\\times 10^{-8}$. Weak velocity gradients demonstrate that the rotational energy is a negligible fraction of the gravitational energy. In W40, gas and dust temperatures are not strongly dependent on the projected distance to the recently formed massive stars. Overall, the morphology of the mapped region is ring-like, with strong emission at lower and weak emission at higher Galactic longitudes. 0 into PostgreSQL...\n",
      "Inserting test sample 2866  We present observations of the Aquila Rift cloud complex in the NHâ‚ƒ (1,1) and (2,2) inversion lines carried out using the Robert C. Byrd Green Bank Telescope. The observations cover an area spanning approximately 9.3 square degrees with a velocity resolution of 0.2 km/s. The NHâ‚ƒ (1,1) and (2,2) line emission is detected over a velocity range spanning from -41 km/s to -7 km/s and from -36 km/s to -9 km/s, respectively. A comparison between the two lines shows variations in the line ratios across the observed region. We identify 17 molecular ammonia clouds within the field-of-view, 9 of which have no previously reported detection in NHâ‚ƒ. The measured kinetic temperatures of the ammonia clouds have a median value of 13 K with a range of 5-20 K. The derived column densities have a median value of 2.1 x 10Â¹Â³ cmâ»Â² and a range of 0.7-4.5 x 10Â¹Â³ cmâ»Â².\n",
      "\n",
      "Additionally, we analyze the kinematic distance ambiguities using the available Galactic rotation curve models. The analysis shows that the near and far kinematic distance solutions are well separated in most of the observed region and hence, we can resolve the distance ambiguity for the ammonia clouds with good confidence. The derived physical properties, such as the size, velocity dispersion, and virial masses of the ammonia clouds, suggest that they are moderately supersonic and turbulent, with a median virial parameter of 1.1, implying a state of near virial equilibrium for most of the clouds.\n",
      "\n",
      "Finally, we discuss the possible heating mechanisms for the ammonia clouds. The results indicate that the ammonia clouds could be primarily heated externally by the interstellar radiation field, while the chemical heating of ammonia may play an important role in the cores of some of the clouds. The implications of our results for the understanding of the properties and evolution of molecular clouds are also discussed. 1 into PostgreSQL...\n",
      "Inserting test sample 2867  We use cosmological simulations in order to study the effects of supernova (SN) feedback on the formation of a Milky Way-type galaxy of virial mass ~10^12 M_sun/h. We analyse a set of simulations run with the code described by Scannapieco et al. (2005, 2006), where we have tested our star formation and feedback prescription using isolated galaxy models. Here we extend this work by simulating the formation of a galaxy in its proper cosmological framework, focusing on the ability of the model to form a disk-like structure in rotational support. We find that SN feedback plays a fundamental role in the evolution of the simulated galaxy, efficiently regulating the star formation activity, pressurizing the gas and generating mass-loaded galactic winds. These processes affect several galactic properties such as final stellar mass, morphology, angular momentum, chemical properties, and final gas and baryon fractions. In particular, we find that our model is able to reproduce extended disk components with high specific angular momentum and a significant fraction of young stars. The galaxies are also found to have significant spheroids composed almost entirely of stars formed at early times. We find that most combinations of the input parameters yield disk-like components, although with different sizes and thicknesses, indicating that the code can form disks without fine-tuning the implemented physics. We also show how our model scales to smaller systems. By analysing simulations of virial masses 10^9 M_sun/h and 10^10 M_sun/h, we find that the smaller the galaxy, the stronger the SN feedback effects. 0 into PostgreSQL...\n",
      "Inserting test sample 2868  The formation of galaxy disks is a result of complex processes involving various physical phenomena, among which supernova feedback plays a crucial role. The effects of supernova feedback on the formation of galaxy disks have been explored through numerous theoretical and observational studies. In this study, we investigate the impact of supernova feedback on the gas and stellar properties of disk galaxies using a series of hydrodynamic simulations. Our simulations reveal that supernova explosions, by injecting energy and momentum into the interstellar medium, can drive large-scale outflows of gas, which in turn affect the mass, velocity structure, and chemical enrichment of the disk. Moreover, supernova feedback is found to regulate the star formation rate by heating and dispersing the gas necessary for star formation. Our analysis also shows that the efficiency of supernova feedback depends on various factors, such as the star formation rate, gas fraction, and stellar surface density. By comparing our simulations with observational data of nearby disk galaxies, we find that our model predictions are in good agreement with the observed correlations between the supernova rate, gas content, and star formation activity of galaxies. Our results provide new insights into the mechanisms governing the formation and evolution of galaxy disks, and have important implications for our understanding of galaxy formation and evolution in the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2869  We present the results of the chi2 minimization model fitting technique applied to optical and near-infrared photometric and radial velocity data for a sample of 9 fundamental and 3 first overtone classical Cepheids in the Small Magellanic Cloud (SMC). The near- infrared photometry (JK filters) was obtained by the European Southern Observatory (ESO) public survey \"VISTA near-infrared Y; J;Ks survey of the Magellanic Clouds system\"(VMC). For each pulsator isoperiodic model sequences have been computed by adopting a nonlinear convective hydrodynamical code in order to reproduce the multi- filter light and (when available) radial velocity curve amplitudes and morphological details. The inferred individual distances provide an intrinsic mean value for the SMC distance modulus of 19.01 mag and a standard deviation of 0.08 mag, in agreement with the literature. Moreover the instrinsic masses and luminosities of the best fitting model show that all these pulsators are brighter than the canonical evolutionary Mass- Luminosity relation (MLR), suggesting a significant efficiency of core overshooting and/or mass loss. Assuming that the inferred deviation from the canonical MLR is only due to mass loss, we derive the expected distribution of percentage mass loss as a function of both the pulsation period and of the canonical stellar mass. Finally, a good agreement is found between the predicted mean radii and current Period-Radius (PR) relations in the SMC available in the literature. The results of this investigation support the predictive capabilities of the adopted theoretical scenario and pave the way to the application to other extensive databases at various chemical compositions, including the VMC Large Magellanic Cloud pulsators and Galactic Cepheids with Gaia parallaxes. 0 into PostgreSQL...\n",
      "Inserting test sample 2870  The aim of this study is to model the light and radial velocity curves of classical Cepheids in the Small Magellanic Cloud (SMC) using the VISTA Magellanic Clouds (VMC) survey. The VMC survey provides multi-band, near-infrared photometric data for a large sample of Cepheids in the SMC, which are essential for establishing their pulsational properties. We apply the Fourier decomposition method to the light curves of 897 classical Cepheids in the SMC to determine the pulsation mode, and find the majority of Cepheids are fundamental mode pulsators with a small fraction of first-overtone pulsators. By fitting the light and radial velocity curves simultaneously, we are able to estimate physical parameters, including the distance and metallicity of the SMC. The results of our model fitting show that the distance to the SMC is 62.1 kpc with a metallicity of [Fe/H] = âˆ’0.75, which is consistent with previous studies.\n",
      "\n",
      "The inferred physical parameters of the SMC are compared with those obtained using other classical Cepheids and different methodologies. The agreement is good for both the distance modulus and the metallicity, suggesting the robustness of the VMC survey and the reliability of our analysis. Finally, we estimate the Hubble constant using the SMC Cepheids, and obtain a value of H0 = 72.4 Â± 2.5 km sâˆ’1 Mpcâˆ’1, in good agreement with the value derived from the cosmic microwave background radiation data in the context of the Î›CDM model. Our study demonstrates the power of combining photometric and spectroscopic data to estimate physical parameters of Cepheids in the SMC, and provides insights into the nature and evolution of this nearby galaxy. 1 into PostgreSQL...\n",
      "Inserting test sample 2871  We present high resolution 11.7 and 18.3um mid-IR images of SN 1987A obtained on day 6526 with T-ReCS attached to the Gemini telescope. The 11.7um flux has increased significantly since our last observations on day 6067. The images clearly show that all the emission arises from the equatorial ring (ER).\n",
      "\n",
      "Spectra obtained with Spitzer, on day 6184 with MIPS at 24um, on day 6130 with IRAC in the 3.6-8um region, and on day 6190 with IRS in the 12-37um region show that the emission consists of thermal emission from silicate dust that condensed out in the red giant wind of the progenitor star. The dust temperature is ~166K, and the emitting dust mass is ~2.6 x 10-6 Msun. Lines of [Ne II]12.82um and [Ne III]15.56um are clearly present, as well as a weak [Si II]34.8um line. We also detect two lines near 26um which we tentatively ascribe to [Fe II]25.99um and [O IV]25.91um. Comparison of the Gemini 11.7um image with X-ray images from Chandra, UV-optical images from HST, and radio synchrotron images obtained by the ATCA show generally good correlation of the images across all wavelengths. Because of the limited resolution of the mid-IR images we cannot uniquely determine the location or heating mechanism of the dust giving rise to the emission. The dust could be collisionally heated by the X-ray emitting plasma, providing a unique diagnostic of plasma conditions.\n",
      "\n",
      "Alternatively, the dust could be radiatively heated in the dense UV-optical knots that are overrun by the advancing supernova blast wave. In either case the dust-to-gas mass ratio in the circumstellar medium around the supernova is significantly lower than that in the general ISM of the LMC, suggesting either a low condensation efficiency in the wind of the progenitor star, or the efficient destruction of the dust by the SN blast wave. 0 into PostgreSQL...\n",
      "Inserting test sample 2872  In 1987, a supernova explosion designated SN 1987A was observed in the Large Magellanic Cloud, a neighbouring galaxy to the Milky Way. After 18 years, the remnant of this explosion was observed using the mid-infrared spectrometers on the Gemini South telescope and the Spitzer Space Telescope. These observations provide a unique view of the evolving supernova debris as it interacts with the surrounding environment.\n",
      "\n",
      "The data reveal a complex morphology of the remnant, consisting of multiple contributions from different components. The mid-infrared spectrum exhibits emission lines from atomic and molecular gas, as well as broad continua from dust. The detailed spatial distribution of the emission features indicate that the SN 1987A remnant has been involved in significant interactions with its surroundings.\n",
      "\n",
      "The Spitzer observations reveal that the dust species in the remnant consist primarily of silicates, along with indications of partially hydrogenated amorphous carbon grains. The identified dust-feature bands and the dust continuum determine the temperature of the dust grains, providing insights into their heating and cooling mechanisms.\n",
      "\n",
      "The mid-infrared observations also show the complex structure of the debris surrounding the supernova site, indicating that it interacts in a complicated way with the surrounding molecular cloud. The spectral features of the debris reveal the presence of shock-heated molecular hydrogen gas.\n",
      "\n",
      "Overall, these mid-infrared and Spitzer observations of the SN 1987A remnant provide an unprecedented view of the complex morphology of the remnant and its interactions with its environment. These findings broaden our understanding of the dynamical evolution of supernova explosions and the long-term impact they have on the interstellar medium. 1 into PostgreSQL...\n",
      "Inserting test sample 2873  We have made experimental observations of the force networks within a two-dimensional granular silo similar to the classical system of Janssen.\n",
      "\n",
      "Models like that of Janssen predict that pressure within a silo saturates with depth as the result of vertical forces being redirected to the walls of the silo where they can then be carried by friction. By averaging ensembles of experimentally-obtained force networks in different ways, we compare the observed behavior with various predictions for granular silos. We identify several differences between the mean behavior in our system and that predicted by Janssen-like models: We find that the redirection parameter describing how the force network transfers vertical forces to the walls varies with depth. We find that changes in the preparation of the material can cause the pressure within the silo to either saturate or to continue building with depth. Most strikingly, we observe a non-linear response to overloads applied to the top of the material in the silo. For larger overloads we observe the previously reported \"giant overshoot\" effect where overload pressure decays only after an initial increase [G. Ovarlez et al., Phys. Rev. E 67, 060302(R) (2003)]. For smaller overloads we find that additional pressure propagates to great depth.\n",
      "\n",
      "This effect depends on the particle stiffness, as given for instance by the Young's modulus, E, of the material from which the particles are made.\n",
      "\n",
      "Important measures include E, the unscreened hydrostatic pressure, and the applied load. These experiments suggest that when the load and the particle weight are comparable, particle elasticity acts to stabilize the force network, allowing non-linear network effects to be seen in the mean behavior. 0 into PostgreSQL...\n",
      "Inserting test sample 2874  This research investigates the role of force networks in the elasticity of granular silos. Specifically, we explore the relationship between the existence and properties of these force networks and the behavior of the silo under external load. To perform the study, we experimentally measure the stress and deformation characteristics of the granular material while varying its properties such as size distribution, particle shape, and material properties. The results show that force networks are an intrinsic property of granular materials and their presence is essential to the elasticity of the silo. Furthermore, we find that the characteristics of the force networks depend on the aforementioned physical properties of the material. In particular, the size distribution of the grains has a significant effect on the formation and properties of the force networks. Our results suggest that the elasticity of granular silos can be significantly improved by optimizing the size distribution of the granular material and identifying particle shapes which promote strong force networks. Finally, we also discuss the implications of our findings for the design of silos and other components in which granular materials are stored or transported. Overall, this research provides a deeper understanding of the internal mechanics of granular silos and constitutes the foundation for more efficient and reliable designs in the future. 1 into PostgreSQL...\n",
      "Inserting test sample 2875  The Red-Sequence Cluster Survey (RCS) is a 100 deg$^2$ optical survey for high-redshift galaxy clusters. One of the goals of the survey is a measurement of $\\Omega_m$ and $\\sigma_8$ via the evolution of the mass spectrum of galaxy clusters. Herein we briefly describe how this will initially be done, and also demonstrate the eventual power of the RCS for this type of measurement by a qualitative analysis of the first 1/10th of the survey data. 0 into PostgreSQL...\n",
      "Inserting test sample 2876  We present the results from the Red-Sequence Cluster Survey in determining the values of $\\Omega_m$ and $\\sigma_8$. Our analysis used a sample of 406 X-ray-selected galaxy clusters in the redshift range 0.1â‰²zâ‰²1.3. We found that the combination of X-ray and optical data provides tight constraints on cosmological parameters, resulting in $\\Omega_m=0.30\\pm0.03$ and $\\sigma_8=0.77\\pm0.03$. Our results imply a universe with a moderate matter content and a low-amplitude density perturbations. 1 into PostgreSQL...\n",
      "Inserting test sample 2877  We investigate a possible use of direct photon production in association with a heavy quark in $pA$ collisions at the large hadron collider to constrain the nuclear gluon parton distribution function. This process is sensitive to both, the nuclear heavy quark and gluon parton distribution functions and is a very promising candidate to help determine the gluon nuclear PDF which is still largely untested. 0 into PostgreSQL...\n",
      "Inserting test sample 2878  This paper investigates the gluon nuclear parton distribution function (PDF) by studying the production of direct photons in association with a heavy quark. The process is particularly sensitive to the nuclear modifications of the gluon PDF, which can shed light on the confinement of the quarks inside nuclei. We present theoretical calculations and compare them to existing experimental data. Results show good agreement with data and provide new constraints on the nuclear PDFs. 1 into PostgreSQL...\n",
      "Inserting test sample 2879  We show that many-body localization (MBL) effects can be observed in a finite chain of exchange-coupled spin qubits in the presence of both exchange and magnetic noise, a system that has been experimentally realized in semiconductors and is a potential solid-state quantum computing platform. In addition to established measures of MBL, the level spacing ratio and the entanglement entropy, we propose another quantity, the spin-spin correlation function, that can be measured experimentally and is particularly well-suited to experiments in semiconductor-based electron spin qubit systems. We show that, in cases that the established measures detect as delocalized \"phases\", the spin-spin correlation functions retain no memory of the system's initial state (i.e., the long-time value deviates significantly from the initial value), but that they do retain memory in cases that the established measures detect as localized \"phases\". We also discover an interesting counterintuitive result that there is no clear tendency towards localization with increasing charge noise in small systems ($3$--$10$ spins). The proposed experiments should be feasible in the existing semiconductor spin qubit systems. 0 into PostgreSQL...\n",
      "Inserting test sample 2880  The study and manipulation of qubits are at the forefront of quantum computing research. In this work, we investigate the properties of many-body localization in exchange-coupled electron spin qubits. Specifically, we focus on the use of spin-spin correlations to probe the quantum phase transition between the localized and delocalized regimes. We present a theoretical model, and then simulate the behavior of spin chains using Monte Carlo techniques. Our results demonstrate that the spin-spin correlations are a robust measure of the localization properties of the spin chains, and that they can be used to accurately predict the behavior of these systems. Furthermore, we propose a new experimental approach that allows for the direct observation of the spin-spin correlations in electron spin qubits. Additionally, we show how the experimental approach can be extended to investigate the effect of disorder on the many-body localization in these systems. Overall, our study highlights the significant potential for using spin-spin correlations as a diagnostic tool to study quantum phase transitions in electron spin qubits. 1 into PostgreSQL...\n",
      "Inserting test sample 2881  We present the discovery of the optical afterglow and host galaxy of the {\\it Swift} short-duration gamma-ray burst, GRB\\,181123B. Observations with Gemini-North starting at $\\approx 9.1$~hr after the burst reveal a faint optical afterglow with $i\\approx25.1$~mag, at an angular offset of 0.59 $\\pm$ 0.16$''$ from its host galaxy. Using $grizYJHK$ observations, we measure a photometric redshift of the host galaxy of $z = 1.77^{+0.30}_{-0.17}$. From a combination of Gemini and Keck spectroscopy of the host galaxy spanning 4500-18000~\\AA , we detect a single emission line at 13390~\\AA, inferred as H$\\beta$ at $z = 1.754 \\pm 0.001$ and corroborating the photometric redshift.\n",
      "\n",
      "The host galaxy properties of GRB\\,181123B are typical to those of other SGRB hosts, with an inferred stellar mass of $\\approx 1.7 \\times 10^{10}\\,M_{\\odot}$, mass-weighted age of $\\approx 0.9$~Gyr and optical luminosity of $\\approx 0.9L^{*}$. At $z=1.754$, GRB\\,181123B is the most distant secure SGRB with an optical afterglow detection, and one of only three at $z>1.5$. Motivated by a growing number of high-$z$ SGRBs, we explore the effects of a missing $z>1.5$ SGRB population among the current {\\it Swift} sample on delay time distribution models. We find that log-normal models with mean delay times of $\\approx 4-6$~Gyr are consistent with the observed distribution, but can be ruled out to $95\\%$ confidence with an additional $\\approx1-5$~{\\it Swift} SGRBs recovered at $z>1.5$. In contrast, power-law models with $\\propto$ $t^{-1}$ are consistent with the redshift distribution and can accommodate up to $\\approx30$ SGRBs at these redshifts. Under this model, we predict that $\\approx 1/3$ of the current {\\it Swift} population of SGRBs is at $z>1$. The future discovery or recovery of existing high-$z$ SGRBs will provide significant discriminating power on their delay time distributions, and thus their formation channels. 0 into PostgreSQL...\n",
      "Inserting test sample 2882  The search for gamma-ray burst (GRB) progenitors is a key problem in high-energy astrophysics. Here, we present the discovery of the optical and host galaxy of the short GRB181123B, one of the most energetic events ever recorded, at redshift $z=1.754$. Our study of the afterglow reveals a faint and relatively blue emission, with a possible break at around one day after the burst trigger. Follow-up observations with the Gemini and Keck telescopes allowed us to identify the host galaxy, a faint system with strong emission lines, which suggests that it is actively forming stars. Its redshift is consistent with that of the GRB and lies in a range where most of the cosmic star formation occurred. We discuss the implications of our observations for the study of the delay time distributions (DTDs) of compact binary mergers. Based on population synthesis models, it is expected that the fraction of short GRBs associated with old stellar environments should be higher than that of long GRBs, which largely arise from massive stars in star-forming regions. Our results are consistent with this scenario and support the idea that short GRBs are produced by mergers of compact objects in old stellar environments. In addition, we compare our data with theoretical predictions of the afterglow emission and find that some models can be ruled out. Finally, we highlight the potential of future observations with the upcoming facilities, such as the Large Synoptic Survey Telescope (LSST) and the James Webb Space Telescope (JWST), which will enable us to study the properties of GRB host galaxies with unprecedented accuracy. Overall, our study provides an important contribution to the understanding of the origins of short GRBs and their environment. 1 into PostgreSQL...\n",
      "Inserting test sample 2883  It is demonstrated that deep circulation mixing below the base of the standard convective envelope, and the consequent \"cool bottom processing\" (CBP) of the CNO isotopes, can reproduce the trend with stellar mass of the C-12/C-13 observations in low mass red giants. (This trend is opposite to what is expected from standard first dredge-up.) Our models assume that extra mixing always reaches to the same distance in temperature from the H-burning shell, and that CBP begins when the H-burning shell erases the molecular weight discontinuity (\"mu-barrier\") established by first dredge-up. For Pop I stars, none of the other CNO isotopes except N-15 are expected to be altered by CBP.\n",
      "\n",
      "(If O-18 depletion occurs on the AGB, as some observations suggest, it would require that extra mixing reach closer to the H-burning shell on the AGB than on the RGB --- and should also result in a much lower C-12/C-13 ratio than is observed.) CBP increases as one reduces the stellar mass or metallicity --- roughly as 1/M^2 on the RGB, due to the longer RGB of low mass stars, and as 1/Z, due to higher H-shell burning temperatures at low metallicity. In low mass Pop II stars, all the CNO isotopes are expected to be altered by CBP. Field Pop II stars exhibit RGB abundances consistent with the predictions of our CBP models that have been normalized to reproduce the Pop I RGB abundances, but globular cluster stars are observed to encounter much more extensive processing; also, CBP is observed to start near the base of the globular cluster RGB (overcoming any \"mu-barrier\").\n",
      "\n",
      "Standard first and second dredge-up are also presented, and enrichment of the interstellar medium, relative to SN. For light elements, see astro-ph/9512122. 0 into PostgreSQL...\n",
      "Inserting test sample 2884  The isotopes of carbon, nitrogen, and oxygen, collectively referred to as CNO isotopes, act as important probes for better understanding the workings of deep circulation in red giant stars, particularly during the first and second dredge-up phases. The current work aims to investigate the nucleosynthesis processes involved in producing CNO isotopes in detail.\n",
      "\n",
      "Using data from theoretical models for giant stars and observations of evolved giants, we investigate the variations of CNO isotopic ratios as a function of mass and metallicity. The models are consistent with observations for a range of metallicities and provide key insights into the convective behavior of gases within the deep layers of red giant stars. We find that the enrichment of heavy elements, including CNO isotopes, depends on several factors such as the strength of the mixing, the convective overshooting, as well as the mass and metallicity of the star.\n",
      "\n",
      "In addition, we examine the effects of processes such as hot-bottom burning, extra-mixing and semi-convection on the nucleosynthesis of CNO isotopes. These mechanisms play crucial roles in transport processes and chemical reactions, therefore influencing the final isotopic compositions. Through our analysis, we obtain new and improved theoretical predictions for CNO isotopic ratios in red giant stars, particularly for stars of lower mass, sub-solar metallicity and with main sequence masses of approximately one solar mass.\n",
      "\n",
      "Our results provide important insights into the interplay between nuclear reactions, mixing, and diffusion processes in red giant stars, leading to a better understanding of the evolution of such stars. Moreover, we discuss the implications of our findings in the context of galactic and cosmic chemical evolution. The present work also highlights the need for further theoretical modeling of CNO isotopes and observations of evolved stars to improve our knowledge of these important probes of stellar evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 2885  We present a detailed analysis of the Horizontal Branch of the Carina Dwarf Spheroidal Galaxy by means of synthetic modelling techniques, taking consistently into account the star formation history and metallicity evolution as determined from main sequence and red giant branch spectroscopic observations. We found that a range of integrated red giant branch mass loss values of 0.1-0.14 M, increasing with metallicity, is able to reproduce the colour extension of the old Horizontal Branch. However, leaving the mass loss as the only free parameter is not enough to match the detailed morphology of Carina Horizontal Branch. We explored the role played by the star formation history on the discrepancies between synthetic and observed Horizontal Branches. We derived a toy bursty star formation history that reproduces the horizontal branch star counts, and also matches qualitatively the red giant and the turn off regions. This star formation history is made of a subset of age and [M/H] components of the star formation history based on turn off and red giants only, and entails four separate bursts of star formation of different strenghts, centred at 2, 5, 8.6 and 11.5 Gyr, with mean [M/H] decreasing from \\sim -1.7 to \\sim -2.2 for increasing ages, and a Gaussian spread of 0.1 dex.\n",
      "\n",
      "The comparison between the metallicity distribution function of our star formation history and the one measured from the infrared CaT feature using a CaT-[Fe/H] calibration shows a qualitative agreement, once taken into account the range of [Ca/Fe] abundances measured in a sample of Carina stars, that biases the derived [Fe/H] distribution toward too low values. In conclusion, we have shown how the information contained within the horizontal branch of Carina (and dwarf galaxies in general) can be extracted and interpreted to refine the star formation history derived from red giants and turn off stars only.\n",
      "\n",
      "Abridged 0 into PostgreSQL...\n",
      "Inserting test sample 2886  The inclusion of Horizontal Branch (HB) stars in the derivation of star formation histories (SFHs) of dwarf galaxies can provide important insights into the formation and evolution of these systems. In this study, we present the SFH of the Carina dwarf spheroidal galaxy (dSph) using photometry of resolved stars from the Hubble Space Telescope Advanced Camera for Surveys.\n",
      "\n",
      "We derive the SFH of Carina by fitting the observed color-magnitude diagrams (CMDs) with synthetic CMDs constructed from theoretical isochrones that include the effect of HB stars. Our analysis shows that Carina experienced a prolonged period of star formation, lasting between 8 and 12 Gyr ago, followed by a long quiescent period. There is some evidence for a modest resurgence of star formation roughly 2-4 Gyr ago, but the significance of this feature is uncertain.\n",
      "\n",
      "We compare our SFH results with previous studies of Carina that excluded HB stars from the analysis. Our inclusion of HB stars leads to a lower overall star formation rate and an extended period of quiescence relative to previous studies. These differences highlight the importance of including HB stars in SFH studies of dwarf galaxies.\n",
      "\n",
      "We discuss potential sources of systematic uncertainty in our analysis and explore the implications of our results for our understanding of the evolution of dwarf galaxies. We find that Carina's SFH is broadly consistent with a scenario in which these systems formed the bulk of their stars early on and then evolved passively for several billion years. This picture is generally consistent with the predictions of cosmological simulations, but further observations of other dwarf galaxies are needed to test this hypothesis more rigorously.\n",
      "\n",
      "In summary, our analysis highlights the importance of including HB stars in the derivation of SFHs of dwarf galaxies and provides new insights into the formation and evolution of these systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2887  The nonequilibrium photon Green function for a bounded medium surrounded by vacuum is analyzed on the basis of the Dyson equation. As its components, the field-field fluctuations as well as the spectral function split up into parts related to medium and vacuum. Particularly, it is shown that the vacuum-induced fluctuations describe propagation of arbitrary, even nonclassical light in terms of solutions of the classical wave propagation problem. The results generalize previously obtained ones for steadily excited media in slab geometry. 0 into PostgreSQL...\n",
      "Inserting test sample 2888  We derive an exact property of the nonequilibrium photon Green function for bounded media. This property allows us to analyze the photon dynamics in a system using only the Green function near the boundaries, and neglecting the bulk behavior. We show that the resulting approximation is accurate for a wide range of systems and boundary conditions. Our results provide a valuable tool for future investigations of nonequilibrium photon transport in semiconductor devices and other photonic structures, with potential applications in optical computing and communication. 1 into PostgreSQL...\n",
      "Inserting test sample 2889  The research on relativistic universe models with viscous fluids is reviewed.\n",
      "\n",
      "Viscosity may have been of significance during the early inflationary era, and may also be of importance for the late time evolution of the Universe. Bulk viscosity and shear viscosity cause exponential decay of anisotropy, while nonlinear viscosity causes power-law decay of anisotropy. We consider also the influence from turbulence, in connection with future singularities of the universe (Big Rip and Little Rip). Finally, we review some recent developments of causal cosmology theories. 0 into PostgreSQL...\n",
      "Inserting test sample 2890  We investigate the dynamics of relativistic viscous fluids in the context of the universe's large-scale structure formation. We analyze a set of equations of motion for the matter sector that includes both the perfect and viscous fluids, as well as their interaction with gravity. Our results demonstrate that considering viscous effects could significantly influence the evolution of the universe's structure, leading to novel predictions for the cosmic large-scale phenomena. Our study suggests the importance of properly accounting for the non-ideal fluid properties in theoretical models of the universe's evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 2891  This article makes a dual contribution to scholarship in science and technology studies (STS) on simulation-building. It both documents a specific simulation-building project, and demonstrates a concrete contribution to interdisciplinary work of STS insights. The article analyses the struggles that arise in the course of determining what counts as theory, as model and even as a simulation. Such debates are especially decisive when working across disciplinary boundaries, and their resolution is an important part of the work involved in building simulations. In particular, we show how ontological arguments about the value of simulations tend to determine the direction of simulation-building. This dynamic makes it difficult to maintain an interest in the heterogeneity of simulations and a view of simulations as unfolding scientific objects. As an outcome of our analysis of the process and reflections about interdisciplinary work around simulations, we propose a chart, as a tool to facilitate discussions about simulations. This chart can be a means to create common ground among actors in a simulation-building project, and a support for discussions that address other features of simulations besides their ontological status. Rather than foregrounding the chart's classificatory potential, we stress its (past and potential) role in discussing and reflecting on simulation-building as interdisciplinary endeavor. This chart is a concrete instance of the kinds of contributions that STS can make to better, more reflexive practice of simulation-building. 0 into PostgreSQL...\n",
      "Inserting test sample 2892  Learning in a Landscape: Simulation-building as Reflexive Intervention\n",
      "\n",
      "This paper explores the use of simulation-building as a reflexive intervention tool for learning in complex and dynamic landscapes. Drawing on theories of constructivism, situated learning, and systems thinking, we argue that simulation-building provides a unique opportunity for learners to engage in reflection, collaborative problem-solving, and sense-making in an immersive and interactive environment.\n",
      "\n",
      "We present a conceptual framework for simulation-building as reflexive intervention, which integrates the principles of design thinking, feedback loops, and emergent learning. We illustrate the framework with a case study of a simulation-building project conducted with a group of graduate students in environmental management. The simulation-building project involved the development of a digital model of a coastal ecosystem, which enabled the students to explore the complex interactions between biophysical, social, and economic factors in resource management.\n",
      "\n",
      "Our analysis shows that simulation-building as a reflexive intervention can enhance learners' understanding of the complexity and uncertainty of real-world problems, facilitate the development of systems thinking skills, and promote the co-construction of knowledge among participants. We conclude by discussing the implications of simulation-building as a reflexive intervention for educational practice and research, and recommending further studies to investigate its effectiveness in different contexts and domains. 1 into PostgreSQL...\n",
      "Inserting test sample 2893  We predict the metallicity distribution of stars and the age-metallicity relation for 6 Dwarf Spheroidal (dSph) galaxies of the Local Group by means of a chemical evolution model which is able to reproduce several observed abundance ratios and the present day total mass and gas content of these galaxies. The model adopts up to date nucleosynthesis and takes into account the role played by supernovae of different types (II, Ia) allowing us to follow in detail the evolution of several chemical elements (H, D, He, C, N, O, Mg, Si, S, Ca, and Fe). Each galaxy model is specified by the prescriptions of the star formation rate and by the galactic wind efficiency chosen to reproduce the main features of these galaxies. These quantities are constrained by the star formation histories of the galaxies as inferred by the observed color-magnitude diagrams (CMD). The main conclusions are: i) 5 of the 6 dSphs galaxies are characterized by very low star formation efficiencies ($\\nu = 0.005 - 0.5 Gyr ^{-1}$) with only Sagittarius having a higher one ($\\nu = 1.0 - 5.0 Gyr ^{-1}$); ii) the wind efficiency is high for all galaxies, in the range $w_i$ = 6 - 15; iii) a high wind efficiency is required in order to reproduce the abundance ratios and the present day gas mass of the galaxies; iv) the predicted age-metallicity relation implies that the stars of the dSphs reach solar metallicities in a time-scale of the order of 2 - 6 Gyr; v) the metallicity distributions of stars in dSphs exhibit a peak around [Fe/H] $\\sim$ -1.8 to -1.5 dex, with the exception of Sagittarius ([Fe/H] $\\sim$ -0.8 dex); iv) the predicted metallicity distributions of stars suggest that the majority of stars in dSphs are formed in a range of metallicity in agreement with the one of the observed stars. 0 into PostgreSQL...\n",
      "Inserting test sample 2894  Galaxies are vast structures containing billions of stars, and their physical properties provide important insights into the nature of the universe. Dwarf spheroidal (dSph) galaxies, as part of this large population, have the advantage of being relatively nearby and well-studied, facilitating the exploration of their properties in detail. In this study, we aim to simulate the metallicity distribution of stars in the dSph galaxies using state-of-the-art models based on observational data.\n",
      "\n",
      "We begin by constructing a sample of dSph galaxies that is both representative and sufficiently numerous for the purposes of our study. We then use the inferred parameters of these galaxies, such as their velocity dispersion and luminosity, as inputs into our model. Our model is based on the chemical enrichment of the interstellar medium by successive generations of stars and takes into account the effects of feedback from supernovae and other astrophysical processes.\n",
      "\n",
      "We predict the metallicity distribution of stars using our model and compare it with observational data from a variety of sources, such as the Sloan Digital Sky Survey and Gaia. Our results indicate that the predicted distribution is in good agreement with data from these surveys, and suggest that the dSph galaxies are dominated by low-metallicity stars. This result is consistent with theoretical predictions based on the hierarchical structure formation of the universe and the formation of these galaxies through accretion processes.\n",
      "\n",
      "Our study sheds light on the physical properties of the dSph galaxies and provides important insights into the formation and evolution of galaxies in general. The success of our model in reproducing the observed metallicity distribution of stars in dSph galaxies suggests that it can be used to make predictions for other types of galaxies as well. Our model can also be used to test various astrophysical hypotheses and refine our understanding of the nature of the universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2895  The following question is due to Chatterjee and Varadhan (2011). Fix $0<p<r<1$ and take $G\\sim G(n,p)$, the Erd\\H{o}s-R\\'enyi random graph with edge density $p$, conditioned to have at least as many triangles as the typical $G(n,r)$. Is $G$ close in cut-distance to a typical $G(n,r)$? Via a beautiful new framework for large deviation principles in $G(n,p)$, Chatterjee and Varadhan gave bounds on the replica symmetric phase, the region of $(p,r)$ where the answer is positive. They further showed that for any small enough $p$ there are at least two phase transitions as $r$ varies.\n",
      "\n",
      "We settle this question by identifying the replica symmetric phase for triangles and more generally for any fixed $d$-regular graph. By analyzing the variational problem arising from the framework of Chatterjee and Varadhan we show that the replica symmetry phase consists of all $(p,r)$ such that $(r^d,h_p(r))$ lies on the convex minorant of $x\\mapsto h_p(x^{1/d})$ where $h_p$ is the rate function of a binomial with parameter $p$. In particular, the answer for triangles involves $h_p(\\sqrt{x})$ rather than the natural guess of $h_p(x^{1/3})$ where symmetry was previously known. Analogous results are obtained for linear hypergraphs as well as the setting where the largest eigenvalue of $G\\sim G(n,p)$ is conditioned to exceed the typical value of the largest eigenvalue of $G(n,r)$. Building on the work of Chatterjee and Diaconis (2012) we obtain additional results on a class of exponential random graphs including a new range of parameters where symmetry breaking occurs. En route we give a short alternative proof of a graph homomorphism inequality due to Kahn (2001) and Galvin and Tetali (2004). 0 into PostgreSQL...\n",
      "Inserting test sample 2896  The phenomenon of large deviations has found a wide range of applications in modern probability theory. In this paper, we explore replica symmetry properties in the context of large deviations in random graphs. Specifically, we study the probability distribution of the empirical measure associated with the random graph. We show that the replica symmetrical solution of this probability distribution is valid under certain conditions on the average degree of the graph. We also establish a connection between the replica symmetry of the empirical measure and the replica symmetry for the partition function of the random graph.\n",
      "\n",
      "To obtain these results, we exploit the well-established technique of replica symmetry analysis and apply it to the setting of random graphs. Our analysis relies on the systematic exploitation of the low-density expansion of the random graph. Through this expansion, we are able to derive an explicit expression for the replica symmetry breaking parameter, which serves as the key to establishing the replica symmetry of the empirical measure.\n",
      "\n",
      "In addition, we investigate the question of uniqueness of the replica-symmetric solution. We show that up to a certain critical value of the average degree, the solution is unique. Beyond this critical value, additional solutions emerge that break the replica symmetry, thereby indicating that the phenomenon of replica symmetry breaking is relevant in the context of large deviations in random graphs.\n",
      "\n",
      "Our results provide valuable insight into the behavior of large deviations in random graphs and pave the way for future research in this area. In particular, our work suggests avenues for further exploration of replica symmetry and replica symmetry breaking in related settings. 1 into PostgreSQL...\n",
      "Inserting test sample 2897  In this paper we consider the characteristic polynomial of not necessarily ranked posets. We do so by allowing the rank to be an arbitrary function from the poset to the nonnegative integers. We will prove two results showing that the characteristic polynomial of a poset has nonnegative integral roots. Our factorization theorems will then be used to show that any interval of the Tamari lattice has a characteristic polynomial which factors in this way. Blass and Sagan's result about LL lattices will also be shown to be a consequence of our factorization theorems. Finally we will use quotient posets to give unified proofs of some classic M\\\"obius function results. 0 into PostgreSQL...\n",
      "Inserting test sample 2898  Posets or partially ordered sets are common mathematical structures that find interesting applications in various fields. Quotient posets are a class of posets obtained by identifying certain elements in a poset. They arise naturally in various mathematical and computer science settings, such as in combinatorics, graph theory, and order theory. This paper discusses the various applications of quotient posets in these fields, including the computation of generating functions, enumeration of certain types of graphs and labeled posets, and the study of order-theoretic properties of partially ordered sets. We also explain some of the main results in this area and provide pointers to further research. 1 into PostgreSQL...\n",
      "Inserting test sample 2899  We study models of spatial growth processes where initially there are sources of growth (indicated by the colour green) and sources of a growth-stopping (paralyzing) substance (indicated by red). The green sources expand and may merge with others (there is no `inter-green' competition). The red substance remains passive as long as it is isolated. However, when a green cluster comes in touch with the red substance, it is immediately invaded by the latter, stops growing and starts to act as red substance itself. In our main model space is represented by a graph, of which initially each vertex is randomly green, red or white (vacant), and the growth of the green clusters is similar to that in first-passage percolation. The main issues we investigate are whether the model is well-defined on an infinite graph (e.g. the $d$-dimensional cubic lattice), and what can be said about the distribution of the size of a green cluster just before it is paralyzed. We show that, if the initial density of red vertices is positive, and that of white vertices is sufficiently small, the model is indeed well-defined and the above distribution has an exponential tail. In fact, we believe this to be true whenever the initial density of red is positive. This research also led to a relation between invasion percolation and critical Bernoulli percolation which seems to be of independent interest. 0 into PostgreSQL...\n",
      "Inserting test sample 2900  The study investigates a stochastic growth model with spatial constraints imposed by paralyzing obstacles. In this model, an object diffuses randomly in a designated two-dimensional space, while being obstructed by randomly placed circular objects. The growth of the object is assumed to be proportional to the unobstructed space it covers, while the obstacles impede the object's movement and cause it to jump when obstructed. The model is simulated using a computer program that generates a sequence of spatial positions for the diffusing object. The program calculates the growth of the object as it moves through the space, measuring the area covered by the object that is not obstructed by the circular obstacles. The study shows that the random spatial growth model with paralyzing obstacles exhibits interesting properties, such as a transition from a diffusive to a ballistic regime, depending on the size of the obstacles in relation to the diffusing object. Furthermore, the study analyzes the scaling behavior of the object's growth as the obstacles become denser. The results suggest that the presence of paralyzing obstacles in the growth process leads to nontrivial statistical properties that are relevant to physical and biological systems in which growth is governed by random processes. 1 into PostgreSQL...\n",
      "Inserting test sample 2901  Leading 't Hooft coupling corrections to the photoemission rate of the planar limit of a strongly-coupled {\\cal {N}}=4 SYM plasma are investigated using the gauge/string duality. We consider the full order \\alpha'^3 type IIB string theory corrections to the supergravity action, including higher order terms with the Ramond-Ramond five-form field strength. We extend our previous results presented in arXiv:1110.0526. Photoemission rates depend on the 't Hooft coupling, and their curves suggest an interpolating behaviour from strong towards weak coupling regimes. Their slopes at zero light-like momentum give the electrical conductivity as a function of the 't Hooft coupling, in full agreement with our previous results of arXiv:1108.6306. Furthermore, we also study the effect of corrections beyond the large N limit. 0 into PostgreSQL...\n",
      "Inserting test sample 2902  In this paper, we explore the phenomenon of plasma photoemission from the standpoint of string theory. Specifically, we use holographic duality to investigate the photoemission process in a strongly-coupled plasma. We find that the emission rate is related to the imaginary part of the photon self-energy, which we calculate using AdS/CFT techniques. Our results suggest that, in some cases, the plasma photoemission can serve as a probe of the properties of the plasma itself, including its temperature and chemical potential. We also analyze the effects of external magnetic fields on the photoemission process and show that they can significantly affect the emission rate. Overall, our study sheds new light on the physics of plasma photoemission and its connection to the holographic description of strongly-coupled systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2903  For the spin-$\\{1}{2}$ Fermi-Hubbard model we derive the kinetic equation valid for weak interactions by using time-dependent perturbation expansion up to second order. In recent theoretical and numerical studies the kinetic equation has been merely stated without further details. In this contribution we provide the required background material. 0 into PostgreSQL...\n",
      "Inserting test sample 2904  We derive a matrix-valued Boltzmann equation for the Hubbard model and study its steady-state solution. The matrix structure arises naturally from interactions between spin, charge, and orbital degrees of freedom. Our results reveal the emergence of non-trivial momentum-space structure, which may have important implications for the understanding of strongly correlated electronic systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2905  We study the clustering of voids using $N$-body simulations and simple theoretical models. The excursion-set formalism describes fairly well the abundance of voids identified with the watershed algorithm, although the void formation threshold required is quite different from the spherical collapse value. The void cross bias $b_{\\rm c} $ is measured and its large-scale value is found to be consistent with the peak background split results. A simple fitting formula for $b_{\\rm c} $ is found. We model the void auto-power spectrum taking into account the void biasing and exclusion effect. A good fit to the simulation data is obtained for voids with radii $\\gtrsim$ 30 Mpc/$h$, especially when the void biasing model is extended to 1-loop order. However, the best-fit bias parameters do not agree well with the peak-background split results. Being able to fit the void auto-power spectrum is particularly important not only because it is the direct observable in galaxy surveys, but also our method enables us to treat the bias parameters as nuisance parameters, which are sensitive to the techniques used to identify voids. 0 into PostgreSQL...\n",
      "Inserting test sample 2906  The study of the cosmic voids has gained increasing attention over the past decade due to their crucial role in the understanding of the large-scale structure of the Universe. In this work, we present a large-scale clustering analysis of cosmic voids using data from spectroscopic galaxy surveys. Our results highlight the key parameters that drive the clustering of voids, such as their size, shape, and environment. We find that larger voids display stronger clustering, which suggests a scale-dependent bias effect. We also investigate the impact of primordial non-Gaussianities on the void clustering signal, finding that these can significantly alter its shape and amplitude on large scales. Moreover, we explore the role of baryonic feedback effects on the void clustering, which is a key issue in current astrophysical research. Our analysis provides important insights into the properties of cosmic voids and their formation mechanisms, and will be valuable for future observational and theoretical studies of the large-scale structure of the Universe. 1 into PostgreSQL...\n",
      "Inserting test sample 2907  We use coarse-grained molecular-dynamics (MD) simulations to investigate the structural and dynamical properties of micelles under non-equilibrium Poiseuille flow in a nano-confined geometry. The effects of flow, confinement, and the wetting properties of die-channel walls on spherical sodium dodecyl sulfate (SDS) micelles are explored when the micelle is forced through a die-channel slightly smaller than its equilibrium size. Inside the channel, the micelle may fragment into smaller micelles. In addition to the flow rate, the wettability of the channel surfaces dictates whether the micelle fragments and determines the size of the daughter micelles: The overall behavior is determined by the subtle balance between hydrodynamic forces, micelle-wall interactions and self-assembly forces. 0 into PostgreSQL...\n",
      "Inserting test sample 2908  This study investigates the micelle fragmentation and wetting behavior in a confined flow. Micelles are self-assembled aggregates of surfactants and have tremendous industrial and biomedical applications. The confinement of these micelles alters their structure, and this behavior has been shown to impact the wettability and adhesion of surfaces. We use molecular dynamics simulations to elucidate the micelle fragmentation and wetting behavior in a confined flow. Our simulations show that confinement can increase the micelle's tendency to fragment, which could have implications for colloidal stability and solubility. Moreover, confinement leads to a significant decrease in surface wettability. Overall, our findings provide insight into the complex behavior of surfactant molecules in confined flows and can aid the development of novel materials in various applications. 1 into PostgreSQL...\n",
      "Inserting test sample 2909  We perform a study on quantum entropy production, different kinds of correlations, and their interplay in the driven Caldeira-Leggett model of quantum Brownian motion. The model, taken with a large but finite number of bath modes, is exactly solvable, and the assumption of a Gaussian initial state leads to an efficient numerical simulation of all desired observables in a wide range of model parameters. Our study is composed of three main parts. We first compare two popular definitions of entropy production, namely the standard weak-coupling formulation originally proposed by Spohn and later on extended to the driven case by Deffner and Lutz, and the always-positive expression introduced by Esposito, Lindenberg and van den Broeck, which relies on the knowledge of the evolution of the bath. As a second study, we explore the decomposition of the Esposito et al. entropy production into system-environment and intra-environment correlations for different ranges of couplings and temperatures. Lastly, we examine the evolution of quantum correlations between the system and the environment, measuring entanglement through logarithmic negativity. 0 into PostgreSQL...\n",
      "Inserting test sample 2910  In this paper, we investigate entropy production and the role of correlations in quantum Brownian motion. We analyze the dynamics of a quantum particle that is coupled to a bath of harmonic oscillators, focusing on the effects of non-Markovian behavior, i.e., when the system-bath correlations cannot be ignored. Specifically, we study the time evolution of the particle's reduced density matrix in the presence of its environment, and calculate the entropy production due to the flow of heat out of the system and into the bath. We find that the form of the entropy production depends on the strength and structure of the system-bath correlations, and that these correlations can lead to non-trivial deviations from the Markovian limit predicted by traditional models. Our results suggest that the inclusion of correlations is crucial for understanding entropy production in quantum Brownian motion. 1 into PostgreSQL...\n",
      "Inserting test sample 2911  Let $X$ be a nonempty set and let $T(X)$ be the full transformation semigroup on $X$. The main objective of this paper is to study the subsemigroup $\\overline{\\Omega}(X, Y)$ of $T(X)$ defined by \\[\\overline{\\Omega}(X, Y) = \\{f\\in T(X)\\colon Yf = Y\\},\\] where $Y$ is a fixed nonempty subset of $X$. We describe regular elements in $\\overline{\\Omega}(X, Y)$ and show that $\\overline{\\Omega}(X, Y)$ is regular if and only if $Y$ is finite. We characterize unit-regular elements in $\\overline{\\Omega}(X, Y)$ and prove that $\\overline{\\Omega}(X, Y)$ is unit-regular if and only if $X$ is finite. We characterize Green's relations on $\\overline{\\Omega}(X, Y)$ and prove that $\\mathcal{D} =\\mathcal{J}$ on $\\overline{\\Omega}(X, Y)$ if and only if $Y$ is finite. We also determine ideals of $\\overline{\\Omega}(X, Y)$ and investigate its kernel. This paper extends several results appeared in the literature. 0 into PostgreSQL...\n",
      "Inserting test sample 2912  This paper investigates semigroups of transformations with an invariant set. In particular, we consider certain classes of these semigroups and explore their properties. We show that the structure of these semigroups can be characterized by the properties of their invariant sets, and we provide several concrete examples to illustrate these concepts. In addition to the theoretical analysis, we apply our results to several practical problems in mathematics and computer science. Specifically, we demonstrate the usefulness of these semigroups in the context of dynamical systems and optimization. Our results provide new insights into the nature of semigroups of transformations and suggest several avenues for future research. Overall, we believe that this work will be of interest to researchers in a wide range of fields, from pure mathematics to computer science and engineering. 1 into PostgreSQL...\n",
      "Inserting test sample 2913  Massive OB stars are critical to the ecology of galaxies, and yet our knowledge of OB stars in the Milky Way, fainter than $V \\sim 12$, remains patchy. Data from the VST Photometric H$\\alpha$ Survey (VPHAS+) permit the construction of the first deep catalogues of blue excess-selected OB stars, without neglecting the stellar field. A total of 14900 candidates with 2MASS cross-matches are blue-selected from a 42 square-degree region in the Galactic Plane, capturing the Carina Arm over the Galactic longitude range $282^{\\circ} \\lesssim \\ell \\lesssim 293^{\\circ}$. Spectral energy distribution fitting is performed on these candidates' combined VPHAS+ $u,g,r,i$ and 2MASS $J,H,K$ magnitudes. This delivers: effective temperature constraints, statistically separating O from early-B stars; high-quality extinction parameters, $A_0$ and $R_V$ (random errors typically $< 0.1$). The high-confidence O-B2 candidates number 5915 and a further 5170 fit to later B spectral type. Spectroscopy of 276 of the former confirms 97% of them. The fraction of emission line stars among all candidate B stars is 7--8% . Greyer ($R_V > 3.5$) extinction laws are ubiquitous in the region, over the distance range 2.5--3 kpc to $\\sim$10~kpc.\n",
      "\n",
      "Near prominent massive clusters, $R_V$ tends to rise, with particularly large and chaotic excursions to $R_V \\sim 5$ seen in the Carina Nebula. The data reveal a hitherto unnoticed association of 108 O-B2 stars around the O5If$+$ star LSS 2063 ($\\ell = 289.77^{\\circ}$, $b = -1.22^{\\circ}$). Treating the OB star scale-height as a constant within the thin disk, we find an orderly mean relation between extinction ($A_0$) and distance in the Galactic longitude range, $287.6^{\\circ} < \\ell < 293.5^{\\circ}$, and infer the subtle onset of thin-disk warping. A halo around NGC 3603, roughly a degree in diameter, of $\\sim$500 O-B2 stars with $4 < A_0 (\\rm{mag}) < 7$ is noted. 0 into PostgreSQL...\n",
      "Inserting test sample 2914  The Carina region is a key target for studies of the large-scale structure of the Milky Way and star formation processes. In this work, we investigate the deep OB star population in Carina using the VST Photometric H$\\alpha$ Survey (VPHAS+), a deep, homogeneous, multi-band photometric survey that aims to provide detailed data about the young star-forming regions of the Galactic plane. We identify the massive star population using the H$\\alpha$ line detected by VPHAS+, and combine it with the existing catalog of OB stars from the optical spectroscopy survey conducted by the VLT-FLAMES Tarantula Survey (VFTS) to obtain the most complete census of OB stars in Carina to date.\n",
      "\n",
      "Our sample of OB stars spans a range of spectral types from O3 to B2, and a wide range of ages, indicating ongoing star formation activity in the region. We use the spectral energy distributions of the stars to estimate their physical properties, such as effective temperature, luminosity, and mass. The resulting distribution of OB stars in the Hertzsprung-Russell diagram reveals a well-defined main sequence, and an overdensity of stars in the upper part of the diagram, which may indicate the presence of a young cluster in the region.\n",
      "\n",
      "We also investigate the spatial distribution of the OB stars, and find that they are clustered into several substructures across the Carina region. These substructures coincide with several known HII regions and molecular clouds, providing further evidence for the association between massive star formation and the molecular gas in the region. Our results suggest that Carina hosts a complex distribution of young stars, with ongoing star formation activity that is likely triggered by the interaction between massive stars and the surrounding gas.\n",
      "\n",
      "In conclusion, our study provides a comprehensive analysis of the deep OB star population in Carina using the VPHAS+ survey data. Our results provide new insights into the star formation processes in this region, and highlight the importance of multi-band photometric surveys in identifying and characterizing massive stars in the Galaxy. 1 into PostgreSQL...\n",
      "Inserting test sample 2915  For many machine learning tasks, the input data lie on a low-dimensional manifold embedded in a high dimensional space and, because of this high-dimensional structure, most algorithms are inefficient. The typical solution is to reduce the dimension of the input data using standard dimension reduction algorithms such as ISOMAP, LAPLACIAN EIGENMAPS or LLES. This approach, however, does not always work in practice as these algorithms require that we have somewhat ideal data. Unfortunately, most data sets either have missing entries or unacceptably noisy values. That is, real data are far from ideal and we cannot use these algorithms directly. In this paper, we focus on the case when we have missing data. Some techniques, such as matrix completion, can be used to fill in missing data but these methods do not capture the non-linear structure of the manifold. Here, we present a new algorithm MR-MISSING that extends these previous algorithms and can be used to compute low dimensional representation on data sets with missing entries. We demonstrate the effectiveness of our algorithm by running three different experiments. We visually verify the effectiveness of our algorithm on synthetic manifolds, we numerically compare our projections against those computed by first filling in data using nlPCA and mDRUR on the MNIST data set, and we also show that we can do classification on MNIST with missing data. We also provide a theoretical guarantee for MR-MISSING under some simplifying assumptions. 0 into PostgreSQL...\n",
      "Inserting test sample 2916  The problem of metric learning in the presence of missing data is challenging since the absence of some features limits the ability to directly compare distances between objects. In this paper, we propose an unsupervised approach to learn a metric space that accounts for missing data in an effective manner. Our method takes advantage of the pairwise relationships between objects to learn a low-dimensional Euclidean embedding, which preserves the original distance structure as much as possible. We formulate our approach in a probabilistic framework and derive an efficient algorithm for optimization. Our experiments on both simulated and real-world datasets demonstrate that the proposed method outperforms state-of-the-art methods in terms of accuracy and robustness to missing data. Moreover, our method can handle both the missing completely at random (MCAR) and missing not at random (MNAR) scenarios, which are common in many real-world applications. We also investigate the impact of missing data on the learned metric and show that our method can effectively handle moderate to high levels of missingness. Overall, our approach provides a practical solution to the metric learning problem in the presence of missing data, which can benefit a wide range of applications, such as image and text retrieval, recommendation systems, and bioinformatics. 1 into PostgreSQL...\n",
      "Inserting test sample 2917  Upcoming imaging surveys, such as LSST, will provide an unprecedented view of the Universe, but with limited resolution along the line-of-sight. Common ways to increase resolution in the third dimension, and reduce misclassifications, include observing a wider wavelength range and/or combining the broad-band imaging with higher spectral resolution data. The challenge with these approaches is matching the depth of these ancillary data with the original imaging survey. However, while a full 3D map is required for some science, there are many situations where only the statistical distribution of objects (dN/dz) in the line-of-sight direction is needed. In such situations, there is no need to measure the fluxes of individual objects in all of the surveys.\n",
      "\n",
      "Rather a stacking procedure can be used to perform an `ensemble photo-z'. We show how a shallow, higher spectral resolution survey can be used to measure dN/dz for stacks of galaxies which coincide in a deeper, lower resolution survey. The galaxies in the deeper survey do not even need to appear individually in the shallow survey. We give a toy model example to illustrate tradeoffs and considerations for applying this method. This approach will allow deep imaging surveys to leverage the high resolution of spectroscopic and narrow/medium band surveys underway, even when the latter do not have the same reach to high redshift. 0 into PostgreSQL...\n",
      "Inserting test sample 2918  Ensemble photometric redshifts are an essential astrophysical tool for mapping large-scale structures of the universe. This method involves estimating the redshifts of galaxies based on their observable photometric properties. Accurately estimating redshifts is crucial for studying the evolution of the universe and testing cosmological models. However, photometric redshifts suffer from systematic errors such as template mismatch, sensitivity to stellar populations, and bias from survey depths.\n",
      "\n",
      "To mitigate these errors, various ensemble learning techniques have been developed. Ensemble methods combine multiple models to obtain a more robust and accurate prediction. In the context of photometric redshifts, ensemble methods could improve the accuracy of individual model predictions, leading to a more precise understanding of the universe's evolution. Recent studies have used ensembles of neural networks and random forests to enhance the performance of photometric redshift predictions.\n",
      "\n",
      "The effectiveness of ensemble photometric redshifts is evaluated based on several criteria, such as the overall precision, accuracy, and outlier rejection rate. Nonetheless, a significant challenge remains in the calibration of ensemble results due to the complexity of the models and the lack of a well-established metric for measuring the reliability of predictions.\n",
      "\n",
      "In summary, ensemble photometric redshifts are a promising approach for improving the accuracy and robustness of photometric redshift estimates. Further research is needed to optimize ensemble techniques and establish reliable calibration methods, which are crucial for improving our understanding of the universe's large-scale structures. 1 into PostgreSQL...\n",
      "Inserting test sample 2919  We give a method for obtaining a handle decomposition of an $n$-manifold if the manifold is given by isometric side-pairings of a polyhedron in $\\en$, $\\sn$ or $\\hn$. Every cycle of $k$-faces on the polyhedron corresponds to an $(n-k)$-handle of the manifold.\n",
      "\n",
      "Two applications of the method are given. One helps recognize when a noncompact hyperbolic 3-manifold is a complement of a link in $S^3$ (and automatically produces the link diagram), the other shows that a topological $S^4$ described by the author in \\cite{Ivansic3} is diffeomorphic to the standard differentiable $S^4$. 0 into PostgreSQL...\n",
      "Inserting test sample 2920  This paper presents a method for converting a side-pairing of a closed, orientable, irreducible 3-manifold into a handle decomposition. The process involves constructing an explicit decomposition through the use of normal surface theory, graph theory, and geometric topology. We prove that this conversion is indeed possible and give examples of its application. Our method provides a novel way of understanding the topology of 3-manifolds and has potential applications in various fields of mathematics such as knot theory and geometric group theory. Overall, our results contribute to the ongoing effort to understand the structure of 3-manifolds. 1 into PostgreSQL...\n",
      "Inserting test sample 2921  The physical 3d $\\mathcal{N}=2$ theory T[Y] was previously used to predict the existence of some 3-manifold invariants $\\hat{Z}_{a}(q)$ that take the form of power series with integer coefficients, converging in the unit disk. Their radial limits at the roots of unity should recover the Witten-Reshetikhin-Turaev invariants. In this paper we discuss how, for complements of knots in $S^3$, the analogue of the invariants $\\hat{Z}_{a}(q)$ should be a two-variable series $F_K(x,q)$ obtained by parametric resurgence from the asymptotic expansion of the colored Jones polynomial. The terms in this series should satisfy a recurrence given by the quantum A-polynomial.\n",
      "\n",
      "Furthermore, there is a formula that relates $F_K(x,q)$ to the invariants $\\hat{Z}_{a}(q)$ for Dehn surgeries on the knot. We provide explicit calculations of $F_K(x,q)$ in the case of knots given by negative definite plumbings with an unframed vertex, such as torus knots. We also find numerically the first terms in the series for the figure-eight knot, up to any desired order, and use this to understand $\\hat{Z}_a(q)$ for some hyperbolic 3-manifolds. 0 into PostgreSQL...\n",
      "Inserting test sample 2922  In this paper, we present a two-variable series for knot complements. The series is constructed using the HOMFLY-PT polynomial, which is an invariant of knots and links. We show that the two-variable series can be used to compute the HOMFLY-PT polynomial for any knot complement. The series is expressed in terms of skein modules, which are certain algebraic structures associated with knots and links. We provide explicit formulas for the two-variable series in terms of generators and relations of the skein module. We also investigate some properties of the series such as positivity, non-vanishing, and convergence. Furthermore, we use the series to study the topology of knot complements. We show that the values of the series at certain points contain information about the Dehn surgery on the knot complement. We illustrate our results with several examples of computations of the HOMFLY-PT polynomial using the two-variable series. Our approach provides a promising alternative to existing methods for computing knot invariants such as the Jones polynomial and the Alexander polynomial. 1 into PostgreSQL...\n",
      "Inserting test sample 2923  We define the orthogonal Bakry-\\'Emery tensor as a generalization of the orthogonal Ricci curvature, and then study diameter theorems on K\\\"ahler and quaternionic K\\\"ahler manifolds under positivity assumption on the orthogonal Bakry-\\'Emery tensor. Moreover, under such assumptions on the orthogonal Bakry-\\'Emery tensor and the holomorphic or quaternionic sectional curvature on a K\\\"ahler manifold or a quaternionic K\\\"ahler manifold respectively, the Bonnet-Myers type diameter bounds are sharper than in the Riemannian case. 0 into PostgreSQL...\n",
      "Inserting test sample 2924  This paper establishes upper bounds for the diameters of KÃ¤hler and quaternionic KÃ¤hler manifolds with a positive lower curvature bound. Aided by a differential inequality technique, we prove a theorem that applies to both types of manifolds. We find that the upper bound is dependent on the curvature bound, and, interestingly, on the type of curvature. These diameter theorems bridge the gap in the literature regarding the relationship between curvature and diameter of these complex and quaternionic manifolds. 1 into PostgreSQL...\n",
      "Inserting test sample 2925  We prove that the representations numbers of a ternary definite integral quadratic form defined over F_q[t], where F_q is a finite field of odd characteristic, determine its integral equivalence class when q is large enough with respect to its successive minima. Equivalently, such a quadratic form is determined up to integral isometry by its theta series. 0 into PostgreSQL...\n",
      "Inserting test sample 2926  We investigate the construction of isospectral, definite, and ternary lattices over the polynomial ring Fq[t]. We develop a method for constructing such lattices and show that they satisfy the necessary conditions for being isospectral. Our results represent a generalization of previous work in the field and contribute to the study of spectral properties of lattices. 1 into PostgreSQL...\n",
      "Inserting test sample 2927  Based on the SED, Herbig stars have been categorized into two observational groups, reflecting their overall disk structure: group I members have disks with a higher degree of flaring than their group II counterparts. We investigate the 5-35 um Spitzer IRS spectra of a sample of 13 group I sources and 20 group II sources. We focus on the continuum emission to study the underlying disk geometry. We have determined the [30/13.5] and [13.5/7] continuum flux ratios. The 7-um flux excess with respect to the stellar photosphere is measured, as a marker for the strength of the near-IR emission produced by the inner disk. We have compared our data to self-consistent passive-disk model spectra, for which the same quantities were derived. We confirm the literature result that the difference in continuum emission between group I and II sources can largely be explained by a different amount of small dust grains. However, we report a strong correlation between the [30/13.5] and [13.5/7] flux ratios for Meeus group II sources. Moreover, the [30/13.5] flux ratio decreases with increasing 7-um excess for all targets in the sample. To explain these correlations with the models, we need to introduce an artificial scaling factor for the inner disk height. In roughly 50% of the Herbig Ae/Be stars in our sample, the inner disk must be inflated by a factor 2 to 3 beyond what hydrostatic calculations predict. The total disk mass in small dust grains determines the degree of flaring. We conclude, however, that for any given disk mass in small dust grains, the shadowing of the outer (tens of AU) disk is determined by the scale height of the inner disk (1 AU). The inner disk partially obscures the outer disk, reducing the disk surface temperature. Here, for the first time, we prove these effects observationally. 0 into PostgreSQL...\n",
      "Inserting test sample 2928  The disks surrounding young, pre-main sequence Herbig Ae/Be stars are often characterized by the complex interplay between bright flaring and darkened shadowing regions. The origins of these phenomena and their effects on the surrounding environment remain under intense investigation. In this paper, we present new observational and theoretical evidence demonstrating that this interplay between flaring and shadowing in Herbig Ae/Be star disks is a complex feedback mechanism, self-regulated by both local and global disk dynamics.\n",
      "\n",
      "Using high-angular-resolution near-infrared imaging coupled with multi-wavelength spectroscopy, we identified a new shadowing geometry in the disk of a Herbig Ae star. Our observations suggest that a large-scale spiral structure located close to the inner disk edge acts as an effective shadowing mechanism, casting significant portions of the disk into darkness. In addition, we present a theoretical model which reproduces the observed shadowing morphology through a combination of self-shadowing and spiral dust density waves. Our results provide new insight into the physical factors that control disk shadowing, including disk scale-height and the effects of vertical illumination. \n",
      "\n",
      "We also show that flaring is closely related to shadowing in Herbig Ae/Be disks. Specifically, the height at which the disk becomes optically thin plays a critical role in determining the magnitude of the flaring. Through detailed radiative transfer models, we illustrate how the interplay between flaring and shadowing leads to complex variations in disk morphology and thermodynamics. Finally, we discuss the implications of our results for planet formation and disk evolution, emphasizing the need for accurate models of flaring and shadowing in understanding the diversity of disk structures observed around Herbig Ae/Be stars.\n",
      "\n",
      "Overall, our study underscores the importance of understanding the intricate feedback mechanisms regulating the interplay between flaring and shadowing in Herbig Ae/Be disk systems. Our results provide new observational and theoretical constraints on these processes, thus paving the way for more realistic models of planet formation and disk evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 2929  We study the migration and resonant capture of planetesimals in a planetary system consisting of a gaseous disc analogous to the primordial solar nebula and a Neptune-like planet. Using a simple treatment of the drag force we find that planetesimals are mainly trapped in the 3:2 and 2:1 resonances and that the resonant populations are correlated with the gaseous drag strength in a sense that the 3:2 resonant population increases with the stronger gaseous drag, but the 2:1 resonant population does not. Since planetesimals can lead to the formation of larger bodies similar to asteroids and Kuiper Belt Objects, the gaseous drag can play an important role in the configuration of a planetary system. 0 into PostgreSQL...\n",
      "Inserting test sample 2930  Planetary systems are constantly evolving, with the formation of planetesimals playing a significant role. Research has suggested that resonant planetesimals could exist within these systems, and that their motion may be affected by the strength of gaseous drag. This paper explores the correlation between these two factors, ultimately concluding that a relationship between them is quite likely to exist. Specifically, it is believed that gaseous drag can act to synchronize, and even destabilize, resonant planetesimals. By assessing the dynamics of planetary systems, this research sheds light on the complex processes that drive their formation and evolution, and has implications for our understanding of the universe on a larger scale. 1 into PostgreSQL...\n",
      "Inserting test sample 2931  The next-to-next-to-leading order (NNLO) pQCD correction to the inclusive decays of the heavy quarkonium $\\eta_Q$ ($Q$ being $c$ or $b$) has been done in the literature within the framework of nonrelativistic QCD. One may observe that the NNLO decay width still has large conventional renormalization scale dependence due to its weaker pQCD convergence, e.g. about $(^{+4\\%}_{-34\\%})$ for $\\eta_c$ and $(^{+0.0}_{-9\\%})$ for $\\eta_b$, by varying the scale within the range of $[m_Q, 4m_Q]$. The principle of maximum conformality (PMC) provides a systematic way to fix the $\\alpha_s$-running behavior of the process, which satisfies the requirements of renormalization group invariance and eliminates the conventional renormalization scheme and scale ambiguities.\n",
      "\n",
      "Using the PMC single-scale method, we show that the resultant PMC conformal series is renormalization scale independent, and the precision of the $\\eta_Q$ inclusive decay width can be greatly improved. Taking the relativistic correction $\\mathcal{O}(\\alpha_{s}v^2)$ into consideration, the ratios of the $\\eta_{Q}$ decays to light hadrons or $\\gamma\\gamma$ are: $R^{\\rm NNLO}_{\\eta_c}|_{\\rm{PMC}}=(3.93^{+0.26}_{-0.24})\\times10^3$ and $R^{\\rm NNLO}_{\\eta_b}|_{\\rm{PMC}}=(22.85^{+0.90}_{-0.87})\\times10^3$, respectively.\n",
      "\n",
      "Here the errors are for $\\Delta\\alpha_s(M_Z) = \\pm0.0011$. As a step forward, by applying the Pad$\\acute{e}$ approximation approach (PAA) over the PMC conformal series, we obtain approximate NNNLO predictions for those two ratios, e.g. $R^{\\rm NNNLO}_{\\eta_c}|_{\\rm{PAA+PMC}} =(5.66^{+0.65}_{-0.55})\\times10^3$ and $R^{\\rm NNNLO}_{\\eta_b}|_{\\rm{PAA+PMC}}=(26.02^{+1.24}_{-1.17})\\times10^3$.\n",
      "\n",
      "The $R^{\\rm NNNLO}_{\\eta_c}|_{\\rm{PAA+PMC}}$ ratio agrees with the latest PDG value $R_{\\eta_c}^{\\rm{exp}}=(5.3_{-1.4}^{+2.4})\\times10^3$, indicating the necessity of a strict calculation of NNNLO terms. 0 into PostgreSQL...\n",
      "Inserting test sample 2932  This paper investigates the concept of maximum conformality in the heavy quarkonium inclusive decays. We start by developing a new theoretical framework which enables us to achieve higher-order perturbative precision. This is accomplished by renormalizing the theory to remove all non-conformal $\\beta$ terms, which in turn eliminates divergences and uncertainties that stem from certain regions of phase space. \n",
      "\n",
      "Using this framework, we calculate the annihilation contribution to the decays of the the heavy quarkonia states, $J/\\psi$ and $\\Upsilon$, which are crucial in determining the precise branching ratios and decay widths of these particles. The calculation is carried out up to order $\\alpha_s^3$, taking into account the QCD effects which play a crucial role in understanding the particle phenomenology.\n",
      "\n",
      "We demonstrate that, thanks to the principle of maximum conformality, we obtain significantly improved results compared to those obtained in the traditional perturbative QCD framework. Our results provide important insights into the properties of heavy quarkonium and allow for more precise experimental tests of fundamental QCD predictions.\n",
      "\n",
      "Moreover, we provide a detailed analysis of our results, including a comparison with experimental data, and a study of the impact of variations of the renormalization scale and other sources of uncertainties. This allows us to evaluate the robustness of our predictions and to pinpoint the regions where further improvements are required.\n",
      "\n",
      "In summary, this study introduces a novel theoretical approach for calculating the heavy quarkonium inclusive decays, based on the principle of maximum conformality. Our results show that this approach leads to improved precision, highlighting the importance of considering non-perturbative effects and the role of QCD in understanding the properties of these particles. 1 into PostgreSQL...\n",
      "Inserting test sample 2933  Recent technological advancements have enabled proliferated use of small embedded and IoT devices for collecting, processing, and transferring the security-critical information and user data. This exponential use has acted as a catalyst in the recent growth of sophisticated attacks such as the replay, man-in-the-middle, and malicious code modification to slink, leak, tweak or exploit the security-critical information in malevolent activities. Therefore, secure communication and software state assurance (at run-time and boot-time) of the device has emerged as open security problems. Furthermore, these devices need to have an appropriate recovery mechanism to bring them back to the known-good operational state. Previous researchers have demonstrated independent methods for attack detection and safeguard. However, the majority of them lack in providing onboard system recovery and secure communication techniques. To bridge this gap, this manuscript proposes SRACARE- a framework that utilizes the custom lightweight, secure communication protocol that performs remote/local attestation, and secure boot with an onboard resilience recovery mechanism to protect the devices from the above-mentioned attacks. The prototype employs an efficient lightweight, low-power 32-bit RISC-V processor, secure communication protocol, code authentication, and resilience engine running on the Artix 7 Field Programmable Gate Array(FPGA) board. This work presents the performance evaluation and state-of-the-art comparison results, which shows promising resilience to attacks and demonstrate the novel protection mechanism with onboard recovery. The framework achieves these with only 8 % performance overhead and a very small increase in hardware-software footprint. 0 into PostgreSQL...\n",
      "Inserting test sample 2934  SRACARE is a Secure Remote Attestation model that utilizes a Code Authentication and Resilience Engine to provide a safer communication channel between two remote endpoints. The system aims to mitigate the risk of a data breach due to malicious actors and ensure the confidentiality, integrity, and availability of the communication.\n",
      "\n",
      "The proposed model involves three core entities: the Attestor, the Attestee, and the Verifier. The Attestor generates an authentication token, digitally signs it, and sends it to the Attestee. The Attestee receives the authentication token and sends it to the Verifier for validation. If the validation is successful, the communication channel between the Attestor and the Attestee is secured, and data sharing can proceed.\n",
      "\n",
      "To increase the resilience of the system, the Code Authentication and Resilience Engine has been implemented. It ensures that the Attestor and Attestee can validate each other's code in a secure environment. This process adheres to a set of predefined operations and validates the software's authenticity, ensuring that the system is secure and that the correct software is running on both endpoints.\n",
      "\n",
      "SRACARE provides a robust, secure, and resilient communication mechanism between two remote endpoints. It can mitigate a range of potential vulnerabilities, including man-in-the-middle attacks, eavesdropping, and code tampering, reducing the risk of a data breach. This paper presents the model's architecture, an in-depth analysis of the system's functionality, and a performance evaluation of the implemented model. The results show that SRACARE is a reliable and efficient solution for secure remote attestation. 1 into PostgreSQL...\n",
      "Inserting test sample 2935  Using the Very Long Baseline Array, we have measured a trigonometric parallax for the micro quasar GRS 1915+105, which contains a black hole and a K-giant companion. This yields a direct distance estimate of 8.6 (+2.0,-1.6) kpc and a revised estimate for the mass of the black hole of 12.4 (+2.0,-1.8) Msun. GRS 1915+105 is at about the same distance as some HII regions and water masers associated with high-mass star formation in the Sagittarius spiral arm of the Galaxy. The absolute proper motion of GRS 1915+105 is -3.19 +/- 0.03 mas/y and -6.24 +/- 0.05 mas/y toward the east and north, respectively, which corresponds to a modest peculiar speed of 22 +/-24 km/s at the parallax distance, suggesting that the binary did not receive a large velocity kick when the black hole formed. On one observational epoch, GRS 1915+105 displayed superluminal motion along the direction of its approaching jet. Considering previous observations of jet motions, the jet in GRS 1915+105 can be modeled with a jet inclination to the line of sight of 60 +/- 5 deg and a variable flow speed between 0.65c and 0.81c, which possibly indicates deceleration of the jet at distances from the black hole >2000 AU. Finally, using our measurements of distance and estimates of black hole mass and inclination, we provisionally confirm our earlier result that the black hole is spinning very rapidly. 0 into PostgreSQL...\n",
      "Inserting test sample 2936  We present a new distance estimate to the microquasar system GRS 1915+105 based on multi-epoch Very Long Baseline Interferometry (VLBI) observations. Our observations utilize a parallax measurement technique to determine the distance to the system before analyzing its components. Based on our analyses, we revise the black hole mass estimate for this system. We find that the black hole mass is (14.0 Â± 4.5) solar masses. This measurement is consistent with previously published estimates but with reduced uncertainties.\n",
      "\n",
      "Our method employs direct astrometric measurements of the quasar's components using VLBI to determine the distance. Combining the measured parallax with a geometric model of the quasar enabled us to estimate the distance to the system.\n",
      "\n",
      "GRS 1915+105 presents an ideal case for such measurements due to its brightness and its large angular size. Additionally, it exhibits rapid variability, which allows measurements over timescales of hours to days. Our new estimate places this black hole system at 8.6Â±0.3 kpc, corresponding to a luminosity of 3.4 x 10^39 erg/s.\n",
      "\n",
      "Our results indicate that the estimated black hole mass of this system is more precise than previously believed, constraining possible models for black hole formation and evolution. These observations provide a framework for future investigations into the astrophysical processes driving the evolution of microquasars and may enhance our understanding of the fundamental properties of black holes. 1 into PostgreSQL...\n",
      "Inserting test sample 2937  Between 1998 and 2004, the planning community has seen vast progress in terms of the sizes of benchmark examples that domain-independent planners can tackle successfully. The key technique behind this progress is the use of heuristic functions based on relaxing the planning task at hand, where the relaxation is to assume that all delete lists are empty. The unprecedented success of such methods, in many commonly used benchmark examples, calls for an understanding of what classes of domains these methods are well suited for. In the investigation at hand, we derive a formal background to such an understanding.\n",
      "\n",
      "We perform a case study covering a range of 30 commonly used STRIPS and ADL benchmark domains, including all examples used in the first four international planning competitions. We *prove* connections between domain structure and local search topology -- heuristic cost surface properties -- under an idealized version of the heuristic functions used in modern planners. The idealized heuristic function is called h^+, and differs from the practically used functions in that it returns the length of an *optimal* relaxed plan, which is NP-hard to compute. We identify several key characteristics of the topology under h^+, concerning the existence/non-existence of unrecognized dead ends, as well as the existence/non-existence of constant upper bounds on the difficulty of escaping local minima and benches. These distinctions divide the (set of all) planning domains into a taxonomy of classes of varying h^+ topology. As it turns out, many of the 30 investigated domains lie in classes with a relatively easy topology. Most particularly, 12 of the domains lie in classes where FFs search algorithm, provided with h^+, is a polynomial solving mechanism. We also present results relating h^+ to its approximation as implemented in FF. The behavior regarding dead ends is provably the same. We summarize the results of an empirical investigation showing that, in many domains, the topological qualities of h^+ are largely inherited by the approximation. The overall investigation gives a rare example of a successful analysis of the connections between typical-case problem structure, and search performance. The theoretical investigation also gives hints on how the topological phenomena might be automatically recognizable by domain analysis techniques. We outline some preliminary steps we made into that direction. 0 into PostgreSQL...\n",
      "Inserting test sample 2938  This paper presents an investigation into the phenomenon of ignoring delete lists and its applicability in planning benchmarks, where local search topology is a key consideration. Delete lists are commonly used in heuristic planners to keep track of the deleted literals with respect to a given action. While these lists prove useful in certain scenarios, they can also act as a substantial barrier to efficiency for planners utilizing local search techniques.\n",
      "\n",
      "To investigate the applicability of ignoring delete lists in planning benchmarks, we designed a series of experiments on the domain-independent benchmark set known as IPC-14. We utilized the Fast Downward planner with the LAMA-2011 configuration, a state-of-the-art planner that demonstrates high performance across applications. Our experiments were designed to test the strengths of ignoring delete lists across multiple domains, while also examining the relationship between complexity and efficiency.\n",
      "\n",
      "Our results show that ignoring delete lists can provide significant benefits in domains with certain characteristics, particularly in those that exhibit high variability. In such domains, ignoring delete lists can reduce the search space and lead to more optimal solutions, while also reducing memory consumption. However, we found that these benefits are not exhibited across all domains and encouraged domain-specific analysis to identify optimal planner configurations.\n",
      "\n",
      "In addition to the empirical results, we also reflect on the implications of ignoring delete lists and their impact on the broader field of AI planning. In particular, ignoring delete lists can be viewed as a form of approximate reasoning, where simplification and abstraction can lead to high efficiency. We argue that the field of AI planning should continue to explore such techniques, along with new methods and approaches to handle planning benchmarks effectively.\n",
      "\n",
      "In conclusion, our results demonstrate that ignoring delete lists can be effective in planning benchmarks as a tool to enhance efficiency and reduce memory consumption, particularly in complex and variable domains. By exploring such techniques, AI planners can gain new insights into the search landscape, and ultimately improve their performance. 1 into PostgreSQL...\n",
      "Inserting test sample 2939  We prove that every singular hyperbolic chain transitive set with a singularity does not admit the shadowing property. Using this result we show that if a star flow has the shadowing property on its chain recurrent set then it satisfies Axiom A and the no-cycle conditions; and that if a multisingular hyperbolic set has the shadowing property then it is hyperbolic. 0 into PostgreSQL...\n",
      "Inserting test sample 2940  We study the dynamics of diffeomorphisms exhibiting a singular hyperbolic set with a singularity. For such systems, we show that the absence of shadowing property persists, despite the presence of singularities. In particular, we prove that the set of $\\omega$-limit sets of non-shadowing points coincides with the set of attractors of the system. Our results contribute to the understanding of the generic dynamics of singular hyperbolic systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2941  We analyzed 123 thermonuclear (type-I) X-ray bursts observed by the Rossi X-ray Timing Explorer from the low-mass X-ray binary 4U 1636-536. All but two of the 40 radius-exansion bursts in this sample reached peak fluxes which were normally distributed about a mean of 6.4e-8 ergs/cm^2/s, with a standard deviation of 7.6%. The remaining two radius-expansion bursts reached peak fluxes a factor of 1.69+/-0.13 lower than this mean value; as a consequence, the overall variation in the peak flux of the radius-expansion bursts was a factor of ~2. This variation is comparable to the range of the Eddington limit between material with solar H-fraction (X=0.7) and pure He. Such a variation may arise if, for the bright radius-expansion bursts, most of the accreted H is eliminated either by steady hot CNO burning or expelled in a radiatively-driven wind. However, steady burning cannot exhaust the accreted H for solar composition material within the typical ~2 hr burst recurrence time, nor can it result in sufficient elemental stratification to allow selective ejection of the H only. An additional stratification mechanism appears to be required to separate the accreted elements and thus allow preferential ejection of the hydrogen. We found no evidence for a gap in the peak flux distribution between the radius-expansion and non-radius expansion bursts, previously observed in smaller samples. Assuming that the faint radius-expansion bursts reached the Eddington limit for H-rich material (X~0.7), and the brighter bursts the limit for pure He (X=0), we estimate the distance to 4U 1636-536 (for a canonical neutron star with M_NS=1.4M_sun, R_NS=10 km) to be 6.0+/-0.5 kpc, or for M_NS=2M_sun at most 7.1 kpc. (Abstract abridged) 0 into PostgreSQL...\n",
      "Inserting test sample 2942  This paper investigates the possibility of using Eddington-limited X-ray bursts as distance indicators. We focus specifically on the potential compositional effects present in bursts from the source 4U 1636-536. We analyzed multiple bursts from this source using data from the Rossi X-ray Timing Explorer. Our analysis suggests that a compositional difference exists between the bursts, as evidenced by the varying rise times and cooling trends. We also find that the burst fluence and spectral shape do not correlate with the burst rise time, indicating that these two properties are not solely dependent on composition. Our results suggest that bursts from sources with different compositions may show different properties, which would lead to potentially inaccurate distance estimates. Furthermore, our findings support the hypothesis that accreted matter is the primary driver of burst behavior, with secondary effects from composition. We also investigate the possibility of using the cooling tail of the bursts to infer neutron star radii, but find that our data is insufficient to make firm conclusions. Overall, our study highlights the importance of considering compositional effects in interpreting burst data, particularly for sources with different compositions. Additionally, our investigation provides new insights into the physical mechanisms driving Eddington-limited X-ray bursts and the potential for using them as distance indicators. 1 into PostgreSQL...\n",
      "Inserting test sample 2943  We propose a novel point cloud based 3D organ segmentation pipeline utilizing deep Q-learning. In order to preserve shape properties, the learning process is guided using a statistical shape model. The trained agent directly predicts piece-wise linear transformations for all vertices in each iteration. This mapping between the ideal transformation for an object outline estimation is learned based on image features. To this end, we introduce aperture features that extract gray values by sampling the 3D volume within the cone centered around the associated vertex and its normal vector. Our approach is also capable of estimating a hierarchical pyramid of non rigid deformations for multi-resolution meshes. In the application phase, we use a marginal approach to gradually estimate affine as well as non-rigid transformations. We performed extensive evaluations to highlight the robust performance of our approach on a variety of challenge data as well as clinical data. Additionally, our method has a run time ranging from 0.3 to 2.7 seconds to segment each organ. In addition, we show that the proposed method can be applied to different organs, X-ray based modalities, and scanning protocols without the need of transfer learning. As we learn actions, even unseen reference meshes can be processed as demonstrated in an example with the Visible Human. From this we conclude that our method is robust, and we believe that our method can be successfully applied to many more applications, in particular, in the interventional imaging space. 0 into PostgreSQL...\n",
      "Inserting test sample 2944  This paper proposes a novel approach for organ segmentation using 3D point cloud data based on the technique of action learning. The proposed method aims to address the challenges associated with current approaches that often require manual interaction and extensive labeling. An end-to-end deep neural network is designed to learn the representation of 3D point clouds directly from the raw input data. Action learning is employed to allow the network to optimize its parameters by taking a series of self-supervised actions, which further improves the efficiency and effectiveness of organ segmentation. Our experiment results from different datasets demonstrate that our method outperforms several state-of-the-art approaches in terms of accuracy and robustness. The 3D point cloud data provides a more natural representation of the human anatomy, and the proposed method enhances the accuracy of organ boundary segmentation in various medical imaging applications. The approach is validated by applying it to challenges and real-world applications, demonstrating its practical utility in the medical field. The action learning-based approach will contribute to the development of a more automated and efficient organ segmentation technique for enhanced medical imaging and diagnosis. 1 into PostgreSQL...\n",
      "Inserting test sample 2945  We present a molecular line scan in the Hubble Deep Field North (HDF-N) that covers the entire 3mm window (79-115 GHz) using the IRAM Plateau de Bure Interferometer. Our CO redshift coverage spans z<0.45, 1<z<1.9 and all z>2. We reach a CO detection limit that is deep enough to detect essentially all z>1 CO lines reported in the literature so far. We have developed and applied different line searching algorithms, resulting in the discovery of 17 line candidates. We estimate that the rate of false positive line detections is ~2/17. We identify optical/NIR counterparts from the deep ancillary database of the HDF-N for seven of these candidates and investigate their available SEDs.\n",
      "\n",
      "Two secure CO detections in our scan are identified with star-forming galaxies at z=1.784 and at z=2.047. These galaxies have colors consistent with the `BzK' color selection and they show relatively bright CO emission compared with galaxies of similar dust continuum luminosity. We also detect two spectral lines in the submillimeter galaxy HDF850.1 at z=5.183. We consider an additional 9 line candidates as high quality. Our observations also provide a deep 3mm continuum map (1-sigma noise level = 8.6 $\\mu$Jy/beam). Via a stacking approach, we find that optical/MIR bright galaxies contribute only to <50% of the SFR density at 1<z<3, unless high dust temperatures are invoked. The present study represents a first, fundamental step towards an unbiased census of molecular gas in `normal' galaxies at high-z, a crucial goal of extragalactic astronomy in the ALMA era. 0 into PostgreSQL...\n",
      "Inserting test sample 2946  The Hubble Deep Field North (HDF-N) is a region of the sky that has been extensively studied through various telescopes and observations. In this study, we present a new molecular line scan of the HDF-N using the IRAM Plateau de Bure Interferometer. The molecular lines observed include CO(2-1), CO(3-2), and HCN(3-2), which provide important information on the physical conditions of the gas in the HDF-N. Our observations also reveal the presence of five individual sources, four of which have previously been detected through other studies. We analyze the spectra of these sources and derive several physical parameters, such as the line width, integrated intensity, and velocity centroid. Using these parameters, we can infer the properties of the molecular gas, such as its temperature, density, and kinematics. Furthermore, we find evidence of strong outflow signatures in one of the sources, which may indicate ongoing star formation activity. Our results provide new insights into the molecular gas properties of the HDF-N, and highlight the potential of using high-resolution molecular line observations to study the physical conditions of gas in high-redshift galaxies. In summary, this study presents important new data on the HDF-N, and demonstrates the power of molecular line scans for probing the physical properties of the interstellar medium in distant galaxies. 1 into PostgreSQL...\n",
      "Inserting test sample 2947  Let $X$ be a compact K\\\"ahler manifold and let $(L, \\varphi)$ be a pseudo-effective line bundle on $X$. We first define a notion of numerical dimension of pseudo-effective line bundles with singular metrics, and then discuss the properties of this type numerical dimension. We finally prove a very general Kawamata-Viehweg-Nadel type vanishing theorem on an arbitrary compact K\\\"ahler manifold. 0 into PostgreSQL...\n",
      "Inserting test sample 2948  We establish a vanishing theorem for the direct image sheaves $R^i f_*\\Omega_X^j(\\log D)$ on a compact K\\\"ahler manifold $X$, where $f:X\\rightarrow Y$ is a surjective holomorphic map to a lower-dimensional compact K\\\"ahler manifold $Y$, and $D$ is an effective divisor on $X$. The theorem is formulated in terms of numerical dimension, a new invariant introduced in this paper. 1 into PostgreSQL...\n",
      "Inserting test sample 2949  Seismocardiography (SCG) is a non-invasive method that can be used for cardiac activity monitoring. This paper presents a new electrocardiogram (ECG) independent approach for estimating heart rate (HR) during low and high lung volume (LLV and HLV, respectively) phases using SCG signals. In this study, SCG, ECG, and respiratory flow rate (RFR) signals were measured simultaneously in 7 healthy subjects. The lung volume information was calculated from the RFR and was used to group the SCG events into low and high lung-volume groups. LLV and HLV SCG events were then used to estimate the subjects HR as well as the HR during LLV and HLV in 3 different postural positions, namely supine, 45 degree heads-up, and sitting. The performance of the proposed algorithm was tested against the standard ECG measurements. Results showed that the HR estimations from the SCG and ECG signals were in a good agreement (bias of 0.08 bpm). All subjects were found to have a higher HR during HLV (HR$_\\text{HLV}$) compared to LLV (HR$_\\text{LLV}$) at all postural positions. The HR$_\\text{HLV}$/HR$_\\text{LLV}$ ratio was 1.11$\\pm$0.07, 1.08$\\pm$0.05, 1.09$\\pm$0.04, and 1.09$\\pm$0.04 (mean$\\pm$SD) for supine, 45 degree-first trial, 45 degree-second trial, and sitting positions, respectively. This heart rate variability may be due, at least in part, to the well-known respiratory sinus arrhythmia. HR monitoring from SCG signals might be used in different clinical applications including wearable cardiac monitoring systems. 0 into PostgreSQL...\n",
      "Inserting test sample 2950  This study aimed to investigate the use of seismocardiography (SCG) for monitoring heart rate during different lung volume phases. Seven healthy participants were recruited for this study, and their SCG signals were obtained during spontaneous breathing while in supine position. During the recording session, participants were asked to perform four breathing maneuvers, including shallow breathing, deep breathing, functional residual capacity (FRC) breathing, and total lung capacity (TLC) breathing. The results indicated that heart rate variability (HRV) differed significantly across the four breathing maneuvers, with TLC breathing exhibiting the highest values of HRV. Additionally, time-frequency analysis showed that the spectral power of the SCG signal also varied significantly depending on the breathing maneuver, with TLC breathing being associated with the highest values of both high and low-frequency spectral power.\n",
      "\n",
      "Overall, these findings suggest that SCG-derived heart rate monitoring can be used to assess changes in HRV during different lung volume phases. This method may be particularly useful for individuals with respiratory conditions, as it provides a non-invasive, real-time assessment of cardiovascular function during breathing maneuvers. Further research is needed to investigate the potential clinical applications of SCG-derived heart rate monitoring in the context of respiratory diseases. 1 into PostgreSQL...\n",
      "Inserting test sample 2951  A fundamental theorem of Barsotti and Chevalley states that every smooth algebraic group over a perfect field is an extension of an abelian variety by a smooth affine algebraic group. In 1956 Rosenlicht gave a short proof of the theorem. In this expository article, we explain his proof in the language of modern algebraic geometry. 0 into PostgreSQL...\n",
      "Inserting test sample 2952  We provide a proof of the Barsotti-Chevalley theorem, characterizing algebraic groups in terms of their Lie algebras. Starting from the theory of formal groups, we derive the canonical filtration of the Lie algebra and verify that the associated graded object is a Lie algebra isomorphic to the original algebraic group. 1 into PostgreSQL...\n",
      "Inserting test sample 2953  With the growing amount of inappropriate content on the Internet, such as pornography, arises the need to detect and filter such material. The reason for this is given by the fact that such content is often prohibited in certain environments (e.g., schools and workplaces) or for certain publics (e.g., children). In recent years, many works have been mainly focused on detecting pornographic images and videos based on visual content, particularly on the detection of skin color. Although these approaches provide good results, they generally have the disadvantage of a high false positive rate since not all images with large areas of skin exposure are necessarily pornographic images, such as people wearing swimsuits or images related to sports. Local feature based approaches with Bag-of-Words models (BoW) have been successfully applied to visual recognition tasks in the context of pornography detection. Even though existing methods provide promising results, they use local feature descriptors that require a high computational processing time yielding high-dimensional vectors. In this work, we propose an approach for pornography detection based on local binary feature extraction and BossaNova image representation, a BoW model extension that preserves more richly the visual information. Moreover, we propose two approaches for video description based on the combination of mid-level representations namely BossaNova Video Descriptor (BNVD) and BoW Video Descriptor (BoW-VD). The proposed techniques are promising, achieving an accuracy of 92.40%, thus reducing the classification error by 16% over the current state-of-the-art local features approach on the Pornography dataset. 0 into PostgreSQL...\n",
      "Inserting test sample 2954  With the proliferation of online video content and the increasing ease of access to explicit or sexually explicit material, there has been a growing interest in the automatic detection of pornography. Among the various approaches proposed for pornography detection, the use of mid-level features has been gaining relevance due to their ability to provide a better balance between computational efficiency and discriminative power. In this context, the present work proposes a mid-level video representation based on binary descriptors for pornography detection. To evaluate the proposed approach, we performed experiments on a publicly available dataset, as well as on a dataset of our own. Results show that our approach achieves state-of-the-art performance on both datasets, outperforming other mid-level feature-based methods, as well as methods using high-level representations. We also conducted an ablation study to investigate the impact of the different components of our approach, showing that the combination of binary descriptors with our proposed pooling strategy is a key factor for achieving high performance. Overall, our results demonstrate the effectiveness of the proposed mid-level video representation for pornography detection, which could be used as a building block for other video content analysis tasks. 1 into PostgreSQL...\n",
      "Inserting test sample 2955  In a generalized network design (GND) problem, a set of resources are assigned to multiple communication requests. Each request contributes its weight to the resources it uses and the total load on a resource is then translated to the cost it incurs via a resource specific cost function. For example, a request may be to establish a virtual circuit, thus contributing to the load on each edge in the circuit. Motivated by energy efficiency applications, recently, there is a growing interest in GND using cost functions that exhibit (dis)economies of scale ((D)oS), namely, cost functions that appear subadditive for small loads and superadditive for larger loads.\n",
      "\n",
      "The current paper advances the existing literature on approximation algorithms for GND problems with (D)oS cost functions in various aspects: (1) we present a generic approximation framework that yields approximation results for a much wider family of requests in both directed and undirected graphs; (2) our framework allows for unrelated weights, thus providing the first non-trivial approximation for the problem of scheduling unrelated parallel machines with (D)oS cost functions; (3) our framework is fully combinatorial and runs in strongly polynomial time; (4) the family of (D)oS cost functions considered in the current paper is more general than the one considered in the existing literature, providing a more accurate abstraction for practical energy conservation scenarios; and (5) we obtain the first approximation ratio for GND with (D)oS cost functions that depends only on the parameters of the resources' technology and does not grow with the number of resources, the number of requests, or their weights. The design of our framework relies heavily on Roughgarden's smoothness toolbox (JACM 2015), thus demonstrating the possible usefulness of this toolbox in the area of approximation algorithms. 0 into PostgreSQL...\n",
      "Inserting test sample 2956  This paper presents a novel method for approximating generalized network design under (dis)economies of scale with applications to energy efficiency. Network design is a complex problem that involves finding an optimal solution for connecting a set of nodes with minimum cost or maximum efficiency. In this work, we extend the traditional network design problem to account for (dis)economies of scale, which arise when the cost or efficiency of a network increases as the size of the network grows.\n",
      "\n",
      "We propose an iterative algorithm that approximates the optimal solution for generalized network design with (dis)economies of scale. Our algorithm is based on a decomposition approach, which involves dividing the problem into smaller subproblems that can be solved easily and efficiently. We show that our method is guaranteed to converge to a near-optimal solution, and we provide numerical experiments to demonstrate its effectiveness.\n",
      "\n",
      "We also apply our method to the problem of energy efficiency in networks. Energy efficiency is becoming an increasingly important consideration in modern networks as energy costs rise and environmental concerns become more pressing. Our algorithm can be used to optimize the energy efficiency of a network by finding the best balance between energy consumption and network performance.\n",
      "\n",
      "Finally, we discuss the implications of our work for future research in network design and energy efficiency. Our results suggest that incorporating (dis)economies of scale into network design models can lead to more accurate and efficient solutions. We also highlight the potential for our method to be extended to other domains beyond energy efficiency, such as transportation or telecommunications. 1 into PostgreSQL...\n",
      "Inserting test sample 2957  In this paper we study a constrained minimization problem for the Willmore functional. For prescribed surface area we consider smooth embeddings of the sphere into the unit ball. We evaluate the dependence of the the minimal Willmore energy of such surfaces on the prescribed surface area and prove corresponding upper and lower bounds. Interesting features arise when the prescribed surface area just exceeds the surface area of the unit sphere. We show that (almost) minimizing surfaces cannot be a $C^2$-small perturbation of the sphere. Indeed they have to be nonconvex and there is a sharp increase in Willmore energy with a square root rate with respect to the increase in surface area. 0 into PostgreSQL...\n",
      "Inserting test sample 2958  This paper investigates the confined structures that exhibit the least bending energy. Specifically, we analyze the deformation of slender structures, including columns and beams, when constrained within certain geometries. By using the principle of minimum potential energy, we obtain the governing equations for the configurations that minimize the total bending energy. Our results reveal that confinement induces particular modes of deformation and that these modes are highly affected by certain geometric parameters and loading conditions. Additionally, we show that the minimization of the bending energy leads to optimal designs for slender structures, as it lowers the risk of buckling and other instabilities. Ultimately, this research contributes to the understanding of optimal structures with minimized bending energy. 1 into PostgreSQL...\n",
      "Inserting test sample 2959  We present the HI eXtreme (HIX) galaxy survey targeting some of the most HI rich galaxies in the southern hemisphere. The 13 HIX galaxies have been selected to host the most massive HI discs at a given stellar luminosity. We compare these galaxies to a control sample of average galaxies detected in the HI Parkes All Sky Survey (HIPASS, Barnes et al. 2001). As the control sample is matched in stellar luminosity, we find that the stellar properties of HIX galaxies are similar to the control sample. Furthermore, the specific star formation rate and optical morphology do not differ between HIX and control galaxies. We find, however, the HIX galaxies to be less efficient in forming stars. For the most HI massive galaxy in our sample (ESO075-G006, $\\rm log\\ M_{HI}\\ [M_{\\odot}] = 10.8$) the kinematic properties are the reason for inefficient star formation and HI excess. Examining the Australian Telescope Compact Array (ATCA) HI imaging and Wide Field Spectrograph (WiFeS) optical spectra of ESO075-G006 reveals an undisturbed galaxy without evidence for recent major, violent accretion events. A tilted-ring fit to the HI disc together with the gas-phase oxygen abundance distribution supports the scenario that gas has been constantly accreted onto ESO07-G006 but the high specific angular momentum makes ESO075-G006 very inefficient in forming stars. Thus a massive HI disc has been built up. 0 into PostgreSQL...\n",
      "Inserting test sample 2960  The HIX galaxy survey aims to study the most gas-rich galaxies detected by the HI Parkes All-Sky Survey (HIPASS), providing a comprehensive view of their properties and evolution. Using the Australia Telescope Compact Array and the Parkes Radio Telescope, we have obtained HI observations of a sample of 100 galaxies, selected based on their HI mass and proximity. Our data reveal that these galaxies typically have a high gas fraction and a low stellar-to-gas ratio, suggesting ongoing or recent star formation activity. We have also found evidence of peculiar gas kinematics in some of these systems, such as warped disks or asymmetric velocity fields, which may be linked to gravitational interactions or environmental effects. Moreover, our analysis of the optical and infrared properties of these galaxies indicates that they span a wide range of morphologies and evolutionary stages, from spiral disks to merger remnants. Taken together, our results shed new light on the formation and evolution of the most gas-rich galaxies in the local universe, and provide a valuable benchmark for models of galaxy formation and evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 2961  We report on a sensitive search for H2 1-0 S(1), 1-0 S(0) and 2-1 S(1) ro-vibrational emission at 2.12, 2.22 and 2.25 micron in a sample of 15 Herbig Ae/Be stars employing CRIRES, the ESO-VLT near-infrared high-resolution spectrograph, at R~90,000. We detect the H2 1-0 S(1) line toward HD 100546 and HD 97048. In the other 13 targets, the line is not detected. The H2 1-0 S(0) and 2-1 S(1) lines are undetected in all sources. This is the first detection of near-IR H2 emission in HD 100546. The H2 1-0 S(1) lines observed in HD 100546 and HD 97048 are observed at a velocity consistent with the rest velocity of both stars, suggesting that they are produced in the circumstellar disk. In HD 97048, the emission is spatially resolved and it is observed to extend at least up to 200 AU. We report an increase of one order of magnitude in the H2 1-0 S(1) line flux with respect to previous measurements taken in 2003 for this star, which suggests line variability. In HD 100546 the emission is tentatively spatially resolved and may extend at least up to 50 AU. Modeling of the H2 1-0 S(1) line profiles and their spatial extent with flat keplerian disks shows that most of the emission is produced at a radius >5 AU. Upper limits to the H2 1-0 S(0)/ 1-0 S(1) and H2 2-1 S(1)/1-0 S(1) line ratios in HD 97048 are consistent with H2 gas at T>2000 K and suggest that the emission observed may be produced by X-ray excitation. The upper limits for the line ratios for HD 100546 are inconclusive. Because the H2 emission is located at large radii, for both sources a thermal emission scenario (i.e., gas heated by collisions with dust) is implausible. We argue that the observation of H2 emission at large radii may be indicative of an extended disk atmosphere at radii >5 AU. This may be explained by a hydrostatic disk in which gas and dust are thermally decoupled or by a disk wind caused by photoevaporation. 0 into PostgreSQL...\n",
      "Inserting test sample 2962  This paper presents the results of our survey for near-infrared H2 emission in two Herbig Ae/Be stars, HD 97048 and HD 100546, with the goal of studying the emission from the outer disks. Using the high-resolution spectra obtained with the CRIRES instrument at the European Southern Observatory's Very Large Telescope, we have detected the H2 emission lines in both stars and have analyzed their line profiles and spatial distributions.\n",
      "\n",
      "In HD 97048, we have detected H2 emission in the red-shifted line wing, extending up to at least 500 AU from the central star. The line profile shows a double-peaked structure, indicating that the emission arises from a rotating disk. We have also detected H2 emission in the blue-shifted line wing, although at a lower significance level.\n",
      "\n",
      "In HD 100546, we have detected H2 emission in the blue-shifted line wing, extending up to at least 200 AU from the central star. The line profile shows a single-peaked structure, suggesting that the emission arises from an infalling/outflowing gas. We have not detected H2 emission in the red-shifted line wing, which may indicate an inclination effect or a lower gas density in that part of the disk.\n",
      "\n",
      "We have modeled the H2 emission using a simple disk model and have derived the physical parameters of the emitting gas. For HD 97048, we find that the disk is inclined by about 50 degrees with respect to the line of sight and has a size of about 600 AU. The gas temperature and density are estimated to be about 1000 K and 10^7 cm^-3, respectively. For HD 100546, we find that the emitting gas has a temperature of about 1500 K and a density of about 10^8 cm^-3.\n",
      "\n",
      "Our results are consistent with the scenario that the H2 emission in Herbig Ae/Be stars arises from the hot and dense gas in the outer disks, near the dust sublimation radius. The disk structure and properties inferred from the H2 emission provide important constraints on the accretion and outflow processes in these objects. Further observations and modeling are needed to fully understand the physical nature of the emitting gas and its relation to the disk structure and evolution. 1 into PostgreSQL...\n",
      "Inserting test sample 2963  Radiative E1 decay widths of $\\rm X(3872)$ are calculated through the relativistic Salpeter method, with the assumption that $\\rm X(3872)$ is the $\\chi_{c1}$(2P) state, which is the radial excited state of $\\chi_{c1}$(1P). We firstly calculated the E1 decay width of $\\chi_{c1}$(1P), the result is in agreement with experimental data excellently, then we calculated the case of $\\rm X(3872)$ with the assignment that it is $\\chi_{c1}$(2P). Results are: ${\\Gamma}({\\rm X(3872)}\\rightarrow \\gamma \\sl J/\\psi)=33.0$ keV, ${\\Gamma}({\\rm X(3872)}\\rightarrow \\gamma \\psi(2S))=146$ keV and ${\\Gamma}({\\rm X(3872)}\\rightarrow \\gamma \\psi(3770))=7.09$ keV. The ratio ${{\\rm Br(X(3872)}\\rightarrow\\gamma\\psi(2{\\rm S}))}/{{\\rm Br(X(3872)}\\rightarrow \\gamma {\\sl J}/\\psi)}=4.4$ agrees with experimental data by BaBar, but larger than the new up-bound reported by Belle recently. With the same method, we also predict the decay widths: ${\\Gamma}(\\chi_{b1}(1\\rm P))\\rightarrow \\gamma \\Upsilon(1\\rm S))=30.0$ keV, ${\\Gamma}(\\chi_{b1}(2\\rm P))\\rightarrow \\gamma \\Upsilon(1\\rm S))=5.65$ keV and ${\\Gamma}(\\chi_{b1}(2\\rm P))\\rightarrow \\gamma \\Upsilon(2S))=15.8$ keV, and the full widths: ${\\Gamma}(\\chi_{b1}(1\\rm P))\\sim 85.7$ keV, ${\\Gamma}(\\chi_{b1}(2\\rm P))\\sim 66.5$ keV. 0 into PostgreSQL...\n",
      "Inserting test sample 2964  The X(3872) particle, first observed in the decay of B mesons in 2003, has generated significant interest due to its unconventional properties. Its proximity to the D0DÂ¯âˆ—0 threshold suggests that it may be a bound state, and its unusually low branching ratio in hadronic channels indicates that it may have a significant non-hadronic component. In this work, we present a study of the radiative E1 decays of X(3872) into a J/Ïˆ and a photon, using a sample of proton-proton collision data collected by the LHCb experiment. We observe a significant signal in the region of interest and measure the branching fraction to be (6.5Â±1.5(stat)Â±0.9(syst))Ã—10âˆ’6, corresponding to a partial width of (30Â±7(stat)Â±4(syst)) eV. This is the first observation of the E1 transition in X(3872) and provides new information on its electromagnetic properties. We compare our results with theoretical calculations and discuss the implications for the nature of X(3872). Our measurements constrain the range of possible interpretations and provide valuable input to models aimed at understanding this enigmatic particle. 1 into PostgreSQL...\n",
      "Inserting test sample 2965  Message passing neural networks (MPNN) have provable limitations, which can be overcome by universal networks. However, universal networks are typically impractical. The only exception is random node initialization (RNI), a data augmentation method that results in provably universal networks. Unfortunately, RNI suffers from severe drawbacks such as slow convergence and high sensitivity to changes in hyperparameters. We transfer powerful techniques from the practical world of graph isomorphism testing to MPNNs, resolving these drawbacks. This culminates in individualization-refinement node initialization (IRNI). We replace the indiscriminate and haphazard randomness used in RNI by a surgical incision of only a few random bits at well-selected nodes. Our novel non-intrusive data-augmentation scheme maintains the networks' universality while resolving the trainability issues. We formally prove the claimed universality and corroborate experimentally -- on synthetic benchmarks sets previously explicitly designed for that purpose -- that IRNI overcomes the limitations of MPNNs. We also verify the practical efficacy of our approach on the standard benchmark data sets PROTEINS and NCI1. 0 into PostgreSQL...\n",
      "Inserting test sample 2966  This research paper explores the concept of trainability for universal graph neural networks (GNNs) through the use of surgical randomness. It proposes a novel approach to improving the training of GNNs, which involves selectively dropping nodes and edges during the training process. The authors show that this method can significantly improve the performance of GNNs on a range of benchmark datasets, and that it is particularly effective for graphs with high levels of sparsity and noise. They also investigate the theoretical properties of this approach and demonstrate that it is consistent with the principles of statistical learning theory. Finally, they discuss the implications of their findings for the development of more powerful and robust GNN models, and suggest future avenues for research in this area. Overall, this work contributes to a deeper understanding of the mechanisms underlying GNNs and offers a promising new direction for their improvement. 1 into PostgreSQL...\n",
      "Inserting test sample 2967  We introduce a new microscopic model of the outages in transmission power grids. This model accounts for the automatic response of the grid to load fluctuations that take place on the scale of minutes, when the optimum power flow adjustments and load shedding controls are unavailable. We describe extreme events, initiated by load fluctuations, which cause cascading failures of loads, generators and lines. Our model is quasi-static in the causal, discrete time and sequential resolution of individual failures. The model, in its simplest realization based on the Directed Current description of the power flow problem, is tested on three standard IEEE systems consisting of 30, 39 and 118 buses. Our statistical analysis suggests a straightforward classification of cascading and islanding phases in terms of the ratios between average number of removed loads, generators and links. The analysis also demonstrates sensitivity to variations in line capacities. Future research challenges in modeling and control of cascading outages over real-world power networks are discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 2968  This paper presents a statistical methodology for the classification of cascading failures in power grids. The proposed framework leverages publicly available data on power grid topology and outage events to identify potential factors that contribute to the occurrence of cascading failures. The methodology relies on a multi-step approach that combines exploratory data analysis, feature selection, and machine learning techniques to develop a predictive model capable of distinguishing between cascading and non-cascading failures. We apply our framework to a realistic test case, a modified version of the IEEE 118 bus system, and demonstrate its efficacy in accurately classifying cascading events while achieving a high level of precision and recall. Our results provide insights into the underlying mechanisms driving cascading failures and showcase the potential of statistical methods in improving the resilience of power grids against catastrophic events. 1 into PostgreSQL...\n",
      "Inserting test sample 2969  We consider unmanned aerial vehicle (UAV)-assisted wireless communication employing UAVs as relay nodes to increase the throughput between a pair of transmitter and receiver. We focus on developing effective methods to position the UAV(s) in the sky in the presence of interference in the environment, the existence of which makes the problem non-trivial and our methodology different from the current art. We study the optimal position planning, which aims to maximize the (average) signal-to-interference-ratio (SIR) of the system, in the presence of: i) one major source of interference, ii) stochastic interference.\n",
      "\n",
      "For each scenario, we first consider utilizing a single UAV in the dual-hop relay mode and determine its optimal position. Afterward, multiple UAVs in the multi-hop relay mode are considered, for which we investigate two novel problems concerned with determining the optimal number of required UAVs and developing an optimal fully distributed position alignment method.\n",
      "\n",
      "Subsequently, we propose a cost-effective method that simultaneously minimizes the number of UAVs and determines their optimal positions to guarantee a certain (average) SIR of the system. Alternatively, for a given number of UAVs, we develop a fully distributed placement algorithm along with its performance guarantee. Numerical simulations are provided to evaluate the performance of our proposed methods. 0 into PostgreSQL...\n",
      "Inserting test sample 2970  Interference avoidance is a critical challenge in designing unmanned aerial vehicle (UAV) networks due to the radio-frequency (RF) interference between the neighboring links. In this paper, we propose an interference avoidance position planning (IAPP) scheme for dual-hop and multi-hop UAV relay networks. The IAPP scheme determines the optimal positioning of the UAVs by considering both the RF interference caused by the transmission over the same frequency and the multi-path propagation of wireless signals. The UAVs' positions are optimized such that the overall network capacity is maximized while the interference between the links is minimized. The proposed scheme employs a hybrid approach combining the particle swarm optimization algorithm and the gradient descent method, which has been shown to achieve a balance between exploration and exploitation for the optimization problem. Simulation results demonstrate that the proposed IAPP scheme outperforms the existing schemes in terms of network capacity, bit error rate and energy efficiency. This work lays a foundation for the practical application of UAV relay networks in various scenarios, such as search-and-rescue missions, environmental monitoring, and disaster response. 1 into PostgreSQL...\n",
      "Inserting test sample 2971  Controlling thermal transport at the nanoscale is vital for many applications. Previously, it has been shown that this control can be achieved with periodically nanostructured two-dimensional phononic crystals, for the case of suspended devices. Here we show that thermal conductance can also be controlled with three-dimensional phononic crystals, allowing the engineering of the thermal contact of more varied devices without the need of suspension in the future. We show experimental results measured at sub-Kelvin temperatures for two different period three-dimensional crystals, as well as for a bulk control structure. The results show that the conductance can be enhanced with the phononic crystal structures in our geometry. This result cannot be fully explained by the simplest theory taking into account the coherent modification of the phonon band structure, calculated with finite element method simulations. 0 into PostgreSQL...\n",
      "Inserting test sample 2972  This study presents a novel approach to control heat conduction in solids using three-dimensional phononic crystals. By tailoring the acoustic properties of the crystal structure, phononic crystals can be designed to allow or impede the transmission of acoustic waves which can be mapped onto thermal transport. Here, we numerically investigate the use of phononic crystals to manipulate thermal conductance. We demonstrate that the presence of bandgaps in the phononic crystal structure results in the suppression of heat flow in certain frequency ranges. Further, by varying the geometry of the crystal, we show that the position and width of these bandgaps can be tuned, enabling greater control over thermal transport. Our findings represent a significant step towards the development of next-generation materials with tailored thermal properties. 1 into PostgreSQL...\n",
      "Inserting test sample 2973  We present multi-sightline absorption spectroscopy of cool gas around three lensing galaxies at z=0.4-0.7. These lenses have half-light radii r_e=2.6-8 kpc and stellar masses of log M*/Ms=10.9-11.4, and therefore resemble nearby passive elliptical galaxies. The lensed QSO sightlines presented here occur at projected distances of d=3-15 kpc (or d~1-2 r_e) from the lensing galaxies, providing for the first time an opportunity to probe both interstellar gas at r~r_e and circumgalactic gas at larger radii r>>re of these distant quiescent galaxies. We observe distinct gas absorption properties among different lenses and among sightlines of individual lenses. Specifically, while the quadruple lens for HE0435-1223 shows no absorption features to very sensitive limits along all four sightlines, strong Mg II, Fe II, Mg I, and Ca II absorption transitions are detected along both sightlines near the double lens for HE0047-1756, and in one of the two sightlines near the double lens for HE1104-1805. The absorbers are resolved into 8-15 individual components with a line-of-sight velocity spread of dv~300-600 km/s. The large ionic column densities, log N>14, observed in two components suggest that these may be Lyman limit or damped Lya absorbers with a significant neutral hydrogen fraction. The majority of the absorbing components exhibit a uniform super solar Fe/Mg ratio with a scatter of <0.1 dex across the full dv range. Given a predominantly old stellar population in these lensing galaxies, we argue that the observed large velocity width and Fe-rich abundance pattern can be explained by SNe Ia enriched gas at radius r~r_e. We show that additional spatial constraints in line-of-sight velocity and relative abundance ratios afforded by a multi-sightline approach provide a powerful tool to resolve the origin of chemically-enriched cool gas in massive halos. 0 into PostgreSQL...\n",
      "Inserting test sample 2974  The interstellar and circumgalactic gas of galaxies have significant implications for their formation, evolution, and environment. In this study, we present observations of the cool gas associated with three massive lensing galaxies at redshifts between 0.4 and 0.7. We use multi-wavelength observations from the Keck, Magellan, and Gemini telescopes to probe the properties of the gas through absorption-line spectroscopy. Our analysis reveals evidence for H I, Mg II, and Fe II gas absorption systems associated with the lensing galaxies, as well as detecting several low-ionization state metal lines. By combining the observed data sets, we construct a detailed picture of the spatial and kinematic properties of the gas, and we analyze the physical conditions and chemical abundances of the gas.\n",
      "\n",
      "We find evidence for significant outflows in two of the three systems, indicating the presence of powerful feedback mechanisms in these massive galaxies. This feedback is likely driven by the high-energy radiation from massive stars and active galactic nuclei that power the central black holes. Our results provide new insight into the physical processes that govern the formation and evolution of massive galaxies, suggesting that outflowing gas plays a key role in shaping the properties of these systems.\n",
      "\n",
      "Additionally, we also investigate the circumgalactic medium surrounding these massive galaxies, finding evidence for extended H I gas and infalling neutral gas clouds. The interaction between the cool gas and the cosmic web can have significant consequences for galaxy evolution, including fueling star formation and triggering galactic winds. Our results support the idea that galaxies are complex and dynamic systems that are intricately connected to their environment.\n",
      "\n",
      "In conclusion, our study presents a comprehensive analysis of the interstellar and circumgalactic gas associated with three massive lensing galaxies at redshifts between 0.4 and 0.7. Our results shed light on the complex physical processes that regulate galaxy evolution and highlight the importance of studying the cool gas in the circumgalactic medium. 1 into PostgreSQL...\n",
      "Inserting test sample 2975  The aim of this work is to investigate the average properties of the intra-cluster medium (ICM) magnetic fields, and to search for possible correlations with the ICM thermal properties and cluster radio emission. We have selected a sample of 39 massive galaxy clusters from the HIghest X-ray FLUx Galaxy Cluster Sample, and used Northern VLA Sky Survey data to analyze the fractional polarization of radio sources out to 10 core radii from the cluster centers. Following Murgia et al (2004), we have investigated how different magnetic field strengths affect the observed polarized emission of sources lying at different projected distances from the cluster center. In addition, statistical tests are performed to investigate the fractional polarization trends in clusters with different thermal and non-thermal properties. We find a trend of the fractional polarization with the cluster impact parameter, with fractional polarization increasing at the cluster periphery and decreasing toward the cluster center. Such trend can be reproduced by a magnetic field model with central value of few $\\mu$G. The logrank statistical test indicates that there are no differences in the depolarization trend observed in cluster with and without radio halo, while the same test indicates significant differences when the depolarization trend of sources in clusters with and without cool core are compared. The comparison between clusters with high and low temperatures does not yields significant differences. Although therole of the gas density should be better accounted for, these results give important indications for models that require a role of the ICM magnetic field to explain the presence of cool core and radio halos in galaxy clusters. 0 into PostgreSQL...\n",
      "Inserting test sample 2976  The intergalactic medium (ICM) is known for its complex structure, including magnetic fields that permeate the space between galaxies. Detailed studies of these magnetic fields have the potential to shed light on the underlying physical processes that govern the structure and evolution of the ICM. One key observable parameter that has been used to study these magnetic fields is fractional polarization, which is defined as the ratio of the polarized component to the total intensity of radiation detected from the source.\n",
      "\n",
      "In recent years, there has been a growing interest in using fractional polarization measurements to map the strength and orientation of magnetic fields in the ICM. In particular, studies have focused on the role of turbulence and shock waves in amplifying and shearing these fields, as well as the influence of galaxy clusters and filaments on their structure.\n",
      "\n",
      "In this paper, we present a detailed analysis of fractional polarization measurements obtained from a large sample of radio sources in the ICM. Our results reveal a complex network of magnetic fields that are influenced by a range of physical processes, including turbulence, shocks, and the presence of nearby galaxies and clusters. We also find evidence for a correlation between the strength of these fields and the properties of the surrounding gas, suggesting that magnetic fields play an important role in shaping the structure and evolution of the ICM.\n",
      "\n",
      "Overall, our study highlights the potential of fractional polarization measurements as a powerful tool for investigating the physical processes that govern the magnetic fields in the ICM. Our findings provide new insights into the complex interplay between magnetic fields and the surrounding gas, and have important implications for our understanding of the formation and evolution of galaxies and galaxy clusters. 1 into PostgreSQL...\n",
      "Inserting test sample 2977  We report the relationship between epitaxial strain and the crystallographic orientation of the in-phase rotation axis and A-site displacements in Pbnm-type perovskite films. Synchrotron diffraction measurements of EuFeO3 films under strain states ranging from 2% compressive to 0.9% tensile on cubic or rhombohedral substrates exhibit a combination of a-a+c- and a+a-c- rotational patterns. We compare the EuFeO3 behavior with previously reported experimental and theoretical work on strained Pbnm-type films on non-orthorhombic substrates, as well as additional measurements from LaGaO3, LaFeO3, and Eu0.7Sr0.3MnO3 films on SrTiO3. Compiling the results from various material systems reveals a general strain dependence in which compressive strain strongly favors a-a+c- and a+a-c- rotation patterns and tensile strain weakly favors a-a-c+ structures. In contrast, EuFeO3 films grown on Pbnm-type GdScO3 under 2.3% tensile strain take on a uniform a-a+c- rotation pattern imprinted from the substrate, despite strain energy considerations that favor the a-a-c+ pattern. These results point to the use of substrate imprinting as a more robust route than strain for tuning the crystallographic orientations of the octahedral rotations and A-site displacements needed to realize rotation-induced hybrid improper ferroelectricity in oxide heterostructures. 0 into PostgreSQL...\n",
      "Inserting test sample 2978  This paper investigates the octahedral rotation patterns in strained EuFeO3 and other Pbnm perovskite films, and evaluates the implications for hybrid improper ferroelectricity. By utilizing transmission electron microscopy and density functional calculations, we found that the octahedral rotations in the EuFeO3 film differ from those in the bulk material, suggesting a correlation between film stress and rotation direction. Additionally, we discovered that the relative orientation of neighboring octahedral rotations controls the hybrid improper polar distortions in Pbnm perovskites, with certain rotation patterns delivering optimal ferroelectric performance. Our findings provide insights into how the octahedral rotation patterns can be fine-tuned in strained perovskite films, possibly unlocking unprecedented functionalities in hybrid improper ferroelectricity. Furthermore, the results of this study suggest that strain engineering, combined with polarization control, could enhance the properties of perovskite-based devices and pave the way towards innovative applications in areas such as memory devices, energy-harvesting, and multifunctional sensors. 1 into PostgreSQL...\n",
      "Inserting test sample 2979  Nonnative residual interactions have attracted increasing attention in recent protein folding researches. Experimental and theoretical investigations had been set out to catch nonnative contacts that might dominate key events in protein folding. However, energetic frustrations caused by nonnative inter-residue interactions are not systematically characterized, due to the complicated folding mechanism. Recently, we studied the folding of a set of homologous all-{\\alpha} proteins and found that nonnative-contact-based energetic frustrations are closely related to protein native-contact networks.\n",
      "\n",
      "In this paper, we studied the folding of nine homologous immunoglobulin-like (Ig-like) \\b{eta}-sandwich proteins and examined energetic frustrations caused by nonnative inter-residue interactions, based on analyses of residual phi-values and contact maps of transition state ensembles. The proteins share similar tertiary structures, thus minimize topology frustration differences in protein folding and highlighting the effects of energetic frustrations as caused by hydrophilic-hydrophobic mutations. Our calculations showed that energetic frustrations have highly heterogeneous effects on the folding of most secondary structures and on the folding correlations between different folding-patches of \\b{eta}-sandwich proteins. The simulations revealed a strong interplay between energetic frustrations and native-contact networks in \\b{eta}-sandwich domains, which ensures that \\b{eta}-sandwich domains follow a common folding mechanism. Our results suggested that the folding processes of \\b{eta}-sandwich proteins might be redesigned by carefully manipulating energetic frustrations at residue level. 0 into PostgreSQL...\n",
      "Inserting test sample 2980  This study investigates the energetic frustrations encountered in protein folding at the residue level through simulation of homologous immunoglobin-like Î²-sandwich proteins. Protein folding is a complex process involving several intermediate states that lead to the native state. However, there are energetic barriers or frustrations that hinder the folding process to achieve the correct structure. Î²-sandwich proteins occupy a prominent place in the protein kingdom, and their fold topology is characterized by a pair of stacked anti-parallel Î²-sheets that form the central core. In this study, replica exchange molecular dynamics simulations were performed on a pair of homologous proteins with diverse sequences but a common fold. The results showed that the folding of both proteins experienced similar energetic barriers, localizing mainly at specific positions within the primary sequence. These positions corresponded to the Î²-turns at the C-terminal end of strands, the sequence of which determines the folding properties of Î²-sandwich proteins. The study provides a new perspective on the role of energetic frustrations in protein folding and highlights the potential of computational methods to investigate protein folding and identify targets for rational protein design. 1 into PostgreSQL...\n",
      "Inserting test sample 2981  The AMPS paradox challenges black hole complementarity by apparently constructing a way for an observer to bring information from the outside of the black hole into its interior if there is no drama at its horizon, making manifest a violation of monogamy of entanglement. We propose a new resolution to the paradox: this violation cannot be explicitly checked by an infalling observer in the finite proper time they have to live after crossing the horizon. Our resolution depends on a weak relaxation of the no-drama condition (we call it \"little drama\") which is the \"complementarity dual\" of scrambling of information on the stretched horizon. When translated to the description of the black hole interior, this implies that the fine-grained quantum information of infalling matter is rapidly diffused across the entire interior while classical observables and coarse-grained geometry remain unaffected. Under the assumption that information has diffused throughout the interior, we consider the difficulty of the information-theoretic task that an observer must perform after crossing the event horizon of a Schwarzschild black hole in order to verify a violation of monogamy of entanglement. We find that the time required to complete a necessary subroutine of this task, namely the decoding of Bell pairs from the interior and the late radiation, takes longer than the maximum amount of time that an observer can spend inside the black hole before hitting the singularity. Therefore, an infalling observer cannot observe monogamy violation before encountering the singularity. 0 into PostgreSQL...\n",
      "Inserting test sample 2982  Rescuing complementarity, a central principle of quantum mechanics, from the paradoxes that have plagued it for decades has been a long-standing challenge in the field. While various interpretations of quantum mechanics have been proposed to address this issue, none of them seem entirely satisfactory. In this paper, we present a novel approach to rescuing complementarity that avoids adding any additional assumptions or theoretical constructs. Our approach relies on a modification of the measurement process itself, which preserves both the complementary observables and their uncertainty relations. We demonstrate the effectiveness of this approach through a series of experiments that verify the preservation of complementarity in various situations. These experiments include measurements of spin and position, as well as entangled systems. Additionally, we compare our approach to other strategies that attempt to rescue complementarity and show that our method produces results that are competitive, if not superior, to those obtained through other approaches. Our results demonstrate that it is possible to rescue complementarity without resorting to ad hoc assumptions or exotic theories, and that this can be achieved with little drama. We believe that our approach can pave the way for a deeper understanding of complementarity, as well as provide new insights into the nature of quantum mechanics. 1 into PostgreSQL...\n",
      "Inserting test sample 2983  In this paper, we present multimodal 2D +3D face recognition method using block based curvelet features. The 3D surface of face (Depth Map) is computed from the stereo face images using stereo vision technique. The statistical measures such as mean, standard deviation, variance and entropy are extracted from each block of curvelet subband for both depth and intensity images independently.In order to compute the decision score, the KNN classifier is employed independently for both intensity and depth map. Further, computed decision scoresof intensity and depth map are combined at decision level to improve the face recognition rate. The combination of intensity and depth map is verified experimentally using benchmark face database. The experimental results show that the proposed multimodal method is better than individual modality. 0 into PostgreSQL...\n",
      "Inserting test sample 2984  Multi-modal face recognition is an emerging area of research in computer vision, which aims to improve the accuracy and robustness of face recognition systems by integrating information from multiple sources, such as visual, thermal, and depth data. In this paper, we propose a novel approach for multi-modal face recognition using block-based curvelet features. Curvelets are a powerful mathematical tool for analyzing and representing images, which can capture complex structures and features at different scales and orientations. Our method integrates the curvelet-based features from multiple modalities to improve the recognition performance and overcome the limitations of individual modalities. Experimental results on several multi-modal face datasets demonstrate the effectiveness of our approach compared to state-of-the-art methods. This work has implications for applications such as surveillance, access control, and biometric identification. 1 into PostgreSQL...\n",
      "Inserting test sample 2985  Three mode parametric instability has been predicted in Advanced gravitational wave detectors. Here we present the first observation of this phenomenon in a large scale suspended optical cavity designed to be comparable to those of advanced gravitational wave detectors. Our results show that previous modelling assumptions that transverse optical modes are stable in frequency except for frequency drifts on a thermal deformation time scale is unlikely to be valid for suspended mass optical cavities. We demonstrate that mirror figure errors cause a dependence of transverse mode offset frequency on spot position. Combined with low frequency residual motion of suspended mirrors, this leads to transverse mode frequency modulation which suppresses the effective parametric gain. We show that this gain suppression mechanism can be enhanced by laser spot dithering or fast thermal modulation. Using Advanced LIGO test mass data and thermal modelling we show that gain suppression factors of 10-20 could be achieved for individual modes, sufficient to greatly ameliorate the parametric instability problem. 0 into PostgreSQL...\n",
      "Inserting test sample 2986  This paper investigates the problem of parametric instability in long optical cavities and proposes a solution for its suppression. Parametric instability is a common phenomenon in optical cavities where the optical field in the cavity interacts with the cavity mirrors to produce mechanical oscillations. These oscillations can lead to instabilities and reduce the quality of the output beam. Our proposed solution involves modulating the frequency of the transverse modes of the cavity, which has been shown to mitigate the instability by breaking the symmetry of the system. The effectiveness of the proposed solution was demonstrated through numerical simulations and experimental results obtained from a prototype cavity. Our findings suggest that dynamic transverse mode frequency modulation can be an effective technique to suppress parametric instability in long optical cavities, which can have significant implications for a range of applications such as gravitational wave detectors and high-power laser systems. 1 into PostgreSQL...\n",
      "Inserting test sample 2987  The well-known word analogy experiments show that the recent word vectors capture fine-grained linguistic regularities in words by linear vector offsets, but it is unclear how well the simple vector offsets can encode visual regularities over words. We study a particular image-word relevance relation in this paper. Our results show that the word vectors of relevant tags for a given image rank ahead of the irrelevant tags, along a principal direction in the word vector space. Inspired by this observation, we propose to solve image tagging by estimating the principal direction for an image. Particularly, we exploit linear mappings and nonlinear deep neural networks to approximate the principal direction from an input image. We arrive at a quite versatile tagging model. It runs fast given a test image, in constant time w.r.t.\\ the training set size. It not only gives superior performance for the conventional tagging task on the NUS-WIDE dataset, but also outperforms competitive baselines on annotating images with previously unseen tags 0 into PostgreSQL...\n",
      "Inserting test sample 2988  In recent years, zero-shot learning has emerged as a new paradigm in the fields of computer vision and machine learning. By leveraging semantic information from auxiliary sources such as text descriptions, zero-shot learning methods allow for image recognition on categories that lack training data. In this paper, we propose a novel fast zero-shot image tagging framework that adopts a multi-modal architecture consisting of both visual and textual representations. Our proposed method is designed to learn a robust cross-modal mapping between the visual and textual domains, enabling efficient image annotation without requiring explicit image category labels. We demonstrate the effectiveness of our approach on several benchmark datasets, achieving superior tagging performance compared to state-of-the-art approaches while maintaining high computational efficiency. Additionally, we conduct extensive ablation studies to analyze the impact of various components in our framework and show the effectiveness of our proposed method in challenging zero-shot tagging scenarios. 1 into PostgreSQL...\n",
      "Inserting test sample 2989  We present the data and our analysis of MIR fine-structure emission lines detected in Spitzer/IRS high-res spectra of 202 local LIRGs observed as part of the GOALS project. We detect emission lines of [SIV], [NeII], [NeV], [NeIII], [SIII]18.7, [OIV], [FeII], [SIII]33.5, and [SiII]. Over 75% of our galaxies are classified as starburst (SB) sources in the MIR. We compare ratios of the emission line fluxes to stellar photo- and shock-ionization models to constrain the gas properties in the SB nuclei. Comparing the [SIV]/[NeII] and [NeIII]/[NeII] ratios to the Starburst99-Mappings III models with an instantaneous burst history, the line ratios suggest that the SB in our LIRGs have ages of 1-4.5Myr, metallicities of 1-2Z_sun, and ionization parameters of 2-8e7cm/s. Based on the [SIII]/[SIII] ratios, the electron density in LIRG nuclei has a median electron density of ~300cm-3 for sources above the low density limit. We also find that strong shocks are likely present in 10 SB sources. A significant fraction of the GOALS sources have resolved neon lines and 5 show velocity differences of >200km/s in [NeIII] or [NeV] relative to [NeII]. Furthermore, 6 SB and 5 AGN LIRGs show a trend of increasing line width with ionization potential, suggesting the possibility of a compact energy source and stratified ISM in their nuclei. We confirm a strong correlation between the [NeII]+[NeIII] emission, as well as [SIII]33.5, with both the IR luminosity and the 24um warm dust emission measured from the spectra. Finally, we find no correlation between the hardness of the radiation field or the line width and the ratio of the total IR to 8um emission (IR8). This may be because the IR luminosity and the MIR fine-structure lines are sensitive to different timescales over the SB, or that IR8 is more sensitive to the geometry of the warm dust region than the radiation field producing the HII region emission. 0 into PostgreSQL...\n",
      "Inserting test sample 2990  Luminous Infrared Galaxies (LIRGs) have garnered considerable attention due to their high energy emission properties, particularly in the mid-infrared (MIR) range. Studying the atomic fine structure emission line spectra of these galaxies can provide insight into the processes occurring within them. This paper presents an analysis of Spitzer/IRS spectra of the LIRGs in the Great Observatories All-Sky LIRG Survey (GOALS), with the aim of characterizing their MIR emission line spectra and the dominant energy sources within these galaxies.\n",
      "\n",
      "We used the SMART software package to extract the spectra of 202 galaxies from the GOALS sample, covering the wavelength range of 5-40 Âµm. The spectra were then fitted using the APEC code for optically thin plasma emission, which allowed us to identify 20 fine-structure emission lines from six heavy elements, namely, Ne, Fe, S, Ar, Si, and P. By comparing our measurements to photoionization models of active galactic nuclei (AGNs) and star-forming regions, we found that the dominant energy source in most of the LIRGs is star formation.\n",
      "\n",
      "We also investigated the correlations among the fine-structure line ratios and various galaxy properties, such as luminosity, infrared color, and star formation rate (SFR). We found that the line ratios involving Ne and Si are strongly correlated with the SFR, indicating that they are good tracers of star formation. On the other hand, the line ratios involving Fe are weakly correlated with SFR, suggesting that Fe emission is primarily associated with the hot plasma of an AGN. We also observed clear trends in the line ratios with galaxy metallicity, with higher ratios of Ne/V and Ne/Fe observed in lower-metallicity galaxies.\n",
      "\n",
      "Overall, our analysis provides valuable insights into the MIR atomic fine-structure emission line spectra of LIRGs, shedding light on the dominant energy sources and the chemical composition of these galaxies. Our results have important implications for the understanding of galaxy evolution and the role of star formation and AGNs in shaping the properties of these fascinating objects. 1 into PostgreSQL...\n",
      "Inserting test sample 2991  We first study the thermodynamics of Bardeen-AdS black hole by the $T$-$r_{h}$ diagram, where T is the Hawking temperature and $r_{h}$ is the radius of event horizon. The cut-off radius which is the minimal radius of the thermodynamical stable Bardeen black hole can be got, and the cut-off radius is the same with the result of the heat capacity analysis. Moreover, by studying the parameter $g$, which is interpreted as a gravitationally collapsed magnetic monopole arising in a specific form of non-linear electrodynamics, in the Bardeen black hole, we can get a critical value $g_{m}$ and different phenomenons with different values of parameter $g$. For $g>g_{m}$, there is no second order phase transition. We also research the thermodynamical stability of the Bardeen black hole by the Gibbs free energy and the heat capacity. In addition, the phase transition is discussed. 0 into PostgreSQL...\n",
      "Inserting test sample 2992  In this paper, we investigate the thermodynamics of the Bardeen black hole in the context of Anti-de Sitter space. We explore the properties of the black hole and its thermodynamic quantities, including the mass, temperature, and entropy. By applying the first law of thermodynamics, we derive the corresponding expressions for the changes in these quantities due to variations of the black hole's mass and charge. Our analysis reveals interesting results regarding the thermodynamic stability of the black hole, which we investigate in detail. Additionally, we examine the thermodynamic phase transitions that can occur in this system, and discuss the corresponding phase diagrams. We conclude by highlighting the implications of our study for the understanding of black hole thermodynamics and the thermodynamics of Anti-de Sitter space. 1 into PostgreSQL...\n",
      "Inserting test sample 2993  We report a measurement of the B^0_s meson lifetime from B_s^0 -> D^-_s X decays, where D_s^- mesons are reconstructed in the D_s^- -> phi pi^- and D_s^- -> K^{*0} K^- decay channels. From approximately 3.7 million hadronic Z^0 decays recorded by the OPAL detector at LEP a sample is selected containing 911 +/- 83 candidates, of which 519 +/- 136 are estimated to be from B_s^0 meson decays. Fitting the distribution of the distance from the beam spot to the decay vertex of the D_s^- candidates with an unbinned likelihood function we measure tau(B_s^0) = 1.72 + 0.20 - 0.19 + 0.18 - 0.17 ps, where the errors are statistical and systematic, respectively. 0 into PostgreSQL...\n",
      "Inserting test sample 2994  In this study, we present a measurement of the lifetime of the B_s^0 meson using the decay chain B_s^0 -> D_s^- pi^+. The decay products are reconstructed in a sample of proton-proton collision data at a center-of-mass energy of 13 TeV collected using the CMS detector at the LHC. The measurement is based on the analysis of the decay-time distribution of the reconstructed D_s^- mesons. Our result, tau(B_s^0) = 1.504 Â± 0.040(stat) Â± 0.021(syst) ps, is consistent with previous measurements and with the world average value. This measurement is important for testing the predictions of the Standard Model and for future searches for new physics in the B_s^0 sector. 1 into PostgreSQL...\n",
      "Inserting test sample 2995  We analyze the RXTE observations of the 2009 outburst of H~1743-322, as well as the observations of the previous five outbursts for comparison. The hardness-intensity diagram (HID) shows a complete counter-clockwise q-track for the 2009 outburst and, interestingly, the track falls in} between a huge one in 2003, with a complete transition to high/soft state, and that of} the failed outburst in 2008. It leaves the low/hard state but does not reach the leftmost edge of the overall HID. While the lowest hardness (6--19 keV/3--6 keV) values} in the HID is about 0.3--0.4 for the 2009 outburst, similar to the ``failed state transition\" seen in the persistent black hole XRB Cyg X-1, the timing analysis shows that a transition to the high soft state occurred. During the low/hard state of the 2009 outburst, the inner radius of the accretion disk is found to be closer to the central black hole and have an anti-correlation with the disk temperature. These results may be understood as the reprocessing} of the hot corona on the disk's} soft X-rays, which can lead to an underestimation of the inner radius of the accretion disk. In the luminosity diagram of the corona versus the disk, the tracks of the outbursts} in 2003 and 2009 cross the line which represents a roughly equal contribution to the entire emission from the thermal and the non-thermal components;} the track of the 2008 outburst has the turn-over falling} on this line. This may be indicative of an emission balance between the corona and the disk, which prevents the state transition from going further than the low/hard state. 0 into PostgreSQL...\n",
      "Inserting test sample 2996  The binary black hole system H~1743-322 underwent a major outburst in 2009, which was observed and studied by the Rossi X-ray Timing Explorer (RXTE) satellite. The outburst started in February 2009 and exhibited complex temporal and spectral behavior over the following months. We present the results of our analysis of the RXTE observations during this outburst.\n",
      "\n",
      "We find that the outburst displayed a number of interesting features, including significant variability in the X-ray spectra and a flaring behavior which was particularly pronounced in the hard X-ray band. We also observed quasi-periodic oscillations in the power density spectra, with a characteristic frequency of around 0.1 Hz. These observations provide additional support for the idea that black hole systems like H~1743-322 can exhibit very complex X-ray variability during outburst events.\n",
      "\n",
      "We used a number of different spectral models to fit the RXTE data, including thermal disk models and Comptonization models. We found that a thermal plus Comptonization model provided the best fit to the data, indicating the presence of both a soft thermal component and a hard, power-law-like component. We analyze the evolution of the spectral parameters over the course of the outburst and find that the spectral shape and flux changed significantly during the transition from the initial hard phase to the intermediate and soft phases.\n",
      "\n",
      "Overall, our analysis of the RXTE observations of the 2009 outburst of H~1743-322 provides further insights into the complex behavior of black hole binary systems during outburst events. Our results shed light on the physical mechanisms at work in these systems, and may have important implications for our understanding of accretion processes in astrophysics. 1 into PostgreSQL...\n",
      "Inserting test sample 2997  Ground state cooling of massive mechanical objects remains a difficult task restricted by the unresolved mechanical sidebands. We propose an optomechanically-induced-transparency cooling scheme to achieve ground state cooling of mechanical motion without the resolved sideband condition in a pure optomechanical system with two mechanical modes coupled to the same optical cavity mode. We show that ground state cooling is achievable for sideband resolution $\\omega_{m}/\\kappa$ as low as 0.003. This provides a new route for quantum manipulation of massive macroscopic devices and high-precision measurements. 0 into PostgreSQL...\n",
      "Inserting test sample 2998  We experimentally demonstrate the use of optomechanically-induced-transparency (OMIT) cooling to prepare the quantum ground state of a massive mechanical resonator. We achieve a cooling factor of 11.1 in our system, which is the highest value reported so far for OMIT cooling. Our approach offers a promising pathway towards achieving ground-state cooling of massive oscillators. We also discuss the potential applications of our technique in precision measurements, quantum information processing, and fundamental tests of quantum mechanics. 1 into PostgreSQL...\n",
      "Inserting test sample 2999  We study a 24\\,$\\mu$m selected sample of 330 galaxies observed with the Infrared Spectrograph for the 5\\,mJy Unbiased Spitzer Extragalactic Survey. We estimate accurate total infrared luminosities by combining mid-IR spectroscopy and mid-to-far infrared photometry, and by utilizing new empirical spectral templates from {\\em Spitzer} data. The infrared luminosities of this sample range mostly from 10$^9$L$_\\odot$ to $10^{13.5}$L$_\\odot$, with 83% in the range 10$^{10}$L$_\\odot$$<$L$_{\\rm IR}$$<10^{12}$L$_\\odot$. The redshifts range from 0.008 to 4.27, with a median of 0.144. The equivalent widths of the 6.2\\,$\\mu$m aromatic feature have a bimodal distribution. We use the 6.2\\,$\\mu$m PAH EW to classify our objects as SB-dominated (44%), SB-AGN composite (22%), and AGN-dominated (34%). The high EW objects (SB-dominated) tend to have steeper mid-IR to far-IR spectral slopes and lower L$_{\\rm IR}$ and redshifts. The low EW objects (AGN-dominated) tend to have less steep spectral slopes and higher L$_{\\rm IR}$ and redshifts. This dichotomy leads to a gross correlation between EW and slope, which does not hold within either group. AGN dominated sources tend to have lower log(L$_{\\rm PAH 7.7\\mu m}$/L$_{\\rm PAH 11.3\\mu m}$) ratios than star-forming galaxies, possibly due to preferential destruction of the smaller aromatics by the AGN. The log(L$_{\\rm PAH 7.7\\mu m}$/L$_{\\rm PAH 11.3\\mu m}$) ratios for star-forming galaxies are lower in our sample than the ratios measured from the nuclear spectra of nearby normal galaxies, most probably indicating a difference in the ionization state or grain size distribution between the nuclear regions and the entire galaxy.\n",
      "\n",
      "Finally, we provide a calibration relating the monochromatic 5.8, 8, 14 and 24um continuum or Aromatic Feature luminosity to L$_{\\rm IR}$ for different types of objects. 0 into PostgreSQL...\n",
      "Inserting test sample 3000  This research paper explores the relationship between infrared luminosities and aromatic features in the 24um Flux Limited Sample of 5MUSES. The study is based on the spectral analysis of 144 galaxies observed by the Spitzer Space Telescope's Infrared Spectrograph. By analyzing the mid-infrared spectra, we were able to determine the luminosity of the polycyclic aromatic hydrocarbon (PAH) emissions and the strength of the aromatic emission features, which are related to star formation and interstellar dust grains.\n",
      "\n",
      "The results show a correlation between the aromatic emission features and the infrared luminosities of galaxies. Specifically, we found that galaxies with higher aromatic feature strengths tended to have higher infrared luminosities. This suggests that there is a connection between the formation of aromatic hydrocarbons and the heating mechanism that produces the infrared luminosity in galaxies.\n",
      "\n",
      "Furthermore, we found that the luminosity of PAH emissions is also correlated with the infrared luminosities of galaxies. This indicates that the presence of PAHs is closely linked to the processes that create the infrared luminosity in galaxies. The results also show that the PAH luminosity tends to increase with increasing infrared luminosity, further supporting the correlation between these two factors.\n",
      "\n",
      "Overall, our findings provide evidence for the importance of the PAH features and luminosities as indicators of star formation and other processes in galaxies. The results may also be useful in understanding the evolution and properties of galaxies, as well as in refining models of interstellar dust grains and their role in galaxy formation and evolution. Further studies with more detailed observations and larger sample sizes will be important in confirming and extending our results. 1 into PostgreSQL...\n",
      "âœ… Test data stored in PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a new table for the test dataset if not exists\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS test_text_embeddings (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        text TEXT UNIQUE,\n",
    "        label INTEGER,\n",
    "        embedding TEXT\n",
    "    );\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "print(\"âœ… Table 'test_text_embeddings' created successfully.\")\n",
    "\n",
    "# Function to insert test data into PostgreSQL\n",
    "def insert_test_data(text, label):\n",
    "    cursor.execute('''\n",
    "        INSERT INTO test_text_embeddings (text, label) \n",
    "        VALUES (%s, %s) ON CONFLICT (text) DO NOTHING;\n",
    "    ''', (text, label))\n",
    "    conn.commit()\n",
    "\n",
    "# Insert test data into PostgreSQL\n",
    "for idx, row in df_test.iterrows():\n",
    "    print(f\"Inserting test sample {idx + 1}  {row['text']} {row['label']} into PostgreSQL...\")\n",
    "    insert_test_data(row[\"text\"], row[\"label\"])\n",
    "\n",
    "print(\"âœ… Test data stored in PostgreSQL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ollama\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def update_test_embedding(text):\n",
    "    \"\"\"\n",
    "    Updates the embedding for a given test text in PostgreSQL.\n",
    "    \"\"\"\n",
    "    # Check if embedding already exists\n",
    "    cursor.execute(\"SELECT embedding FROM test_text_embeddings WHERE text = %s\", (text,))\n",
    "    result = cursor.fetchone()\n",
    "\n",
    "    if result and result[0]:  # Embedding exists\n",
    "        return None  # Skip reprocessing\n",
    "\n",
    "    # Generate embedding if missing\n",
    "    try:\n",
    "        embedding_data = ollama.embeddings(model=\"mxbai-embed-large\", prompt=text)\n",
    "        embedding_tensor = torch.tensor(embedding_data[\"embedding\"], dtype=torch.float32)\n",
    "\n",
    "        # Convert tensor to string for PostgreSQL storage\n",
    "        embedding_str = \",\".join(map(str, embedding_tensor.tolist()))\n",
    "\n",
    "        # Update the row with the new embedding\n",
    "        cursor.execute('''\n",
    "            UPDATE test_text_embeddings \n",
    "            SET embedding = %s WHERE text = %s;\n",
    "        ''', (embedding_str, text))\n",
    "        conn.commit()\n",
    "\n",
    "        return embedding_tensor\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating test embedding: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Found 3000 test samples that need embeddings.\n",
      "âœ… Processed 100/3000 (3.33%)\n",
      "âœ… Processed 200/3000 (6.67%)\n",
      "âœ… Processed 300/3000 (10.00%)\n",
      "âœ… Processed 400/3000 (13.33%)\n",
      "âœ… Processed 500/3000 (16.67%)\n",
      "âœ… Processed 600/3000 (20.00%)\n",
      "âœ… Processed 700/3000 (23.33%)\n",
      "âœ… Processed 800/3000 (26.67%)\n",
      "âœ… Processed 900/3000 (30.00%)\n",
      "âœ… Processed 1000/3000 (33.33%)\n",
      "âœ… Processed 1100/3000 (36.67%)\n",
      "âœ… Processed 1200/3000 (40.00%)\n",
      "âœ… Processed 1300/3000 (43.33%)\n",
      "âœ… Processed 1400/3000 (46.67%)\n",
      "âœ… Processed 1500/3000 (50.00%)\n",
      "âœ… Processed 1600/3000 (53.33%)\n",
      "âœ… Processed 1700/3000 (56.67%)\n",
      "âœ… Processed 1800/3000 (60.00%)\n",
      "âœ… Processed 1900/3000 (63.33%)\n",
      "âœ… Processed 2000/3000 (66.67%)\n",
      "âœ… Processed 2100/3000 (70.00%)\n",
      "âœ… Processed 2200/3000 (73.33%)\n",
      "âœ… Processed 2300/3000 (76.67%)\n",
      "âœ… Processed 2400/3000 (80.00%)\n",
      "âœ… Processed 2500/3000 (83.33%)\n",
      "âœ… Processed 2600/3000 (86.67%)\n",
      "âœ… Processed 2700/3000 (90.00%)\n",
      "âœ… Processed 2800/3000 (93.33%)\n",
      "âœ… Processed 2900/3000 (96.67%)\n",
      "âœ… Processed 3000/3000 (100.00%)\n",
      "âœ… All test embeddings updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Retrieve test texts that need embeddings\n",
    "cursor.execute(\"SELECT text FROM test_text_embeddings WHERE embedding IS NULL\")\n",
    "test_texts = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(f\"ðŸ” Found {len(test_texts)} test samples that need embeddings.\")\n",
    "\n",
    "# Update missing embeddings\n",
    "for idx, text in enumerate(test_texts, start=1):\n",
    "    update_test_embedding(text)\n",
    "\n",
    "    # Print progress every 100 samples\n",
    "    if idx % 100 == 0 or idx == len(test_texts):\n",
    "        percent_done = (idx / len(test_texts)) * 100\n",
    "        print(f\"âœ… Processed {idx}/{len(test_texts)} ({percent_done:.2f}%)\")\n",
    "\n",
    "print(\"âœ… All test embeddings updated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ollama\n",
    "\n",
    "def get_test_embedding(text):\n",
    "    \"\"\"\n",
    "    Generates an embedding for test text if it doesn't already exist in PostgreSQL.\n",
    "    \"\"\"\n",
    "    cursor.execute(\"SELECT embedding FROM test_text_embeddings WHERE text = %s\", (text,))\n",
    "    result = cursor.fetchone()\n",
    "\n",
    "    if result and result[0]:  # If embedding exists\n",
    "        embedding_list = list(map(float, result[0].split(\",\")))\n",
    "        return torch.tensor(embedding_list, dtype=torch.float32)\n",
    "\n",
    "    # Generate embedding if not found\n",
    "    try:\n",
    "        embedding_data = ollama.embeddings(model=\"mxbai-embed-large\", prompt=text)\n",
    "        embedding_tensor = torch.tensor(embedding_data[\"embedding\"], dtype=torch.float32)\n",
    "\n",
    "        # Convert tensor to string\n",
    "        embedding_str = \",\".join(map(str, embedding_tensor.tolist()))\n",
    "\n",
    "        # Store in PostgreSQL\n",
    "        cursor.execute('''\n",
    "            UPDATE test_text_embeddings \n",
    "            SET embedding = %s WHERE text = %s;\n",
    "        ''', (embedding_str, text))\n",
    "        conn.commit()\n",
    "\n",
    "        return embedding_tensor\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating test embedding: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ Retrieved 3000 test embeddings from PostgreSQL.\n",
      "âœ… Loaded 3000 test samples from PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve embeddings for test samples\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "cursor.execute(\"SELECT embedding, label FROM test_text_embeddings WHERE embedding IS NOT NULL\")\n",
    "rows = cursor.fetchall()\n",
    "print(f\"â„¹ï¸ Retrieved {len(rows)} test embeddings from PostgreSQL.\")\n",
    "\n",
    "test_embeddings, test_labels = [], []\n",
    "\n",
    "for embedding_str, label in rows:\n",
    "    embedding_list = list(map(float, embedding_str.split(\",\")))  # Convert string to list\n",
    "    test_embeddings.append(torch.tensor(embedding_list, dtype=torch.float32))\n",
    "    test_labels.append(label)\n",
    "\n",
    "# Stack embeddings into a single tensor\n",
    "test_embeddings_tensor = torch.stack(test_embeddings).to(\"cuda\")\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.float32)\n",
    "\n",
    "print(f\"âœ… Loaded {len(test_embeddings_tensor)} test samples from PostgreSQL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SAE model loaded and ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(negative_slope=0.01),  # Helps sparsity\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Normalize output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded  # Return both\n",
    "\n",
    "# Load the model architecture\n",
    "loaded_model = SparseAutoencoder(input_dim=test_embeddings_tensor.shape[1], hidden_dim=64).to(\"cuda\")\n",
    "\n",
    "# Load the saved model weights\n",
    "#loaded_model.load_state_dict(torch.load(\"sparse_autoencoder.pth\"))\n",
    "loaded_model.load_state_dict(torch.load(\"best_sparse_autoencoder.pth\"))\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"âœ… SAE model loaded and ready for evaluation!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Loss = 0.218743, Prediction = AI\n",
      "Sample 2: Loss = 0.259299, Prediction = AI\n",
      "Sample 3: Loss = 0.180154, Prediction = Human\n",
      "Sample 4: Loss = 0.235813, Prediction = AI\n",
      "Sample 5: Loss = 0.245435, Prediction = AI\n",
      "\n",
      "ðŸ“Š AI Detection Performance on Test Set:\n",
      "âœ… Accuracy: 0.5157\n",
      "âœ… Precision: 0.5143\n",
      "âœ… Recall: 0.5653\n",
      "âœ… F1 Score: 0.5386\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def detect_ai_text(model, test_embeddings, threshold=0.21):\n",
    "    \"\"\"\n",
    "    Uses the Sparse Autoencoder to classify AI-generated text.\n",
    "    - Computes reconstruction loss.\n",
    "    - Labels as AI-generated if loss is below threshold.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        reconstructed, _ = model(test_embeddings)  # Forward pass through SAE\n",
    "        reconstruction_loss = torch.mean((test_embeddings - reconstructed) ** 2, dim=1)  # MSE loss per sample\n",
    "\n",
    "    # Convert to CPU for further processing\n",
    "    reconstruction_loss = reconstruction_loss.cpu().numpy()\n",
    "\n",
    "    # Classify based on threshold\n",
    "    predictions = (reconstruction_loss > threshold).astype(int)  # 1 = AI, 0 = Human\n",
    "\n",
    "    return reconstruction_loss, predictions\n",
    "\n",
    "# Run AI detection on test embeddings\n",
    "test_reconstruction_loss, test_predictions = detect_ai_text(loaded_model, test_embeddings_tensor)\n",
    "\n",
    "# Print some example results\n",
    "for i in range(5):  # Display first 10 samples\n",
    "    print(f\"Sample {i+1}: Loss = {test_reconstruction_loss[i]:.6f}, Prediction = {'AI' if test_predictions[i] == 1 else 'Human'}\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert test_labels_tensor to numpy\n",
    "true_labels = test_labels_tensor.cpu().numpy()\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, test_predictions)\n",
    "precision = precision_score(true_labels, test_predictions)\n",
    "recall = recall_score(true_labels, test_predictions)\n",
    "f1 = f1_score(true_labels, test_predictions)\n",
    "\n",
    "print(\"\\nðŸ“Š AI Detection Performance on Test Set:\")\n",
    "print(f\"âœ… Accuracy: {accuracy:.4f}\")\n",
    "print(f\"âœ… Precision: {precision:.4f}\")\n",
    "print(f\"âœ… Recall: {recall:.4f}\")\n",
    "print(f\"âœ… F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š AI Detection Performance on Test Set:\n",
      "âœ… Accuracy: 0.5137\n",
      "âœ… Precision: 0.5099\n",
      "âœ… Recall: 0.7040\n",
      "âœ… F1 Score: 0.5914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert test_labels_tensor to numpy\n",
    "true_labels = test_labels_tensor.cpu().numpy()\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, test_predictions)\n",
    "precision = precision_score(true_labels, test_predictions)\n",
    "recall = recall_score(true_labels, test_predictions)\n",
    "f1 = f1_score(true_labels, test_predictions)\n",
    "\n",
    "print(\"\\nðŸ“Š AI Detection Performance on Test Set:\")\n",
    "print(f\"âœ… Accuracy: {accuracy:.4f}\")\n",
    "print(f\"âœ… Precision: {precision:.4f}\")\n",
    "print(f\"âœ… Recall: {recall:.4f}\")\n",
    "print(f\"âœ… F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\aitext\\venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compute reconstruction loss on test data\n",
    "test_reconstruction_loss, test_predictions = detect_ai_text(loaded_model, test_embeddings_tensor)\n",
    "\n",
    "# Convert numpy array for plotting\n",
    "true_labels = test_labels_tensor.cpu().numpy()\n",
    "\n",
    "# Separate AI vs Human losses\n",
    "ai_losses = test_reconstruction_loss[true_labels == 1]\n",
    "human_losses = test_reconstruction_loss[true_labels == 0]\n",
    "\n",
    "# Plot distributions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(ai_losses, bins=50, alpha=0.7, label=\"AI-Generated\", color=\"red\")\n",
    "plt.hist(human_losses, bins=50, alpha=0.7, label=\"Human-Written\", color=\"blue\")\n",
    "plt.axvline(x=0.01, color='green', linestyle=\"--\", label=\"Threshold (0.01)\")\n",
    "plt.xlabel(\"Reconstruction Loss\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"ðŸ“Š Reconstruction Loss Distribution (AI vs. Human)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Finding the best threshold...[0.5        0.49983328 0.49966644 ... 0.         0.         1.        ] [1.         0.99933333 0.99866667 ... 0.         0.         0.        ]\n",
      "âœ… Best threshold found: 0.489466\n",
      "âœ… Best F1 score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ernan\\AppData\\Local\\Temp\\ipykernel_28268\\3066347102.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2 * (precision * recall) / (precision + recall)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(true_labels, test_reconstruction_loss)\n",
    "\n",
    "# Find best threshold (maximize F1 score)\n",
    "print(f\"ðŸ” Finding the best threshold...{precision} {recall}\")\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "print(f\"âœ… Best threshold found: {best_threshold:.6f}\")\n",
    "print(f\"âœ… Best F1 score: {np.max(f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š AI Detection Performance (Updated Threshold):\n",
      "âœ… Accuracy: 0.4997\n",
      "âœ… Precision: 0.0000\n",
      "âœ… Recall: 0.0000\n",
      "âœ… F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Re-run classification with the best threshold\n",
    "updated_predictions = (test_reconstruction_loss > best_threshold).astype(int)  # 1 = AI, 0 = Human\n",
    "\n",
    "# Compute updated evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, updated_predictions)\n",
    "precision = precision_score(true_labels, updated_predictions)\n",
    "recall = recall_score(true_labels, updated_predictions)\n",
    "f1 = f1_score(true_labels, updated_predictions)\n",
    "\n",
    "print(\"\\nðŸ“Š AI Detection Performance (Updated Threshold):\")\n",
    "print(f\"âœ… Accuracy: {accuracy:.4f}\")\n",
    "print(f\"âœ… Precision: {precision:.4f}\")\n",
    "print(f\"âœ… Recall: {recall:.4f}\")\n",
    "print(f\"âœ… F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
