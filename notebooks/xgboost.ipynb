{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieved 14000 embeddings from PostgreSQL.\n",
      "✅ Loaded 14000 embeddings from PostgreSQL.\n",
      "✅ Residual embeddings shape: torch.Size([14000, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Retrieve embeddings from PostgreSQL\n",
    "cursor.execute(\"SELECT embedding, label FROM text_embeddings\")\n",
    "rows = cursor.fetchall()\n",
    "print(f\"✅ Retrieved {len(rows)} embeddings from PostgreSQL.\")\n",
    "\n",
    "# Convert embeddings back into tensors\n",
    "embeddings, labels = [], []\n",
    "\n",
    "for embedding_str, label in rows:\n",
    "    embedding_list = list(map(float, embedding_str.split(\",\")))  # Convert string to list\n",
    "    embeddings.append(torch.tensor(embedding_list, dtype=torch.float32))\n",
    "    labels.append(label)\n",
    "\n",
    "# Stack embeddings into a single tensor\n",
    "residual_embeddings = torch.stack(embeddings)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "print(f\"✅ Loaded {len(residual_embeddings)} embeddings from PostgreSQL.\")\n",
    "print(f\"✅ Residual embeddings shape: {residual_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] - Train Loss: 0.542243 | Val Loss: 0.530349\n",
      "Epoch [2/500] - Train Loss: 0.530320 | Val Loss: 0.514546\n",
      "Epoch [3/500] - Train Loss: 0.514485 | Val Loss: 0.492822\n",
      "Epoch [4/500] - Train Loss: 0.492728 | Val Loss: 0.466045\n",
      "Epoch [5/500] - Train Loss: 0.465920 | Val Loss: 0.435853\n",
      "Epoch [6/500] - Train Loss: 0.435699 | Val Loss: 0.404141\n",
      "Epoch [7/500] - Train Loss: 0.403957 | Val Loss: 0.372778\n",
      "Epoch [8/500] - Train Loss: 0.372566 | Val Loss: 0.343405\n",
      "Epoch [9/500] - Train Loss: 0.343175 | Val Loss: 0.317249\n",
      "Epoch [10/500] - Train Loss: 0.317004 | Val Loss: 0.295005\n",
      "Epoch [11/500] - Train Loss: 0.294749 | Val Loss: 0.276895\n",
      "Epoch [12/500] - Train Loss: 0.276625 | Val Loss: 0.262737\n",
      "Epoch [13/500] - Train Loss: 0.262454 | Val Loss: 0.252053\n",
      "Epoch [14/500] - Train Loss: 0.251760 | Val Loss: 0.244254\n",
      "Epoch [15/500] - Train Loss: 0.243952 | Val Loss: 0.238718\n",
      "Epoch [16/500] - Train Loss: 0.238407 | Val Loss: 0.234817\n",
      "Epoch [17/500] - Train Loss: 0.234497 | Val Loss: 0.232048\n",
      "Epoch [18/500] - Train Loss: 0.231717 | Val Loss: 0.230095\n",
      "Epoch [19/500] - Train Loss: 0.229754 | Val Loss: 0.228748\n",
      "Epoch [20/500] - Train Loss: 0.228403 | Val Loss: 0.227842\n",
      "Epoch [21/500] - Train Loss: 0.227494 | Val Loss: 0.227207\n",
      "Epoch [22/500] - Train Loss: 0.226860 | Val Loss: 0.226676\n",
      "Epoch [23/500] - Train Loss: 0.226332 | Val Loss: 0.226168\n",
      "Epoch [24/500] - Train Loss: 0.225829 | Val Loss: 0.225690\n",
      "Epoch [25/500] - Train Loss: 0.225357 | Val Loss: 0.225240\n",
      "Epoch [26/500] - Train Loss: 0.224912 | Val Loss: 0.224810\n",
      "Epoch [27/500] - Train Loss: 0.224486 | Val Loss: 0.224412\n",
      "Epoch [28/500] - Train Loss: 0.224092 | Val Loss: 0.224061\n",
      "Epoch [29/500] - Train Loss: 0.223742 | Val Loss: 0.223764\n",
      "Epoch [30/500] - Train Loss: 0.223443 | Val Loss: 0.223480\n",
      "Epoch [31/500] - Train Loss: 0.223149 | Val Loss: 0.223158\n",
      "Epoch [32/500] - Train Loss: 0.222813 | Val Loss: 0.222814\n",
      "Epoch [33/500] - Train Loss: 0.222453 | Val Loss: 0.222477\n",
      "Epoch [34/500] - Train Loss: 0.222102 | Val Loss: 0.222163\n",
      "Epoch [35/500] - Train Loss: 0.221778 | Val Loss: 0.221870\n",
      "Epoch [36/500] - Train Loss: 0.221478 | Val Loss: 0.221598\n",
      "Epoch [37/500] - Train Loss: 0.221202 | Val Loss: 0.221318\n",
      "Epoch [38/500] - Train Loss: 0.220921 | Val Loss: 0.221010\n",
      "Epoch [39/500] - Train Loss: 0.220617 | Val Loss: 0.220690\n",
      "Epoch [40/500] - Train Loss: 0.220303 | Val Loss: 0.220405\n",
      "Epoch [41/500] - Train Loss: 0.220025 | Val Loss: 0.220160\n",
      "Epoch [42/500] - Train Loss: 0.219785 | Val Loss: 0.219919\n",
      "Epoch [43/500] - Train Loss: 0.219547 | Val Loss: 0.219668\n",
      "Epoch [44/500] - Train Loss: 0.219297 | Val Loss: 0.219405\n",
      "Epoch [45/500] - Train Loss: 0.219034 | Val Loss: 0.219136\n",
      "Epoch [46/500] - Train Loss: 0.218763 | Val Loss: 0.218872\n",
      "Epoch [47/500] - Train Loss: 0.218495 | Val Loss: 0.218618\n",
      "Epoch [48/500] - Train Loss: 0.218237 | Val Loss: 0.218375\n",
      "Epoch [49/500] - Train Loss: 0.217990 | Val Loss: 0.218133\n",
      "Epoch [50/500] - Train Loss: 0.217746 | Val Loss: 0.217883\n",
      "Epoch [51/500] - Train Loss: 0.217494 | Val Loss: 0.217625\n",
      "Epoch [52/500] - Train Loss: 0.217235 | Val Loss: 0.217366\n",
      "Epoch [53/500] - Train Loss: 0.216977 | Val Loss: 0.217114\n",
      "Epoch [54/500] - Train Loss: 0.216725 | Val Loss: 0.216869\n",
      "Epoch [55/500] - Train Loss: 0.216480 | Val Loss: 0.216630\n",
      "Epoch [56/500] - Train Loss: 0.216242 | Val Loss: 0.216393\n",
      "Epoch [57/500] - Train Loss: 0.216006 | Val Loss: 0.216156\n",
      "Epoch [58/500] - Train Loss: 0.215771 | Val Loss: 0.215920\n",
      "Epoch [59/500] - Train Loss: 0.215536 | Val Loss: 0.215684\n",
      "Epoch [60/500] - Train Loss: 0.215302 | Val Loss: 0.215454\n",
      "Epoch [61/500] - Train Loss: 0.215073 | Val Loss: 0.215236\n",
      "Epoch [62/500] - Train Loss: 0.214855 | Val Loss: 0.215035\n",
      "Epoch [63/500] - Train Loss: 0.214653 | Val Loss: 0.214836\n",
      "Epoch [64/500] - Train Loss: 0.214451 | Val Loss: 0.214632\n",
      "Epoch [65/500] - Train Loss: 0.214244 | Val Loss: 0.214438\n",
      "Epoch [66/500] - Train Loss: 0.214047 | Val Loss: 0.214257\n",
      "Epoch [67/500] - Train Loss: 0.213865 | Val Loss: 0.214064\n",
      "Epoch [68/500] - Train Loss: 0.213672 | Val Loss: 0.213863\n",
      "Epoch [69/500] - Train Loss: 0.213473 | Val Loss: 0.213678\n",
      "Epoch [70/500] - Train Loss: 0.213290 | Val Loss: 0.213502\n",
      "Epoch [71/500] - Train Loss: 0.213116 | Val Loss: 0.213324\n",
      "Epoch [72/500] - Train Loss: 0.212941 | Val Loss: 0.213150\n",
      "Epoch [73/500] - Train Loss: 0.212767 | Val Loss: 0.212986\n",
      "Epoch [74/500] - Train Loss: 0.212602 | Val Loss: 0.212843\n",
      "Epoch [75/500] - Train Loss: 0.212458 | Val Loss: 0.212708\n",
      "Epoch [76/500] - Train Loss: 0.212321 | Val Loss: 0.212547\n",
      "Epoch [77/500] - Train Loss: 0.212156 | Val Loss: 0.212379\n",
      "Epoch [78/500] - Train Loss: 0.211986 | Val Loss: 0.212222\n",
      "Epoch [79/500] - Train Loss: 0.211829 | Val Loss: 0.212081\n",
      "Epoch [80/500] - Train Loss: 0.211687 | Val Loss: 0.211947\n",
      "Epoch [81/500] - Train Loss: 0.211554 | Val Loss: 0.211797\n",
      "Epoch [82/500] - Train Loss: 0.211406 | Val Loss: 0.211638\n",
      "Epoch [83/500] - Train Loss: 0.211248 | Val Loss: 0.211485\n",
      "Epoch [84/500] - Train Loss: 0.211097 | Val Loss: 0.211337\n",
      "Epoch [85/500] - Train Loss: 0.210952 | Val Loss: 0.211191\n",
      "Epoch [86/500] - Train Loss: 0.210809 | Val Loss: 0.211061\n",
      "Epoch [87/500] - Train Loss: 0.210683 | Val Loss: 0.210946\n",
      "Epoch [88/500] - Train Loss: 0.210572 | Val Loss: 0.210817\n",
      "Epoch [89/500] - Train Loss: 0.210444 | Val Loss: 0.210674\n",
      "Epoch [90/500] - Train Loss: 0.210299 | Val Loss: 0.210541\n",
      "Epoch [91/500] - Train Loss: 0.210163 | Val Loss: 0.210429\n",
      "Epoch [92/500] - Train Loss: 0.210048 | Val Loss: 0.210321\n",
      "Epoch [93/500] - Train Loss: 0.209937 | Val Loss: 0.210205\n",
      "Epoch [94/500] - Train Loss: 0.209819 | Val Loss: 0.210087\n",
      "Epoch [95/500] - Train Loss: 0.209701 | Val Loss: 0.209973\n",
      "Epoch [96/500] - Train Loss: 0.209587 | Val Loss: 0.209861\n",
      "Epoch [97/500] - Train Loss: 0.209476 | Val Loss: 0.209750\n",
      "Epoch [98/500] - Train Loss: 0.209365 | Val Loss: 0.209639\n",
      "Epoch [99/500] - Train Loss: 0.209254 | Val Loss: 0.209530\n",
      "Epoch [100/500] - Train Loss: 0.209146 | Val Loss: 0.209423\n",
      "Epoch [101/500] - Train Loss: 0.209039 | Val Loss: 0.209318\n",
      "Epoch [102/500] - Train Loss: 0.208933 | Val Loss: 0.209214\n",
      "Epoch [103/500] - Train Loss: 0.208826 | Val Loss: 0.209108\n",
      "Epoch [104/500] - Train Loss: 0.208718 | Val Loss: 0.209003\n",
      "Epoch [105/500] - Train Loss: 0.208611 | Val Loss: 0.208901\n",
      "Epoch [106/500] - Train Loss: 0.208508 | Val Loss: 0.208799\n",
      "Epoch [107/500] - Train Loss: 0.208405 | Val Loss: 0.208688\n",
      "Epoch [108/500] - Train Loss: 0.208295 | Val Loss: 0.208572\n",
      "Epoch [109/500] - Train Loss: 0.208179 | Val Loss: 0.208458\n",
      "Epoch [110/500] - Train Loss: 0.208065 | Val Loss: 0.208356\n",
      "Epoch [111/500] - Train Loss: 0.207963 | Val Loss: 0.208272\n",
      "Epoch [112/500] - Train Loss: 0.207879 | Val Loss: 0.208173\n",
      "Epoch [113/500] - Train Loss: 0.207781 | Val Loss: 0.208064\n",
      "Epoch [114/500] - Train Loss: 0.207674 | Val Loss: 0.207963\n",
      "Epoch [115/500] - Train Loss: 0.207573 | Val Loss: 0.207856\n",
      "Epoch [116/500] - Train Loss: 0.207466 | Val Loss: 0.207747\n",
      "Epoch [117/500] - Train Loss: 0.207356 | Val Loss: 0.207646\n",
      "Epoch [118/500] - Train Loss: 0.207254 | Val Loss: 0.207555\n",
      "Epoch [119/500] - Train Loss: 0.207162 | Val Loss: 0.207467\n",
      "Epoch [120/500] - Train Loss: 0.207075 | Val Loss: 0.207371\n",
      "Epoch [121/500] - Train Loss: 0.206979 | Val Loss: 0.207263\n",
      "Epoch [122/500] - Train Loss: 0.206871 | Val Loss: 0.207151\n",
      "Epoch [123/500] - Train Loss: 0.206760 | Val Loss: 0.207049\n",
      "Epoch [124/500] - Train Loss: 0.206659 | Val Loss: 0.206962\n",
      "Epoch [125/500] - Train Loss: 0.206573 | Val Loss: 0.206877\n",
      "Epoch [126/500] - Train Loss: 0.206488 | Val Loss: 0.206786\n",
      "Epoch [127/500] - Train Loss: 0.206396 | Val Loss: 0.206694\n",
      "Epoch [128/500] - Train Loss: 0.206303 | Val Loss: 0.206604\n",
      "Epoch [129/500] - Train Loss: 0.206212 | Val Loss: 0.206517\n",
      "Epoch [130/500] - Train Loss: 0.206124 | Val Loss: 0.206431\n",
      "Epoch [131/500] - Train Loss: 0.206037 | Val Loss: 0.206344\n",
      "Epoch [132/500] - Train Loss: 0.205949 | Val Loss: 0.206256\n",
      "Epoch [133/500] - Train Loss: 0.205861 | Val Loss: 0.206169\n",
      "Epoch [134/500] - Train Loss: 0.205774 | Val Loss: 0.206084\n",
      "Epoch [135/500] - Train Loss: 0.205690 | Val Loss: 0.205998\n",
      "Epoch [136/500] - Train Loss: 0.205605 | Val Loss: 0.205910\n",
      "Epoch [137/500] - Train Loss: 0.205517 | Val Loss: 0.205820\n",
      "Epoch [138/500] - Train Loss: 0.205428 | Val Loss: 0.205731\n",
      "Epoch [139/500] - Train Loss: 0.205339 | Val Loss: 0.205644\n",
      "Epoch [140/500] - Train Loss: 0.205252 | Val Loss: 0.205559\n",
      "Epoch [141/500] - Train Loss: 0.205166 | Val Loss: 0.205475\n",
      "Epoch [142/500] - Train Loss: 0.205081 | Val Loss: 0.205392\n",
      "Epoch [143/500] - Train Loss: 0.204997 | Val Loss: 0.205311\n",
      "Epoch [144/500] - Train Loss: 0.204915 | Val Loss: 0.205230\n",
      "Epoch [145/500] - Train Loss: 0.204834 | Val Loss: 0.205151\n",
      "Epoch [146/500] - Train Loss: 0.204754 | Val Loss: 0.205069\n",
      "Epoch [147/500] - Train Loss: 0.204672 | Val Loss: 0.204986\n",
      "Epoch [148/500] - Train Loss: 0.204590 | Val Loss: 0.204905\n",
      "Epoch [149/500] - Train Loss: 0.204509 | Val Loss: 0.204825\n",
      "Epoch [150/500] - Train Loss: 0.204430 | Val Loss: 0.204746\n",
      "Epoch [151/500] - Train Loss: 0.204353 | Val Loss: 0.204670\n",
      "Epoch [152/500] - Train Loss: 0.204277 | Val Loss: 0.204595\n",
      "Epoch [153/500] - Train Loss: 0.204203 | Val Loss: 0.204521\n",
      "Epoch [154/500] - Train Loss: 0.204129 | Val Loss: 0.204445\n",
      "Epoch [155/500] - Train Loss: 0.204054 | Val Loss: 0.204369\n",
      "Epoch [156/500] - Train Loss: 0.203978 | Val Loss: 0.204293\n",
      "Epoch [157/500] - Train Loss: 0.203902 | Val Loss: 0.204219\n",
      "Epoch [158/500] - Train Loss: 0.203827 | Val Loss: 0.204146\n",
      "Epoch [159/500] - Train Loss: 0.203754 | Val Loss: 0.204075\n",
      "Epoch [160/500] - Train Loss: 0.203681 | Val Loss: 0.204002\n",
      "Epoch [161/500] - Train Loss: 0.203608 | Val Loss: 0.203929\n",
      "Epoch [162/500] - Train Loss: 0.203535 | Val Loss: 0.203855\n",
      "Epoch [163/500] - Train Loss: 0.203461 | Val Loss: 0.203783\n",
      "Epoch [164/500] - Train Loss: 0.203388 | Val Loss: 0.203713\n",
      "Epoch [165/500] - Train Loss: 0.203319 | Val Loss: 0.203646\n",
      "Epoch [166/500] - Train Loss: 0.203252 | Val Loss: 0.203580\n",
      "Epoch [167/500] - Train Loss: 0.203186 | Val Loss: 0.203512\n",
      "Epoch [168/500] - Train Loss: 0.203119 | Val Loss: 0.203442\n",
      "Epoch [169/500] - Train Loss: 0.203049 | Val Loss: 0.203373\n",
      "Epoch [170/500] - Train Loss: 0.202980 | Val Loss: 0.203305\n",
      "Epoch [171/500] - Train Loss: 0.202913 | Val Loss: 0.203239\n",
      "Epoch [172/500] - Train Loss: 0.202847 | Val Loss: 0.203173\n",
      "Epoch [173/500] - Train Loss: 0.202781 | Val Loss: 0.203108\n",
      "Epoch [174/500] - Train Loss: 0.202716 | Val Loss: 0.203043\n",
      "Epoch [175/500] - Train Loss: 0.202651 | Val Loss: 0.202978\n",
      "Epoch [176/500] - Train Loss: 0.202586 | Val Loss: 0.202912\n",
      "Epoch [177/500] - Train Loss: 0.202520 | Val Loss: 0.202847\n",
      "Epoch [178/500] - Train Loss: 0.202455 | Val Loss: 0.202782\n",
      "Epoch [179/500] - Train Loss: 0.202390 | Val Loss: 0.202718\n",
      "Epoch [180/500] - Train Loss: 0.202326 | Val Loss: 0.202654\n",
      "Epoch [181/500] - Train Loss: 0.202262 | Val Loss: 0.202589\n",
      "Epoch [182/500] - Train Loss: 0.202198 | Val Loss: 0.202524\n",
      "Epoch [183/500] - Train Loss: 0.202133 | Val Loss: 0.202459\n",
      "Epoch [184/500] - Train Loss: 0.202069 | Val Loss: 0.202395\n",
      "Epoch [185/500] - Train Loss: 0.202005 | Val Loss: 0.202330\n",
      "Epoch [186/500] - Train Loss: 0.201941 | Val Loss: 0.202266\n",
      "Epoch [187/500] - Train Loss: 0.201876 | Val Loss: 0.202201\n",
      "Epoch [188/500] - Train Loss: 0.201811 | Val Loss: 0.202135\n",
      "Epoch [189/500] - Train Loss: 0.201745 | Val Loss: 0.202068\n",
      "Epoch [190/500] - Train Loss: 0.201677 | Val Loss: 0.201999\n",
      "Epoch [191/500] - Train Loss: 0.201608 | Val Loss: 0.201927\n",
      "Epoch [192/500] - Train Loss: 0.201537 | Val Loss: 0.201853\n",
      "Epoch [193/500] - Train Loss: 0.201463 | Val Loss: 0.201776\n",
      "Epoch [194/500] - Train Loss: 0.201388 | Val Loss: 0.201706\n",
      "Epoch [195/500] - Train Loss: 0.201320 | Val Loss: 0.201633\n",
      "Epoch [196/500] - Train Loss: 0.201247 | Val Loss: 0.201557\n",
      "Epoch [197/500] - Train Loss: 0.201171 | Val Loss: 0.201481\n",
      "Epoch [198/500] - Train Loss: 0.201094 | Val Loss: 0.201404\n",
      "Epoch [199/500] - Train Loss: 0.201016 | Val Loss: 0.201325\n",
      "Epoch [200/500] - Train Loss: 0.200936 | Val Loss: 0.201250\n",
      "Epoch [201/500] - Train Loss: 0.200861 | Val Loss: 0.201178\n",
      "Epoch [202/500] - Train Loss: 0.200789 | Val Loss: 0.201102\n",
      "Epoch [203/500] - Train Loss: 0.200714 | Val Loss: 0.201021\n",
      "Epoch [204/500] - Train Loss: 0.200634 | Val Loss: 0.200937\n",
      "Epoch [205/500] - Train Loss: 0.200550 | Val Loss: 0.200858\n",
      "Epoch [206/500] - Train Loss: 0.200470 | Val Loss: 0.200786\n",
      "Epoch [207/500] - Train Loss: 0.200397 | Val Loss: 0.200720\n",
      "Epoch [208/500] - Train Loss: 0.200332 | Val Loss: 0.200653\n",
      "Epoch [209/500] - Train Loss: 0.200264 | Val Loss: 0.200575\n",
      "Epoch [210/500] - Train Loss: 0.200184 | Val Loss: 0.200495\n",
      "Epoch [211/500] - Train Loss: 0.200104 | Val Loss: 0.200423\n",
      "Epoch [212/500] - Train Loss: 0.200030 | Val Loss: 0.200358\n",
      "Epoch [213/500] - Train Loss: 0.199963 | Val Loss: 0.200295\n",
      "Epoch [214/500] - Train Loss: 0.199898 | Val Loss: 0.200230\n",
      "Epoch [215/500] - Train Loss: 0.199831 | Val Loss: 0.200162\n",
      "Epoch [216/500] - Train Loss: 0.199762 | Val Loss: 0.200091\n",
      "Epoch [217/500] - Train Loss: 0.199692 | Val Loss: 0.200018\n",
      "Epoch [218/500] - Train Loss: 0.199620 | Val Loss: 0.199946\n",
      "Epoch [219/500] - Train Loss: 0.199548 | Val Loss: 0.199873\n",
      "Epoch [220/500] - Train Loss: 0.199476 | Val Loss: 0.199801\n",
      "Epoch [221/500] - Train Loss: 0.199404 | Val Loss: 0.199733\n",
      "Epoch [222/500] - Train Loss: 0.199334 | Val Loss: 0.199670\n",
      "Epoch [223/500] - Train Loss: 0.199270 | Val Loss: 0.199606\n",
      "Epoch [224/500] - Train Loss: 0.199205 | Val Loss: 0.199539\n",
      "Epoch [225/500] - Train Loss: 0.199137 | Val Loss: 0.199472\n",
      "Epoch [226/500] - Train Loss: 0.199069 | Val Loss: 0.199406\n",
      "Epoch [227/500] - Train Loss: 0.199003 | Val Loss: 0.199340\n",
      "Epoch [228/500] - Train Loss: 0.198937 | Val Loss: 0.199273\n",
      "Epoch [229/500] - Train Loss: 0.198870 | Val Loss: 0.199207\n",
      "Epoch [230/500] - Train Loss: 0.198803 | Val Loss: 0.199142\n",
      "Epoch [231/500] - Train Loss: 0.198736 | Val Loss: 0.199077\n",
      "Epoch [232/500] - Train Loss: 0.198670 | Val Loss: 0.199011\n",
      "Epoch [233/500] - Train Loss: 0.198602 | Val Loss: 0.198948\n",
      "Epoch [234/500] - Train Loss: 0.198538 | Val Loss: 0.198888\n",
      "Epoch [235/500] - Train Loss: 0.198476 | Val Loss: 0.198828\n",
      "Epoch [236/500] - Train Loss: 0.198415 | Val Loss: 0.198769\n",
      "Epoch [237/500] - Train Loss: 0.198355 | Val Loss: 0.198707\n",
      "Epoch [238/500] - Train Loss: 0.198293 | Val Loss: 0.198645\n",
      "Epoch [239/500] - Train Loss: 0.198231 | Val Loss: 0.198585\n",
      "Epoch [240/500] - Train Loss: 0.198172 | Val Loss: 0.198526\n",
      "Epoch [241/500] - Train Loss: 0.198113 | Val Loss: 0.198467\n",
      "Epoch [242/500] - Train Loss: 0.198054 | Val Loss: 0.198408\n",
      "Epoch [243/500] - Train Loss: 0.197994 | Val Loss: 0.198350\n",
      "Epoch [244/500] - Train Loss: 0.197935 | Val Loss: 0.198293\n",
      "Epoch [245/500] - Train Loss: 0.197878 | Val Loss: 0.198239\n",
      "Epoch [246/500] - Train Loss: 0.197822 | Val Loss: 0.198184\n",
      "Epoch [247/500] - Train Loss: 0.197765 | Val Loss: 0.198126\n",
      "Epoch [248/500] - Train Loss: 0.197707 | Val Loss: 0.198068\n",
      "Epoch [249/500] - Train Loss: 0.197649 | Val Loss: 0.198013\n",
      "Epoch [250/500] - Train Loss: 0.197593 | Val Loss: 0.197957\n",
      "Epoch [251/500] - Train Loss: 0.197538 | Val Loss: 0.197901\n",
      "Epoch [252/500] - Train Loss: 0.197482 | Val Loss: 0.197846\n",
      "Epoch [253/500] - Train Loss: 0.197426 | Val Loss: 0.197791\n",
      "Epoch [254/500] - Train Loss: 0.197371 | Val Loss: 0.197737\n",
      "Epoch [255/500] - Train Loss: 0.197316 | Val Loss: 0.197683\n",
      "Epoch [256/500] - Train Loss: 0.197263 | Val Loss: 0.197630\n",
      "Epoch [257/500] - Train Loss: 0.197209 | Val Loss: 0.197576\n",
      "Epoch [258/500] - Train Loss: 0.197154 | Val Loss: 0.197522\n",
      "Epoch [259/500] - Train Loss: 0.197100 | Val Loss: 0.197468\n",
      "Epoch [260/500] - Train Loss: 0.197045 | Val Loss: 0.197414\n",
      "Epoch [261/500] - Train Loss: 0.196992 | Val Loss: 0.197360\n",
      "Epoch [262/500] - Train Loss: 0.196938 | Val Loss: 0.197307\n",
      "Epoch [263/500] - Train Loss: 0.196884 | Val Loss: 0.197253\n",
      "Epoch [264/500] - Train Loss: 0.196831 | Val Loss: 0.197200\n",
      "Epoch [265/500] - Train Loss: 0.196777 | Val Loss: 0.197147\n",
      "Epoch [266/500] - Train Loss: 0.196724 | Val Loss: 0.197095\n",
      "Epoch [267/500] - Train Loss: 0.196671 | Val Loss: 0.197043\n",
      "Epoch [268/500] - Train Loss: 0.196618 | Val Loss: 0.196990\n",
      "Epoch [269/500] - Train Loss: 0.196564 | Val Loss: 0.196937\n",
      "Epoch [270/500] - Train Loss: 0.196511 | Val Loss: 0.196883\n",
      "Epoch [271/500] - Train Loss: 0.196456 | Val Loss: 0.196829\n",
      "Epoch [272/500] - Train Loss: 0.196401 | Val Loss: 0.196775\n",
      "Epoch [273/500] - Train Loss: 0.196346 | Val Loss: 0.196722\n",
      "Epoch [274/500] - Train Loss: 0.196292 | Val Loss: 0.196669\n",
      "Epoch [275/500] - Train Loss: 0.196238 | Val Loss: 0.196612\n",
      "Epoch [276/500] - Train Loss: 0.196180 | Val Loss: 0.196555\n",
      "Epoch [277/500] - Train Loss: 0.196121 | Val Loss: 0.196497\n",
      "Epoch [278/500] - Train Loss: 0.196062 | Val Loss: 0.196438\n",
      "Epoch [279/500] - Train Loss: 0.196002 | Val Loss: 0.196382\n",
      "Epoch [280/500] - Train Loss: 0.195945 | Val Loss: 0.196327\n",
      "Epoch [281/500] - Train Loss: 0.195888 | Val Loss: 0.196267\n",
      "Epoch [282/500] - Train Loss: 0.195827 | Val Loss: 0.196207\n",
      "Epoch [283/500] - Train Loss: 0.195767 | Val Loss: 0.196150\n",
      "Epoch [284/500] - Train Loss: 0.195709 | Val Loss: 0.196093\n",
      "Epoch [285/500] - Train Loss: 0.195651 | Val Loss: 0.196034\n",
      "Epoch [286/500] - Train Loss: 0.195591 | Val Loss: 0.195974\n",
      "Epoch [287/500] - Train Loss: 0.195531 | Val Loss: 0.195917\n",
      "Epoch [288/500] - Train Loss: 0.195472 | Val Loss: 0.195861\n",
      "Epoch [289/500] - Train Loss: 0.195415 | Val Loss: 0.195804\n",
      "Epoch [290/500] - Train Loss: 0.195357 | Val Loss: 0.195745\n",
      "Epoch [291/500] - Train Loss: 0.195297 | Val Loss: 0.195688\n",
      "Epoch [292/500] - Train Loss: 0.195239 | Val Loss: 0.195631\n",
      "Epoch [293/500] - Train Loss: 0.195181 | Val Loss: 0.195573\n",
      "Epoch [294/500] - Train Loss: 0.195122 | Val Loss: 0.195514\n",
      "Epoch [295/500] - Train Loss: 0.195061 | Val Loss: 0.195457\n",
      "Epoch [296/500] - Train Loss: 0.195002 | Val Loss: 0.195407\n",
      "Epoch [297/500] - Train Loss: 0.194951 | Val Loss: 0.195361\n",
      "Epoch [298/500] - Train Loss: 0.194903 | Val Loss: 0.195309\n",
      "Epoch [299/500] - Train Loss: 0.194849 | Val Loss: 0.195251\n",
      "Epoch [300/500] - Train Loss: 0.194790 | Val Loss: 0.195197\n",
      "Epoch [301/500] - Train Loss: 0.194735 | Val Loss: 0.195146\n",
      "Epoch [302/500] - Train Loss: 0.194683 | Val Loss: 0.195096\n",
      "Epoch [303/500] - Train Loss: 0.194633 | Val Loss: 0.195045\n",
      "Epoch [304/500] - Train Loss: 0.194581 | Val Loss: 0.194993\n",
      "Epoch [305/500] - Train Loss: 0.194529 | Val Loss: 0.194942\n",
      "Epoch [306/500] - Train Loss: 0.194476 | Val Loss: 0.194890\n",
      "Epoch [307/500] - Train Loss: 0.194423 | Val Loss: 0.194840\n",
      "Epoch [308/500] - Train Loss: 0.194371 | Val Loss: 0.194792\n",
      "Epoch [309/500] - Train Loss: 0.194322 | Val Loss: 0.194746\n",
      "Epoch [310/500] - Train Loss: 0.194274 | Val Loss: 0.194698\n",
      "Epoch [311/500] - Train Loss: 0.194225 | Val Loss: 0.194650\n",
      "Epoch [312/500] - Train Loss: 0.194174 | Val Loss: 0.194601\n",
      "Epoch [313/500] - Train Loss: 0.194125 | Val Loss: 0.194553\n",
      "Epoch [314/500] - Train Loss: 0.194076 | Val Loss: 0.194506\n",
      "Epoch [315/500] - Train Loss: 0.194028 | Val Loss: 0.194460\n",
      "Epoch [316/500] - Train Loss: 0.193982 | Val Loss: 0.194415\n",
      "Epoch [317/500] - Train Loss: 0.193937 | Val Loss: 0.194370\n",
      "Epoch [318/500] - Train Loss: 0.193890 | Val Loss: 0.194323\n",
      "Epoch [319/500] - Train Loss: 0.193842 | Val Loss: 0.194277\n",
      "Epoch [320/500] - Train Loss: 0.193796 | Val Loss: 0.194232\n",
      "Epoch [321/500] - Train Loss: 0.193750 | Val Loss: 0.194187\n",
      "Epoch [322/500] - Train Loss: 0.193704 | Val Loss: 0.194142\n",
      "Epoch [323/500] - Train Loss: 0.193658 | Val Loss: 0.194097\n",
      "Epoch [324/500] - Train Loss: 0.193612 | Val Loss: 0.194051\n",
      "Epoch [325/500] - Train Loss: 0.193564 | Val Loss: 0.194004\n",
      "Epoch [326/500] - Train Loss: 0.193517 | Val Loss: 0.193958\n",
      "Epoch [327/500] - Train Loss: 0.193470 | Val Loss: 0.193912\n",
      "Epoch [328/500] - Train Loss: 0.193424 | Val Loss: 0.193865\n",
      "Epoch [329/500] - Train Loss: 0.193376 | Val Loss: 0.193817\n",
      "Epoch [330/500] - Train Loss: 0.193327 | Val Loss: 0.193770\n",
      "Epoch [331/500] - Train Loss: 0.193278 | Val Loss: 0.193726\n",
      "Epoch [332/500] - Train Loss: 0.193232 | Val Loss: 0.193682\n",
      "Epoch [333/500] - Train Loss: 0.193186 | Val Loss: 0.193633\n",
      "Epoch [334/500] - Train Loss: 0.193137 | Val Loss: 0.193584\n",
      "Epoch [335/500] - Train Loss: 0.193087 | Val Loss: 0.193537\n",
      "Epoch [336/500] - Train Loss: 0.193040 | Val Loss: 0.193491\n",
      "Epoch [337/500] - Train Loss: 0.192993 | Val Loss: 0.193444\n",
      "Epoch [338/500] - Train Loss: 0.192945 | Val Loss: 0.193396\n",
      "Epoch [339/500] - Train Loss: 0.192896 | Val Loss: 0.193347\n",
      "Epoch [340/500] - Train Loss: 0.192847 | Val Loss: 0.193302\n",
      "Epoch [341/500] - Train Loss: 0.192801 | Val Loss: 0.193262\n",
      "Epoch [342/500] - Train Loss: 0.192759 | Val Loss: 0.193216\n",
      "Epoch [343/500] - Train Loss: 0.192713 | Val Loss: 0.193169\n",
      "Epoch [344/500] - Train Loss: 0.192666 | Val Loss: 0.193125\n",
      "Epoch [345/500] - Train Loss: 0.192622 | Val Loss: 0.193083\n",
      "Epoch [346/500] - Train Loss: 0.192579 | Val Loss: 0.193041\n",
      "Epoch [347/500] - Train Loss: 0.192537 | Val Loss: 0.192999\n",
      "Epoch [348/500] - Train Loss: 0.192494 | Val Loss: 0.192958\n",
      "Epoch [349/500] - Train Loss: 0.192451 | Val Loss: 0.192916\n",
      "Epoch [350/500] - Train Loss: 0.192408 | Val Loss: 0.192875\n",
      "Epoch [351/500] - Train Loss: 0.192366 | Val Loss: 0.192834\n",
      "Epoch [352/500] - Train Loss: 0.192325 | Val Loss: 0.192794\n",
      "Epoch [353/500] - Train Loss: 0.192285 | Val Loss: 0.192753\n",
      "Epoch [354/500] - Train Loss: 0.192244 | Val Loss: 0.192713\n",
      "Epoch [355/500] - Train Loss: 0.192203 | Val Loss: 0.192673\n",
      "Epoch [356/500] - Train Loss: 0.192163 | Val Loss: 0.192634\n",
      "Epoch [357/500] - Train Loss: 0.192123 | Val Loss: 0.192595\n",
      "Epoch [358/500] - Train Loss: 0.192084 | Val Loss: 0.192557\n",
      "Epoch [359/500] - Train Loss: 0.192044 | Val Loss: 0.192518\n",
      "Epoch [360/500] - Train Loss: 0.192004 | Val Loss: 0.192478\n",
      "Epoch [361/500] - Train Loss: 0.191964 | Val Loss: 0.192439\n",
      "Epoch [362/500] - Train Loss: 0.191925 | Val Loss: 0.192400\n",
      "Epoch [363/500] - Train Loss: 0.191885 | Val Loss: 0.192360\n",
      "Epoch [364/500] - Train Loss: 0.191845 | Val Loss: 0.192321\n",
      "Epoch [365/500] - Train Loss: 0.191806 | Val Loss: 0.192284\n",
      "Epoch [366/500] - Train Loss: 0.191769 | Val Loss: 0.192246\n",
      "Epoch [367/500] - Train Loss: 0.191730 | Val Loss: 0.192207\n",
      "Epoch [368/500] - Train Loss: 0.191691 | Val Loss: 0.192170\n",
      "Epoch [369/500] - Train Loss: 0.191653 | Val Loss: 0.192133\n",
      "Epoch [370/500] - Train Loss: 0.191615 | Val Loss: 0.192095\n",
      "Epoch [371/500] - Train Loss: 0.191577 | Val Loss: 0.192056\n",
      "Epoch [372/500] - Train Loss: 0.191538 | Val Loss: 0.192017\n",
      "Epoch [373/500] - Train Loss: 0.191498 | Val Loss: 0.191977\n",
      "Epoch [374/500] - Train Loss: 0.191458 | Val Loss: 0.191938\n",
      "Epoch [375/500] - Train Loss: 0.191418 | Val Loss: 0.191899\n",
      "Epoch [376/500] - Train Loss: 0.191378 | Val Loss: 0.191858\n",
      "Epoch [377/500] - Train Loss: 0.191336 | Val Loss: 0.191816\n",
      "Epoch [378/500] - Train Loss: 0.191293 | Val Loss: 0.191774\n",
      "Epoch [379/500] - Train Loss: 0.191251 | Val Loss: 0.191731\n",
      "Epoch [380/500] - Train Loss: 0.191207 | Val Loss: 0.191686\n",
      "Epoch [381/500] - Train Loss: 0.191163 | Val Loss: 0.191642\n",
      "Epoch [382/500] - Train Loss: 0.191118 | Val Loss: 0.191598\n",
      "Epoch [383/500] - Train Loss: 0.191074 | Val Loss: 0.191553\n",
      "Epoch [384/500] - Train Loss: 0.191029 | Val Loss: 0.191508\n",
      "Epoch [385/500] - Train Loss: 0.190983 | Val Loss: 0.191464\n",
      "Epoch [386/500] - Train Loss: 0.190938 | Val Loss: 0.191419\n",
      "Epoch [387/500] - Train Loss: 0.190893 | Val Loss: 0.191374\n",
      "Epoch [388/500] - Train Loss: 0.190849 | Val Loss: 0.191330\n",
      "Epoch [389/500] - Train Loss: 0.190804 | Val Loss: 0.191286\n",
      "Epoch [390/500] - Train Loss: 0.190760 | Val Loss: 0.191243\n",
      "Epoch [391/500] - Train Loss: 0.190717 | Val Loss: 0.191201\n",
      "Epoch [392/500] - Train Loss: 0.190675 | Val Loss: 0.191159\n",
      "Epoch [393/500] - Train Loss: 0.190633 | Val Loss: 0.191119\n",
      "Epoch [394/500] - Train Loss: 0.190592 | Val Loss: 0.191079\n",
      "Epoch [395/500] - Train Loss: 0.190552 | Val Loss: 0.191041\n",
      "Epoch [396/500] - Train Loss: 0.190513 | Val Loss: 0.191002\n",
      "Epoch [397/500] - Train Loss: 0.190474 | Val Loss: 0.190965\n",
      "Epoch [398/500] - Train Loss: 0.190436 | Val Loss: 0.190928\n",
      "Epoch [399/500] - Train Loss: 0.190399 | Val Loss: 0.190892\n",
      "Epoch [400/500] - Train Loss: 0.190362 | Val Loss: 0.190855\n",
      "Epoch [401/500] - Train Loss: 0.190326 | Val Loss: 0.190820\n",
      "Epoch [402/500] - Train Loss: 0.190290 | Val Loss: 0.190784\n",
      "Epoch [403/500] - Train Loss: 0.190254 | Val Loss: 0.190749\n",
      "Epoch [404/500] - Train Loss: 0.190218 | Val Loss: 0.190713\n",
      "Epoch [405/500] - Train Loss: 0.190182 | Val Loss: 0.190678\n",
      "Epoch [406/500] - Train Loss: 0.190146 | Val Loss: 0.190642\n",
      "Epoch [407/500] - Train Loss: 0.190109 | Val Loss: 0.190607\n",
      "Epoch [408/500] - Train Loss: 0.190073 | Val Loss: 0.190570\n",
      "Epoch [409/500] - Train Loss: 0.190036 | Val Loss: 0.190534\n",
      "Epoch [410/500] - Train Loss: 0.189998 | Val Loss: 0.190497\n",
      "Epoch [411/500] - Train Loss: 0.189960 | Val Loss: 0.190459\n",
      "Epoch [412/500] - Train Loss: 0.189921 | Val Loss: 0.190421\n",
      "Epoch [413/500] - Train Loss: 0.189882 | Val Loss: 0.190382\n",
      "Epoch [414/500] - Train Loss: 0.189842 | Val Loss: 0.190343\n",
      "Epoch [415/500] - Train Loss: 0.189802 | Val Loss: 0.190303\n",
      "Epoch [416/500] - Train Loss: 0.189761 | Val Loss: 0.190264\n",
      "Epoch [417/500] - Train Loss: 0.189720 | Val Loss: 0.190224\n",
      "Epoch [418/500] - Train Loss: 0.189679 | Val Loss: 0.190183\n",
      "Epoch [419/500] - Train Loss: 0.189636 | Val Loss: 0.190142\n",
      "Epoch [420/500] - Train Loss: 0.189593 | Val Loss: 0.190099\n",
      "Epoch [421/500] - Train Loss: 0.189549 | Val Loss: 0.190058\n",
      "Epoch [422/500] - Train Loss: 0.189505 | Val Loss: 0.190016\n",
      "Epoch [423/500] - Train Loss: 0.189462 | Val Loss: 0.189975\n",
      "Epoch [424/500] - Train Loss: 0.189420 | Val Loss: 0.189933\n",
      "Epoch [425/500] - Train Loss: 0.189379 | Val Loss: 0.189891\n",
      "Epoch [426/500] - Train Loss: 0.189335 | Val Loss: 0.189848\n",
      "Epoch [427/500] - Train Loss: 0.189291 | Val Loss: 0.189805\n",
      "Epoch [428/500] - Train Loss: 0.189246 | Val Loss: 0.189764\n",
      "Epoch [429/500] - Train Loss: 0.189204 | Val Loss: 0.189728\n",
      "Epoch [430/500] - Train Loss: 0.189166 | Val Loss: 0.189691\n",
      "Epoch [431/500] - Train Loss: 0.189126 | Val Loss: 0.189649\n",
      "Epoch [432/500] - Train Loss: 0.189083 | Val Loss: 0.189608\n",
      "Epoch [433/500] - Train Loss: 0.189042 | Val Loss: 0.189570\n",
      "Epoch [434/500] - Train Loss: 0.189002 | Val Loss: 0.189532\n",
      "Epoch [435/500] - Train Loss: 0.188964 | Val Loss: 0.189494\n",
      "Epoch [436/500] - Train Loss: 0.188926 | Val Loss: 0.189457\n",
      "Epoch [437/500] - Train Loss: 0.188888 | Val Loss: 0.189420\n",
      "Epoch [438/500] - Train Loss: 0.188851 | Val Loss: 0.189385\n",
      "Epoch [439/500] - Train Loss: 0.188814 | Val Loss: 0.189351\n",
      "Epoch [440/500] - Train Loss: 0.188778 | Val Loss: 0.189317\n",
      "Epoch [441/500] - Train Loss: 0.188743 | Val Loss: 0.189283\n",
      "Epoch [442/500] - Train Loss: 0.188707 | Val Loss: 0.189249\n",
      "Epoch [443/500] - Train Loss: 0.188673 | Val Loss: 0.189217\n",
      "Epoch [444/500] - Train Loss: 0.188640 | Val Loss: 0.189183\n",
      "Epoch [445/500] - Train Loss: 0.188606 | Val Loss: 0.189148\n",
      "Epoch [446/500] - Train Loss: 0.188571 | Val Loss: 0.189114\n",
      "Epoch [447/500] - Train Loss: 0.188536 | Val Loss: 0.189080\n",
      "Epoch [448/500] - Train Loss: 0.188502 | Val Loss: 0.189047\n",
      "Epoch [449/500] - Train Loss: 0.188468 | Val Loss: 0.189012\n",
      "Epoch [450/500] - Train Loss: 0.188434 | Val Loss: 0.188980\n",
      "Epoch [451/500] - Train Loss: 0.188400 | Val Loss: 0.188948\n",
      "Epoch [452/500] - Train Loss: 0.188368 | Val Loss: 0.188916\n",
      "Epoch [453/500] - Train Loss: 0.188336 | Val Loss: 0.188884\n",
      "Epoch [454/500] - Train Loss: 0.188304 | Val Loss: 0.188853\n",
      "Epoch [455/500] - Train Loss: 0.188272 | Val Loss: 0.188822\n",
      "Epoch [456/500] - Train Loss: 0.188240 | Val Loss: 0.188792\n",
      "Epoch [457/500] - Train Loss: 0.188208 | Val Loss: 0.188761\n",
      "Epoch [458/500] - Train Loss: 0.188177 | Val Loss: 0.188731\n",
      "Epoch [459/500] - Train Loss: 0.188146 | Val Loss: 0.188701\n",
      "Epoch [460/500] - Train Loss: 0.188116 | Val Loss: 0.188671\n",
      "Epoch [461/500] - Train Loss: 0.188085 | Val Loss: 0.188641\n",
      "Epoch [462/500] - Train Loss: 0.188054 | Val Loss: 0.188611\n",
      "Epoch [463/500] - Train Loss: 0.188023 | Val Loss: 0.188581\n",
      "Epoch [464/500] - Train Loss: 0.187994 | Val Loss: 0.188552\n",
      "Epoch [465/500] - Train Loss: 0.187964 | Val Loss: 0.188523\n",
      "Epoch [466/500] - Train Loss: 0.187934 | Val Loss: 0.188494\n",
      "Epoch [467/500] - Train Loss: 0.187904 | Val Loss: 0.188463\n",
      "Epoch [468/500] - Train Loss: 0.187873 | Val Loss: 0.188432\n",
      "Epoch [469/500] - Train Loss: 0.187841 | Val Loss: 0.188402\n",
      "Epoch [470/500] - Train Loss: 0.187811 | Val Loss: 0.188376\n",
      "Epoch [471/500] - Train Loss: 0.187785 | Val Loss: 0.188347\n",
      "Epoch [472/500] - Train Loss: 0.187755 | Val Loss: 0.188317\n",
      "Epoch [473/500] - Train Loss: 0.187724 | Val Loss: 0.188290\n",
      "Epoch [474/500] - Train Loss: 0.187696 | Val Loss: 0.188263\n",
      "Epoch [475/500] - Train Loss: 0.187668 | Val Loss: 0.188236\n",
      "Epoch [476/500] - Train Loss: 0.187639 | Val Loss: 0.188207\n",
      "Epoch [477/500] - Train Loss: 0.187610 | Val Loss: 0.188179\n",
      "Epoch [478/500] - Train Loss: 0.187581 | Val Loss: 0.188150\n",
      "Epoch [479/500] - Train Loss: 0.187552 | Val Loss: 0.188123\n",
      "Epoch [480/500] - Train Loss: 0.187525 | Val Loss: 0.188096\n",
      "Epoch [481/500] - Train Loss: 0.187497 | Val Loss: 0.188068\n",
      "Epoch [482/500] - Train Loss: 0.187468 | Val Loss: 0.188041\n",
      "Epoch [483/500] - Train Loss: 0.187441 | Val Loss: 0.188016\n",
      "Epoch [484/500] - Train Loss: 0.187414 | Val Loss: 0.187990\n",
      "Epoch [485/500] - Train Loss: 0.187387 | Val Loss: 0.187964\n",
      "Epoch [486/500] - Train Loss: 0.187360 | Val Loss: 0.187937\n",
      "Epoch [487/500] - Train Loss: 0.187333 | Val Loss: 0.187911\n",
      "Epoch [488/500] - Train Loss: 0.187307 | Val Loss: 0.187886\n",
      "Epoch [489/500] - Train Loss: 0.187281 | Val Loss: 0.187860\n",
      "Epoch [490/500] - Train Loss: 0.187255 | Val Loss: 0.187834\n",
      "Epoch [491/500] - Train Loss: 0.187229 | Val Loss: 0.187809\n",
      "Epoch [492/500] - Train Loss: 0.187203 | Val Loss: 0.187784\n",
      "Epoch [493/500] - Train Loss: 0.187178 | Val Loss: 0.187758\n",
      "Epoch [494/500] - Train Loss: 0.187152 | Val Loss: 0.187733\n",
      "Epoch [495/500] - Train Loss: 0.187126 | Val Loss: 0.187707\n",
      "Epoch [496/500] - Train Loss: 0.187101 | Val Loss: 0.187682\n",
      "Epoch [497/500] - Train Loss: 0.187075 | Val Loss: 0.187657\n",
      "Epoch [498/500] - Train Loss: 0.187050 | Val Loss: 0.187632\n",
      "Epoch [499/500] - Train Loss: 0.187024 | Val Loss: 0.187608\n",
      "Epoch [500/500] - Train Loss: 0.187000 | Val Loss: 0.187583\n",
      "✅ SAE training completed!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Split into 80% train and 20% validation\n",
    "train_embeddings, val_embeddings, train_labels, val_labels = train_test_split(\n",
    "    residual_embeddings, labels_tensor, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "train_embeddings, val_embeddings = train_embeddings.to(\"cuda\"), val_embeddings.to(\"cuda\")\n",
    "\n",
    "# Define SAE model\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "# Initialize SAE\n",
    "input_dim = residual_embeddings.shape[1]  # Embedding size\n",
    "hidden_dim = 128  # Feature extraction size\n",
    "sae = SparseAutoencoder(input_dim, hidden_dim).to(\"cuda\")\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(sae.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define L1 regularization (sparsity constraint)\n",
    "l1_lambda = 0.0001  # Adjust this value based on tuning\n",
    "\n",
    "# Train SAE\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    sae.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    reconstructed, encoded = sae(train_embeddings)\n",
    "    train_loss = loss_fn(reconstructed, train_embeddings)\n",
    "    \n",
    "    # Add L1 penalty\n",
    "    l1_penalty = l1_lambda * torch.norm(encoded, 1)  \n",
    "    total_loss = train_loss + l1_penalty  \n",
    "\n",
    "\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute validation loss\n",
    "    sae.eval()\n",
    "    with torch.no_grad():\n",
    "        val_reconstructed, val_encoded = sae(val_embeddings)\n",
    "        val_loss = loss_fn(val_reconstructed, val_embeddings)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss.item():.6f} | Val Loss: {val_loss.item():.6f}\")\n",
    "\n",
    "print(\"✅ SAE training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted features shape: (11200, 128)\n",
      "✅ Train labels shape: (11200,)\n"
     ]
    }
   ],
   "source": [
    "# Extract features from train and validation sets\n",
    "sae.eval()\n",
    "with torch.no_grad():\n",
    "    train_features = sae.encoder(train_embeddings).cpu().numpy()\n",
    "    val_features = sae.encoder(val_embeddings).cpu().numpy()\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "train_labels_np = train_labels.cpu().numpy()\n",
    "val_labels_np = val_labels.cpu().numpy()\n",
    "\n",
    "print(f\"✅ Extracted features shape: {train_features.shape}\")\n",
    "print(f\"✅ Train labels shape: {train_labels_np.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\aitext\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:10:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 XGBoost Classification Performance:\n",
      "✅ Accuracy: 0.7704\n",
      "✅ Precision: 0.7694\n",
      "✅ Recall: 0.7786\n",
      "✅ F1 Score: 0.7740\n",
      "\n",
      "✅ XGBoost model training completed!\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Train XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "objective=\"binary:logistic\", \n",
    "    eval_metric=\"logloss\", \n",
    "    use_label_encoder=False,\n",
    "    max_depth=7,  # Increase depth\n",
    "    learning_rate=0.01,  # Lower learning rate for better optimization\n",
    "    n_estimators=500  # More trees for improved accuracy\n",
    ")\n",
    "\n",
    "xgb_model.fit(train_features, train_labels_np)\n",
    "\n",
    "# Predict on validation set\n",
    "val_preds = xgb_model.predict(val_features)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(val_labels_np, val_preds)\n",
    "precision = precision_score(val_labels_np, val_preds)\n",
    "recall = recall_score(val_labels_np, val_preds)\n",
    "f1 = f1_score(val_labels_np, val_preds)\n",
    "\n",
    "print(\"\\n📊 XGBoost Classification Performance:\")\n",
    "print(f\"✅ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"✅ Precision: {precision:.4f}\")\n",
    "print(f\"✅ Recall: {recall:.4f}\")\n",
    "print(f\"✅ F1 Score: {f1:.4f}\")\n",
    "print(\"\\n✅ XGBoost model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save SAE encoder\n",
    "torch.save(sae.state_dict(), \"sparse_autoencoder.pth\")\n",
    "\n",
    "# Save XGBoost model\n",
    "joblib.dump(xgb_model, \"xgboost_classifier.pkl\")\n",
    "\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
